{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Elegy Elegy is a framework-agnostic Trainer interface for the Jax ecosystem. Main Features Easy-to-use : Elegy provides a Keras-like high-level API that makes it very easy to do common tasks. Flexible : Elegy provides a functional Pytorch Lightning-like low-level API that provides maximal flexibility when needed. Agnostic : Elegy supports a variety of frameworks including Flax, Haiku, and Optax on the high-level API, and it is 100% framework-agnostic on the low-level API. Compatible : Elegy can consume a wide variety of common data sources including TensorFlow Datasets, Pytorch DataLoaders, Python generators, and Numpy pytrees. For more information take a look at the Documentation . Installation Install Elegy using pip: pip install elegy For Windows users we recommend the Windows subsystem for linux 2 WSL2 since jax does not support it yet. Quick Start: High-level API Elegy's high-level API provides a very simple interface you can use by implementing following steps: 1. Define the architecture inside a Module . We will use Flax Linen for this example: import flax.linen as nn import jax class MLP (nn . Module): @nn . compact def call ( self , x): x = nn . Dense( 300 )(x) x = jax . nn . relu(x) x = nn . Dense( 10 )(x) return x 2. Create a Model from this module and specify additional things like losses, metrics, and optimizers: import elegy , optax model = elegy . Model( module = MLP(), loss = [ elegy . losses . SparseCategoricalCrossentropy(from_logits = True ), elegy . regularizers . GlobalL2(l =1e-5 ), ], metrics = elegy . metrics . SparseCategoricalAccuracy(), optimizer = optax . rmsprop( 1e-3 ), ) 3. Train the model using the fit method: model . fit( x = X_train, y = y_train, epochs =100 , steps_per_epoch =200 , batch_size =64 , validation_data = (X_test, y_test), shuffle = True , callbacks = [elegy . callbacks . TensorBoard( \"summaries\" )] ) Quick Start: Low-level API In Elegy's low-level API lets you define exactly what goes on during training, testing, and inference. Lets define the test_step to implement a linear classifier in pure jax: 1. Calculate our loss, logs, and states: class LinearClassifier (elegy . Model): # request parameters by name via depending injection. # names: x, y_true, sample_weight, class_weight, states, initializing def test_step ( self , x, # inputs y_true, # labels states: elegy . States, # model state initializing: bool , # if True we should initialize our parameters ): rng: elegy . RNGSeq = states . rng # flatten + scale x = jnp . reshape(x, (x . shape[ 0 ], -1 )) / 255 # initialize or use existing parameters if initializing: w = jax . random . uniform( rng . next(), shape = [np . prod(x . shape[ 1 :]), 10 ] ) b = jax . random . uniform(rng . next(), shape = [ 1 ]) else : w, b = states . net_params # model logits = jnp . dot(x, w) + b # categorical crossentropy loss labels = jax . nn . one_hot(y_true, 10 ) loss = jnp . mean( - jnp . sum(labels * jax . nn . log_softmax(logits), axis =-1 )) accuracy = jnp . mean(jnp . argmax(logits, axis =-1 ) == y_true) # metrics logs = dict ( accuracy = accuracy, loss = loss, ) return loss, logs, states . update(net_params = (w, b)) 2. Instantiate our LinearClassifier with an optimizer: model = LinearClassifier( optimizer = optax . rmsprop( 1e-3 ), ) 3. Train the model using the fit method: model . fit( x = X_train, y = y_train, epochs =100 , steps_per_epoch =200 , batch_size =64 , validation_data = (X_test, y_test), shuffle = True , callbacks = [elegy . callbacks . TensorBoard( \"summaries\" )] ) Using Jax Frameworks It is straightforward to integrate other functional JAX libraries with this low-level API: class LinearClassifier (elegy . Model): def test_step ( self , x, y_true, states: elegy . States, initializing: bool ): rng: elegy . RNGSeq = states . rng x = jnp . reshape(x, (x . shape[ 0 ], -1 )) / 255 if initializing: logits, variables = self . module . init_with_output( { \"params\" : rng . next(), \"dropout\" : rng . next()}, x ) else : variables = dict (params = states . net_params, ** states . net_states) logits, variables = self . module . apply( variables, x, rngs = { \"dropout\" : rng . next()}, mutable = True ) net_states, net_params = variables . pop( \"params\" ) labels = jax . nn . one_hot(y_true, 10 ) loss = jnp . mean( - jnp . sum(labels * jax . nn . log_softmax(logits), axis =-1 )) accuracy = jnp . mean(jnp . argmax(logits, axis =-1 ) == y_true) logs = dict (accuracy = accuracy, loss = loss) return loss, logs, states . update(net_params = net_params, net_states = net_states) More Info Getting Started: High-level API tutorial. Getting Started: Low-level API tutorial. Elegy's Documentation . The examples directory. What is Jax? Examples To run the examples first install some required packages: pip install -r examples/requirements.txt Now run the example: python examples/flax_mnist_vae.py Contributing Deep Learning is evolving at an incredible pace, there is so much to do and so few hands. If you wish to contribute anything from a loss or metric to a new awesome feature for Elegy just open an issue or send a PR! For more information check out our Contributing Guide . About Us We are some friends passionate about ML. License Apache Citing Elegy To cite this project: BibTeX @software{elegy2020repository, author = {PoetsAI}, title = {Elegy: A framework-agnostic Trainer interface for the Jax ecosystem}, url = {https://github.com/poets-ai/elegy}, version = {0.7.1}, year = {2020}, } Where the current version may be retrieved either from the Release tag or the file elegy/__init__.py and the year corresponds to the project's release year.","title":"Introduction"},{"location":"#elegy","text":"Elegy is a framework-agnostic Trainer interface for the Jax ecosystem.","title":"Elegy"},{"location":"#main-features","text":"Easy-to-use : Elegy provides a Keras-like high-level API that makes it very easy to do common tasks. Flexible : Elegy provides a functional Pytorch Lightning-like low-level API that provides maximal flexibility when needed. Agnostic : Elegy supports a variety of frameworks including Flax, Haiku, and Optax on the high-level API, and it is 100% framework-agnostic on the low-level API. Compatible : Elegy can consume a wide variety of common data sources including TensorFlow Datasets, Pytorch DataLoaders, Python generators, and Numpy pytrees. For more information take a look at the Documentation .","title":"Main Features"},{"location":"#installation","text":"Install Elegy using pip: pip install elegy For Windows users we recommend the Windows subsystem for linux 2 WSL2 since jax does not support it yet.","title":"Installation"},{"location":"#quick-start-high-level-api","text":"Elegy's high-level API provides a very simple interface you can use by implementing following steps: 1. Define the architecture inside a Module . We will use Flax Linen for this example: import flax.linen as nn import jax class MLP (nn . Module): @nn . compact def call ( self , x): x = nn . Dense( 300 )(x) x = jax . nn . relu(x) x = nn . Dense( 10 )(x) return x 2. Create a Model from this module and specify additional things like losses, metrics, and optimizers: import elegy , optax model = elegy . Model( module = MLP(), loss = [ elegy . losses . SparseCategoricalCrossentropy(from_logits = True ), elegy . regularizers . GlobalL2(l =1e-5 ), ], metrics = elegy . metrics . SparseCategoricalAccuracy(), optimizer = optax . rmsprop( 1e-3 ), ) 3. Train the model using the fit method: model . fit( x = X_train, y = y_train, epochs =100 , steps_per_epoch =200 , batch_size =64 , validation_data = (X_test, y_test), shuffle = True , callbacks = [elegy . callbacks . TensorBoard( \"summaries\" )] )","title":"Quick Start: High-level API"},{"location":"#quick-start-low-level-api","text":"In Elegy's low-level API lets you define exactly what goes on during training, testing, and inference. Lets define the test_step to implement a linear classifier in pure jax: 1. Calculate our loss, logs, and states: class LinearClassifier (elegy . Model): # request parameters by name via depending injection. # names: x, y_true, sample_weight, class_weight, states, initializing def test_step ( self , x, # inputs y_true, # labels states: elegy . States, # model state initializing: bool , # if True we should initialize our parameters ): rng: elegy . RNGSeq = states . rng # flatten + scale x = jnp . reshape(x, (x . shape[ 0 ], -1 )) / 255 # initialize or use existing parameters if initializing: w = jax . random . uniform( rng . next(), shape = [np . prod(x . shape[ 1 :]), 10 ] ) b = jax . random . uniform(rng . next(), shape = [ 1 ]) else : w, b = states . net_params # model logits = jnp . dot(x, w) + b # categorical crossentropy loss labels = jax . nn . one_hot(y_true, 10 ) loss = jnp . mean( - jnp . sum(labels * jax . nn . log_softmax(logits), axis =-1 )) accuracy = jnp . mean(jnp . argmax(logits, axis =-1 ) == y_true) # metrics logs = dict ( accuracy = accuracy, loss = loss, ) return loss, logs, states . update(net_params = (w, b)) 2. Instantiate our LinearClassifier with an optimizer: model = LinearClassifier( optimizer = optax . rmsprop( 1e-3 ), ) 3. Train the model using the fit method: model . fit( x = X_train, y = y_train, epochs =100 , steps_per_epoch =200 , batch_size =64 , validation_data = (X_test, y_test), shuffle = True , callbacks = [elegy . callbacks . TensorBoard( \"summaries\" )] )","title":"Quick Start: Low-level API"},{"location":"#using-jax-frameworks","text":"It is straightforward to integrate other functional JAX libraries with this low-level API: class LinearClassifier (elegy . Model): def test_step ( self , x, y_true, states: elegy . States, initializing: bool ): rng: elegy . RNGSeq = states . rng x = jnp . reshape(x, (x . shape[ 0 ], -1 )) / 255 if initializing: logits, variables = self . module . init_with_output( { \"params\" : rng . next(), \"dropout\" : rng . next()}, x ) else : variables = dict (params = states . net_params, ** states . net_states) logits, variables = self . module . apply( variables, x, rngs = { \"dropout\" : rng . next()}, mutable = True ) net_states, net_params = variables . pop( \"params\" ) labels = jax . nn . one_hot(y_true, 10 ) loss = jnp . mean( - jnp . sum(labels * jax . nn . log_softmax(logits), axis =-1 )) accuracy = jnp . mean(jnp . argmax(logits, axis =-1 ) == y_true) logs = dict (accuracy = accuracy, loss = loss) return loss, logs, states . update(net_params = net_params, net_states = net_states)","title":"Using Jax Frameworks"},{"location":"#more-info","text":"Getting Started: High-level API tutorial. Getting Started: Low-level API tutorial. Elegy's Documentation . The examples directory. What is Jax?","title":"More Info"},{"location":"#examples","text":"To run the examples first install some required packages: pip install -r examples/requirements.txt Now run the example: python examples/flax_mnist_vae.py","title":"Examples"},{"location":"#contributing","text":"Deep Learning is evolving at an incredible pace, there is so much to do and so few hands. If you wish to contribute anything from a loss or metric to a new awesome feature for Elegy just open an issue or send a PR! For more information check out our Contributing Guide .","title":"Contributing"},{"location":"#about-us","text":"We are some friends passionate about ML.","title":"About Us"},{"location":"#license","text":"Apache","title":"License"},{"location":"#citing-elegy","text":"To cite this project: BibTeX @software{elegy2020repository, author = {PoetsAI}, title = {Elegy: A framework-agnostic Trainer interface for the Jax ecosystem}, url = {https://github.com/poets-ai/elegy}, version = {0.7.1}, year = {2020}, } Where the current version may be retrieved either from the Release tag or the file elegy/__init__.py and the year corresponds to the project's release year.","title":"Citing Elegy"},{"location":"contributing/","text":"Contributing This is a short guide on how to start contributing to Elegy along with some best practices for the project. Setup We use poetry >= 1.1.4 , the easiest way to setup a development environment is run: poetry config virtualenvs.in-project true --local poetry install In order for Jax to recognize your GPU, you will probably have to install it again using the command below. PYTHON_VERSION = cp38 CUDA_VERSION = cuda101 # alternatives: cuda100, cuda101, cuda102, cuda110, check your cuda version PLATFORM = manylinux2010_x86_64 # alternatives: manylinux2010_x86_64 BASE_URL = 'https://storage.googleapis.com/jax-releases' pip install --upgrade $BASE_URL / $CUDA_VERSION /jaxlib-0.1.55- $PYTHON_VERSION -none- $PLATFORM .whl pip install --upgrade jax Gitpod An alternative way to contribute is using gitpod which creates a vscode-based cloud development enviroment. To get started just login at gitpod, grant the appropriate permissions to github, and open the following link: https://gitpod.io/#https://github.com/poets-ai/elegy We have built a python 3.8 enviroment and all development dependencies will install when the enviroment starts. Creating Losses and Metrics For this you can follow these guidelines: Each loss / metric should be defined in its own file. Inherit from either elegy.losses.loss.Loss or elegy.metrics.metric.Metric or an existing class that inherits from them. Try to use an existing metric or loss as a template You must provide documentation for the following: The class definition. The __init__ method. The call method. Try to port the documentation + signature from its Keras counter part. If so you must give credits to the original source file. You must include tests. If you there exists an equivalent loss/metric in Keras you must test numerical equivalence between both. Testing To execute all the tests just run pytest Documentation We use mkdocs . If you create a new object that requires documentation please do the following: Add a markdown file inside /docs/api in the appropriate location according to the project's structure. This file must: Contain the path of function / class as header Use mkdocstring to render the API information. Example: # elegy.losses.BinaryCrossentropy ::: elegy.losses.BinaryCrossentropy selection: inherited_members: true members: - call - __init__ Add and entry to mkdocs.yml inside nav pointing to this file. Checkout mkdocs.yml . To build and visualize the documentation locally run mkdocs serve Creating a PR Before sending a pull request make sure all test run and code is formatted with black : black . Changelog CHANGELOG.md is automatically generated using github-changelog-generator , to update the changelog just run: docker run -it --rm -v ( pwd ) :/usr/local/src/your-app ferrarimarco/github-changelog-generator -u poets-ai -p elegy -t <TOKEN> where <TOKEN> token can be obtained from Github at Personal access tokens , you only have to give permission for the repo section.","title":"Contributing"},{"location":"contributing/#contributing","text":"This is a short guide on how to start contributing to Elegy along with some best practices for the project.","title":"Contributing"},{"location":"contributing/#setup","text":"We use poetry >= 1.1.4 , the easiest way to setup a development environment is run: poetry config virtualenvs.in-project true --local poetry install In order for Jax to recognize your GPU, you will probably have to install it again using the command below. PYTHON_VERSION = cp38 CUDA_VERSION = cuda101 # alternatives: cuda100, cuda101, cuda102, cuda110, check your cuda version PLATFORM = manylinux2010_x86_64 # alternatives: manylinux2010_x86_64 BASE_URL = 'https://storage.googleapis.com/jax-releases' pip install --upgrade $BASE_URL / $CUDA_VERSION /jaxlib-0.1.55- $PYTHON_VERSION -none- $PLATFORM .whl pip install --upgrade jax","title":"Setup"},{"location":"contributing/#gitpod","text":"An alternative way to contribute is using gitpod which creates a vscode-based cloud development enviroment. To get started just login at gitpod, grant the appropriate permissions to github, and open the following link: https://gitpod.io/#https://github.com/poets-ai/elegy We have built a python 3.8 enviroment and all development dependencies will install when the enviroment starts.","title":"Gitpod"},{"location":"contributing/#creating-losses-and-metrics","text":"For this you can follow these guidelines: Each loss / metric should be defined in its own file. Inherit from either elegy.losses.loss.Loss or elegy.metrics.metric.Metric or an existing class that inherits from them. Try to use an existing metric or loss as a template You must provide documentation for the following: The class definition. The __init__ method. The call method. Try to port the documentation + signature from its Keras counter part. If so you must give credits to the original source file. You must include tests. If you there exists an equivalent loss/metric in Keras you must test numerical equivalence between both.","title":"Creating Losses and Metrics"},{"location":"contributing/#testing","text":"To execute all the tests just run pytest","title":"Testing"},{"location":"contributing/#documentation","text":"We use mkdocs . If you create a new object that requires documentation please do the following: Add a markdown file inside /docs/api in the appropriate location according to the project's structure. This file must: Contain the path of function / class as header Use mkdocstring to render the API information. Example: # elegy.losses.BinaryCrossentropy ::: elegy.losses.BinaryCrossentropy selection: inherited_members: true members: - call - __init__ Add and entry to mkdocs.yml inside nav pointing to this file. Checkout mkdocs.yml . To build and visualize the documentation locally run mkdocs serve","title":"Documentation"},{"location":"contributing/#creating-a-pr","text":"Before sending a pull request make sure all test run and code is formatted with black : black .","title":"Creating a PR"},{"location":"contributing/#changelog","text":"CHANGELOG.md is automatically generated using github-changelog-generator , to update the changelog just run: docker run -it --rm -v ( pwd ) :/usr/local/src/your-app ferrarimarco/github-changelog-generator -u poets-ai -p elegy -t <TOKEN> where <TOKEN> token can be obtained from Github at Personal access tokens , you only have to give permission for the repo section.","title":"Changelog"},{"location":"elegy-module/","text":"Elegy Module This is a guide to Elegy's underlying Module System. It will help get a better understanding of how Elegy interacts with Jax at the lower level, certain details about the hooks system and how it differs from other Deep Learning frameworks. Traditional Object Oriented Style We will begin by exploring how other frameworks define Modules / Layers. It is very common to use Object Oriented architectures as backbones of Module systems as it helps frameworks keep track of the parameters and states each module might require. Here we will create some very basic Linear and MLP modules which will seem very familiar: class Linear : def __init__ ( self , n_in, n_out): super () . __init__ () self . w = self . add_parameter( \"w\" , [x . shape[ -1 ], self . n_out], initializer = elegy . initializers . RandomUniform(), ) self . b = self . add_parameter( \"b\" , [n_out], initializer = elegy . initializers . RandomUniform()) def call ( self , x): return jnp . dot(x, self . w) + self . b class MLP (elegy . Module): def __init__ ( self , n_in): self . linear1 = Linear(n_in, 64 ) self . linear2 = Linear( 64 , 32 ) self . linear3 = Linear( 32 , 1 ) def call ( self , x): x = self . linear1(x) x = jax . nn . relu(x) x = self . linear2(x) x = jax . nn . relu(x) x = self . linear3(x) return x Here we just defined a simple linear layer and used it inside a MLP with 3 layers. Pytorch and Keras users should feel very familiar with this type of code: we define parameters or other submodules in the __init__ method, and use them during the call (forward) method. Keras users might complain that if we do things this way we loose the ability to do shape inference, but don't worry, we will fix that latter. Fow now it is important to notice that here we use our first hook: add_parameter . Elegy Hooks Hooks are a way in which we can manage state while preserving functional purity (in the end). Elegy's hook system is ported and expanded from Haiku, but hook-based functional architectures in other areas like web development have proven valuable, React Hooks being a recent notable success. In Elegy we have the following list of hooks: Hook Description self.add_parameter Gives us access to trainable and non-trainable parameters. elegy.hooks.add_loss Lets us declare a loss from some intermediate module. elegy.hooks.add_metric Lets us declare a metric in some intermediate module. elegy.hooks.add_summary Lets us declare a summary in some intermediate module. elegy.training Tells us whether training is currently happening or not. elegy.next_key Gives us access to a unique PRNGKey we can pass to functions like jax.random.uniform and friends. Note If you use existing Module s you might not need to worry much about these hooks, but keep them in mind if you are developing your own custom modules. Module Hooks: Functional Style In the initial example we used hooks in a very shy manner to replicate the behavior of of other frameworks, now we will go beyond. The first thing we need to know is that: Quote Modules are hooks This means that module instantiation taps into the hook system, and that hooks are aware of the module in which they are executing in. In practice this will mean that we will be able to move a lot of the code defined on the __init__ method to the call method: class Linear : def __init__ ( self , n_out): super () . __init__ () self . n_out = n_out def call ( self , x): w = self . add_parameter( \"w\" , [x . shape[ -1 ], self . n_out], initializer = elegy . initializers . RandomUniform(), ) b = self . add_parameter( \"b\" , [ self . n_out], initializer = jnp . zeros) return jnp . dot(x, w) + b class MLP (elegy . Module): def call ( self , x): x = Linear( 64 )(x) x = jax . nn . relu(x) x = Linear( 32 )(x) x = jax . nn . relu(x) x = Linear( 1 )(x) return x What happened here? Lets decompose it into two parts. First we moved the add_parameter definitions on the Linear module to the call method: class Linear : def __init__ ( self , n_out): super () . __init__ () self . n_out = n_out def call ( self , x): w = self . add_parameter( \"w\" , [x . shape[ -1 ], self . n_out], initializer = elegy . initializers . RandomUniform(), ) b = self . add_parameter( \"b\" , [ self . n_out], initializer = jnp . zeros) return jnp . dot(x, w) + b As you see this allows us to do shape inference since we have access to the inputs when defining our parameter's shape. Second, we also moved the instantiation of the Linear modules in MLP from __init__ to call : class MLP (elegy . Module): def call ( self , x): x = Linear( 64 )(x) x = jax . nn . relu(x) x = Linear( 32 )(x) x = jax . nn . relu(x) x = Linear( 1 )(x) return x Here we are using Modules as hooks. While it may appear as if we are instantiating 3 new Linear modules on every call , Elegy is actually caching them behind the scenes with the help of Python metaclasses. There is one important rule you have to follow: Quote You must use hooks unconditionally This motto comes from React and it means that the module always has to call the same amount of hooks, and for module hooks specifically they must be called in the same order. For example the following code is invalid: def call ( self , x): if x . shape[ 0 ] > 5 : x = elegy . nn . Conv2D( 32 , [ 3 , 3 ])(x) x = elegy . nn . Linear( 48 , [ 3 , 3 ])(x) else : x = elegy . nn . Linear( 48 , [ 3 , 3 ])(x) x = elegy . nn . Conv2D( 32 , [ 3 , 3 ])(x) return x Here Linear and Conv2D are dangerously swapped based on some condition. If you want to do this you can just clare them unconditionally and use them inside the condition: def call ( self , x): linear = elegy . nn . Linear( 48 , [ 3 , 3 ]) conv2d = elegy . nn . Conv2D( 32 , [ 3 , 3 ]) if x . shape[ 0 ] > 5 : x = conv2d(x) x = linear(x) else : x = linear(x) x = conv2d(x) return x Hooks Preserve References In our MLP class we where able to create the Linear modules at their call site, this simplified our code a lot but we've seem to lost the reference to these modules. Having reference to other modules is critical for being able to e.g. easily compose modules that might be trained separately like in transfer learning, or being able to easily decompose / extract a sub-module and use it separately like when using the decoder of a VAE by itself to generate new samples. Because of this, Elegy actually assigns all submodules, parameters, and states as properties of the module: x = np . random . uniform(size = ( 15 , 3 )) mlp = MLP() mlp(x) linear, linear_1, linear_2 = mlp . linear, mlp . linear_1, mlp . linear_2 y_pred = mlp(x) assert linear is mlp . linear and linear_1 is mlp . linear_1 and linear_2 is mlp . linear_2 As you see we were able to access all the linear layer references. More over, we verified that these reference don't change during execution. Each submodule gets assigned to a a unique field name based on its class name and order of creation. You can customize this name by using the name argument available in the Module 's constructor. Low-level Training Loop A big theme in Jax is that state and computation are separate, this is a requirement because in order for combinators like jax.grad and jax.jit to work you need pure functions. Elegy as you've seen is object oriented so additional effort ir required to properly convert all the global states and Module parameters an inputs to a function so Jax can track them. To achieve Elegy implements its own jit and value_and_grad function wrappers that handle this for you. Lets create a low level training loop using the previous definition MLP along with these functions: x = np . random . uniform(size = ( 15 , 3 )) y = np . random . uniform(size = ( 15 , 1 )) mlp = MLP() def loss_fn (x, y): y_pred = mlp(x) return jnp . mean(jnp . square(y - y_pred)) def update (x, y): loss, gradients = elegy . value_and_grad(loss_fn, modules = mlp)(x, y) parameters = mlp . get_parameters(trainable = True ) new_parameters = jax . tree_multimap( lambda p, g: p - 0.01 * g, parameters, gradients ) mlp . set_parameters(new_parameters) return loss update_jit = elegy . hooks . jit(update, modules = mlp) for step in range ( 1000 ): loss = update_jit(x, y) print (step, loss) Here we created the functions loss_fn and update , plus a minimal training loop. Loss loss_fn calculate the Mean Square Error while update uses value_and_grad to calculate the gradient of the loss with respect to the trainable parameters of mlp . def update (x, y): loss, gradients = elegy . value_and_grad(loss_fn, modules = mlp)(x, y) parameters = mlp . get_parameters(trainable = True ) new_parameters = jax . tree_multimap( lambda p, g: p - 0.01 * g, parameters, gradients ) mlp . set_parameters(new_parameters) return loss After that we just use tree_multimap to implement Gradient Descent and get our new_parameters and then use the set_parameters method our Module to update its state. def update (x, y): loss, gradients = elegy . value_and_grad(loss_fn, modules = mlp)(x, y) parameters = mlp . get_parameters(trainable = True ) new_parameters = jax . tree_multimap( lambda p, g: p - 0.01 * g, parameters, gradients ) mlp . set_parameters(new_parameters) return loss Having our update function we can use elegy.hooks.jit to create an optimized version of our computation and create a minimal training loop. update_jit = elegy . hooks . jit(update, modules = mlp) for step in range ( 1000 ): loss = update_jit(x, y) print (step, loss) Notice that even though we are jitting update which has the set_parameters side effect (normally forbidden in Jax), learning is happening because update_jit automatically keeps track of changes to the parameters of mlp and updates them for us. Something similar is done in elegy.value_and_grad as you saw previously. Note Elegy has 2 types states: module state for the parameters of models and global state where Elegy keeps track of certain variables like an RNG for convenience. Elegy's jit behaves just like its Jax counterpart except that its aware of the changes in state such that: Jax properly recompiles if something changes. The jitted function behaves similar to its eager version in that it propagates changes in state inwards and outwards (this only applies to Elegy states, not arbitrary side effects). High Level Equivalent If all this seems a bit too manual for you don't worry, you can can easily express all the previous in a few lines of code using an elegy.Model : model = elegy . Model( module = elegy . nn . Sequential( lambda : [ elegy . nn . Linear( 64 ), jax . nn . relu, elegy . nn . Linear( 32 ), jax . nn . relu, elegy . nn . Linear( 1 ), ] ), loss = elegy . losses . MeanSquaredError(), ) model . fit( x = np . random . uniform(size = ( 15 , 3 )), y = np . random . uniform(size = ( 15 , 1 )), batch_size =15 , epochs =1000 , )","title":"Elegy Module"},{"location":"elegy-module/#elegy-module","text":"This is a guide to Elegy's underlying Module System. It will help get a better understanding of how Elegy interacts with Jax at the lower level, certain details about the hooks system and how it differs from other Deep Learning frameworks.","title":"Elegy Module"},{"location":"elegy-module/#traditional-object-oriented-style","text":"We will begin by exploring how other frameworks define Modules / Layers. It is very common to use Object Oriented architectures as backbones of Module systems as it helps frameworks keep track of the parameters and states each module might require. Here we will create some very basic Linear and MLP modules which will seem very familiar: class Linear : def __init__ ( self , n_in, n_out): super () . __init__ () self . w = self . add_parameter( \"w\" , [x . shape[ -1 ], self . n_out], initializer = elegy . initializers . RandomUniform(), ) self . b = self . add_parameter( \"b\" , [n_out], initializer = elegy . initializers . RandomUniform()) def call ( self , x): return jnp . dot(x, self . w) + self . b class MLP (elegy . Module): def __init__ ( self , n_in): self . linear1 = Linear(n_in, 64 ) self . linear2 = Linear( 64 , 32 ) self . linear3 = Linear( 32 , 1 ) def call ( self , x): x = self . linear1(x) x = jax . nn . relu(x) x = self . linear2(x) x = jax . nn . relu(x) x = self . linear3(x) return x Here we just defined a simple linear layer and used it inside a MLP with 3 layers. Pytorch and Keras users should feel very familiar with this type of code: we define parameters or other submodules in the __init__ method, and use them during the call (forward) method. Keras users might complain that if we do things this way we loose the ability to do shape inference, but don't worry, we will fix that latter. Fow now it is important to notice that here we use our first hook: add_parameter .","title":"Traditional Object Oriented Style"},{"location":"elegy-module/#elegy-hooks","text":"Hooks are a way in which we can manage state while preserving functional purity (in the end). Elegy's hook system is ported and expanded from Haiku, but hook-based functional architectures in other areas like web development have proven valuable, React Hooks being a recent notable success. In Elegy we have the following list of hooks: Hook Description self.add_parameter Gives us access to trainable and non-trainable parameters. elegy.hooks.add_loss Lets us declare a loss from some intermediate module. elegy.hooks.add_metric Lets us declare a metric in some intermediate module. elegy.hooks.add_summary Lets us declare a summary in some intermediate module. elegy.training Tells us whether training is currently happening or not. elegy.next_key Gives us access to a unique PRNGKey we can pass to functions like jax.random.uniform and friends. Note If you use existing Module s you might not need to worry much about these hooks, but keep them in mind if you are developing your own custom modules.","title":"Elegy Hooks"},{"location":"elegy-module/#module-hooks-functional-style","text":"In the initial example we used hooks in a very shy manner to replicate the behavior of of other frameworks, now we will go beyond. The first thing we need to know is that: Quote Modules are hooks This means that module instantiation taps into the hook system, and that hooks are aware of the module in which they are executing in. In practice this will mean that we will be able to move a lot of the code defined on the __init__ method to the call method: class Linear : def __init__ ( self , n_out): super () . __init__ () self . n_out = n_out def call ( self , x): w = self . add_parameter( \"w\" , [x . shape[ -1 ], self . n_out], initializer = elegy . initializers . RandomUniform(), ) b = self . add_parameter( \"b\" , [ self . n_out], initializer = jnp . zeros) return jnp . dot(x, w) + b class MLP (elegy . Module): def call ( self , x): x = Linear( 64 )(x) x = jax . nn . relu(x) x = Linear( 32 )(x) x = jax . nn . relu(x) x = Linear( 1 )(x) return x What happened here? Lets decompose it into two parts. First we moved the add_parameter definitions on the Linear module to the call method: class Linear : def __init__ ( self , n_out): super () . __init__ () self . n_out = n_out def call ( self , x): w = self . add_parameter( \"w\" , [x . shape[ -1 ], self . n_out], initializer = elegy . initializers . RandomUniform(), ) b = self . add_parameter( \"b\" , [ self . n_out], initializer = jnp . zeros) return jnp . dot(x, w) + b As you see this allows us to do shape inference since we have access to the inputs when defining our parameter's shape. Second, we also moved the instantiation of the Linear modules in MLP from __init__ to call : class MLP (elegy . Module): def call ( self , x): x = Linear( 64 )(x) x = jax . nn . relu(x) x = Linear( 32 )(x) x = jax . nn . relu(x) x = Linear( 1 )(x) return x Here we are using Modules as hooks. While it may appear as if we are instantiating 3 new Linear modules on every call , Elegy is actually caching them behind the scenes with the help of Python metaclasses. There is one important rule you have to follow: Quote You must use hooks unconditionally This motto comes from React and it means that the module always has to call the same amount of hooks, and for module hooks specifically they must be called in the same order. For example the following code is invalid: def call ( self , x): if x . shape[ 0 ] > 5 : x = elegy . nn . Conv2D( 32 , [ 3 , 3 ])(x) x = elegy . nn . Linear( 48 , [ 3 , 3 ])(x) else : x = elegy . nn . Linear( 48 , [ 3 , 3 ])(x) x = elegy . nn . Conv2D( 32 , [ 3 , 3 ])(x) return x Here Linear and Conv2D are dangerously swapped based on some condition. If you want to do this you can just clare them unconditionally and use them inside the condition: def call ( self , x): linear = elegy . nn . Linear( 48 , [ 3 , 3 ]) conv2d = elegy . nn . Conv2D( 32 , [ 3 , 3 ]) if x . shape[ 0 ] > 5 : x = conv2d(x) x = linear(x) else : x = linear(x) x = conv2d(x) return x","title":"Module Hooks: Functional Style"},{"location":"elegy-module/#hooks-preserve-references","text":"In our MLP class we where able to create the Linear modules at their call site, this simplified our code a lot but we've seem to lost the reference to these modules. Having reference to other modules is critical for being able to e.g. easily compose modules that might be trained separately like in transfer learning, or being able to easily decompose / extract a sub-module and use it separately like when using the decoder of a VAE by itself to generate new samples. Because of this, Elegy actually assigns all submodules, parameters, and states as properties of the module: x = np . random . uniform(size = ( 15 , 3 )) mlp = MLP() mlp(x) linear, linear_1, linear_2 = mlp . linear, mlp . linear_1, mlp . linear_2 y_pred = mlp(x) assert linear is mlp . linear and linear_1 is mlp . linear_1 and linear_2 is mlp . linear_2 As you see we were able to access all the linear layer references. More over, we verified that these reference don't change during execution. Each submodule gets assigned to a a unique field name based on its class name and order of creation. You can customize this name by using the name argument available in the Module 's constructor.","title":"Hooks Preserve References"},{"location":"elegy-module/#low-level-training-loop","text":"A big theme in Jax is that state and computation are separate, this is a requirement because in order for combinators like jax.grad and jax.jit to work you need pure functions. Elegy as you've seen is object oriented so additional effort ir required to properly convert all the global states and Module parameters an inputs to a function so Jax can track them. To achieve Elegy implements its own jit and value_and_grad function wrappers that handle this for you. Lets create a low level training loop using the previous definition MLP along with these functions: x = np . random . uniform(size = ( 15 , 3 )) y = np . random . uniform(size = ( 15 , 1 )) mlp = MLP() def loss_fn (x, y): y_pred = mlp(x) return jnp . mean(jnp . square(y - y_pred)) def update (x, y): loss, gradients = elegy . value_and_grad(loss_fn, modules = mlp)(x, y) parameters = mlp . get_parameters(trainable = True ) new_parameters = jax . tree_multimap( lambda p, g: p - 0.01 * g, parameters, gradients ) mlp . set_parameters(new_parameters) return loss update_jit = elegy . hooks . jit(update, modules = mlp) for step in range ( 1000 ): loss = update_jit(x, y) print (step, loss) Here we created the functions loss_fn and update , plus a minimal training loop. Loss loss_fn calculate the Mean Square Error while update uses value_and_grad to calculate the gradient of the loss with respect to the trainable parameters of mlp . def update (x, y): loss, gradients = elegy . value_and_grad(loss_fn, modules = mlp)(x, y) parameters = mlp . get_parameters(trainable = True ) new_parameters = jax . tree_multimap( lambda p, g: p - 0.01 * g, parameters, gradients ) mlp . set_parameters(new_parameters) return loss After that we just use tree_multimap to implement Gradient Descent and get our new_parameters and then use the set_parameters method our Module to update its state. def update (x, y): loss, gradients = elegy . value_and_grad(loss_fn, modules = mlp)(x, y) parameters = mlp . get_parameters(trainable = True ) new_parameters = jax . tree_multimap( lambda p, g: p - 0.01 * g, parameters, gradients ) mlp . set_parameters(new_parameters) return loss Having our update function we can use elegy.hooks.jit to create an optimized version of our computation and create a minimal training loop. update_jit = elegy . hooks . jit(update, modules = mlp) for step in range ( 1000 ): loss = update_jit(x, y) print (step, loss) Notice that even though we are jitting update which has the set_parameters side effect (normally forbidden in Jax), learning is happening because update_jit automatically keeps track of changes to the parameters of mlp and updates them for us. Something similar is done in elegy.value_and_grad as you saw previously. Note Elegy has 2 types states: module state for the parameters of models and global state where Elegy keeps track of certain variables like an RNG for convenience. Elegy's jit behaves just like its Jax counterpart except that its aware of the changes in state such that: Jax properly recompiles if something changes. The jitted function behaves similar to its eager version in that it propagates changes in state inwards and outwards (this only applies to Elegy states, not arbitrary side effects).","title":"Low-level Training Loop"},{"location":"elegy-module/#high-level-equivalent","text":"If all this seems a bit too manual for you don't worry, you can can easily express all the previous in a few lines of code using an elegy.Model : model = elegy . Model( module = elegy . nn . Sequential( lambda : [ elegy . nn . Linear( 64 ), jax . nn . relu, elegy . nn . Linear( 32 ), jax . nn . relu, elegy . nn . Linear( 1 ), ] ), loss = elegy . losses . MeanSquaredError(), ) model . fit( x = np . random . uniform(size = ( 15 , 3 )), y = np . random . uniform(size = ( 15 , 1 )), batch_size =15 , epochs =1000 , )","title":"High Level Equivalent"},{"location":"na/","text":"Not Available \ud83d\udea7 This page is not available yet, we are working on it \ud83d\udea7","title":"Not Available"},{"location":"na/#not-available","text":"\ud83d\udea7 This page is not available yet, we are working on it \ud83d\udea7","title":"Not Available"},{"location":"api/GeneralizedModule/","text":"elegy.GeneralizedModule __class__ inherited Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). __instancecheck__ ( cls , instance ) special Override for isinstance(instance, cls). Source code in elegy/generalized_module/generalized_module.py def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) __new__ ( mcls , name , bases , namespace , ** kwargs ) special staticmethod Create and return a new object. See help(type) for accurate signature. Source code in elegy/generalized_module/generalized_module.py def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls __subclasscheck__ ( cls , subclass ) special Override for issubclass(subclass, cls). Source code in elegy/generalized_module/generalized_module.py def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) register ( cls , subclass ) Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/generalized_module/generalized_module.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"GeneralizedModule"},{"location":"api/GeneralizedModule/#elegygeneralizedmodule","text":"","title":"elegy.GeneralizedModule"},{"location":"api/GeneralizedModule/#elegy.generalized_module.generalized_module.GeneralizedModule","text":"","title":"elegy.generalized_module.generalized_module.GeneralizedModule"},{"location":"api/GeneralizedModule/#elegy.generalized_module.generalized_module.GeneralizedModule.__class__","text":"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()).","title":"__class__"},{"location":"api/GeneralizedModule/#elegy.generalized_module.generalized_module.GeneralizedModule.__class__.__instancecheck__","text":"Override for isinstance(instance, cls). Source code in elegy/generalized_module/generalized_module.py def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance )","title":"__instancecheck__()"},{"location":"api/GeneralizedModule/#elegy.generalized_module.generalized_module.GeneralizedModule.__class__.__new__","text":"Create and return a new object. See help(type) for accurate signature. Source code in elegy/generalized_module/generalized_module.py def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls","title":"__new__()"},{"location":"api/GeneralizedModule/#elegy.generalized_module.generalized_module.GeneralizedModule.__class__.__subclasscheck__","text":"Override for issubclass(subclass, cls). Source code in elegy/generalized_module/generalized_module.py def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass )","title":"__subclasscheck__()"},{"location":"api/GeneralizedModule/#elegy.generalized_module.generalized_module.GeneralizedModule.__class__.register","text":"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/generalized_module/generalized_module.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"register()"},{"location":"api/GeneralizedOptimizer/","text":"elegy.GeneralizedOptimizer __class__ inherited Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). __instancecheck__ ( cls , instance ) special Override for isinstance(instance, cls). Source code in elegy/generalized_optimizer/generalized_optimizer.py def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) __new__ ( mcls , name , bases , namespace , ** kwargs ) special staticmethod Create and return a new object. See help(type) for accurate signature. Source code in elegy/generalized_optimizer/generalized_optimizer.py def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls __subclasscheck__ ( cls , subclass ) special Override for issubclass(subclass, cls). Source code in elegy/generalized_optimizer/generalized_optimizer.py def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) register ( cls , subclass ) Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/generalized_optimizer/generalized_optimizer.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"GeneralizedOptimizer"},{"location":"api/GeneralizedOptimizer/#elegygeneralizedoptimizer","text":"","title":"elegy.GeneralizedOptimizer"},{"location":"api/GeneralizedOptimizer/#elegy.generalized_optimizer.generalized_optimizer.GeneralizedOptimizer","text":"","title":"elegy.generalized_optimizer.generalized_optimizer.GeneralizedOptimizer"},{"location":"api/GeneralizedOptimizer/#elegy.generalized_optimizer.generalized_optimizer.GeneralizedOptimizer.__class__","text":"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()).","title":"__class__"},{"location":"api/GeneralizedOptimizer/#elegy.generalized_optimizer.generalized_optimizer.GeneralizedOptimizer.__class__.__instancecheck__","text":"Override for isinstance(instance, cls). Source code in elegy/generalized_optimizer/generalized_optimizer.py def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance )","title":"__instancecheck__()"},{"location":"api/GeneralizedOptimizer/#elegy.generalized_optimizer.generalized_optimizer.GeneralizedOptimizer.__class__.__new__","text":"Create and return a new object. See help(type) for accurate signature. Source code in elegy/generalized_optimizer/generalized_optimizer.py def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls","title":"__new__()"},{"location":"api/GeneralizedOptimizer/#elegy.generalized_optimizer.generalized_optimizer.GeneralizedOptimizer.__class__.__subclasscheck__","text":"Override for issubclass(subclass, cls). Source code in elegy/generalized_optimizer/generalized_optimizer.py def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass )","title":"__subclasscheck__()"},{"location":"api/GeneralizedOptimizer/#elegy.generalized_optimizer.generalized_optimizer.GeneralizedOptimizer.__class__.register","text":"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/generalized_optimizer/generalized_optimizer.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"register()"},{"location":"api/GradStep/","text":"elegy.GradStep GradStep(loss, logs, states, grads) __getnewargs__ ( self ) special Return self as a plain tuple. Used by copy and pickle. Source code in elegy/model/model_core.py def __getnewargs__ ( self ): 'Return self as a plain tuple. Used by copy and pickle.' return _tuple ( self ) __new__ ( _cls , loss , logs , states , grads ) special staticmethod Create new instance of GradStep(loss, logs, states, grads) __repr__ ( self ) special Return a nicely formatted representation string Source code in elegy/model/model_core.py def __repr__ ( self ): 'Return a nicely formatted representation string' return self . __class__ . __name__ + repr_fmt % self","title":"GradStep"},{"location":"api/GradStep/#elegygradstep","text":"","title":"elegy.GradStep"},{"location":"api/GradStep/#elegy.model.model_core.GradStep","text":"GradStep(loss, logs, states, grads)","title":"elegy.model.model_core.GradStep"},{"location":"api/GradStep/#elegy.model.model_core.GradStep.__getnewargs__","text":"Return self as a plain tuple. Used by copy and pickle. Source code in elegy/model/model_core.py def __getnewargs__ ( self ): 'Return self as a plain tuple. Used by copy and pickle.' return _tuple ( self )","title":"__getnewargs__()"},{"location":"api/GradStep/#elegy.model.model_core.GradStep.__new__","text":"Create new instance of GradStep(loss, logs, states, grads)","title":"__new__()"},{"location":"api/GradStep/#elegy.model.model_core.GradStep.__repr__","text":"Return a nicely formatted representation string Source code in elegy/model/model_core.py def __repr__ ( self ): 'Return a nicely formatted representation string' return self . __class__ . __name__ + repr_fmt % self","title":"__repr__()"},{"location":"api/HaikuModule/","text":"elegy.HaikuModule __class__ inherited Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). __instancecheck__ ( cls , instance ) special Override for isinstance(instance, cls). Source code in elegy/generalized_module/haiku_module.py def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) __new__ ( mcls , name , bases , namespace , ** kwargs ) special staticmethod Create and return a new object. See help(type) for accurate signature. Source code in elegy/generalized_module/haiku_module.py def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls __subclasscheck__ ( cls , subclass ) special Override for issubclass(subclass, cls). Source code in elegy/generalized_module/haiku_module.py def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) register ( cls , subclass ) Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/generalized_module/haiku_module.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"HaikuModule"},{"location":"api/HaikuModule/#elegyhaikumodule","text":"","title":"elegy.HaikuModule"},{"location":"api/HaikuModule/#elegy.generalized_module.haiku_module.HaikuModule","text":"","title":"elegy.generalized_module.haiku_module.HaikuModule"},{"location":"api/HaikuModule/#elegy.generalized_module.haiku_module.HaikuModule.__class__","text":"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()).","title":"__class__"},{"location":"api/HaikuModule/#elegy.generalized_module.haiku_module.HaikuModule.__class__.__instancecheck__","text":"Override for isinstance(instance, cls). Source code in elegy/generalized_module/haiku_module.py def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance )","title":"__instancecheck__()"},{"location":"api/HaikuModule/#elegy.generalized_module.haiku_module.HaikuModule.__class__.__new__","text":"Create and return a new object. See help(type) for accurate signature. Source code in elegy/generalized_module/haiku_module.py def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls","title":"__new__()"},{"location":"api/HaikuModule/#elegy.generalized_module.haiku_module.HaikuModule.__class__.__subclasscheck__","text":"Override for issubclass(subclass, cls). Source code in elegy/generalized_module/haiku_module.py def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass )","title":"__subclasscheck__()"},{"location":"api/HaikuModule/#elegy.generalized_module.haiku_module.HaikuModule.__class__.register","text":"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/generalized_module/haiku_module.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"register()"},{"location":"api/Loss/","text":"elegy.Loss Loss base class. To be implemented by subclasses: call() : Contains the logic for loss calculation. Example subclass implementation: class MeanSquaredError (Loss): def call ( self , y_true, y_pred): return jnp . mean(jnp . square(y_pred - y_true), axis =-1 ) Please see the [Modules, Losses, and Metrics Guide] (https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#losses) for more details on this. __init__ ( self , reduction = None , weight = None , on = None , name = None ) special Initializes Loss class. Parameters: Name Type Description Default reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None name Optional[str] Optional name for the instance, if not provided lower snake_case version of the name of the class is used instead. None Source code in elegy/losses/loss.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , name : tp . Optional [ str ] = None , ): \"\"\" Initializes `Loss` class. Arguments: reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). name: Optional name for the instance, if not provided lower snake_case version of the name of the class is used instead. \"\"\" self . name = name if name is not None else utils . get_name ( self ) self . weight = weight if weight is not None else 1.0 self . _reduction = ( reduction if reduction is not None else Reduction . SUM_OVER_BATCH_SIZE ) self . _labels_filter = ( on ,) if isinstance ( on , ( str , int )) else on self . _signature_f = self . call","title":"Loss"},{"location":"api/Loss/#elegyloss","text":"","title":"elegy.Loss"},{"location":"api/Loss/#elegy.losses.loss.Loss","text":"Loss base class. To be implemented by subclasses: call() : Contains the logic for loss calculation. Example subclass implementation: class MeanSquaredError (Loss): def call ( self , y_true, y_pred): return jnp . mean(jnp . square(y_pred - y_true), axis =-1 ) Please see the [Modules, Losses, and Metrics Guide] (https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#losses) for more details on this.","title":"elegy.losses.loss.Loss"},{"location":"api/Loss/#elegy.losses.loss.Loss.__init__","text":"Initializes Loss class. Parameters: Name Type Description Default reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None name Optional[str] Optional name for the instance, if not provided lower snake_case version of the name of the class is used instead. None Source code in elegy/losses/loss.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , name : tp . Optional [ str ] = None , ): \"\"\" Initializes `Loss` class. Arguments: reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). name: Optional name for the instance, if not provided lower snake_case version of the name of the class is used instead. \"\"\" self . name = name if name is not None else utils . get_name ( self ) self . weight = weight if weight is not None else 1.0 self . _reduction = ( reduction if reduction is not None else Reduction . SUM_OVER_BATCH_SIZE ) self . _labels_filter = ( on ,) if isinstance ( on , ( str , int )) else on self . _signature_f = self . call","title":"__init__()"},{"location":"api/Losses/","text":"elegy.Losses","title":"Losses"},{"location":"api/Losses/#elegylosses","text":"","title":"elegy.Losses"},{"location":"api/Losses/#elegy.model.model.Losses","text":"","title":"elegy.model.model.Losses"},{"location":"api/Metric/","text":"elegy.Metric Encapsulates metric logic and state. Metrics accumulate state between apply s such that their output value reflect the metric as if calculated on the whole data given up to that point. Usage: m = SomeMetric() _, state = m . init()(x) for x in batch: result = m . apply(state)(x) print ( 'Final result: ' , result) Usage with the Model API: >>> import elegy , jax , optax >>> model = elegy . Model( ... module = elegy . nn . Sequential( ... lambda : [ ... elegy . nn . Flatten(), ... elegy . nn . Linear( 300 ), ... jax . nn . relu, ... elegy . nn . Linear( 10 ), ... ] ... ), ... loss = [ ... elegy . losses . SparseCategoricalCrossentropy(from_logits = True ), ... ], ... metrics = [ ... elegy . metrics . SparseCategoricalAccuracy() ... ], ... optimizer = optax . rmsprop( 1e-3 ), ... ) To be implemented by subclasses: call() : All state variables should be created in this method by calling self.add_parameter(..., trainable=False) , update this state by calling self.update_parameter(...) , and return a result based on these states. Example subclass implementation: >>> class Accuracy (elegy . Metric): ... def call ( self , y_true, y_pred): ... ... total = self . add_parameter( \"total\" , lambda : jnp . array( 0 ), trainable = False ) ... count = self . add_parameter( \"count\" , lambda : jnp . array( 0 ), trainable = False ) ... ... total += jnp . sum(y_true == y_pred) ... count += jnp . prod(y_true . shape) ... ... self . update_parameter( \"total\" , total) ... self . update_parameter( \"count\" , count) ... ... return total / count __init__ ( self , on = None , ** kwargs ) special Base Metric constructor. Parameters: Name Type Description Default on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/metrics/metric.py def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Base Metric constructor. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" super () . __init__ ( ** kwargs ) self . _labels_filter = ( on ,) if isinstance ( on , ( str , int )) else on","title":"Metric"},{"location":"api/Metric/#elegymetric","text":"","title":"elegy.Metric"},{"location":"api/Metric/#elegy.metrics.metric.Metric","text":"Encapsulates metric logic and state. Metrics accumulate state between apply s such that their output value reflect the metric as if calculated on the whole data given up to that point. Usage: m = SomeMetric() _, state = m . init()(x) for x in batch: result = m . apply(state)(x) print ( 'Final result: ' , result) Usage with the Model API: >>> import elegy , jax , optax >>> model = elegy . Model( ... module = elegy . nn . Sequential( ... lambda : [ ... elegy . nn . Flatten(), ... elegy . nn . Linear( 300 ), ... jax . nn . relu, ... elegy . nn . Linear( 10 ), ... ] ... ), ... loss = [ ... elegy . losses . SparseCategoricalCrossentropy(from_logits = True ), ... ], ... metrics = [ ... elegy . metrics . SparseCategoricalAccuracy() ... ], ... optimizer = optax . rmsprop( 1e-3 ), ... ) To be implemented by subclasses: call() : All state variables should be created in this method by calling self.add_parameter(..., trainable=False) , update this state by calling self.update_parameter(...) , and return a result based on these states. Example subclass implementation: >>> class Accuracy (elegy . Metric): ... def call ( self , y_true, y_pred): ... ... total = self . add_parameter( \"total\" , lambda : jnp . array( 0 ), trainable = False ) ... count = self . add_parameter( \"count\" , lambda : jnp . array( 0 ), trainable = False ) ... ... total += jnp . sum(y_true == y_pred) ... count += jnp . prod(y_true . shape) ... ... self . update_parameter( \"total\" , total) ... self . update_parameter( \"count\" , count) ... ... return total / count","title":"elegy.metrics.metric.Metric"},{"location":"api/Metric/#elegy.metrics.metric.Metric.__init__","text":"Base Metric constructor. Parameters: Name Type Description Default on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/metrics/metric.py def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Base Metric constructor. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" super () . __init__ ( ** kwargs ) self . _labels_filter = ( on ,) if isinstance ( on , ( str , int )) else on","title":"__init__()"},{"location":"api/Metrics/","text":"elegy.Metrics","title":"Metrics"},{"location":"api/Metrics/#elegymetrics","text":"","title":"elegy.Metrics"},{"location":"api/Metrics/#elegy.model.model.Metrics","text":"","title":"elegy.model.model.Metrics"},{"location":"api/Model/","text":"elegy.Model Model is a framework-agnostic trainer interface that is tasked with performing training, evaluation, and inference. It provides 2 main APIs: The high-level API provides the Keras-like experience of simplicity and speed. The user provides things like the a modules, losses, metrics, and callbacks and Elegy takes care of the rest. The low-level API provides the Pytorch Lightning-like experience of experisiveness and power. The user can override methods like pred_step , test_step , and train_step to perform jax-based computation with very few restrictions. High-level API To use the high-level API you first have to define a network architecture in a Module, currently we support Modules from Flax, Haiku, Elegy, and pure jax functions. Using elegy.Module this could look like this: >>> import elegy , jax , optax >>> import jax.numpy as jnp >>> class MLP (elegy . Module): ... def call ( self , x: jnp . ndarray) -> jnp . ndarray: ... x = elegy . nn . Linear( 5 )(x) ... x = jax . nn . relu(x) ... x = elegy . nn . Linear( 2 )(x) ... return x You can pass this architecture to the Model API along with losses, metrics, optimizer, etc: >>> model = elegy . Model( ... module = MLP(), ... loss = [ ... elegy . losses . SparseCategoricalCrossentropy(from_logits = True ), ... elegy . regularizers . GlobalL2(l =1e-5 ), ... ], ... metrics = elegy . metrics . SparseCategoricalAccuracy(), ... optimizer = optax . rmsprop( 1e-3 ), ... ) Once the model is created, you can train the model with model.fit() , or use the model to do prediction with model.predict() . Checkout Getting Started for additional details. >>> x = jnp . ones(shape = [ 12 , 10 ]) >>> y = jnp . ones(shape = [ 12 ]) >>> history = model . fit(x, y) # doctest: +SKIP Model supports optax optimizers as well as elegy.Optimizer which has a feature for definint + monitoring custom learning rate schedules, it is implemented on top of optax and you can use it like this: >>> model = elegy . Model( ... module = MLP(), ... loss = elegy . losses . SparseCategoricalCrossentropy(from_logits = True ), ... metrics = elegy . metrics . SparseCategoricalAccuracy(), ... optimizer = elegy . Optimizer( ... # one or more optax optimizers as *args, ... # these will be passed to optax.chain(...) ... optax . adam( 1.0 ), # <---- important to set this to 1.0 ... ... lr_schedule = lambda step, epoch: 1 / (epoch * 100 + step), ... steps_per_epoch =1000 , ... ), ... run_eagerly = True , ... ) Notice how we set the learning rate parameter of the adam optimizer to 1.0 , this is necessary if you want the logged lr be closer to the \"actual\" learning rate since this feature was implemented by chaining an additional optax.scale_by_schedule at the end. High-level API evaluate ( self , x = None , y = None , verbose = 1 , batch_size = None , sample_weight = None , class_weight = None , steps = None , callbacks = None , drop_remaining = False ) inherited Returns the loss value & metrics values for the model in test mode. Computation is done in batches. Parameters: Name Type Description Default x Optional[Any] Input data. It could be: A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. A generator returning (inputs,) , (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types generator is given in the Unpacking behavior for iterator-like inputs section of Model.fit . None y Optional[Any] Target data. Like the input data x , it could be either Numpy or Jax array(s). It should be consistent with x . If x is a generator, y should not be specified (since targets will be obtained from x ). None verbose int 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 1 batch_size Optional[int] Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of generator (since they generate batches). None sample_weight Optional[numpy.ndarray] Optional Numpy/Jax array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples). This argument is not supported when x is generator, instead provide the sample_weights as the third element of x . None steps Optional[int] Integer or None . Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of None . This argument is not supported with array inputs. None callbacks Union[List[elegy.callbacks.callback.Callback], elegy.callbacks.callback_list.CallbackList] List of elegy.callbacks.callback.Callback instances. List of callbacks to apply during training. None See the discussion of Unpacking behavior for iterator-like inputs for Model.fit . Returns: Type Description Dict[str, Union[numpy.ndarray, float]] A dictionary for mapping the losses and metrics names to the values obtained. Exceptions: Type Description ValueError in case of invalid arguments. Source code in elegy/model/model.py def evaluate ( self , x : tp . Optional [ tp . Any ] = None , y : tp . Optional [ tp . Any ] = None , verbose : int = 1 , batch_size : tp . Optional [ int ] = None , sample_weight : tp . Optional [ np . ndarray ] = None , class_weight : tp . Optional [ tp . Mapping [ str , float ]] = None , steps : tp . Optional [ int ] = None , callbacks : tp . Union [ tp . List [ Callback ], CallbackList , None ] = None , drop_remaining : bool = False , ) -> types . Logs : \"\"\"Returns the loss value & metrics values for the model in test mode. Computation is done in batches. Arguments: x: Input data. It could be: - A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. - A generator returning `(inputs,)`, `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types generator is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. y: Target data. Like the input data `x`, it could be either Numpy or Jax array(s). It should be consistent with `x`. If `x` is a generator, `y` should not be specified (since targets will be obtained from `x`). verbose: 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of generator (since they generate batches). sample_weight: Optional Numpy/Jax array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples). This argument is not supported when `x` is generator, instead provide the sample_weights as the third element of `x`. steps: Integer or `None`. Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of `None`. This argument is not supported with array inputs. callbacks: List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. See the discussion of `Unpacking behavior for iterator-like inputs` for [`Model.fit`][elegy.model.model.Model.fit]. Returns: A dictionary for mapping the losses and metrics names to the values obtained. Raises: ValueError: in case of invalid arguments. \"\"\" if x is None : x = {} if not self . initialized : self . init ( x = x , y = y , batch_size = batch_size , class_weight = class_weight , sample_weight = sample_weight , ) data_handler = DataHandler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , shuffle = False , training = False , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , CallbackList ): callbacks = CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps , ) callbacks . on_test_begin () logs = {} for _ , iterator in data_handler . enumerate_epochs (): self . reset_metrics () with data_handler . catch_stop_iteration (): for step in data_handler . steps (): callbacks . on_test_batch_begin ( step ) batch = next ( iterator ) x_batch , y_batch , sample_weight = unpack_x_y_sample_weight ( batch ) if drop_remaining and not data_utils . has_batch_size ( batch , data_handler . batch_size ): continue tmp_logs = self . test_on_batch ( x = x_batch , y = y_batch , sample_weight = sample_weight , class_weight = None , ) tmp_logs . update ({ \"size\" : data_handler . batch_size }) logs = tmp_logs callbacks . on_test_batch_end ( step , logs ) callbacks . on_test_end () return logs fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = 1 , callbacks = None , validation_split = 0.0 , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , drop_remaining = True ) inherited Trains the model for a fixed number of epochs (iterations on a dataset). Parameters: Name Type Description Default x Optional[Any] Input data. It could be: A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. A generator returning (inputs,) , (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for generator type is given below. None y Optional[Any] Target data. Like the input data x , it could be either Numpy or Jax array(s). It should be consistent with x . If x is a generator, y should not be specified (since targets will be obtained from x ). None batch_size Optional[int] Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of generator (since they generate batches). None epochs int Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided. Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. 1 verbose int 0, 1, 2 or 3. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch 3 = table. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). 1 callbacks Union[List[elegy.callbacks.callback.Callback], elegy.callbacks.callback_list.CallbackList] List of elegy.callbacks.callback.Callback instances. List of callbacks to apply during training. None validation_split float Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a generator. 0.0 validation_data Union[Tuple, Iterable] Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . validation_data could be: tuple (x_val, y_val) of Numpy/Jax arrays, list of arrays or mappings tuple (x_val, y_val, val_sample_weights) of Numpy/Jax arrays, list of arrays or mappings generator For the first two cases, batch_size must be provided. For the last case, validation_steps should be provided, and should follow the same convention for yielding data as x . Note that validation_data does not support all the data types that are supported in x , eg, dict. None shuffle bool Boolean (whether to shuffle the training data before each epoch). This argument is ignored when x is a generator. Has no effect when steps_per_epoch is not None . True class_weight Optional[Mapping[str, float]] Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. None sample_weight Optional[numpy.ndarray] Optional Numpy/Jax array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples). This argument is not supported when x is generator, instead provide the sample_weights as the third element of x . None initial_epoch int Integer. Epoch at which to start training (useful for resuming a previous training run). 0 steps_per_epoch Optional[int] Integer or None . Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input arrays such as jax data arrays, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. When passing a generator, you must specify the steps_per_epoch argument. This argument is not supported with array inputs. None validation_steps Optional[int] Only relevant if validation_data is provided and is a generator. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the validation_data dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. None validation_batch_size Optional[int] Integer or None . Number of samples per validation batch. If unspecified, will default to batch_size . Do not specify the validation_batch_size if your data is in the form of generators (since they generate batches). None validation_freq int Only relevant if validation data is provided. Integer or collections_abc.Container instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. 1 Unpacking behavior for iterator-like inputs: A common pattern is to pass a generator, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Elegy requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. ({\"x0\": x0, \"x1\": x1}, y) . Elegy will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: namedtuple(\"example_tuple\", [\"y\", \"x\"]) it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"]) where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to x . As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: Type Description History A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Exceptions: Type Description ValueError In case of mismatch between the provided input data and what the model expects. Source code in elegy/model/model.py def fit ( self , x : tp . Optional [ tp . Any ] = None , y : tp . Optional [ tp . Any ] = None , batch_size : tp . Optional [ int ] = None , epochs : int = 1 , verbose : int = 1 , callbacks : tp . Union [ tp . List [ Callback ], CallbackList , None ] = None , validation_split : float = 0.0 , validation_data : tp . Union [ tp . Tuple , tp . Iterable , None ] = None , shuffle : bool = True , class_weight : tp . Optional [ tp . Mapping [ str , float ]] = None , sample_weight : tp . Optional [ np . ndarray ] = None , initial_epoch : int = 0 , steps_per_epoch : tp . Optional [ int ] = None , validation_steps : tp . Optional [ int ] = None , validation_batch_size : tp . Optional [ int ] = None , validation_freq : int = 1 , drop_remaining : bool = True , ) -> History : \"\"\" Trains the model for a fixed number of epochs (iterations on a dataset). Arguments: x: Input data. It could be: - A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. - A generator returning `(inputs,)`, `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for generator type is given below. y: Target data. Like the input data `x`, it could be either Numpy or Jax array(s). It should be consistent with `x`. If `x` is a generator, `y` should not be specified (since targets will be obtained from `x`). batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of generator (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire `x` and `y` data provided. Note that in conjunction with `initial_epoch`, `epochs` is to be understood as \"final epoch\". The model is not trained for a number of iterations given by `epochs`, but merely until the epoch of index `epochs` is reached. verbose: 0, 1, 2 or 3. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch 3 = table. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the `x` and `y` data provided, before shuffling. This argument is not supported when `x` is a generator. validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. `validation_data` will override `validation_split`. `validation_data` could be: - tuple `(x_val, y_val)` of Numpy/Jax arrays, list of arrays or mappings - tuple `(x_val, y_val, val_sample_weights)` of Numpy/Jax arrays, list of arrays or mappings - generator For the first two cases, `batch_size` must be provided. For the last case, `validation_steps` should be provided, and should follow the same convention for yielding data as `x`. Note that `validation_data` does not support all the data types that are supported in `x`, eg, dict. shuffle: Boolean (whether to shuffle the training data before each epoch). This argument is ignored when `x` is a generator. Has no effect when `steps_per_epoch` is not `None`. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy/Jax array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples). This argument is not supported when `x` is generator, instead provide the sample_weights as the third element of `x`. initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or `None`. Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input arrays such as jax data arrays, the default `None` is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. When passing a generator, you must specify the `steps_per_epoch` argument. This argument is not supported with array inputs. validation_steps: Only relevant if `validation_data` is provided and is a generator. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the `validation_data` dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or `None`. Number of samples per validation batch. If unspecified, will default to `batch_size`. Do not specify the `validation_batch_size` if your data is in the form of generators (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or `collections_abc.Container` instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. `validation_freq=2` runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. `validation_freq=[1, 2, 10]` runs validation at the end of the 1st, 2nd, and 10th epochs. Unpacking behavior for iterator-like inputs: A common pattern is to pass a generator, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Elegy requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Elegy will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: `namedtuple(\"example_tuple\", [\"y\", \"x\"])` it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])` where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to `x`. As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A `History` object. Its `History.history` attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: ValueError: In case of mismatch between the provided input data and what the model expects. \"\"\" if x is None : x = {} if not self . initialized : self . init ( x = x , y = y , batch_size = batch_size , class_weight = class_weight , sample_weight = sample_weight , ) if validation_split : # Create the validation data using the training data. Only supported for # `Jax Numpy` and `NumPy` input. ( x , y , sample_weight ), validation_data = train_validation_split ( ( x , y , sample_weight ), validation_split = validation_split , shuffle = False ) self . stop_training = False data_handler = DataHandler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps_per_epoch , initial_epoch = initial_epoch , epochs = epochs , shuffle = shuffle , class_weight = class_weight , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , CallbackList ): callbacks = CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = epochs , steps = data_handler . inferred_steps , ) callbacks . on_train_begin () # data_handler._initial_epoch = ( # pylint: disable=protected-access # self._maybe_load_initial_epoch_from_ckpt(initial_epoch)) epoch_logs = {} for epoch , iterator in data_handler . enumerate_epochs (): self . reset_metrics () callbacks . on_epoch_begin ( epoch ) logs = {} with data_handler . catch_stop_iteration (): for step in data_handler . steps (): callbacks . on_train_batch_begin ( step ) batch = next ( iterator ) # sample_weight = batch[2] if len(batch) == 3 else None x_batch , y_batch , sample_weight = unpack_x_y_sample_weight ( batch ) if drop_remaining and not data_utils . has_batch_size ( batch , data_handler . batch_size ): continue tmp_logs = self . train_on_batch ( x = x_batch , y = y_batch , sample_weight = sample_weight , class_weight = class_weight , ) tmp_logs . update ({ \"size\" : data_handler . batch_size }) # print(epoch, step, tmp_logs[\"accuracy\"], batch[0].shape) logs = tmp_logs callbacks . on_train_batch_end ( step , logs ) if self . stop_training : break epoch_logs = copy ( logs ) epoch_logs . update ({ \"size\" : data_handler . batch_size }) # Run validation. if ( validation_data and self . _should_eval ( epoch , validation_freq ) and not self . stop_training ): val_x , val_y , val_sample_weight = unpack_x_y_sample_weight ( validation_data ) try : val_logs = self . evaluate ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps = validation_steps , callbacks = callbacks , # return_dict=True, drop_remaining = drop_remaining , ) val_logs = { \"val_\" + name : val for name , val in val_logs . items ()} epoch_logs . update ( val_logs ) except ( types . MissingMethod , types . MissingModule ) as e : pass callbacks . on_epoch_end ( epoch , epoch_logs ) if self . stop_training : break callbacks . on_train_end ( epoch_logs ) return self . history load ( self , path ) inherited Loads all weights + states from a folder structure. You can load states from other models that have slightly different architecture as long as long as it preserves the ordering of the haiku.Params + haiku.State structures, adding or removing layers is fine as long as they don't have weights, new layers with weights will be initialized from scratch. Parameters: Name Type Description Default path Union[str, pathlib.Path] path to a saved model's directory. required Source code in elegy/model/model.py def load ( self , path : tp . Union [ str , pathlib . Path ], ) -> None : \"\"\" Loads all weights + states from a folder structure. You can load states from other models that have slightly different architecture as long as long as it preserves the ordering of the `haiku.Params` + `haiku.State` structures, adding or removing layers is fine as long as they don't have weights, new layers with weights will be initialized from scratch. Arguments: path: path to a saved model's directory. \"\"\" if isinstance ( path , str ): path = pathlib . Path ( path ) self . states = cloudpickle . loads (( path / \"states.pkl\" ) . read_bytes ()) self . initial_states = cloudpickle . loads ( ( path / \"initial_states.pkl\" ) . read_bytes () ) predict ( self , x = None , verbose = 0 , batch_size = None , steps = None , callbacks = None , drop_remaining = False , initialize = False ) inherited Generates output predictions for the input samples. Computation is done in batches. Parameters: Name Type Description Default x Optional[Any] Input data. It could be: A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. A generator returning (inputs,) , (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types generator is given in the Unpacking behavior for iterator-like inputs section of Model.fit . None batch_size Optional[int] Integer or None . Number of samples per batch. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of generators (since they generate batches). None verbose int Verbosity mode, 0 or 1. 0 steps Optional[int] Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None . None callbacks Union[List[elegy.callbacks.callback.Callback], elegy.callbacks.callback_list.CallbackList] List of elegy.callbacks.callback.Callback instances. List of callbacks to apply during training. None See the discussion of Unpacking behavior for iterator-like inputs for Model.fit . Note that Model.predict uses the same interpretation rules as Model.fit and Model.evaluate , so inputs must be unambiguous for all three methods. Returns: Type Description Any Numpy array(s) of predictions. Exceptions: Type Description ValueError In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. Source code in elegy/model/model.py def predict ( self , x : tp . Optional [ tp . Any ] = None , verbose : int = 0 , batch_size : tp . Optional [ int ] = None , steps : tp . Optional [ int ] = None , callbacks : tp . Union [ tp . List [ Callback ], CallbackList , None ] = None , drop_remaining : bool = False , initialize : bool = False , ) -> tp . Any : \"\"\"Generates output predictions for the input samples. Computation is done in batches. Arguments: x: Input data. It could be: - A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. - A generator returning `(inputs,)`, `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types generator is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. batch_size: Integer or `None`. Number of samples per batch. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of generators (since they generate batches). verbose: Verbosity mode, 0 or 1. steps: Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of `None`. callbacks: List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. See the discussion of `Unpacking behavior for iterator-like inputs` for [`Model.fit`][elegy.model.model.Model.fit]. Note that Model.predict uses the same interpretation rules as `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all three methods. Returns: Numpy array(s) of predictions. Raises: ValueError: In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. \"\"\" if x is None : x = {} if not self . initialized : if initialize : self . init ( x = x , batch_size = batch_size ) else : raise types . ModelNotInitialized ( f \"Model not initialized, please execute `model.init` or `model.init_on_batch` \" \"before running this method, or pass `initialize=True` to initialize with the available data \" \"(this might not initialize the optimizer).\" ) outputs = None data_handler = DataHandler ( x = x , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , shuffle = False , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , CallbackList ): callbacks = CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps , ) callbacks . on_predict_begin () for _ , iterator in data_handler . enumerate_epochs (): self . reset_metrics () with data_handler . catch_stop_iteration (): for step in data_handler . steps (): callbacks . on_predict_batch_begin ( step ) batch = next ( iterator ) if drop_remaining and not data_utils . has_batch_size ( batch , data_handler . batch_size ): continue tmp_batch_outputs = self . predict_on_batch ( x = batch [ 0 ]) batch_outputs = tmp_batch_outputs if outputs is None : outputs = map_structure ( lambda batch_output : [ batch_output ], batch_outputs ) else : outputs = map_structure ( map_append , outputs , batch_outputs , ) callbacks . on_predict_batch_end ( step , { \"outputs\" : batch_outputs , \"size\" : data_handler . batch_size }, ) callbacks . on_predict_end () all_outputs = map_structure ( jnp . concatenate , outputs ) return all_outputs predict_on_batch ( self , x ) inherited Returns predictions for a single batch of samples. Parameters: Name Type Description Default x Any Input data. A Numpy/Jax array (or array-like), or possibly nested python structure of dict, list, tuple that contain arrays as leafs. required Returns: Type Description Any Jax array(s) of predictions. Exceptions: Type Description ValueError In case of mismatch between given number of inputs and expectations of the model. Source code in elegy/model/model.py def predict_on_batch ( self , x : tp . Any ) -> tp . Any : \"\"\" Returns predictions for a single batch of samples. Arguments: x: Input data. A Numpy/Jax array (or array-like), or possibly nested python structure of dict, list, tuple that contain arrays as leafs. Returns: Jax array(s) of predictions. Raises: ValueError: In case of mismatch between given number of inputs and expectations of the model. \"\"\" if not self . initialized : raise types . ModelNotInitialized ( f \"Model not initialized, please execute `init` or `init_on_batch` before running this method.\" ) initializing = False training = False method = self . call_pred_step if self . run_eagerly else self . call_pred_step_jit states = self . states . copy () if self . run_eagerly else self . states y_pred , self . states = method ( x , states , initializing , training , ) return y_pred save ( self , path ) inherited Saves the model to disk. It creates a directory that includes: {path}/model.pkl : The Model object instance serialized with pickle , this allows you to re-instantiate the model later. {path}/states.pkl : The Model.states serialized with pickle . {path}/initial_states.pkl : The Model.initial_states serialized with pickle . This allows you to save the entirety of the states of a model in a directory structure which can be fully restored via Model.load if the model is already instiated or elegy.model.load to load the model instance from its pickled version. import elegy model . save( 'my_model' ) # creates folder at 'my_model' del model # deletes the existing model # returns a model identical to the previous one model = elegy . model . load( 'my_model' ) Parameters: Name Type Description Default path Union[str, pathlib.Path] path where model structure will be saved. required Source code in elegy/model/model.py def save ( self , path : tp . Union [ str , pathlib . Path ], ) -> None : \"\"\" Saves the model to disk. It creates a directory that includes: - `{path}/model.pkl`: The `Model` object instance serialized with `pickle`, this allows you to re-instantiate the model later. - `{path}/states.pkl`: The `Model.states` serialized with `pickle`. - `{path}/initial_states.pkl`: The `Model.initial_states` serialized with `pickle`. This allows you to save the entirety of the states of a model in a directory structure which can be fully restored via `Model.load` if the model is already instiated or `elegy.model.load` to load the model instance from its pickled version. ```python import elegy model.save('my_model') # creates folder at 'my_model' del model # deletes the existing model # returns a model identical to the previous one model = elegy.model.load('my_model') ``` Arguments: path: path where model structure will be saved. \"\"\" if isinstance ( path , str ): path = pathlib . Path ( path ) path . mkdir ( parents = True , exist_ok = True ) with open ( path / \"states.pkl\" , \"wb\" ) as f : cloudpickle . dump ( self . states , f ) with open ( path / \"initial_states.pkl\" , \"wb\" ) as f : cloudpickle . dump ( self . initial_states , f ) with open ( path / \"model.pkl\" , \"wb\" ) as f : cloudpickle . dump ( self , f ) summary ( self , x = None , depth = 2 , tablefmt = 'fancy_grid' , return_repr = False , initialize = False , eval_shape = True , ** tablulate_kwargs ) inherited Prints a summary of the network. Parameters: Name Type Description Default x Optional[Any] A sample of inputs to the network. None depth int The level number of nested level which will be showed. Information about summaries from modules deeper than depth will be aggregated together. 2 tablefmt str A string representing the style of the table generated by tabulate . See python-tabulate for more options. 'fancy_grid' tablulate_kwargs Additional keyword arguments passed to tabulate . See python-tabulate for more options. {} Source code in elegy/model/model.py def summary ( self , x : tp . Optional [ tp . Any ] = None , depth : int = 2 , tablefmt : str = \"fancy_grid\" , return_repr : bool = False , initialize : bool = False , eval_shape : bool = True , ** tablulate_kwargs , ) -> tp . Optional [ str ]: \"\"\" Prints a summary of the network. Arguments: x: A sample of inputs to the network. depth: The level number of nested level which will be showed. Information about summaries from modules deeper than `depth` will be aggregated together. tablefmt: A string representing the style of the table generated by `tabulate`. See [python-tabulate](https://github.com/astanin/python-tabulate) for more options. tablulate_kwargs: Additional keyword arguments passed to `tabulate`. See [python-tabulate](https://github.com/astanin/python-tabulate) for more options. \"\"\" if x is None : x = {} entries : tp . List [ types . SummaryTableEntry ] states = self . states . copy () if self . run_eagerly else self . states method = ( self . call_summary_step if self . run_eagerly else self . call_summary_step_jit ) if eval_shape : entries = jax . eval_shape ( self . call_summary_step , x , states ) else : entries = method ( x , states ) total_entry = entries [ - 1 ] entries = entries [: - 1 ] depth_groups : tp . Dict [ str , tp . List [ types . SummaryTableEntry ]] = toolz . groupby ( lambda entry : \"/\" . join ( entry . path . split ( \"/\" )[: depth ]), entries ) entries = [ utils . get_grouped_entry ( entry , depth_groups ) for entry in entries if entry . path in depth_groups ] main_table = Table ( show_header = True , show_lines = True , show_footer = True , # box=rich.box.HORIZONTALS, ) main_table . add_column ( \"Layer\" ) main_table . add_column ( \"Outputs Shape\" ) main_table . add_column ( \"Trainable \\n Parameters\" ) main_table . add_column ( \"Non-trainable \\n Parameters\" ) rows : tp . List [ tp . List [ str ]] = [] rows . append ([ \"Inputs\" , utils . format_output ( x ), \"\" , \"\" ]) for entry in entries : rows . append ( [ f \" { entry . path } {{ pad }} \" + ( f \"[dim] { entry . module_type_name } [/]\" if entry . module_type_name else \"\" ), utils . format_output ( entry . output_value ), f \"[green] { entry . trainable_params_count : , } [/] {{ pad }} { utils . format_size ( entry . trainable_params_size ) } \" if entry . trainable_params_count > 0 else \"\" , f \"[green] { entry . non_trainable_params_count : , } [/] {{ pad }} { utils . format_size ( entry . non_trainable_params_size ) } \" if entry . non_trainable_params_count > 0 else \"\" , ] ) # global summaries params_count = total_entry . trainable_params_count params_size = total_entry . trainable_params_size states_count = total_entry . non_trainable_params_count states_size = total_entry . non_trainable_params_size total_count = params_count + states_count total_size = params_size + states_size rows . append ( [ \"\" , \"Total\" , ( f \"[green] { params_count : , } [/] {{ pad }} { utils . format_size ( params_size ) } \" if params_count > 0 else \"\" ), ( f \"[green] { states_count : , } [/] {{ pad }} { utils . format_size ( states_size ) } \" if states_count > 0 else \"\" ), ] ) # add padding for col in range ( 4 ): max_length = max ( len ( line . split ( \" {pad} \" )[ 0 ]) for row in rows for line in row [ col ] . split ( \" \\n \" ) ) for row in rows : row [ col ] = \" \\n \" . join ( line . format ( pad = \" \" * ( max_length - len ( line . rstrip () . split ( \" {pad} \" )[ 0 ])) ) for line in row [ col ] . rstrip () . split ( \" \\n \" ) ) for row in rows [: - 1 ]: main_table . add_row ( * row ) main_table . columns [ 1 ] . footer = Text . from_markup ( rows [ - 1 ][ 1 ], justify = \"right\" ) main_table . columns [ 2 ] . footer = rows [ - 1 ][ 2 ] main_table . columns [ 3 ] . footer = rows [ - 1 ][ 3 ] main_table . caption_style = \"bold\" main_table . caption = ( \" \\n Total Parameters: \" + f \"[green] { total_count : , } [/] { utils . format_size ( total_size ) } \" if total_count > 0 else \"\" ) summary = \" \\n \" + utils . get_table_repr ( main_table ) print ( summary ) if return_repr : return summary test_on_batch ( self , x , y = None , sample_weight = None , class_weight = None ) inherited Test the model on a single batch of samples. Parameters: Name Type Description Default x Any Input data. It could be: A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. required y Union[numpy.ndarray, Mapping[str, Any], Tuple] Target data. Like the input data x , it could be either Numpy array(s) or Jax array(s). None sample_weight Optional[numpy.ndarray] Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. None Returns: Type Description Dict[str, Union[numpy.ndarray, float]] A logs dictionary of containing the main loss as well as all other losses and metrics. Exceptions: Type Description ValueError In case of invalid user-provided arguments. Source code in elegy/model/model.py def test_on_batch ( self , x : tp . Any , y : tp . Union [ np . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple , None ] = None , sample_weight : tp . Optional [ np . ndarray ] = None , class_weight : tp . Optional [ np . ndarray ] = None , ) -> types . Logs : \"\"\" Test the model on a single batch of samples. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or Jax array(s). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. Returns: A `logs` dictionary of containing the main `loss` as well as all other losses and metrics. Raises: ValueError: In case of invalid user-provided arguments. \"\"\" self . init_on_batch ( x = x , y_true = y , sample_weight = sample_weight , class_weight = class_weight , ) initializing = False training = False method = self . call_test_step if self . run_eagerly else self . call_test_step_jit states = self . states . copy () if self . run_eagerly else self . states loss , logs , self . states = method ( x , y , sample_weight , class_weight , states , initializing , training , ) return logs train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None ) inherited Runs a single gradient update on a single batch of data. Parameters: Name Type Description Default x Any Input data. It could be: A Numpy array (or array-like), or a iterable of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. required y Union[numpy.ndarray, Mapping[str, Any], Tuple] Target data. Like the input data x , it could be either Numpy array(s) or Jax array(s). It should be consistent with x (you cannot have Numpy inputs and array targets, or inversely). None sample_weight Optional[numpy.ndarray] Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. None class_weight Optional[Any] Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. None Returns: Type Description Dict[str, Union[numpy.ndarray, float]] A logs dictionary of containing the main loss as well as all other losses and metrics. Exceptions: Type Description ValueError In case of invalid user-provided arguments. Source code in elegy/model/model.py def train_on_batch ( self , x : tp . Any , y : tp . Union [ np . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple , None ] = None , sample_weight : tp . Optional [ np . ndarray ] = None , class_weight : tp . Optional [ tp . Any ] = None , ) -> types . Logs : \"\"\" Runs a single gradient update on a single batch of data. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a iterable of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or Jax array(s). It should be consistent with `x` (you cannot have Numpy inputs and array targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. Returns: A `logs` dictionary of containing the main `loss` as well as all other losses and metrics. Raises: ValueError: In case of invalid user-provided arguments. \"\"\" self . init_on_batch ( x = x , y_true = y , sample_weight = sample_weight , class_weight = class_weight , ) initializing = False training = True method = self . call_train_step if self . run_eagerly else self . call_train_step_jit states = self . states . copy () if self . run_eagerly else self . states logs , self . states = method ( x , y , sample_weight , class_weight , states , initializing , training , ) return logs","title":"Model"},{"location":"api/Model/#elegymodel","text":"","title":"elegy.Model"},{"location":"api/Model/#elegy.model.model.Model","text":"Model is a framework-agnostic trainer interface that is tasked with performing training, evaluation, and inference. It provides 2 main APIs: The high-level API provides the Keras-like experience of simplicity and speed. The user provides things like the a modules, losses, metrics, and callbacks and Elegy takes care of the rest. The low-level API provides the Pytorch Lightning-like experience of experisiveness and power. The user can override methods like pred_step , test_step , and train_step to perform jax-based computation with very few restrictions.","title":"elegy.model.model.Model"},{"location":"api/Model/#elegy.model.model.Model--high-level-api","text":"To use the high-level API you first have to define a network architecture in a Module, currently we support Modules from Flax, Haiku, Elegy, and pure jax functions. Using elegy.Module this could look like this: >>> import elegy , jax , optax >>> import jax.numpy as jnp >>> class MLP (elegy . Module): ... def call ( self , x: jnp . ndarray) -> jnp . ndarray: ... x = elegy . nn . Linear( 5 )(x) ... x = jax . nn . relu(x) ... x = elegy . nn . Linear( 2 )(x) ... return x You can pass this architecture to the Model API along with losses, metrics, optimizer, etc: >>> model = elegy . Model( ... module = MLP(), ... loss = [ ... elegy . losses . SparseCategoricalCrossentropy(from_logits = True ), ... elegy . regularizers . GlobalL2(l =1e-5 ), ... ], ... metrics = elegy . metrics . SparseCategoricalAccuracy(), ... optimizer = optax . rmsprop( 1e-3 ), ... ) Once the model is created, you can train the model with model.fit() , or use the model to do prediction with model.predict() . Checkout Getting Started for additional details. >>> x = jnp . ones(shape = [ 12 , 10 ]) >>> y = jnp . ones(shape = [ 12 ]) >>> history = model . fit(x, y) # doctest: +SKIP Model supports optax optimizers as well as elegy.Optimizer which has a feature for definint + monitoring custom learning rate schedules, it is implemented on top of optax and you can use it like this: >>> model = elegy . Model( ... module = MLP(), ... loss = elegy . losses . SparseCategoricalCrossentropy(from_logits = True ), ... metrics = elegy . metrics . SparseCategoricalAccuracy(), ... optimizer = elegy . Optimizer( ... # one or more optax optimizers as *args, ... # these will be passed to optax.chain(...) ... optax . adam( 1.0 ), # <---- important to set this to 1.0 ... ... lr_schedule = lambda step, epoch: 1 / (epoch * 100 + step), ... steps_per_epoch =1000 , ... ), ... run_eagerly = True , ... ) Notice how we set the learning rate parameter of the adam optimizer to 1.0 , this is necessary if you want the logged lr be closer to the \"actual\" learning rate since this feature was implemented by chaining an additional optax.scale_by_schedule at the end.","title":"High-level API"},{"location":"api/Model/#elegy.model.model.Model--high-level-api_1","text":"","title":"High-level API"},{"location":"api/Model/#elegy.model.model.Model.evaluate","text":"Returns the loss value & metrics values for the model in test mode. Computation is done in batches. Parameters: Name Type Description Default x Optional[Any] Input data. It could be: A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. A generator returning (inputs,) , (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types generator is given in the Unpacking behavior for iterator-like inputs section of Model.fit . None y Optional[Any] Target data. Like the input data x , it could be either Numpy or Jax array(s). It should be consistent with x . If x is a generator, y should not be specified (since targets will be obtained from x ). None verbose int 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 1 batch_size Optional[int] Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of generator (since they generate batches). None sample_weight Optional[numpy.ndarray] Optional Numpy/Jax array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples). This argument is not supported when x is generator, instead provide the sample_weights as the third element of x . None steps Optional[int] Integer or None . Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of None . This argument is not supported with array inputs. None callbacks Union[List[elegy.callbacks.callback.Callback], elegy.callbacks.callback_list.CallbackList] List of elegy.callbacks.callback.Callback instances. List of callbacks to apply during training. None See the discussion of Unpacking behavior for iterator-like inputs for Model.fit . Returns: Type Description Dict[str, Union[numpy.ndarray, float]] A dictionary for mapping the losses and metrics names to the values obtained. Exceptions: Type Description ValueError in case of invalid arguments. Source code in elegy/model/model.py def evaluate ( self , x : tp . Optional [ tp . Any ] = None , y : tp . Optional [ tp . Any ] = None , verbose : int = 1 , batch_size : tp . Optional [ int ] = None , sample_weight : tp . Optional [ np . ndarray ] = None , class_weight : tp . Optional [ tp . Mapping [ str , float ]] = None , steps : tp . Optional [ int ] = None , callbacks : tp . Union [ tp . List [ Callback ], CallbackList , None ] = None , drop_remaining : bool = False , ) -> types . Logs : \"\"\"Returns the loss value & metrics values for the model in test mode. Computation is done in batches. Arguments: x: Input data. It could be: - A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. - A generator returning `(inputs,)`, `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types generator is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. y: Target data. Like the input data `x`, it could be either Numpy or Jax array(s). It should be consistent with `x`. If `x` is a generator, `y` should not be specified (since targets will be obtained from `x`). verbose: 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of generator (since they generate batches). sample_weight: Optional Numpy/Jax array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples). This argument is not supported when `x` is generator, instead provide the sample_weights as the third element of `x`. steps: Integer or `None`. Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of `None`. This argument is not supported with array inputs. callbacks: List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. See the discussion of `Unpacking behavior for iterator-like inputs` for [`Model.fit`][elegy.model.model.Model.fit]. Returns: A dictionary for mapping the losses and metrics names to the values obtained. Raises: ValueError: in case of invalid arguments. \"\"\" if x is None : x = {} if not self . initialized : self . init ( x = x , y = y , batch_size = batch_size , class_weight = class_weight , sample_weight = sample_weight , ) data_handler = DataHandler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , shuffle = False , training = False , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , CallbackList ): callbacks = CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps , ) callbacks . on_test_begin () logs = {} for _ , iterator in data_handler . enumerate_epochs (): self . reset_metrics () with data_handler . catch_stop_iteration (): for step in data_handler . steps (): callbacks . on_test_batch_begin ( step ) batch = next ( iterator ) x_batch , y_batch , sample_weight = unpack_x_y_sample_weight ( batch ) if drop_remaining and not data_utils . has_batch_size ( batch , data_handler . batch_size ): continue tmp_logs = self . test_on_batch ( x = x_batch , y = y_batch , sample_weight = sample_weight , class_weight = None , ) tmp_logs . update ({ \"size\" : data_handler . batch_size }) logs = tmp_logs callbacks . on_test_batch_end ( step , logs ) callbacks . on_test_end () return logs","title":"evaluate()"},{"location":"api/Model/#elegy.model.model.Model.fit","text":"Trains the model for a fixed number of epochs (iterations on a dataset). Parameters: Name Type Description Default x Optional[Any] Input data. It could be: A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. A generator returning (inputs,) , (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for generator type is given below. None y Optional[Any] Target data. Like the input data x , it could be either Numpy or Jax array(s). It should be consistent with x . If x is a generator, y should not be specified (since targets will be obtained from x ). None batch_size Optional[int] Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of generator (since they generate batches). None epochs int Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided. Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. 1 verbose int 0, 1, 2 or 3. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch 3 = table. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). 1 callbacks Union[List[elegy.callbacks.callback.Callback], elegy.callbacks.callback_list.CallbackList] List of elegy.callbacks.callback.Callback instances. List of callbacks to apply during training. None validation_split float Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a generator. 0.0 validation_data Union[Tuple, Iterable] Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . validation_data could be: tuple (x_val, y_val) of Numpy/Jax arrays, list of arrays or mappings tuple (x_val, y_val, val_sample_weights) of Numpy/Jax arrays, list of arrays or mappings generator For the first two cases, batch_size must be provided. For the last case, validation_steps should be provided, and should follow the same convention for yielding data as x . Note that validation_data does not support all the data types that are supported in x , eg, dict. None shuffle bool Boolean (whether to shuffle the training data before each epoch). This argument is ignored when x is a generator. Has no effect when steps_per_epoch is not None . True class_weight Optional[Mapping[str, float]] Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. None sample_weight Optional[numpy.ndarray] Optional Numpy/Jax array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples). This argument is not supported when x is generator, instead provide the sample_weights as the third element of x . None initial_epoch int Integer. Epoch at which to start training (useful for resuming a previous training run). 0 steps_per_epoch Optional[int] Integer or None . Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input arrays such as jax data arrays, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. When passing a generator, you must specify the steps_per_epoch argument. This argument is not supported with array inputs. None validation_steps Optional[int] Only relevant if validation_data is provided and is a generator. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the validation_data dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. None validation_batch_size Optional[int] Integer or None . Number of samples per validation batch. If unspecified, will default to batch_size . Do not specify the validation_batch_size if your data is in the form of generators (since they generate batches). None validation_freq int Only relevant if validation data is provided. Integer or collections_abc.Container instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. 1 Unpacking behavior for iterator-like inputs: A common pattern is to pass a generator, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Elegy requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. ({\"x0\": x0, \"x1\": x1}, y) . Elegy will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: namedtuple(\"example_tuple\", [\"y\", \"x\"]) it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"]) where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to x . As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: Type Description History A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Exceptions: Type Description ValueError In case of mismatch between the provided input data and what the model expects. Source code in elegy/model/model.py def fit ( self , x : tp . Optional [ tp . Any ] = None , y : tp . Optional [ tp . Any ] = None , batch_size : tp . Optional [ int ] = None , epochs : int = 1 , verbose : int = 1 , callbacks : tp . Union [ tp . List [ Callback ], CallbackList , None ] = None , validation_split : float = 0.0 , validation_data : tp . Union [ tp . Tuple , tp . Iterable , None ] = None , shuffle : bool = True , class_weight : tp . Optional [ tp . Mapping [ str , float ]] = None , sample_weight : tp . Optional [ np . ndarray ] = None , initial_epoch : int = 0 , steps_per_epoch : tp . Optional [ int ] = None , validation_steps : tp . Optional [ int ] = None , validation_batch_size : tp . Optional [ int ] = None , validation_freq : int = 1 , drop_remaining : bool = True , ) -> History : \"\"\" Trains the model for a fixed number of epochs (iterations on a dataset). Arguments: x: Input data. It could be: - A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. - A generator returning `(inputs,)`, `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for generator type is given below. y: Target data. Like the input data `x`, it could be either Numpy or Jax array(s). It should be consistent with `x`. If `x` is a generator, `y` should not be specified (since targets will be obtained from `x`). batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of generator (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire `x` and `y` data provided. Note that in conjunction with `initial_epoch`, `epochs` is to be understood as \"final epoch\". The model is not trained for a number of iterations given by `epochs`, but merely until the epoch of index `epochs` is reached. verbose: 0, 1, 2 or 3. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch 3 = table. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the `x` and `y` data provided, before shuffling. This argument is not supported when `x` is a generator. validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. `validation_data` will override `validation_split`. `validation_data` could be: - tuple `(x_val, y_val)` of Numpy/Jax arrays, list of arrays or mappings - tuple `(x_val, y_val, val_sample_weights)` of Numpy/Jax arrays, list of arrays or mappings - generator For the first two cases, `batch_size` must be provided. For the last case, `validation_steps` should be provided, and should follow the same convention for yielding data as `x`. Note that `validation_data` does not support all the data types that are supported in `x`, eg, dict. shuffle: Boolean (whether to shuffle the training data before each epoch). This argument is ignored when `x` is a generator. Has no effect when `steps_per_epoch` is not `None`. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy/Jax array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples). This argument is not supported when `x` is generator, instead provide the sample_weights as the third element of `x`. initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or `None`. Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input arrays such as jax data arrays, the default `None` is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. When passing a generator, you must specify the `steps_per_epoch` argument. This argument is not supported with array inputs. validation_steps: Only relevant if `validation_data` is provided and is a generator. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the `validation_data` dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or `None`. Number of samples per validation batch. If unspecified, will default to `batch_size`. Do not specify the `validation_batch_size` if your data is in the form of generators (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or `collections_abc.Container` instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. `validation_freq=2` runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. `validation_freq=[1, 2, 10]` runs validation at the end of the 1st, 2nd, and 10th epochs. Unpacking behavior for iterator-like inputs: A common pattern is to pass a generator, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Elegy requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Elegy will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: `namedtuple(\"example_tuple\", [\"y\", \"x\"])` it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])` where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to `x`. As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A `History` object. Its `History.history` attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: ValueError: In case of mismatch between the provided input data and what the model expects. \"\"\" if x is None : x = {} if not self . initialized : self . init ( x = x , y = y , batch_size = batch_size , class_weight = class_weight , sample_weight = sample_weight , ) if validation_split : # Create the validation data using the training data. Only supported for # `Jax Numpy` and `NumPy` input. ( x , y , sample_weight ), validation_data = train_validation_split ( ( x , y , sample_weight ), validation_split = validation_split , shuffle = False ) self . stop_training = False data_handler = DataHandler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps_per_epoch , initial_epoch = initial_epoch , epochs = epochs , shuffle = shuffle , class_weight = class_weight , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , CallbackList ): callbacks = CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = epochs , steps = data_handler . inferred_steps , ) callbacks . on_train_begin () # data_handler._initial_epoch = ( # pylint: disable=protected-access # self._maybe_load_initial_epoch_from_ckpt(initial_epoch)) epoch_logs = {} for epoch , iterator in data_handler . enumerate_epochs (): self . reset_metrics () callbacks . on_epoch_begin ( epoch ) logs = {} with data_handler . catch_stop_iteration (): for step in data_handler . steps (): callbacks . on_train_batch_begin ( step ) batch = next ( iterator ) # sample_weight = batch[2] if len(batch) == 3 else None x_batch , y_batch , sample_weight = unpack_x_y_sample_weight ( batch ) if drop_remaining and not data_utils . has_batch_size ( batch , data_handler . batch_size ): continue tmp_logs = self . train_on_batch ( x = x_batch , y = y_batch , sample_weight = sample_weight , class_weight = class_weight , ) tmp_logs . update ({ \"size\" : data_handler . batch_size }) # print(epoch, step, tmp_logs[\"accuracy\"], batch[0].shape) logs = tmp_logs callbacks . on_train_batch_end ( step , logs ) if self . stop_training : break epoch_logs = copy ( logs ) epoch_logs . update ({ \"size\" : data_handler . batch_size }) # Run validation. if ( validation_data and self . _should_eval ( epoch , validation_freq ) and not self . stop_training ): val_x , val_y , val_sample_weight = unpack_x_y_sample_weight ( validation_data ) try : val_logs = self . evaluate ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps = validation_steps , callbacks = callbacks , # return_dict=True, drop_remaining = drop_remaining , ) val_logs = { \"val_\" + name : val for name , val in val_logs . items ()} epoch_logs . update ( val_logs ) except ( types . MissingMethod , types . MissingModule ) as e : pass callbacks . on_epoch_end ( epoch , epoch_logs ) if self . stop_training : break callbacks . on_train_end ( epoch_logs ) return self . history","title":"fit()"},{"location":"api/Model/#elegy.model.model.Model.load","text":"Loads all weights + states from a folder structure. You can load states from other models that have slightly different architecture as long as long as it preserves the ordering of the haiku.Params + haiku.State structures, adding or removing layers is fine as long as they don't have weights, new layers with weights will be initialized from scratch. Parameters: Name Type Description Default path Union[str, pathlib.Path] path to a saved model's directory. required Source code in elegy/model/model.py def load ( self , path : tp . Union [ str , pathlib . Path ], ) -> None : \"\"\" Loads all weights + states from a folder structure. You can load states from other models that have slightly different architecture as long as long as it preserves the ordering of the `haiku.Params` + `haiku.State` structures, adding or removing layers is fine as long as they don't have weights, new layers with weights will be initialized from scratch. Arguments: path: path to a saved model's directory. \"\"\" if isinstance ( path , str ): path = pathlib . Path ( path ) self . states = cloudpickle . loads (( path / \"states.pkl\" ) . read_bytes ()) self . initial_states = cloudpickle . loads ( ( path / \"initial_states.pkl\" ) . read_bytes () )","title":"load()"},{"location":"api/Model/#elegy.model.model.Model.predict","text":"Generates output predictions for the input samples. Computation is done in batches. Parameters: Name Type Description Default x Optional[Any] Input data. It could be: A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. A generator returning (inputs,) , (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types generator is given in the Unpacking behavior for iterator-like inputs section of Model.fit . None batch_size Optional[int] Integer or None . Number of samples per batch. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of generators (since they generate batches). None verbose int Verbosity mode, 0 or 1. 0 steps Optional[int] Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None . None callbacks Union[List[elegy.callbacks.callback.Callback], elegy.callbacks.callback_list.CallbackList] List of elegy.callbacks.callback.Callback instances. List of callbacks to apply during training. None See the discussion of Unpacking behavior for iterator-like inputs for Model.fit . Note that Model.predict uses the same interpretation rules as Model.fit and Model.evaluate , so inputs must be unambiguous for all three methods. Returns: Type Description Any Numpy array(s) of predictions. Exceptions: Type Description ValueError In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. Source code in elegy/model/model.py def predict ( self , x : tp . Optional [ tp . Any ] = None , verbose : int = 0 , batch_size : tp . Optional [ int ] = None , steps : tp . Optional [ int ] = None , callbacks : tp . Union [ tp . List [ Callback ], CallbackList , None ] = None , drop_remaining : bool = False , initialize : bool = False , ) -> tp . Any : \"\"\"Generates output predictions for the input samples. Computation is done in batches. Arguments: x: Input data. It could be: - A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. - A generator returning `(inputs,)`, `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types generator is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. batch_size: Integer or `None`. Number of samples per batch. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of generators (since they generate batches). verbose: Verbosity mode, 0 or 1. steps: Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of `None`. callbacks: List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. See the discussion of `Unpacking behavior for iterator-like inputs` for [`Model.fit`][elegy.model.model.Model.fit]. Note that Model.predict uses the same interpretation rules as `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all three methods. Returns: Numpy array(s) of predictions. Raises: ValueError: In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. \"\"\" if x is None : x = {} if not self . initialized : if initialize : self . init ( x = x , batch_size = batch_size ) else : raise types . ModelNotInitialized ( f \"Model not initialized, please execute `model.init` or `model.init_on_batch` \" \"before running this method, or pass `initialize=True` to initialize with the available data \" \"(this might not initialize the optimizer).\" ) outputs = None data_handler = DataHandler ( x = x , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , shuffle = False , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , CallbackList ): callbacks = CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps , ) callbacks . on_predict_begin () for _ , iterator in data_handler . enumerate_epochs (): self . reset_metrics () with data_handler . catch_stop_iteration (): for step in data_handler . steps (): callbacks . on_predict_batch_begin ( step ) batch = next ( iterator ) if drop_remaining and not data_utils . has_batch_size ( batch , data_handler . batch_size ): continue tmp_batch_outputs = self . predict_on_batch ( x = batch [ 0 ]) batch_outputs = tmp_batch_outputs if outputs is None : outputs = map_structure ( lambda batch_output : [ batch_output ], batch_outputs ) else : outputs = map_structure ( map_append , outputs , batch_outputs , ) callbacks . on_predict_batch_end ( step , { \"outputs\" : batch_outputs , \"size\" : data_handler . batch_size }, ) callbacks . on_predict_end () all_outputs = map_structure ( jnp . concatenate , outputs ) return all_outputs","title":"predict()"},{"location":"api/Model/#elegy.model.model.Model.predict_on_batch","text":"Returns predictions for a single batch of samples. Parameters: Name Type Description Default x Any Input data. A Numpy/Jax array (or array-like), or possibly nested python structure of dict, list, tuple that contain arrays as leafs. required Returns: Type Description Any Jax array(s) of predictions. Exceptions: Type Description ValueError In case of mismatch between given number of inputs and expectations of the model. Source code in elegy/model/model.py def predict_on_batch ( self , x : tp . Any ) -> tp . Any : \"\"\" Returns predictions for a single batch of samples. Arguments: x: Input data. A Numpy/Jax array (or array-like), or possibly nested python structure of dict, list, tuple that contain arrays as leafs. Returns: Jax array(s) of predictions. Raises: ValueError: In case of mismatch between given number of inputs and expectations of the model. \"\"\" if not self . initialized : raise types . ModelNotInitialized ( f \"Model not initialized, please execute `init` or `init_on_batch` before running this method.\" ) initializing = False training = False method = self . call_pred_step if self . run_eagerly else self . call_pred_step_jit states = self . states . copy () if self . run_eagerly else self . states y_pred , self . states = method ( x , states , initializing , training , ) return y_pred","title":"predict_on_batch()"},{"location":"api/Model/#elegy.model.model.Model.save","text":"Saves the model to disk. It creates a directory that includes: {path}/model.pkl : The Model object instance serialized with pickle , this allows you to re-instantiate the model later. {path}/states.pkl : The Model.states serialized with pickle . {path}/initial_states.pkl : The Model.initial_states serialized with pickle . This allows you to save the entirety of the states of a model in a directory structure which can be fully restored via Model.load if the model is already instiated or elegy.model.load to load the model instance from its pickled version. import elegy model . save( 'my_model' ) # creates folder at 'my_model' del model # deletes the existing model # returns a model identical to the previous one model = elegy . model . load( 'my_model' ) Parameters: Name Type Description Default path Union[str, pathlib.Path] path where model structure will be saved. required Source code in elegy/model/model.py def save ( self , path : tp . Union [ str , pathlib . Path ], ) -> None : \"\"\" Saves the model to disk. It creates a directory that includes: - `{path}/model.pkl`: The `Model` object instance serialized with `pickle`, this allows you to re-instantiate the model later. - `{path}/states.pkl`: The `Model.states` serialized with `pickle`. - `{path}/initial_states.pkl`: The `Model.initial_states` serialized with `pickle`. This allows you to save the entirety of the states of a model in a directory structure which can be fully restored via `Model.load` if the model is already instiated or `elegy.model.load` to load the model instance from its pickled version. ```python import elegy model.save('my_model') # creates folder at 'my_model' del model # deletes the existing model # returns a model identical to the previous one model = elegy.model.load('my_model') ``` Arguments: path: path where model structure will be saved. \"\"\" if isinstance ( path , str ): path = pathlib . Path ( path ) path . mkdir ( parents = True , exist_ok = True ) with open ( path / \"states.pkl\" , \"wb\" ) as f : cloudpickle . dump ( self . states , f ) with open ( path / \"initial_states.pkl\" , \"wb\" ) as f : cloudpickle . dump ( self . initial_states , f ) with open ( path / \"model.pkl\" , \"wb\" ) as f : cloudpickle . dump ( self , f )","title":"save()"},{"location":"api/Model/#elegy.model.model.Model.summary","text":"Prints a summary of the network. Parameters: Name Type Description Default x Optional[Any] A sample of inputs to the network. None depth int The level number of nested level which will be showed. Information about summaries from modules deeper than depth will be aggregated together. 2 tablefmt str A string representing the style of the table generated by tabulate . See python-tabulate for more options. 'fancy_grid' tablulate_kwargs Additional keyword arguments passed to tabulate . See python-tabulate for more options. {} Source code in elegy/model/model.py def summary ( self , x : tp . Optional [ tp . Any ] = None , depth : int = 2 , tablefmt : str = \"fancy_grid\" , return_repr : bool = False , initialize : bool = False , eval_shape : bool = True , ** tablulate_kwargs , ) -> tp . Optional [ str ]: \"\"\" Prints a summary of the network. Arguments: x: A sample of inputs to the network. depth: The level number of nested level which will be showed. Information about summaries from modules deeper than `depth` will be aggregated together. tablefmt: A string representing the style of the table generated by `tabulate`. See [python-tabulate](https://github.com/astanin/python-tabulate) for more options. tablulate_kwargs: Additional keyword arguments passed to `tabulate`. See [python-tabulate](https://github.com/astanin/python-tabulate) for more options. \"\"\" if x is None : x = {} entries : tp . List [ types . SummaryTableEntry ] states = self . states . copy () if self . run_eagerly else self . states method = ( self . call_summary_step if self . run_eagerly else self . call_summary_step_jit ) if eval_shape : entries = jax . eval_shape ( self . call_summary_step , x , states ) else : entries = method ( x , states ) total_entry = entries [ - 1 ] entries = entries [: - 1 ] depth_groups : tp . Dict [ str , tp . List [ types . SummaryTableEntry ]] = toolz . groupby ( lambda entry : \"/\" . join ( entry . path . split ( \"/\" )[: depth ]), entries ) entries = [ utils . get_grouped_entry ( entry , depth_groups ) for entry in entries if entry . path in depth_groups ] main_table = Table ( show_header = True , show_lines = True , show_footer = True , # box=rich.box.HORIZONTALS, ) main_table . add_column ( \"Layer\" ) main_table . add_column ( \"Outputs Shape\" ) main_table . add_column ( \"Trainable \\n Parameters\" ) main_table . add_column ( \"Non-trainable \\n Parameters\" ) rows : tp . List [ tp . List [ str ]] = [] rows . append ([ \"Inputs\" , utils . format_output ( x ), \"\" , \"\" ]) for entry in entries : rows . append ( [ f \" { entry . path } {{ pad }} \" + ( f \"[dim] { entry . module_type_name } [/]\" if entry . module_type_name else \"\" ), utils . format_output ( entry . output_value ), f \"[green] { entry . trainable_params_count : , } [/] {{ pad }} { utils . format_size ( entry . trainable_params_size ) } \" if entry . trainable_params_count > 0 else \"\" , f \"[green] { entry . non_trainable_params_count : , } [/] {{ pad }} { utils . format_size ( entry . non_trainable_params_size ) } \" if entry . non_trainable_params_count > 0 else \"\" , ] ) # global summaries params_count = total_entry . trainable_params_count params_size = total_entry . trainable_params_size states_count = total_entry . non_trainable_params_count states_size = total_entry . non_trainable_params_size total_count = params_count + states_count total_size = params_size + states_size rows . append ( [ \"\" , \"Total\" , ( f \"[green] { params_count : , } [/] {{ pad }} { utils . format_size ( params_size ) } \" if params_count > 0 else \"\" ), ( f \"[green] { states_count : , } [/] {{ pad }} { utils . format_size ( states_size ) } \" if states_count > 0 else \"\" ), ] ) # add padding for col in range ( 4 ): max_length = max ( len ( line . split ( \" {pad} \" )[ 0 ]) for row in rows for line in row [ col ] . split ( \" \\n \" ) ) for row in rows : row [ col ] = \" \\n \" . join ( line . format ( pad = \" \" * ( max_length - len ( line . rstrip () . split ( \" {pad} \" )[ 0 ])) ) for line in row [ col ] . rstrip () . split ( \" \\n \" ) ) for row in rows [: - 1 ]: main_table . add_row ( * row ) main_table . columns [ 1 ] . footer = Text . from_markup ( rows [ - 1 ][ 1 ], justify = \"right\" ) main_table . columns [ 2 ] . footer = rows [ - 1 ][ 2 ] main_table . columns [ 3 ] . footer = rows [ - 1 ][ 3 ] main_table . caption_style = \"bold\" main_table . caption = ( \" \\n Total Parameters: \" + f \"[green] { total_count : , } [/] { utils . format_size ( total_size ) } \" if total_count > 0 else \"\" ) summary = \" \\n \" + utils . get_table_repr ( main_table ) print ( summary ) if return_repr : return summary","title":"summary()"},{"location":"api/Model/#elegy.model.model.Model.test_on_batch","text":"Test the model on a single batch of samples. Parameters: Name Type Description Default x Any Input data. It could be: A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. required y Union[numpy.ndarray, Mapping[str, Any], Tuple] Target data. Like the input data x , it could be either Numpy array(s) or Jax array(s). None sample_weight Optional[numpy.ndarray] Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. None Returns: Type Description Dict[str, Union[numpy.ndarray, float]] A logs dictionary of containing the main loss as well as all other losses and metrics. Exceptions: Type Description ValueError In case of invalid user-provided arguments. Source code in elegy/model/model.py def test_on_batch ( self , x : tp . Any , y : tp . Union [ np . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple , None ] = None , sample_weight : tp . Optional [ np . ndarray ] = None , class_weight : tp . Optional [ np . ndarray ] = None , ) -> types . Logs : \"\"\" Test the model on a single batch of samples. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or Jax array(s). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. Returns: A `logs` dictionary of containing the main `loss` as well as all other losses and metrics. Raises: ValueError: In case of invalid user-provided arguments. \"\"\" self . init_on_batch ( x = x , y_true = y , sample_weight = sample_weight , class_weight = class_weight , ) initializing = False training = False method = self . call_test_step if self . run_eagerly else self . call_test_step_jit states = self . states . copy () if self . run_eagerly else self . states loss , logs , self . states = method ( x , y , sample_weight , class_weight , states , initializing , training , ) return logs","title":"test_on_batch()"},{"location":"api/Model/#elegy.model.model.Model.train_on_batch","text":"Runs a single gradient update on a single batch of data. Parameters: Name Type Description Default x Any Input data. It could be: A Numpy array (or array-like), or a iterable of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. required y Union[numpy.ndarray, Mapping[str, Any], Tuple] Target data. Like the input data x , it could be either Numpy array(s) or Jax array(s). It should be consistent with x (you cannot have Numpy inputs and array targets, or inversely). None sample_weight Optional[numpy.ndarray] Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. None class_weight Optional[Any] Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. None Returns: Type Description Dict[str, Union[numpy.ndarray, float]] A logs dictionary of containing the main loss as well as all other losses and metrics. Exceptions: Type Description ValueError In case of invalid user-provided arguments. Source code in elegy/model/model.py def train_on_batch ( self , x : tp . Any , y : tp . Union [ np . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple , None ] = None , sample_weight : tp . Optional [ np . ndarray ] = None , class_weight : tp . Optional [ tp . Any ] = None , ) -> types . Logs : \"\"\" Runs a single gradient update on a single batch of data. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a iterable of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or Jax array(s). It should be consistent with `x` (you cannot have Numpy inputs and array targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. Returns: A `logs` dictionary of containing the main `loss` as well as all other losses and metrics. Raises: ValueError: In case of invalid user-provided arguments. \"\"\" self . init_on_batch ( x = x , y_true = y , sample_weight = sample_weight , class_weight = class_weight , ) initializing = False training = True method = self . call_train_step if self . run_eagerly else self . call_train_step_jit states = self . states . copy () if self . run_eagerly else self . states logs , self . states = method ( x , y , sample_weight , class_weight , states , initializing , training , ) return logs","title":"train_on_batch()"},{"location":"api/ModelBase/","text":"elegy.ModelBase Model is tasked with performing training, evaluation, and inference for a given elegy.Module or haiku.Module . To create a Model you first have to define its architecture in a Module : >>> import elegy , jax >>> import jax.numpy as jnp >>> class MLP (elegy . Module): ... def call ( self , x: jnp . ndarray) -> jnp . ndarray: ... x = elegy . nn . Flatten()(x) ... x = elegy . nn . Linear( 5 )(x) ... x = jax . nn . relu(x) ... x = elegy . nn . Linear( 2 )(x) ... return x >>> mlp = MLP() >>> x = jnp . ones(shape = [ 10 , 2 ]) >>> y_pred, parameters, collections = mlp . init(rng = elegy . RNGSeq( 42 ))(x) >>> y_pred . shape ( 10 , 2 ) You can pass use Module with the Model API: model = elegy . Model( module = MLP(), loss = [ elegy . losses . SparseCategoricalCrossentropy(from_logits = True ), elegy . regularizers . GlobalL2(l =1e-5 ), ], metrics = elegy . metrics . SparseCategoricalAccuracy(), optimizer = optax . rmsprop( 1e-3 ), ) Once the model is created, you can train the model with model.fit() , or use the model to do prediction with model.predict() . Checkout Getting Started for additional details. Model supports defining + monitoring custom learning rate schedules by passing an instance of elegy.Optimizer instead of an optax object: model = elegy . Model( module = MLP(n1 =3 , n2 =1 ), loss = elegy . losses . SparseCategoricalCrossentropy(from_logits = True ), metrics = elegy . metrics . SparseCategoricalAccuracy(), optimizer = elegy . Optimizer( optax . adam( 1.0 ), # <---- important to set this to 1.0 lr_schedule = lambda step, epoch: 1 / (epoch * 100 + step), steps_per_epoch =1000 , ), run_eagerly = True , ) history = model . fit( ... ) assert \"lr\" in history . history Notice how we set the learning rate parameter of the adam optimizer to 1.0 , this is necessary if you want the logged lr be closer to the \"actual\" learning rate because we implement this feature by chaining an additional optax.scale_by_schedule at the end. evaluate ( self , x = None , y = None , verbose = 1 , batch_size = None , sample_weight = None , class_weight = None , steps = None , callbacks = None , drop_remaining = False ) Returns the loss value & metrics values for the model in test mode. Computation is done in batches. Parameters: Name Type Description Default x Optional[Any] Input data. It could be: A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. A generator returning (inputs,) , (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types generator is given in the Unpacking behavior for iterator-like inputs section of Model.fit . None y Optional[Any] Target data. Like the input data x , it could be either Numpy or Jax array(s). It should be consistent with x . If x is a generator, y should not be specified (since targets will be obtained from x ). None verbose int 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 1 batch_size Optional[int] Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of generator (since they generate batches). None sample_weight Optional[numpy.ndarray] Optional Numpy/Jax array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples). This argument is not supported when x is generator, instead provide the sample_weights as the third element of x . None steps Optional[int] Integer or None . Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of None . This argument is not supported with array inputs. None callbacks Union[List[elegy.callbacks.callback.Callback], elegy.callbacks.callback_list.CallbackList] List of elegy.callbacks.callback.Callback instances. List of callbacks to apply during training. None See the discussion of Unpacking behavior for iterator-like inputs for Model.fit . Returns: Type Description Dict[str, Union[numpy.ndarray, float]] A dictionary for mapping the losses and metrics names to the values obtained. Exceptions: Type Description ValueError in case of invalid arguments. Source code in elegy/model/model_base.py def evaluate ( self , x : tp . Optional [ tp . Any ] = None , y : tp . Optional [ tp . Any ] = None , verbose : int = 1 , batch_size : tp . Optional [ int ] = None , sample_weight : tp . Optional [ np . ndarray ] = None , class_weight : tp . Optional [ tp . Mapping [ str , float ]] = None , steps : tp . Optional [ int ] = None , callbacks : tp . Union [ tp . List [ Callback ], CallbackList , None ] = None , drop_remaining : bool = False , ) -> types . Logs : \"\"\"Returns the loss value & metrics values for the model in test mode. Computation is done in batches. Arguments: x: Input data. It could be: - A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. - A generator returning `(inputs,)`, `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types generator is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. y: Target data. Like the input data `x`, it could be either Numpy or Jax array(s). It should be consistent with `x`. If `x` is a generator, `y` should not be specified (since targets will be obtained from `x`). verbose: 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of generator (since they generate batches). sample_weight: Optional Numpy/Jax array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples). This argument is not supported when `x` is generator, instead provide the sample_weights as the third element of `x`. steps: Integer or `None`. Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of `None`. This argument is not supported with array inputs. callbacks: List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. See the discussion of `Unpacking behavior for iterator-like inputs` for [`Model.fit`][elegy.model.model.Model.fit]. Returns: A dictionary for mapping the losses and metrics names to the values obtained. Raises: ValueError: in case of invalid arguments. \"\"\" if x is None : x = {} if not self . initialized : self . init ( x = x , y = y , batch_size = batch_size , class_weight = class_weight , sample_weight = sample_weight , ) data_handler = DataHandler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , shuffle = False , training = False , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , CallbackList ): callbacks = CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps , ) callbacks . on_test_begin () logs = {} for _ , iterator in data_handler . enumerate_epochs (): self . reset_metrics () with data_handler . catch_stop_iteration (): for step in data_handler . steps (): callbacks . on_test_batch_begin ( step ) batch = next ( iterator ) x_batch , y_batch , sample_weight = unpack_x_y_sample_weight ( batch ) if drop_remaining and not data_utils . has_batch_size ( batch , data_handler . batch_size ): continue tmp_logs = self . test_on_batch ( x = x_batch , y = y_batch , sample_weight = sample_weight , class_weight = None , ) tmp_logs . update ({ \"size\" : data_handler . batch_size }) logs = tmp_logs callbacks . on_test_batch_end ( step , logs ) callbacks . on_test_end () return logs fit ( self , x = None , y = None , batch_size = None , epochs = 1 , verbose = 1 , callbacks = None , validation_split = 0.0 , validation_data = None , shuffle = True , class_weight = None , sample_weight = None , initial_epoch = 0 , steps_per_epoch = None , validation_steps = None , validation_batch_size = None , validation_freq = 1 , drop_remaining = True ) Trains the model for a fixed number of epochs (iterations on a dataset). Parameters: Name Type Description Default x Optional[Any] Input data. It could be: A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. A generator returning (inputs,) , (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for generator type is given below. None y Optional[Any] Target data. Like the input data x , it could be either Numpy or Jax array(s). It should be consistent with x . If x is a generator, y should not be specified (since targets will be obtained from x ). None batch_size Optional[int] Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of generator (since they generate batches). None epochs int Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided. Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. 1 verbose int 0, 1, 2 or 3. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch 3 = table. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). 1 callbacks Union[List[elegy.callbacks.callback.Callback], elegy.callbacks.callback_list.CallbackList] List of elegy.callbacks.callback.Callback instances. List of callbacks to apply during training. None validation_split float Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a generator. 0.0 validation_data Union[Tuple, Iterable] Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . validation_data could be: tuple (x_val, y_val) of Numpy/Jax arrays, list of arrays or mappings tuple (x_val, y_val, val_sample_weights) of Numpy/Jax arrays, list of arrays or mappings generator For the first two cases, batch_size must be provided. For the last case, validation_steps should be provided, and should follow the same convention for yielding data as x . Note that validation_data does not support all the data types that are supported in x , eg, dict. None shuffle bool Boolean (whether to shuffle the training data before each epoch). This argument is ignored when x is a generator. Has no effect when steps_per_epoch is not None . True class_weight Optional[Mapping[str, float]] Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. None sample_weight Optional[numpy.ndarray] Optional Numpy/Jax array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples). This argument is not supported when x is generator, instead provide the sample_weights as the third element of x . None initial_epoch int Integer. Epoch at which to start training (useful for resuming a previous training run). 0 steps_per_epoch Optional[int] Integer or None . Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input arrays such as jax data arrays, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. When passing a generator, you must specify the steps_per_epoch argument. This argument is not supported with array inputs. None validation_steps Optional[int] Only relevant if validation_data is provided and is a generator. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the validation_data dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. None validation_batch_size Optional[int] Integer or None . Number of samples per validation batch. If unspecified, will default to batch_size . Do not specify the validation_batch_size if your data is in the form of generators (since they generate batches). None validation_freq int Only relevant if validation data is provided. Integer or collections_abc.Container instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. 1 Unpacking behavior for iterator-like inputs: A common pattern is to pass a generator, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Elegy requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. ({\"x0\": x0, \"x1\": x1}, y) . Elegy will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: namedtuple(\"example_tuple\", [\"y\", \"x\"]) it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"]) where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to x . As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: Type Description History A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Exceptions: Type Description ValueError In case of mismatch between the provided input data and what the model expects. Source code in elegy/model/model_base.py def fit ( self , x : tp . Optional [ tp . Any ] = None , y : tp . Optional [ tp . Any ] = None , batch_size : tp . Optional [ int ] = None , epochs : int = 1 , verbose : int = 1 , callbacks : tp . Union [ tp . List [ Callback ], CallbackList , None ] = None , validation_split : float = 0.0 , validation_data : tp . Union [ tp . Tuple , tp . Iterable , None ] = None , shuffle : bool = True , class_weight : tp . Optional [ tp . Mapping [ str , float ]] = None , sample_weight : tp . Optional [ np . ndarray ] = None , initial_epoch : int = 0 , steps_per_epoch : tp . Optional [ int ] = None , validation_steps : tp . Optional [ int ] = None , validation_batch_size : tp . Optional [ int ] = None , validation_freq : int = 1 , drop_remaining : bool = True , ) -> History : \"\"\" Trains the model for a fixed number of epochs (iterations on a dataset). Arguments: x: Input data. It could be: - A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. - A generator returning `(inputs,)`, `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for generator type is given below. y: Target data. Like the input data `x`, it could be either Numpy or Jax array(s). It should be consistent with `x`. If `x` is a generator, `y` should not be specified (since targets will be obtained from `x`). batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of generator (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire `x` and `y` data provided. Note that in conjunction with `initial_epoch`, `epochs` is to be understood as \"final epoch\". The model is not trained for a number of iterations given by `epochs`, but merely until the epoch of index `epochs` is reached. verbose: 0, 1, 2 or 3. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch 3 = table. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the `x` and `y` data provided, before shuffling. This argument is not supported when `x` is a generator. validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. `validation_data` will override `validation_split`. `validation_data` could be: - tuple `(x_val, y_val)` of Numpy/Jax arrays, list of arrays or mappings - tuple `(x_val, y_val, val_sample_weights)` of Numpy/Jax arrays, list of arrays or mappings - generator For the first two cases, `batch_size` must be provided. For the last case, `validation_steps` should be provided, and should follow the same convention for yielding data as `x`. Note that `validation_data` does not support all the data types that are supported in `x`, eg, dict. shuffle: Boolean (whether to shuffle the training data before each epoch). This argument is ignored when `x` is a generator. Has no effect when `steps_per_epoch` is not `None`. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy/Jax array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples). This argument is not supported when `x` is generator, instead provide the sample_weights as the third element of `x`. initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or `None`. Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input arrays such as jax data arrays, the default `None` is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. When passing a generator, you must specify the `steps_per_epoch` argument. This argument is not supported with array inputs. validation_steps: Only relevant if `validation_data` is provided and is a generator. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the `validation_data` dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or `None`. Number of samples per validation batch. If unspecified, will default to `batch_size`. Do not specify the `validation_batch_size` if your data is in the form of generators (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or `collections_abc.Container` instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. `validation_freq=2` runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. `validation_freq=[1, 2, 10]` runs validation at the end of the 1st, 2nd, and 10th epochs. Unpacking behavior for iterator-like inputs: A common pattern is to pass a generator, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Elegy requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Elegy will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: `namedtuple(\"example_tuple\", [\"y\", \"x\"])` it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])` where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to `x`. As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A `History` object. Its `History.history` attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: ValueError: In case of mismatch between the provided input data and what the model expects. \"\"\" if x is None : x = {} if not self . initialized : self . init ( x = x , y = y , batch_size = batch_size , class_weight = class_weight , sample_weight = sample_weight , ) if validation_split : # Create the validation data using the training data. Only supported for # `Jax Numpy` and `NumPy` input. ( x , y , sample_weight ), validation_data = train_validation_split ( ( x , y , sample_weight ), validation_split = validation_split , shuffle = False ) self . stop_training = False data_handler = DataHandler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps_per_epoch , initial_epoch = initial_epoch , epochs = epochs , shuffle = shuffle , class_weight = class_weight , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , CallbackList ): callbacks = CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = epochs , steps = data_handler . inferred_steps , ) callbacks . on_train_begin () # data_handler._initial_epoch = ( # pylint: disable=protected-access # self._maybe_load_initial_epoch_from_ckpt(initial_epoch)) epoch_logs = {} for epoch , iterator in data_handler . enumerate_epochs (): self . reset_metrics () callbacks . on_epoch_begin ( epoch ) logs = {} with data_handler . catch_stop_iteration (): for step in data_handler . steps (): callbacks . on_train_batch_begin ( step ) batch = next ( iterator ) # sample_weight = batch[2] if len(batch) == 3 else None x_batch , y_batch , sample_weight = unpack_x_y_sample_weight ( batch ) if drop_remaining and not data_utils . has_batch_size ( batch , data_handler . batch_size ): continue tmp_logs = self . train_on_batch ( x = x_batch , y = y_batch , sample_weight = sample_weight , class_weight = class_weight , ) tmp_logs . update ({ \"size\" : data_handler . batch_size }) # print(epoch, step, tmp_logs[\"accuracy\"], batch[0].shape) logs = tmp_logs callbacks . on_train_batch_end ( step , logs ) if self . stop_training : break epoch_logs = copy ( logs ) epoch_logs . update ({ \"size\" : data_handler . batch_size }) # Run validation. if ( validation_data and self . _should_eval ( epoch , validation_freq ) and not self . stop_training ): val_x , val_y , val_sample_weight = unpack_x_y_sample_weight ( validation_data ) try : val_logs = self . evaluate ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps = validation_steps , callbacks = callbacks , # return_dict=True, drop_remaining = drop_remaining , ) val_logs = { \"val_\" + name : val for name , val in val_logs . items ()} epoch_logs . update ( val_logs ) except ( types . MissingMethod , types . MissingModule ) as e : pass callbacks . on_epoch_end ( epoch , epoch_logs ) if self . stop_training : break callbacks . on_train_end ( epoch_logs ) return self . history load ( self , path ) inherited Loads all weights + states from a folder structure. You can load states from other models that have slightly different architecture as long as long as it preserves the ordering of the haiku.Params + haiku.State structures, adding or removing layers is fine as long as they don't have weights, new layers with weights will be initialized from scratch. Parameters: Name Type Description Default path Union[str, pathlib.Path] path to a saved model's directory. required Source code in elegy/model/model_base.py def load ( self , path : tp . Union [ str , pathlib . Path ], ) -> None : \"\"\" Loads all weights + states from a folder structure. You can load states from other models that have slightly different architecture as long as long as it preserves the ordering of the `haiku.Params` + `haiku.State` structures, adding or removing layers is fine as long as they don't have weights, new layers with weights will be initialized from scratch. Arguments: path: path to a saved model's directory. \"\"\" if isinstance ( path , str ): path = pathlib . Path ( path ) self . states = cloudpickle . loads (( path / \"states.pkl\" ) . read_bytes ()) self . initial_states = cloudpickle . loads ( ( path / \"initial_states.pkl\" ) . read_bytes () ) predict ( self , x = None , verbose = 0 , batch_size = None , steps = None , callbacks = None , drop_remaining = False , initialize = False ) Generates output predictions for the input samples. Computation is done in batches. Parameters: Name Type Description Default x Optional[Any] Input data. It could be: A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. A generator returning (inputs,) , (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types generator is given in the Unpacking behavior for iterator-like inputs section of Model.fit . None batch_size Optional[int] Integer or None . Number of samples per batch. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of generators (since they generate batches). None verbose int Verbosity mode, 0 or 1. 0 steps Optional[int] Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None . None callbacks Union[List[elegy.callbacks.callback.Callback], elegy.callbacks.callback_list.CallbackList] List of elegy.callbacks.callback.Callback instances. List of callbacks to apply during training. None See the discussion of Unpacking behavior for iterator-like inputs for Model.fit . Note that Model.predict uses the same interpretation rules as Model.fit and Model.evaluate , so inputs must be unambiguous for all three methods. Returns: Type Description Any Numpy array(s) of predictions. Exceptions: Type Description ValueError In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. Source code in elegy/model/model_base.py def predict ( self , x : tp . Optional [ tp . Any ] = None , verbose : int = 0 , batch_size : tp . Optional [ int ] = None , steps : tp . Optional [ int ] = None , callbacks : tp . Union [ tp . List [ Callback ], CallbackList , None ] = None , drop_remaining : bool = False , initialize : bool = False , ) -> tp . Any : \"\"\"Generates output predictions for the input samples. Computation is done in batches. Arguments: x: Input data. It could be: - A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. - A generator returning `(inputs,)`, `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types generator is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. batch_size: Integer or `None`. Number of samples per batch. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of generators (since they generate batches). verbose: Verbosity mode, 0 or 1. steps: Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of `None`. callbacks: List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. See the discussion of `Unpacking behavior for iterator-like inputs` for [`Model.fit`][elegy.model.model.Model.fit]. Note that Model.predict uses the same interpretation rules as `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all three methods. Returns: Numpy array(s) of predictions. Raises: ValueError: In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. \"\"\" if x is None : x = {} if not self . initialized : if initialize : self . init ( x = x , batch_size = batch_size ) else : raise types . ModelNotInitialized ( f \"Model not initialized, please execute `model.init` or `model.init_on_batch` \" \"before running this method, or pass `initialize=True` to initialize with the available data \" \"(this might not initialize the optimizer).\" ) outputs = None data_handler = DataHandler ( x = x , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , shuffle = False , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , CallbackList ): callbacks = CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps , ) callbacks . on_predict_begin () for _ , iterator in data_handler . enumerate_epochs (): self . reset_metrics () with data_handler . catch_stop_iteration (): for step in data_handler . steps (): callbacks . on_predict_batch_begin ( step ) batch = next ( iterator ) if drop_remaining and not data_utils . has_batch_size ( batch , data_handler . batch_size ): continue tmp_batch_outputs = self . predict_on_batch ( x = batch [ 0 ]) batch_outputs = tmp_batch_outputs if outputs is None : outputs = map_structure ( lambda batch_output : [ batch_output ], batch_outputs ) else : outputs = map_structure ( map_append , outputs , batch_outputs , ) callbacks . on_predict_batch_end ( step , { \"outputs\" : batch_outputs , \"size\" : data_handler . batch_size }, ) callbacks . on_predict_end () all_outputs = map_structure ( jnp . concatenate , outputs ) return all_outputs predict_on_batch ( self , x ) inherited Returns predictions for a single batch of samples. Parameters: Name Type Description Default x Any Input data. A Numpy/Jax array (or array-like), or possibly nested python structure of dict, list, tuple that contain arrays as leafs. required Returns: Type Description Any Jax array(s) of predictions. Exceptions: Type Description ValueError In case of mismatch between given number of inputs and expectations of the model. Source code in elegy/model/model_base.py def predict_on_batch ( self , x : tp . Any ) -> tp . Any : \"\"\" Returns predictions for a single batch of samples. Arguments: x: Input data. A Numpy/Jax array (or array-like), or possibly nested python structure of dict, list, tuple that contain arrays as leafs. Returns: Jax array(s) of predictions. Raises: ValueError: In case of mismatch between given number of inputs and expectations of the model. \"\"\" if not self . initialized : raise types . ModelNotInitialized ( f \"Model not initialized, please execute `init` or `init_on_batch` before running this method.\" ) initializing = False training = False method = self . call_pred_step if self . run_eagerly else self . call_pred_step_jit states = self . states . copy () if self . run_eagerly else self . states y_pred , self . states = method ( x , states , initializing , training , ) return y_pred save ( self , path ) inherited Saves the model to disk. It creates a directory that includes: {path}/model.pkl : The Model object instance serialized with pickle , this allows you to re-instantiate the model later. {path}/states.pkl : The Model.states serialized with pickle . {path}/initial_states.pkl : The Model.initial_states serialized with pickle . This allows you to save the entirety of the states of a model in a directory structure which can be fully restored via Model.load if the model is already instiated or elegy.model.load to load the model instance from its pickled version. import elegy model . save( 'my_model' ) # creates folder at 'my_model' del model # deletes the existing model # returns a model identical to the previous one model = elegy . model . load( 'my_model' ) Parameters: Name Type Description Default path Union[str, pathlib.Path] path where model structure will be saved. required Source code in elegy/model/model_base.py def save ( self , path : tp . Union [ str , pathlib . Path ], ) -> None : \"\"\" Saves the model to disk. It creates a directory that includes: - `{path}/model.pkl`: The `Model` object instance serialized with `pickle`, this allows you to re-instantiate the model later. - `{path}/states.pkl`: The `Model.states` serialized with `pickle`. - `{path}/initial_states.pkl`: The `Model.initial_states` serialized with `pickle`. This allows you to save the entirety of the states of a model in a directory structure which can be fully restored via `Model.load` if the model is already instiated or `elegy.model.load` to load the model instance from its pickled version. ```python import elegy model.save('my_model') # creates folder at 'my_model' del model # deletes the existing model # returns a model identical to the previous one model = elegy.model.load('my_model') ``` Arguments: path: path where model structure will be saved. \"\"\" if isinstance ( path , str ): path = pathlib . Path ( path ) path . mkdir ( parents = True , exist_ok = True ) with open ( path / \"states.pkl\" , \"wb\" ) as f : cloudpickle . dump ( self . states , f ) with open ( path / \"initial_states.pkl\" , \"wb\" ) as f : cloudpickle . dump ( self . initial_states , f ) with open ( path / \"model.pkl\" , \"wb\" ) as f : cloudpickle . dump ( self , f ) summary ( self , x = None , depth = 2 , tablefmt = 'fancy_grid' , return_repr = False , initialize = False , eval_shape = True , ** tablulate_kwargs ) Prints a summary of the network. Parameters: Name Type Description Default x Optional[Any] A sample of inputs to the network. None depth int The level number of nested level which will be showed. Information about summaries from modules deeper than depth will be aggregated together. 2 tablefmt str A string representing the style of the table generated by tabulate . See python-tabulate for more options. 'fancy_grid' tablulate_kwargs Additional keyword arguments passed to tabulate . See python-tabulate for more options. {} Source code in elegy/model/model_base.py def summary ( self , x : tp . Optional [ tp . Any ] = None , depth : int = 2 , tablefmt : str = \"fancy_grid\" , return_repr : bool = False , initialize : bool = False , eval_shape : bool = True , ** tablulate_kwargs , ) -> tp . Optional [ str ]: \"\"\" Prints a summary of the network. Arguments: x: A sample of inputs to the network. depth: The level number of nested level which will be showed. Information about summaries from modules deeper than `depth` will be aggregated together. tablefmt: A string representing the style of the table generated by `tabulate`. See [python-tabulate](https://github.com/astanin/python-tabulate) for more options. tablulate_kwargs: Additional keyword arguments passed to `tabulate`. See [python-tabulate](https://github.com/astanin/python-tabulate) for more options. \"\"\" if x is None : x = {} entries : tp . List [ types . SummaryTableEntry ] states = self . states . copy () if self . run_eagerly else self . states method = ( self . call_summary_step if self . run_eagerly else self . call_summary_step_jit ) if eval_shape : entries = jax . eval_shape ( self . call_summary_step , x , states ) else : entries = method ( x , states ) total_entry = entries [ - 1 ] entries = entries [: - 1 ] depth_groups : tp . Dict [ str , tp . List [ types . SummaryTableEntry ]] = toolz . groupby ( lambda entry : \"/\" . join ( entry . path . split ( \"/\" )[: depth ]), entries ) entries = [ utils . get_grouped_entry ( entry , depth_groups ) for entry in entries if entry . path in depth_groups ] main_table = Table ( show_header = True , show_lines = True , show_footer = True , # box=rich.box.HORIZONTALS, ) main_table . add_column ( \"Layer\" ) main_table . add_column ( \"Outputs Shape\" ) main_table . add_column ( \"Trainable \\n Parameters\" ) main_table . add_column ( \"Non-trainable \\n Parameters\" ) rows : tp . List [ tp . List [ str ]] = [] rows . append ([ \"Inputs\" , utils . format_output ( x ), \"\" , \"\" ]) for entry in entries : rows . append ( [ f \" { entry . path } {{ pad }} \" + ( f \"[dim] { entry . module_type_name } [/]\" if entry . module_type_name else \"\" ), utils . format_output ( entry . output_value ), f \"[green] { entry . trainable_params_count : , } [/] {{ pad }} { utils . format_size ( entry . trainable_params_size ) } \" if entry . trainable_params_count > 0 else \"\" , f \"[green] { entry . non_trainable_params_count : , } [/] {{ pad }} { utils . format_size ( entry . non_trainable_params_size ) } \" if entry . non_trainable_params_count > 0 else \"\" , ] ) # global summaries params_count = total_entry . trainable_params_count params_size = total_entry . trainable_params_size states_count = total_entry . non_trainable_params_count states_size = total_entry . non_trainable_params_size total_count = params_count + states_count total_size = params_size + states_size rows . append ( [ \"\" , \"Total\" , ( f \"[green] { params_count : , } [/] {{ pad }} { utils . format_size ( params_size ) } \" if params_count > 0 else \"\" ), ( f \"[green] { states_count : , } [/] {{ pad }} { utils . format_size ( states_size ) } \" if states_count > 0 else \"\" ), ] ) # add padding for col in range ( 4 ): max_length = max ( len ( line . split ( \" {pad} \" )[ 0 ]) for row in rows for line in row [ col ] . split ( \" \\n \" ) ) for row in rows : row [ col ] = \" \\n \" . join ( line . format ( pad = \" \" * ( max_length - len ( line . rstrip () . split ( \" {pad} \" )[ 0 ])) ) for line in row [ col ] . rstrip () . split ( \" \\n \" ) ) for row in rows [: - 1 ]: main_table . add_row ( * row ) main_table . columns [ 1 ] . footer = Text . from_markup ( rows [ - 1 ][ 1 ], justify = \"right\" ) main_table . columns [ 2 ] . footer = rows [ - 1 ][ 2 ] main_table . columns [ 3 ] . footer = rows [ - 1 ][ 3 ] main_table . caption_style = \"bold\" main_table . caption = ( \" \\n Total Parameters: \" + f \"[green] { total_count : , } [/] { utils . format_size ( total_size ) } \" if total_count > 0 else \"\" ) summary = \" \\n \" + utils . get_table_repr ( main_table ) print ( summary ) if return_repr : return summary test_on_batch ( self , x , y = None , sample_weight = None , class_weight = None ) inherited Test the model on a single batch of samples. Parameters: Name Type Description Default x Any Input data. It could be: A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. required y Union[numpy.ndarray, Mapping[str, Any], Tuple] Target data. Like the input data x , it could be either Numpy array(s) or Jax array(s). None sample_weight Optional[numpy.ndarray] Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. None Returns: Type Description Dict[str, Union[numpy.ndarray, float]] A logs dictionary of containing the main loss as well as all other losses and metrics. Exceptions: Type Description ValueError In case of invalid user-provided arguments. Source code in elegy/model/model_base.py def test_on_batch ( self , x : tp . Any , y : tp . Union [ np . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple , None ] = None , sample_weight : tp . Optional [ np . ndarray ] = None , class_weight : tp . Optional [ np . ndarray ] = None , ) -> types . Logs : \"\"\" Test the model on a single batch of samples. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or Jax array(s). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. Returns: A `logs` dictionary of containing the main `loss` as well as all other losses and metrics. Raises: ValueError: In case of invalid user-provided arguments. \"\"\" self . init_on_batch ( x = x , y_true = y , sample_weight = sample_weight , class_weight = class_weight , ) initializing = False training = False method = self . call_test_step if self . run_eagerly else self . call_test_step_jit states = self . states . copy () if self . run_eagerly else self . states loss , logs , self . states = method ( x , y , sample_weight , class_weight , states , initializing , training , ) return logs train_on_batch ( self , x , y = None , sample_weight = None , class_weight = None ) inherited Runs a single gradient update on a single batch of data. Parameters: Name Type Description Default x Any Input data. It could be: A Numpy array (or array-like), or a iterable of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. required y Union[numpy.ndarray, Mapping[str, Any], Tuple] Target data. Like the input data x , it could be either Numpy array(s) or Jax array(s). It should be consistent with x (you cannot have Numpy inputs and array targets, or inversely). None sample_weight Optional[numpy.ndarray] Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. None class_weight Optional[Any] Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. None Returns: Type Description Dict[str, Union[numpy.ndarray, float]] A logs dictionary of containing the main loss as well as all other losses and metrics. Exceptions: Type Description ValueError In case of invalid user-provided arguments. Source code in elegy/model/model_base.py def train_on_batch ( self , x : tp . Any , y : tp . Union [ np . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple , None ] = None , sample_weight : tp . Optional [ np . ndarray ] = None , class_weight : tp . Optional [ tp . Any ] = None , ) -> types . Logs : \"\"\" Runs a single gradient update on a single batch of data. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a iterable of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or Jax array(s). It should be consistent with `x` (you cannot have Numpy inputs and array targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. Returns: A `logs` dictionary of containing the main `loss` as well as all other losses and metrics. Raises: ValueError: In case of invalid user-provided arguments. \"\"\" self . init_on_batch ( x = x , y_true = y , sample_weight = sample_weight , class_weight = class_weight , ) initializing = False training = True method = self . call_train_step if self . run_eagerly else self . call_train_step_jit states = self . states . copy () if self . run_eagerly else self . states logs , self . states = method ( x , y , sample_weight , class_weight , states , initializing , training , ) return logs","title":"ModelBase"},{"location":"api/ModelBase/#elegymodelbase","text":"","title":"elegy.ModelBase"},{"location":"api/ModelBase/#elegy.model.model_base.ModelBase","text":"Model is tasked with performing training, evaluation, and inference for a given elegy.Module or haiku.Module . To create a Model you first have to define its architecture in a Module : >>> import elegy , jax >>> import jax.numpy as jnp >>> class MLP (elegy . Module): ... def call ( self , x: jnp . ndarray) -> jnp . ndarray: ... x = elegy . nn . Flatten()(x) ... x = elegy . nn . Linear( 5 )(x) ... x = jax . nn . relu(x) ... x = elegy . nn . Linear( 2 )(x) ... return x >>> mlp = MLP() >>> x = jnp . ones(shape = [ 10 , 2 ]) >>> y_pred, parameters, collections = mlp . init(rng = elegy . RNGSeq( 42 ))(x) >>> y_pred . shape ( 10 , 2 ) You can pass use Module with the Model API: model = elegy . Model( module = MLP(), loss = [ elegy . losses . SparseCategoricalCrossentropy(from_logits = True ), elegy . regularizers . GlobalL2(l =1e-5 ), ], metrics = elegy . metrics . SparseCategoricalAccuracy(), optimizer = optax . rmsprop( 1e-3 ), ) Once the model is created, you can train the model with model.fit() , or use the model to do prediction with model.predict() . Checkout Getting Started for additional details. Model supports defining + monitoring custom learning rate schedules by passing an instance of elegy.Optimizer instead of an optax object: model = elegy . Model( module = MLP(n1 =3 , n2 =1 ), loss = elegy . losses . SparseCategoricalCrossentropy(from_logits = True ), metrics = elegy . metrics . SparseCategoricalAccuracy(), optimizer = elegy . Optimizer( optax . adam( 1.0 ), # <---- important to set this to 1.0 lr_schedule = lambda step, epoch: 1 / (epoch * 100 + step), steps_per_epoch =1000 , ), run_eagerly = True , ) history = model . fit( ... ) assert \"lr\" in history . history Notice how we set the learning rate parameter of the adam optimizer to 1.0 , this is necessary if you want the logged lr be closer to the \"actual\" learning rate because we implement this feature by chaining an additional optax.scale_by_schedule at the end.","title":"elegy.model.model_base.ModelBase"},{"location":"api/ModelBase/#elegy.model.model_base.ModelBase.evaluate","text":"Returns the loss value & metrics values for the model in test mode. Computation is done in batches. Parameters: Name Type Description Default x Optional[Any] Input data. It could be: A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. A generator returning (inputs,) , (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types generator is given in the Unpacking behavior for iterator-like inputs section of Model.fit . None y Optional[Any] Target data. Like the input data x , it could be either Numpy or Jax array(s). It should be consistent with x . If x is a generator, y should not be specified (since targets will be obtained from x ). None verbose int 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 1 batch_size Optional[int] Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of generator (since they generate batches). None sample_weight Optional[numpy.ndarray] Optional Numpy/Jax array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples). This argument is not supported when x is generator, instead provide the sample_weights as the third element of x . None steps Optional[int] Integer or None . Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of None . This argument is not supported with array inputs. None callbacks Union[List[elegy.callbacks.callback.Callback], elegy.callbacks.callback_list.CallbackList] List of elegy.callbacks.callback.Callback instances. List of callbacks to apply during training. None See the discussion of Unpacking behavior for iterator-like inputs for Model.fit . Returns: Type Description Dict[str, Union[numpy.ndarray, float]] A dictionary for mapping the losses and metrics names to the values obtained. Exceptions: Type Description ValueError in case of invalid arguments. Source code in elegy/model/model_base.py def evaluate ( self , x : tp . Optional [ tp . Any ] = None , y : tp . Optional [ tp . Any ] = None , verbose : int = 1 , batch_size : tp . Optional [ int ] = None , sample_weight : tp . Optional [ np . ndarray ] = None , class_weight : tp . Optional [ tp . Mapping [ str , float ]] = None , steps : tp . Optional [ int ] = None , callbacks : tp . Union [ tp . List [ Callback ], CallbackList , None ] = None , drop_remaining : bool = False , ) -> types . Logs : \"\"\"Returns the loss value & metrics values for the model in test mode. Computation is done in batches. Arguments: x: Input data. It could be: - A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. - A generator returning `(inputs,)`, `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types generator is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. y: Target data. Like the input data `x`, it could be either Numpy or Jax array(s). It should be consistent with `x`. If `x` is a generator, `y` should not be specified (since targets will be obtained from `x`). verbose: 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of generator (since they generate batches). sample_weight: Optional Numpy/Jax array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples). This argument is not supported when `x` is generator, instead provide the sample_weights as the third element of `x`. steps: Integer or `None`. Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of `None`. This argument is not supported with array inputs. callbacks: List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. See the discussion of `Unpacking behavior for iterator-like inputs` for [`Model.fit`][elegy.model.model.Model.fit]. Returns: A dictionary for mapping the losses and metrics names to the values obtained. Raises: ValueError: in case of invalid arguments. \"\"\" if x is None : x = {} if not self . initialized : self . init ( x = x , y = y , batch_size = batch_size , class_weight = class_weight , sample_weight = sample_weight , ) data_handler = DataHandler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , shuffle = False , training = False , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , CallbackList ): callbacks = CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps , ) callbacks . on_test_begin () logs = {} for _ , iterator in data_handler . enumerate_epochs (): self . reset_metrics () with data_handler . catch_stop_iteration (): for step in data_handler . steps (): callbacks . on_test_batch_begin ( step ) batch = next ( iterator ) x_batch , y_batch , sample_weight = unpack_x_y_sample_weight ( batch ) if drop_remaining and not data_utils . has_batch_size ( batch , data_handler . batch_size ): continue tmp_logs = self . test_on_batch ( x = x_batch , y = y_batch , sample_weight = sample_weight , class_weight = None , ) tmp_logs . update ({ \"size\" : data_handler . batch_size }) logs = tmp_logs callbacks . on_test_batch_end ( step , logs ) callbacks . on_test_end () return logs","title":"evaluate()"},{"location":"api/ModelBase/#elegy.model.model_base.ModelBase.fit","text":"Trains the model for a fixed number of epochs (iterations on a dataset). Parameters: Name Type Description Default x Optional[Any] Input data. It could be: A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. A generator returning (inputs,) , (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for generator type is given below. None y Optional[Any] Target data. Like the input data x , it could be either Numpy or Jax array(s). It should be consistent with x . If x is a generator, y should not be specified (since targets will be obtained from x ). None batch_size Optional[int] Integer or None . Number of samples per gradient update. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of generator (since they generate batches). None epochs int Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided. Note that in conjunction with initial_epoch , epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs , but merely until the epoch of index epochs is reached. 1 verbose int 0, 1, 2 or 3. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch 3 = table. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). 1 callbacks Union[List[elegy.callbacks.callback.Callback], elegy.callbacks.callback_list.CallbackList] List of elegy.callbacks.callback.Callback instances. List of callbacks to apply during training. None validation_split float Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a generator. 0.0 validation_data Union[Tuple, Iterable] Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . validation_data could be: tuple (x_val, y_val) of Numpy/Jax arrays, list of arrays or mappings tuple (x_val, y_val, val_sample_weights) of Numpy/Jax arrays, list of arrays or mappings generator For the first two cases, batch_size must be provided. For the last case, validation_steps should be provided, and should follow the same convention for yielding data as x . Note that validation_data does not support all the data types that are supported in x , eg, dict. None shuffle bool Boolean (whether to shuffle the training data before each epoch). This argument is ignored when x is a generator. Has no effect when steps_per_epoch is not None . True class_weight Optional[Mapping[str, float]] Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. None sample_weight Optional[numpy.ndarray] Optional Numpy/Jax array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples). This argument is not supported when x is generator, instead provide the sample_weights as the third element of x . None initial_epoch int Integer. Epoch at which to start training (useful for resuming a previous training run). 0 steps_per_epoch Optional[int] Integer or None . Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input arrays such as jax data arrays, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. When passing a generator, you must specify the steps_per_epoch argument. This argument is not supported with array inputs. None validation_steps Optional[int] Only relevant if validation_data is provided and is a generator. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the validation_data dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. None validation_batch_size Optional[int] Integer or None . Number of samples per validation batch. If unspecified, will default to batch_size . Do not specify the validation_batch_size if your data is in the form of generators (since they generate batches). None validation_freq int Only relevant if validation data is provided. Integer or collections_abc.Container instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. validation_freq=2 runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. validation_freq=[1, 2, 10] runs validation at the end of the 1st, 2nd, and 10th epochs. 1 Unpacking behavior for iterator-like inputs: A common pattern is to pass a generator, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Elegy requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. ({\"x0\": x0, \"x1\": x1}, y) . Elegy will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: namedtuple(\"example_tuple\", [\"y\", \"x\"]) it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"]) where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to x . As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: Type Description History A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Exceptions: Type Description ValueError In case of mismatch between the provided input data and what the model expects. Source code in elegy/model/model_base.py def fit ( self , x : tp . Optional [ tp . Any ] = None , y : tp . Optional [ tp . Any ] = None , batch_size : tp . Optional [ int ] = None , epochs : int = 1 , verbose : int = 1 , callbacks : tp . Union [ tp . List [ Callback ], CallbackList , None ] = None , validation_split : float = 0.0 , validation_data : tp . Union [ tp . Tuple , tp . Iterable , None ] = None , shuffle : bool = True , class_weight : tp . Optional [ tp . Mapping [ str , float ]] = None , sample_weight : tp . Optional [ np . ndarray ] = None , initial_epoch : int = 0 , steps_per_epoch : tp . Optional [ int ] = None , validation_steps : tp . Optional [ int ] = None , validation_batch_size : tp . Optional [ int ] = None , validation_freq : int = 1 , drop_remaining : bool = True , ) -> History : \"\"\" Trains the model for a fixed number of epochs (iterations on a dataset). Arguments: x: Input data. It could be: - A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. - A generator returning `(inputs,)`, `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for generator type is given below. y: Target data. Like the input data `x`, it could be either Numpy or Jax array(s). It should be consistent with `x`. If `x` is a generator, `y` should not be specified (since targets will be obtained from `x`). batch_size: Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of generator (since they generate batches). epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire `x` and `y` data provided. Note that in conjunction with `initial_epoch`, `epochs` is to be understood as \"final epoch\". The model is not trained for a number of iterations given by `epochs`, but merely until the epoch of index `epochs` is reached. verbose: 0, 1, 2 or 3. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch 3 = table. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment). callbacks: List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the `x` and `y` data provided, before shuffling. This argument is not supported when `x` is a generator. validation_data: Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. `validation_data` will override `validation_split`. `validation_data` could be: - tuple `(x_val, y_val)` of Numpy/Jax arrays, list of arrays or mappings - tuple `(x_val, y_val, val_sample_weights)` of Numpy/Jax arrays, list of arrays or mappings - generator For the first two cases, `batch_size` must be provided. For the last case, `validation_steps` should be provided, and should follow the same convention for yielding data as `x`. Note that `validation_data` does not support all the data types that are supported in `x`, eg, dict. shuffle: Boolean (whether to shuffle the training data before each epoch). This argument is ignored when `x` is a generator. Has no effect when `steps_per_epoch` is not `None`. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. sample_weight: Optional Numpy/Jax array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples). This argument is not supported when `x` is generator, instead provide the sample_weights as the third element of `x`. initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run). steps_per_epoch: Integer or `None`. Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input arrays such as jax data arrays, the default `None` is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. When passing a generator, you must specify the `steps_per_epoch` argument. This argument is not supported with array inputs. validation_steps: Only relevant if `validation_data` is provided and is a generator. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If 'validation_steps' is None, validation will run until the `validation_data` dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If 'validation_steps' is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time. validation_batch_size: Integer or `None`. Number of samples per validation batch. If unspecified, will default to `batch_size`. Do not specify the `validation_batch_size` if your data is in the form of generators (since they generate batches). validation_freq: Only relevant if validation data is provided. Integer or `collections_abc.Container` instance (e.g. list, tuple, etc.). If an integer, specifies how many training epochs to run before a new validation run is performed, e.g. `validation_freq=2` runs validation every 2 epochs. If a Container, specifies the epochs on which to run validation, e.g. `validation_freq=[1, 2, 10]` runs validation at the end of the 1st, 2nd, and 10th epochs. Unpacking behavior for iterator-like inputs: A common pattern is to pass a generator, which will in fact yield not only features (x) but optionally targets (y) and sample weights. Elegy requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for y and sample_weight respectively. Any other type provided will be wrapped in a length one tuple, effectively treating everything as 'x'. When yielding dicts, they should still adhere to the top-level tuple structure. e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Elegy will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the namedtuple. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: `namedtuple(\"example_tuple\", [\"y\", \"x\"])` it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])` where it is unclear if the tuple was intended to be unpacked into x, y, and sample_weight or passed through as a single element to `x`. As a result the data processing code will simply raise a ValueError if it encounters a namedtuple. (Along with instructions to remedy the issue.) Returns: A `History` object. Its `History.history` attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). Raises: ValueError: In case of mismatch between the provided input data and what the model expects. \"\"\" if x is None : x = {} if not self . initialized : self . init ( x = x , y = y , batch_size = batch_size , class_weight = class_weight , sample_weight = sample_weight , ) if validation_split : # Create the validation data using the training data. Only supported for # `Jax Numpy` and `NumPy` input. ( x , y , sample_weight ), validation_data = train_validation_split ( ( x , y , sample_weight ), validation_split = validation_split , shuffle = False ) self . stop_training = False data_handler = DataHandler ( x = x , y = y , sample_weight = sample_weight , batch_size = batch_size , steps_per_epoch = steps_per_epoch , initial_epoch = initial_epoch , epochs = epochs , shuffle = shuffle , class_weight = class_weight , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , CallbackList ): callbacks = CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = epochs , steps = data_handler . inferred_steps , ) callbacks . on_train_begin () # data_handler._initial_epoch = ( # pylint: disable=protected-access # self._maybe_load_initial_epoch_from_ckpt(initial_epoch)) epoch_logs = {} for epoch , iterator in data_handler . enumerate_epochs (): self . reset_metrics () callbacks . on_epoch_begin ( epoch ) logs = {} with data_handler . catch_stop_iteration (): for step in data_handler . steps (): callbacks . on_train_batch_begin ( step ) batch = next ( iterator ) # sample_weight = batch[2] if len(batch) == 3 else None x_batch , y_batch , sample_weight = unpack_x_y_sample_weight ( batch ) if drop_remaining and not data_utils . has_batch_size ( batch , data_handler . batch_size ): continue tmp_logs = self . train_on_batch ( x = x_batch , y = y_batch , sample_weight = sample_weight , class_weight = class_weight , ) tmp_logs . update ({ \"size\" : data_handler . batch_size }) # print(epoch, step, tmp_logs[\"accuracy\"], batch[0].shape) logs = tmp_logs callbacks . on_train_batch_end ( step , logs ) if self . stop_training : break epoch_logs = copy ( logs ) epoch_logs . update ({ \"size\" : data_handler . batch_size }) # Run validation. if ( validation_data and self . _should_eval ( epoch , validation_freq ) and not self . stop_training ): val_x , val_y , val_sample_weight = unpack_x_y_sample_weight ( validation_data ) try : val_logs = self . evaluate ( x = val_x , y = val_y , sample_weight = val_sample_weight , batch_size = validation_batch_size or batch_size , steps = validation_steps , callbacks = callbacks , # return_dict=True, drop_remaining = drop_remaining , ) val_logs = { \"val_\" + name : val for name , val in val_logs . items ()} epoch_logs . update ( val_logs ) except ( types . MissingMethod , types . MissingModule ) as e : pass callbacks . on_epoch_end ( epoch , epoch_logs ) if self . stop_training : break callbacks . on_train_end ( epoch_logs ) return self . history","title":"fit()"},{"location":"api/ModelBase/#elegy.model.model_base.ModelBase.load","text":"Loads all weights + states from a folder structure. You can load states from other models that have slightly different architecture as long as long as it preserves the ordering of the haiku.Params + haiku.State structures, adding or removing layers is fine as long as they don't have weights, new layers with weights will be initialized from scratch. Parameters: Name Type Description Default path Union[str, pathlib.Path] path to a saved model's directory. required Source code in elegy/model/model_base.py def load ( self , path : tp . Union [ str , pathlib . Path ], ) -> None : \"\"\" Loads all weights + states from a folder structure. You can load states from other models that have slightly different architecture as long as long as it preserves the ordering of the `haiku.Params` + `haiku.State` structures, adding or removing layers is fine as long as they don't have weights, new layers with weights will be initialized from scratch. Arguments: path: path to a saved model's directory. \"\"\" if isinstance ( path , str ): path = pathlib . Path ( path ) self . states = cloudpickle . loads (( path / \"states.pkl\" ) . read_bytes ()) self . initial_states = cloudpickle . loads ( ( path / \"initial_states.pkl\" ) . read_bytes () )","title":"load()"},{"location":"api/ModelBase/#elegy.model.model_base.ModelBase.predict","text":"Generates output predictions for the input samples. Computation is done in batches. Parameters: Name Type Description Default x Optional[Any] Input data. It could be: A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. A generator returning (inputs,) , (inputs, targets) or (inputs, targets, sample_weights) . A more detailed description of unpacking behavior for iterator types generator is given in the Unpacking behavior for iterator-like inputs section of Model.fit . None batch_size Optional[int] Integer or None . Number of samples per batch. If unspecified, batch_size will default to 32. Do not specify the batch_size if your data is in the form of generators (since they generate batches). None verbose int Verbosity mode, 0 or 1. 0 steps Optional[int] Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of None . None callbacks Union[List[elegy.callbacks.callback.Callback], elegy.callbacks.callback_list.CallbackList] List of elegy.callbacks.callback.Callback instances. List of callbacks to apply during training. None See the discussion of Unpacking behavior for iterator-like inputs for Model.fit . Note that Model.predict uses the same interpretation rules as Model.fit and Model.evaluate , so inputs must be unambiguous for all three methods. Returns: Type Description Any Numpy array(s) of predictions. Exceptions: Type Description ValueError In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. Source code in elegy/model/model_base.py def predict ( self , x : tp . Optional [ tp . Any ] = None , verbose : int = 0 , batch_size : tp . Optional [ int ] = None , steps : tp . Optional [ int ] = None , callbacks : tp . Union [ tp . List [ Callback ], CallbackList , None ] = None , drop_remaining : bool = False , initialize : bool = False , ) -> tp . Any : \"\"\"Generates output predictions for the input samples. Computation is done in batches. Arguments: x: Input data. It could be: - A Numpy or Jax array (or array-like), or a list of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. - A generator returning `(inputs,)`, `(inputs, targets)` or `(inputs, targets, sample_weights)`. A more detailed description of unpacking behavior for iterator types generator is given in the `Unpacking behavior for iterator-like inputs` section of `Model.fit`. batch_size: Integer or `None`. Number of samples per batch. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of generators (since they generate batches). verbose: Verbosity mode, 0 or 1. steps: Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of `None`. callbacks: List of [elegy.callbacks.callback.Callback][] instances. List of callbacks to apply during training. See the discussion of `Unpacking behavior for iterator-like inputs` for [`Model.fit`][elegy.model.model.Model.fit]. Note that Model.predict uses the same interpretation rules as `Model.fit` and `Model.evaluate`, so inputs must be unambiguous for all three methods. Returns: Numpy array(s) of predictions. Raises: ValueError: In case of mismatch between the provided input data and the model's expectations, or in case a stateful model receives a number of samples that is not a multiple of the batch size. \"\"\" if x is None : x = {} if not self . initialized : if initialize : self . init ( x = x , batch_size = batch_size ) else : raise types . ModelNotInitialized ( f \"Model not initialized, please execute `model.init` or `model.init_on_batch` \" \"before running this method, or pass `initialize=True` to initialize with the available data \" \"(this might not initialize the optimizer).\" ) outputs = None data_handler = DataHandler ( x = x , batch_size = batch_size , steps_per_epoch = steps , initial_epoch = 0 , epochs = 1 , shuffle = False , ) # Container that configures and calls `tf.keras.Callback`s. if not isinstance ( callbacks , CallbackList ): callbacks = CallbackList ( callbacks , add_history = True , add_progbar = verbose != 0 , model = self , verbose = verbose , epochs = 1 , steps = data_handler . inferred_steps , ) callbacks . on_predict_begin () for _ , iterator in data_handler . enumerate_epochs (): self . reset_metrics () with data_handler . catch_stop_iteration (): for step in data_handler . steps (): callbacks . on_predict_batch_begin ( step ) batch = next ( iterator ) if drop_remaining and not data_utils . has_batch_size ( batch , data_handler . batch_size ): continue tmp_batch_outputs = self . predict_on_batch ( x = batch [ 0 ]) batch_outputs = tmp_batch_outputs if outputs is None : outputs = map_structure ( lambda batch_output : [ batch_output ], batch_outputs ) else : outputs = map_structure ( map_append , outputs , batch_outputs , ) callbacks . on_predict_batch_end ( step , { \"outputs\" : batch_outputs , \"size\" : data_handler . batch_size }, ) callbacks . on_predict_end () all_outputs = map_structure ( jnp . concatenate , outputs ) return all_outputs","title":"predict()"},{"location":"api/ModelBase/#elegy.model.model_base.ModelBase.predict_on_batch","text":"Returns predictions for a single batch of samples. Parameters: Name Type Description Default x Any Input data. A Numpy/Jax array (or array-like), or possibly nested python structure of dict, list, tuple that contain arrays as leafs. required Returns: Type Description Any Jax array(s) of predictions. Exceptions: Type Description ValueError In case of mismatch between given number of inputs and expectations of the model. Source code in elegy/model/model_base.py def predict_on_batch ( self , x : tp . Any ) -> tp . Any : \"\"\" Returns predictions for a single batch of samples. Arguments: x: Input data. A Numpy/Jax array (or array-like), or possibly nested python structure of dict, list, tuple that contain arrays as leafs. Returns: Jax array(s) of predictions. Raises: ValueError: In case of mismatch between given number of inputs and expectations of the model. \"\"\" if not self . initialized : raise types . ModelNotInitialized ( f \"Model not initialized, please execute `init` or `init_on_batch` before running this method.\" ) initializing = False training = False method = self . call_pred_step if self . run_eagerly else self . call_pred_step_jit states = self . states . copy () if self . run_eagerly else self . states y_pred , self . states = method ( x , states , initializing , training , ) return y_pred","title":"predict_on_batch()"},{"location":"api/ModelBase/#elegy.model.model_base.ModelBase.save","text":"Saves the model to disk. It creates a directory that includes: {path}/model.pkl : The Model object instance serialized with pickle , this allows you to re-instantiate the model later. {path}/states.pkl : The Model.states serialized with pickle . {path}/initial_states.pkl : The Model.initial_states serialized with pickle . This allows you to save the entirety of the states of a model in a directory structure which can be fully restored via Model.load if the model is already instiated or elegy.model.load to load the model instance from its pickled version. import elegy model . save( 'my_model' ) # creates folder at 'my_model' del model # deletes the existing model # returns a model identical to the previous one model = elegy . model . load( 'my_model' ) Parameters: Name Type Description Default path Union[str, pathlib.Path] path where model structure will be saved. required Source code in elegy/model/model_base.py def save ( self , path : tp . Union [ str , pathlib . Path ], ) -> None : \"\"\" Saves the model to disk. It creates a directory that includes: - `{path}/model.pkl`: The `Model` object instance serialized with `pickle`, this allows you to re-instantiate the model later. - `{path}/states.pkl`: The `Model.states` serialized with `pickle`. - `{path}/initial_states.pkl`: The `Model.initial_states` serialized with `pickle`. This allows you to save the entirety of the states of a model in a directory structure which can be fully restored via `Model.load` if the model is already instiated or `elegy.model.load` to load the model instance from its pickled version. ```python import elegy model.save('my_model') # creates folder at 'my_model' del model # deletes the existing model # returns a model identical to the previous one model = elegy.model.load('my_model') ``` Arguments: path: path where model structure will be saved. \"\"\" if isinstance ( path , str ): path = pathlib . Path ( path ) path . mkdir ( parents = True , exist_ok = True ) with open ( path / \"states.pkl\" , \"wb\" ) as f : cloudpickle . dump ( self . states , f ) with open ( path / \"initial_states.pkl\" , \"wb\" ) as f : cloudpickle . dump ( self . initial_states , f ) with open ( path / \"model.pkl\" , \"wb\" ) as f : cloudpickle . dump ( self , f )","title":"save()"},{"location":"api/ModelBase/#elegy.model.model_base.ModelBase.summary","text":"Prints a summary of the network. Parameters: Name Type Description Default x Optional[Any] A sample of inputs to the network. None depth int The level number of nested level which will be showed. Information about summaries from modules deeper than depth will be aggregated together. 2 tablefmt str A string representing the style of the table generated by tabulate . See python-tabulate for more options. 'fancy_grid' tablulate_kwargs Additional keyword arguments passed to tabulate . See python-tabulate for more options. {} Source code in elegy/model/model_base.py def summary ( self , x : tp . Optional [ tp . Any ] = None , depth : int = 2 , tablefmt : str = \"fancy_grid\" , return_repr : bool = False , initialize : bool = False , eval_shape : bool = True , ** tablulate_kwargs , ) -> tp . Optional [ str ]: \"\"\" Prints a summary of the network. Arguments: x: A sample of inputs to the network. depth: The level number of nested level which will be showed. Information about summaries from modules deeper than `depth` will be aggregated together. tablefmt: A string representing the style of the table generated by `tabulate`. See [python-tabulate](https://github.com/astanin/python-tabulate) for more options. tablulate_kwargs: Additional keyword arguments passed to `tabulate`. See [python-tabulate](https://github.com/astanin/python-tabulate) for more options. \"\"\" if x is None : x = {} entries : tp . List [ types . SummaryTableEntry ] states = self . states . copy () if self . run_eagerly else self . states method = ( self . call_summary_step if self . run_eagerly else self . call_summary_step_jit ) if eval_shape : entries = jax . eval_shape ( self . call_summary_step , x , states ) else : entries = method ( x , states ) total_entry = entries [ - 1 ] entries = entries [: - 1 ] depth_groups : tp . Dict [ str , tp . List [ types . SummaryTableEntry ]] = toolz . groupby ( lambda entry : \"/\" . join ( entry . path . split ( \"/\" )[: depth ]), entries ) entries = [ utils . get_grouped_entry ( entry , depth_groups ) for entry in entries if entry . path in depth_groups ] main_table = Table ( show_header = True , show_lines = True , show_footer = True , # box=rich.box.HORIZONTALS, ) main_table . add_column ( \"Layer\" ) main_table . add_column ( \"Outputs Shape\" ) main_table . add_column ( \"Trainable \\n Parameters\" ) main_table . add_column ( \"Non-trainable \\n Parameters\" ) rows : tp . List [ tp . List [ str ]] = [] rows . append ([ \"Inputs\" , utils . format_output ( x ), \"\" , \"\" ]) for entry in entries : rows . append ( [ f \" { entry . path } {{ pad }} \" + ( f \"[dim] { entry . module_type_name } [/]\" if entry . module_type_name else \"\" ), utils . format_output ( entry . output_value ), f \"[green] { entry . trainable_params_count : , } [/] {{ pad }} { utils . format_size ( entry . trainable_params_size ) } \" if entry . trainable_params_count > 0 else \"\" , f \"[green] { entry . non_trainable_params_count : , } [/] {{ pad }} { utils . format_size ( entry . non_trainable_params_size ) } \" if entry . non_trainable_params_count > 0 else \"\" , ] ) # global summaries params_count = total_entry . trainable_params_count params_size = total_entry . trainable_params_size states_count = total_entry . non_trainable_params_count states_size = total_entry . non_trainable_params_size total_count = params_count + states_count total_size = params_size + states_size rows . append ( [ \"\" , \"Total\" , ( f \"[green] { params_count : , } [/] {{ pad }} { utils . format_size ( params_size ) } \" if params_count > 0 else \"\" ), ( f \"[green] { states_count : , } [/] {{ pad }} { utils . format_size ( states_size ) } \" if states_count > 0 else \"\" ), ] ) # add padding for col in range ( 4 ): max_length = max ( len ( line . split ( \" {pad} \" )[ 0 ]) for row in rows for line in row [ col ] . split ( \" \\n \" ) ) for row in rows : row [ col ] = \" \\n \" . join ( line . format ( pad = \" \" * ( max_length - len ( line . rstrip () . split ( \" {pad} \" )[ 0 ])) ) for line in row [ col ] . rstrip () . split ( \" \\n \" ) ) for row in rows [: - 1 ]: main_table . add_row ( * row ) main_table . columns [ 1 ] . footer = Text . from_markup ( rows [ - 1 ][ 1 ], justify = \"right\" ) main_table . columns [ 2 ] . footer = rows [ - 1 ][ 2 ] main_table . columns [ 3 ] . footer = rows [ - 1 ][ 3 ] main_table . caption_style = \"bold\" main_table . caption = ( \" \\n Total Parameters: \" + f \"[green] { total_count : , } [/] { utils . format_size ( total_size ) } \" if total_count > 0 else \"\" ) summary = \" \\n \" + utils . get_table_repr ( main_table ) print ( summary ) if return_repr : return summary","title":"summary()"},{"location":"api/ModelBase/#elegy.model.model_base.ModelBase.test_on_batch","text":"Test the model on a single batch of samples. Parameters: Name Type Description Default x Any Input data. It could be: A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. required y Union[numpy.ndarray, Mapping[str, Any], Tuple] Target data. Like the input data x , it could be either Numpy array(s) or Jax array(s). None sample_weight Optional[numpy.ndarray] Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. None Returns: Type Description Dict[str, Union[numpy.ndarray, float]] A logs dictionary of containing the main loss as well as all other losses and metrics. Exceptions: Type Description ValueError In case of invalid user-provided arguments. Source code in elegy/model/model_base.py def test_on_batch ( self , x : tp . Any , y : tp . Union [ np . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple , None ] = None , sample_weight : tp . Optional [ np . ndarray ] = None , class_weight : tp . Optional [ np . ndarray ] = None , ) -> types . Logs : \"\"\" Test the model on a single batch of samples. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a list of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or Jax array(s). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. Returns: A `logs` dictionary of containing the main `loss` as well as all other losses and metrics. Raises: ValueError: In case of invalid user-provided arguments. \"\"\" self . init_on_batch ( x = x , y_true = y , sample_weight = sample_weight , class_weight = class_weight , ) initializing = False training = False method = self . call_test_step if self . run_eagerly else self . call_test_step_jit states = self . states . copy () if self . run_eagerly else self . states loss , logs , self . states = method ( x , y , sample_weight , class_weight , states , initializing , training , ) return logs","title":"test_on_batch()"},{"location":"api/ModelBase/#elegy.model.model_base.ModelBase.train_on_batch","text":"Runs a single gradient update on a single batch of data. Parameters: Name Type Description Default x Any Input data. It could be: A Numpy array (or array-like), or a iterable of arrays (in case the model has multiple inputs). A dict mapping input names to the corresponding arrays, if the model has named inputs. required y Union[numpy.ndarray, Mapping[str, Any], Tuple] Target data. Like the input data x , it could be either Numpy array(s) or Jax array(s). It should be consistent with x (you cannot have Numpy inputs and array targets, or inversely). None sample_weight Optional[numpy.ndarray] Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. None class_weight Optional[Any] Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. None Returns: Type Description Dict[str, Union[numpy.ndarray, float]] A logs dictionary of containing the main loss as well as all other losses and metrics. Exceptions: Type Description ValueError In case of invalid user-provided arguments. Source code in elegy/model/model_base.py def train_on_batch ( self , x : tp . Any , y : tp . Union [ np . ndarray , tp . Mapping [ str , tp . Any ], tp . Tuple , None ] = None , sample_weight : tp . Optional [ np . ndarray ] = None , class_weight : tp . Optional [ tp . Any ] = None , ) -> types . Logs : \"\"\" Runs a single gradient update on a single batch of data. Arguments: x: Input data. It could be: - A Numpy array (or array-like), or a iterable of arrays (in case the model has multiple inputs). - A dict mapping input names to the corresponding arrays, if the model has named inputs. y: Target data. Like the input data `x`, it could be either Numpy array(s) or Jax array(s). It should be consistent with `x` (you cannot have Numpy inputs and array targets, or inversely). sample_weight: Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape (samples, sequence_length), to apply a different weight to every timestep of every sample. class_weight: Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. Returns: A `logs` dictionary of containing the main `loss` as well as all other losses and metrics. Raises: ValueError: In case of invalid user-provided arguments. \"\"\" self . init_on_batch ( x = x , y_true = y , sample_weight = sample_weight , class_weight = class_weight , ) initializing = False training = True method = self . call_train_step if self . run_eagerly else self . call_train_step_jit states = self . states . copy () if self . run_eagerly else self . states logs , self . states = method ( x , y , sample_weight , class_weight , states , initializing , training , ) return logs","title":"train_on_batch()"},{"location":"api/Module/","text":"elegy.Module Basic Elegy Module. For more information check out the Module System guide . __init__ ( self , name = None , dtype =< class ' jax . _src . numpy . lax_numpy . float32 '>) special Initializes the current module with the given name. Subclasses should call this constructor before creating other modules or variables such that those modules are named correctly. Parameters: Name Type Description Default name Optional[str] An optional string name for the class. If name is not provided then the class name for the current instance is converted to lower_snake_case and used instead. None Source code in elegy/module.py def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Any = jnp . float32 ): \"\"\" Initializes the current module with the given name. Subclasses should call this constructor before creating other modules or variables such that those modules are named correctly. Arguments: name: An optional string name for the class. If ``name`` is not provided then the class name for the current instance is converted to ``lower_snake_case`` and used instead. \"\"\" self . name = name if name else utils . lower_snake_case ( self . __class__ . __name__ ) self . dtype = dtype self . _submodules = {} self . _submodule_name = {} self . _dynamic_submodules = [] self . _default_params = {} self . _scope_params = None self . _spec = {} self . _initialized = False self . _signature_f = self . call self . jit_step () add_parameter ( self , name , initializer , collection = None , trainable = True , regularizer = None , constraint = None ) Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required initializer Callable[[], Any] A callable that takes not arguments returns the initial value. required collection Optional[str] Optional name of the parameter collection, if not defined it will be se to \"parameters\" if trainable=True else it will be set to \"states\" . None trainable bool Specify whether this parameter should be added to the default trainable \"parameters\" collection or to the default non-trainable \"states\" collection. If collection is passed this parameter will ignored. True regularizer Optional[Callable[[Any], jax._src.numpy.lax_numpy.ndarray]] Regularizer instance (callable). None constraint Optional[Callable[[Any], Any]] Constraint instance (callable). None Returns: Type Description Any The value of the parameter. Source code in elegy/module.py def add_parameter ( self , name : str , initializer : tp . Callable [[], tp . Any ], collection : tp . Optional [ str ] = None , trainable : bool = True , regularizer : tp . Optional [ tp . Callable [[ tp . Any ], jnp . ndarray ]] = None , constraint : tp . Optional [ tp . Callable [[ tp . Any ], tp . Any ]] = None , ) -> tp . Any : \"\"\" Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. initializer: A callable that takes not arguments returns the initial value. collection: Optional name of the parameter collection, if not defined it will be se to `\"parameters\"` if `trainable=True` else it will be set to `\"states\"`. trainable: Specify whether this parameter should be added to the default trainable `\"parameters\"` collection or to the default non-trainable `\"states\"` collection. If collection is passed this parameter will ignored. regularizer: Regularizer instance (callable). constraint: Constraint instance (callable). Returns: The value of the parameter. \"\"\" if collection is None : collection = \"parameters\" if trainable else \"states\" if self . _scope_params is None : self . _scope_params = {} if name not in self . _scope_params : if not self . is_initializing (): raise ValueError ( f \"Cannot add parameter { name } outside `init`\" ) if name in self . _default_params : parameter = self . _default_params [ name ] assert collection == parameter . collection initial_value = parameter . value else : initial_value = initializer () initial_value = jax . tree_map ( jnp . asarray , initial_value ) parameter = types . Parameter ( collection , initial_value ) self . _register_parameter ( name , parameter ) self . _scope_params [ name ] = parameter parameter = self . _scope_params [ name ] if parameter . collection != collection : raise ValueError ( f \"types.Parameter { name } previously found in collection { parameter . collection } \" f \"but currently being added for collection { collection } \" ) value = parameter . value if constraint is not None : value = constraint ( value ) if regularizer is not None : hooks . add_loss ( name = utils . get_name ( regularizer ), value = regularizer ( value ), ) return value init ( self , * , rng = None , set_defaults = False ) Initializes the module, Source code in elegy/module.py def init ( self , * , rng : tp . Optional [ types . RNGSeq ] = None , set_defaults : bool = False , ) -> tp . Callable [ ... , tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ] ]: \"\"\" Initializes the module, \"\"\" def init_callable ( * args , ** kwargs ) -> tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ]: with module_context ( module = self , collections = None , initializing = True , training = True , rng = rng ): y = self ( * args , ** kwargs ) parameters , collections = self . get_parameters_internal () self . _mark_initialized_recursive () if set_defaults : self . set_default_parameters ( parameters , collections ) return y , parameters , collections init_callable . _signature_f = self . call return init_callable","title":"Module"},{"location":"api/Module/#elegymodule","text":"","title":"elegy.Module"},{"location":"api/Module/#elegy.module.Module","text":"Basic Elegy Module. For more information check out the Module System guide .","title":"elegy.module.Module"},{"location":"api/Module/#elegy.module.Module.__init__","text":"Initializes the current module with the given name. Subclasses should call this constructor before creating other modules or variables such that those modules are named correctly. Parameters: Name Type Description Default name Optional[str] An optional string name for the class. If name is not provided then the class name for the current instance is converted to lower_snake_case and used instead. None Source code in elegy/module.py def __init__ ( self , name : tp . Optional [ str ] = None , dtype : tp . Any = jnp . float32 ): \"\"\" Initializes the current module with the given name. Subclasses should call this constructor before creating other modules or variables such that those modules are named correctly. Arguments: name: An optional string name for the class. If ``name`` is not provided then the class name for the current instance is converted to ``lower_snake_case`` and used instead. \"\"\" self . name = name if name else utils . lower_snake_case ( self . __class__ . __name__ ) self . dtype = dtype self . _submodules = {} self . _submodule_name = {} self . _dynamic_submodules = [] self . _default_params = {} self . _scope_params = None self . _spec = {} self . _initialized = False self . _signature_f = self . call self . jit_step ()","title":"__init__()"},{"location":"api/Module/#elegy.module.Module.add_parameter","text":"Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required initializer Callable[[], Any] A callable that takes not arguments returns the initial value. required collection Optional[str] Optional name of the parameter collection, if not defined it will be se to \"parameters\" if trainable=True else it will be set to \"states\" . None trainable bool Specify whether this parameter should be added to the default trainable \"parameters\" collection or to the default non-trainable \"states\" collection. If collection is passed this parameter will ignored. True regularizer Optional[Callable[[Any], jax._src.numpy.lax_numpy.ndarray]] Regularizer instance (callable). None constraint Optional[Callable[[Any], Any]] Constraint instance (callable). None Returns: Type Description Any The value of the parameter. Source code in elegy/module.py def add_parameter ( self , name : str , initializer : tp . Callable [[], tp . Any ], collection : tp . Optional [ str ] = None , trainable : bool = True , regularizer : tp . Optional [ tp . Callable [[ tp . Any ], jnp . ndarray ]] = None , constraint : tp . Optional [ tp . Callable [[ tp . Any ], tp . Any ]] = None , ) -> tp . Any : \"\"\" Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. initializer: A callable that takes not arguments returns the initial value. collection: Optional name of the parameter collection, if not defined it will be se to `\"parameters\"` if `trainable=True` else it will be set to `\"states\"`. trainable: Specify whether this parameter should be added to the default trainable `\"parameters\"` collection or to the default non-trainable `\"states\"` collection. If collection is passed this parameter will ignored. regularizer: Regularizer instance (callable). constraint: Constraint instance (callable). Returns: The value of the parameter. \"\"\" if collection is None : collection = \"parameters\" if trainable else \"states\" if self . _scope_params is None : self . _scope_params = {} if name not in self . _scope_params : if not self . is_initializing (): raise ValueError ( f \"Cannot add parameter { name } outside `init`\" ) if name in self . _default_params : parameter = self . _default_params [ name ] assert collection == parameter . collection initial_value = parameter . value else : initial_value = initializer () initial_value = jax . tree_map ( jnp . asarray , initial_value ) parameter = types . Parameter ( collection , initial_value ) self . _register_parameter ( name , parameter ) self . _scope_params [ name ] = parameter parameter = self . _scope_params [ name ] if parameter . collection != collection : raise ValueError ( f \"types.Parameter { name } previously found in collection { parameter . collection } \" f \"but currently being added for collection { collection } \" ) value = parameter . value if constraint is not None : value = constraint ( value ) if regularizer is not None : hooks . add_loss ( name = utils . get_name ( regularizer ), value = regularizer ( value ), ) return value","title":"add_parameter()"},{"location":"api/Module/#elegy.module.Module.init","text":"Initializes the module, Source code in elegy/module.py def init ( self , * , rng : tp . Optional [ types . RNGSeq ] = None , set_defaults : bool = False , ) -> tp . Callable [ ... , tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ] ]: \"\"\" Initializes the module, \"\"\" def init_callable ( * args , ** kwargs ) -> tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ]: with module_context ( module = self , collections = None , initializing = True , training = True , rng = rng ): y = self ( * args , ** kwargs ) parameters , collections = self . get_parameters_internal () self . _mark_initialized_recursive () if set_defaults : self . set_default_parameters ( parameters , collections ) return y , parameters , collections init_callable . _signature_f = self . call return init_callable","title":"init()"},{"location":"api/Optimizer/","text":"elegy.Optimizer A Module that wraps around optax optimizers. __class__ inherited Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). __instancecheck__ ( cls , instance ) special Override for isinstance(instance, cls). Source code in elegy/optimizer.py def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) __new__ ( mcls , name , bases , namespace , ** kwargs ) special staticmethod Create and return a new object. See help(type) for accurate signature. Source code in elegy/optimizer.py def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls __subclasscheck__ ( cls , subclass ) special Override for issubclass(subclass, cls). Source code in elegy/optimizer.py def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) register ( cls , subclass ) Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/optimizer.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) __init__ ( self , * optimizer , * , lr_schedule = None , steps_per_epoch = None , ** kwargs ) special Parameters: Name Type Description Default optimizer GradientTransformation An optax GradientTransformation object, if more than one is passed via *args then they are grouped using optax.chain . () lr_schedule Optional[elegy.optimizer.LRScheduler] A optional callable of the form def lr_schedule(step: int, epoch: Optional[int]) -> float that returns the learning rate schedule at each time step. If steps_per_epoch is given then epoch is calculated, else epoch is None. None steps_per_epoch Union[int, jax._src.numpy.lax_numpy.ndarray, numpy.ndarray] The number of steps to in an epoch, needed to caculate epoch from step . None Source code in elegy/optimizer.py def __init__ ( self , * optimizer : optax . GradientTransformation , lr_schedule : tp . Optional [ LRScheduler ] = None , steps_per_epoch : tp . Union [ int , jnp . ndarray , np . ndarray , None ] = None , ** kwargs , ): r \"\"\" Arguments: optimizer: An optax `GradientTransformation` object, if more than one is passed via `*args` then they are grouped using `optax.chain`. lr_schedule: A optional callable of the form `def lr_schedule(step: int, epoch: Optional[int]) -> float` that returns the learning rate schedule at each time step. If `steps_per_epoch` is given then epoch is calculated, else epoch is None. steps_per_epoch: The number of steps to in an epoch, needed to caculate `epoch` from `step`. \"\"\" if len ( optimizer ) == 0 : raise ValueError ( \"Must pass atleast 1 optimizer, got 0\" ) elif lr_schedule is not None : # do this to preserve reference after re-assign latter base_schedule = lr_schedule def lr_schedule_ ( step : jnp . ndarray ) -> jnp . ndarray : epoch : tp . Any = ( step // steps_per_epoch if steps_per_epoch is not None else None ) return base_schedule ( step , epoch ) optimizer = optax . chain ( * optimizer , optax . scale_by_schedule ( lr_schedule_ ), ) lr_schedule = lr_schedule_ elif len ( optimizer ) == 1 : optimizer = optimizer [ 0 ] else : optimizer = optax . chain ( * optimizer ) self . optimizer = optimizer self . lr_schedule = lr_schedule current_lr ( self , optimizer_states ) Returns the learning rate scaled by schedule(s) that will be used for the next training step Source code in elegy/optimizer.py def current_lr ( self , optimizer_states : types . OptimizerStates ) -> tp . Optional [ jnp . ndarray ]: \"\"\"Returns the learning rate scaled by schedule(s) that will be used for the next training step\"\"\" if self . lr_schedule is not None : step = optimizer_states [ - 1 ] . count return self . lr_schedule ( step )","title":"Optimizer"},{"location":"api/Optimizer/#elegyoptimizer","text":"","title":"elegy.Optimizer"},{"location":"api/Optimizer/#elegy.optimizer.Optimizer","text":"A Module that wraps around optax optimizers.","title":"elegy.optimizer.Optimizer"},{"location":"api/Optimizer/#elegy.optimizer.Optimizer.__class__","text":"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()).","title":"__class__"},{"location":"api/Optimizer/#elegy.optimizer.Optimizer.__class__.__instancecheck__","text":"Override for isinstance(instance, cls). Source code in elegy/optimizer.py def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance )","title":"__instancecheck__()"},{"location":"api/Optimizer/#elegy.optimizer.Optimizer.__class__.__new__","text":"Create and return a new object. See help(type) for accurate signature. Source code in elegy/optimizer.py def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls","title":"__new__()"},{"location":"api/Optimizer/#elegy.optimizer.Optimizer.__class__.__subclasscheck__","text":"Override for issubclass(subclass, cls). Source code in elegy/optimizer.py def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass )","title":"__subclasscheck__()"},{"location":"api/Optimizer/#elegy.optimizer.Optimizer.__class__.register","text":"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/optimizer.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"register()"},{"location":"api/Optimizer/#elegy.optimizer.Optimizer.__init__","text":"Parameters: Name Type Description Default optimizer GradientTransformation An optax GradientTransformation object, if more than one is passed via *args then they are grouped using optax.chain . () lr_schedule Optional[elegy.optimizer.LRScheduler] A optional callable of the form def lr_schedule(step: int, epoch: Optional[int]) -> float that returns the learning rate schedule at each time step. If steps_per_epoch is given then epoch is calculated, else epoch is None. None steps_per_epoch Union[int, jax._src.numpy.lax_numpy.ndarray, numpy.ndarray] The number of steps to in an epoch, needed to caculate epoch from step . None Source code in elegy/optimizer.py def __init__ ( self , * optimizer : optax . GradientTransformation , lr_schedule : tp . Optional [ LRScheduler ] = None , steps_per_epoch : tp . Union [ int , jnp . ndarray , np . ndarray , None ] = None , ** kwargs , ): r \"\"\" Arguments: optimizer: An optax `GradientTransformation` object, if more than one is passed via `*args` then they are grouped using `optax.chain`. lr_schedule: A optional callable of the form `def lr_schedule(step: int, epoch: Optional[int]) -> float` that returns the learning rate schedule at each time step. If `steps_per_epoch` is given then epoch is calculated, else epoch is None. steps_per_epoch: The number of steps to in an epoch, needed to caculate `epoch` from `step`. \"\"\" if len ( optimizer ) == 0 : raise ValueError ( \"Must pass atleast 1 optimizer, got 0\" ) elif lr_schedule is not None : # do this to preserve reference after re-assign latter base_schedule = lr_schedule def lr_schedule_ ( step : jnp . ndarray ) -> jnp . ndarray : epoch : tp . Any = ( step // steps_per_epoch if steps_per_epoch is not None else None ) return base_schedule ( step , epoch ) optimizer = optax . chain ( * optimizer , optax . scale_by_schedule ( lr_schedule_ ), ) lr_schedule = lr_schedule_ elif len ( optimizer ) == 1 : optimizer = optimizer [ 0 ] else : optimizer = optax . chain ( * optimizer ) self . optimizer = optimizer self . lr_schedule = lr_schedule","title":"__init__()"},{"location":"api/Optimizer/#elegy.optimizer.Optimizer.current_lr","text":"Returns the learning rate scaled by schedule(s) that will be used for the next training step Source code in elegy/optimizer.py def current_lr ( self , optimizer_states : types . OptimizerStates ) -> tp . Optional [ jnp . ndarray ]: \"\"\"Returns the learning rate scaled by schedule(s) that will be used for the next training step\"\"\" if self . lr_schedule is not None : step = optimizer_states [ - 1 ] . count return self . lr_schedule ( step )","title":"current_lr()"},{"location":"api/OutputStates/","text":"elegy.OutputStates OutputStates(preds, params, states) __getnewargs__ ( self ) special Return self as a plain tuple. Used by copy and pickle. Source code in elegy/types.py def __getnewargs__ ( self ): 'Return self as a plain tuple. Used by copy and pickle.' return _tuple ( self ) __new__ ( _cls , preds , params , states ) special staticmethod Create new instance of OutputStates(preds, params, states) __repr__ ( self ) special Return a nicely formatted representation string Source code in elegy/types.py def __repr__ ( self ): 'Return a nicely formatted representation string' return self . __class__ . __name__ + repr_fmt % self","title":"OutputStates"},{"location":"api/OutputStates/#elegyoutputstates","text":"","title":"elegy.OutputStates"},{"location":"api/OutputStates/#elegy.types.OutputStates","text":"OutputStates(preds, params, states)","title":"elegy.types.OutputStates"},{"location":"api/OutputStates/#elegy.types.OutputStates.__getnewargs__","text":"Return self as a plain tuple. Used by copy and pickle. Source code in elegy/types.py def __getnewargs__ ( self ): 'Return self as a plain tuple. Used by copy and pickle.' return _tuple ( self )","title":"__getnewargs__()"},{"location":"api/OutputStates/#elegy.types.OutputStates.__new__","text":"Create new instance of OutputStates(preds, params, states)","title":"__new__()"},{"location":"api/OutputStates/#elegy.types.OutputStates.__repr__","text":"Return a nicely formatted representation string Source code in elegy/types.py def __repr__ ( self ): 'Return a nicely formatted representation string' return self . __class__ . __name__ + repr_fmt % self","title":"__repr__()"},{"location":"api/PredStep/","text":"elegy.PredStep PredStep(y_pred, states) __getnewargs__ ( self ) special Return self as a plain tuple. Used by copy and pickle. Source code in elegy/model/model_core.py def __getnewargs__ ( self ): 'Return self as a plain tuple. Used by copy and pickle.' return _tuple ( self ) __new__ ( _cls , y_pred , states ) special staticmethod Create new instance of PredStep(y_pred, states) __repr__ ( self ) special Return a nicely formatted representation string Source code in elegy/model/model_core.py def __repr__ ( self ): 'Return a nicely formatted representation string' return self . __class__ . __name__ + repr_fmt % self","title":"PredStep"},{"location":"api/PredStep/#elegypredstep","text":"","title":"elegy.PredStep"},{"location":"api/PredStep/#elegy.model.model_core.PredStep","text":"PredStep(y_pred, states)","title":"elegy.model.model_core.PredStep"},{"location":"api/PredStep/#elegy.model.model_core.PredStep.__getnewargs__","text":"Return self as a plain tuple. Used by copy and pickle. Source code in elegy/model/model_core.py def __getnewargs__ ( self ): 'Return self as a plain tuple. Used by copy and pickle.' return _tuple ( self )","title":"__getnewargs__()"},{"location":"api/PredStep/#elegy.model.model_core.PredStep.__new__","text":"Create new instance of PredStep(y_pred, states)","title":"__new__()"},{"location":"api/PredStep/#elegy.model.model_core.PredStep.__repr__","text":"Return a nicely formatted representation string Source code in elegy/model/model_core.py def __repr__ ( self ): 'Return a nicely formatted representation string' return self . __class__ . __name__ + repr_fmt % self","title":"__repr__()"},{"location":"api/RNGSeq/","text":"elegy.RNGSeq","title":"RNGSeq"},{"location":"api/RNGSeq/#elegyrngseq","text":"","title":"elegy.RNGSeq"},{"location":"api/RNGSeq/#elegy.types.RNGSeq","text":"","title":"elegy.types.RNGSeq"},{"location":"api/States/","text":"elegy.States __class__ inherited Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). __instancecheck__ ( cls , instance ) special Override for isinstance(instance, cls). Source code in elegy/types.py def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) __new__ ( mcls , name , bases , namespace , ** kwargs ) special staticmethod Create and return a new object. See help(type) for accurate signature. Source code in elegy/types.py def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls __subclasscheck__ ( cls , subclass ) special Override for issubclass(subclass, cls). Source code in elegy/types.py def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) register ( cls , subclass ) Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/types.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) get ( self , key , default = None ) inherited D.get(k[,d]) -> D[k] if k in D, else d. d defaults to None. Source code in elegy/types.py def get ( self , key , default = None ): 'D.get(k[,d]) -> D[k] if k in D, else d. d defaults to None.' try : return self [ key ] except KeyError : return default items ( self ) inherited D.items() -> a set-like object providing a view on D's items Source code in elegy/types.py def items ( self ): \"D.items() -> a set-like object providing a view on D's items\" return ItemsView ( self ) keys ( self ) inherited D.keys() -> a set-like object providing a view on D's keys Source code in elegy/types.py def keys ( self ): \"D.keys() -> a set-like object providing a view on D's keys\" return KeysView ( self ) maybe_update ( self , ** kwargs ) Returns a new States object, updating attributes that are not yet present. Source code in elegy/types.py def maybe_update ( self , ** kwargs ) -> \"States\" : \"\"\"Returns a new States object, updating attributes that are not yet present.\"\"\" kwargs = { key : value for key , value in kwargs . items () if key not in self . __dict__ or self . __dict__ [ key ] is None } return self . update ( ** kwargs ) update ( self , ** kwargs ) Returns a new States object, updating all attributes from kwargs. Source code in elegy/types.py def update ( self , ** kwargs ) -> \"States\" : \"\"\"Returns a new States object, updating all attributes from kwargs.\"\"\" data = self . __dict__ . copy () data . update ( kwargs ) return States ( data ) update_known ( * self , ** kwargs ) Returns a new States object, updating attributes that are already present. e.g: states.update_known(**locals()) Source code in elegy/types.py def update_known ( * self , ** kwargs ) -> \"States\" : \"\"\"Returns a new States object, updating attributes that are already present. e.g: states.update_known(**locals())\"\"\" # NOTE: first argument is *self to allow the **locals() syntax inside bound methods # which have their own self inside locals() # otherwise will get a \"got multiple values for argument 'self'\" error\" assert len ( self ) == 1 , \"States.update_known() called with positional arguments\" self = self [ 0 ] kwargs = { key : value for key , value in kwargs . items () if key in self . __dict__ } return self . update ( ** kwargs ) values ( self ) inherited D.values() -> an object providing a view on D's values Source code in elegy/types.py def values ( self ): \"D.values() -> an object providing a view on D's values\" return ValuesView ( self )","title":"States"},{"location":"api/States/#elegystates","text":"","title":"elegy.States"},{"location":"api/States/#elegy.types.States","text":"","title":"elegy.types.States"},{"location":"api/States/#elegy.types.States.__class__","text":"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()).","title":"__class__"},{"location":"api/States/#elegy.types.States.__class__.__instancecheck__","text":"Override for isinstance(instance, cls). Source code in elegy/types.py def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance )","title":"__instancecheck__()"},{"location":"api/States/#elegy.types.States.__class__.__new__","text":"Create and return a new object. See help(type) for accurate signature. Source code in elegy/types.py def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls","title":"__new__()"},{"location":"api/States/#elegy.types.States.__class__.__subclasscheck__","text":"Override for issubclass(subclass, cls). Source code in elegy/types.py def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass )","title":"__subclasscheck__()"},{"location":"api/States/#elegy.types.States.__class__.register","text":"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/types.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"register()"},{"location":"api/States/#elegy.types.States.get","text":"D.get(k[,d]) -> D[k] if k in D, else d. d defaults to None. Source code in elegy/types.py def get ( self , key , default = None ): 'D.get(k[,d]) -> D[k] if k in D, else d. d defaults to None.' try : return self [ key ] except KeyError : return default","title":"get()"},{"location":"api/States/#elegy.types.States.items","text":"D.items() -> a set-like object providing a view on D's items Source code in elegy/types.py def items ( self ): \"D.items() -> a set-like object providing a view on D's items\" return ItemsView ( self )","title":"items()"},{"location":"api/States/#elegy.types.States.keys","text":"D.keys() -> a set-like object providing a view on D's keys Source code in elegy/types.py def keys ( self ): \"D.keys() -> a set-like object providing a view on D's keys\" return KeysView ( self )","title":"keys()"},{"location":"api/States/#elegy.types.States.maybe_update","text":"Returns a new States object, updating attributes that are not yet present. Source code in elegy/types.py def maybe_update ( self , ** kwargs ) -> \"States\" : \"\"\"Returns a new States object, updating attributes that are not yet present.\"\"\" kwargs = { key : value for key , value in kwargs . items () if key not in self . __dict__ or self . __dict__ [ key ] is None } return self . update ( ** kwargs )","title":"maybe_update()"},{"location":"api/States/#elegy.types.States.update","text":"Returns a new States object, updating all attributes from kwargs. Source code in elegy/types.py def update ( self , ** kwargs ) -> \"States\" : \"\"\"Returns a new States object, updating all attributes from kwargs.\"\"\" data = self . __dict__ . copy () data . update ( kwargs ) return States ( data )","title":"update()"},{"location":"api/States/#elegy.types.States.update_known","text":"Returns a new States object, updating attributes that are already present. e.g: states.update_known(**locals()) Source code in elegy/types.py def update_known ( * self , ** kwargs ) -> \"States\" : \"\"\"Returns a new States object, updating attributes that are already present. e.g: states.update_known(**locals())\"\"\" # NOTE: first argument is *self to allow the **locals() syntax inside bound methods # which have their own self inside locals() # otherwise will get a \"got multiple values for argument 'self'\" error\" assert len ( self ) == 1 , \"States.update_known() called with positional arguments\" self = self [ 0 ] kwargs = { key : value for key , value in kwargs . items () if key in self . __dict__ } return self . update ( ** kwargs )","title":"update_known()"},{"location":"api/States/#elegy.types.States.values","text":"D.values() -> an object providing a view on D's values Source code in elegy/types.py def values ( self ): \"D.values() -> an object providing a view on D's values\" return ValuesView ( self )","title":"values()"},{"location":"api/TestStep/","text":"elegy.TestStep TestStep(loss, logs, states) __getnewargs__ ( self ) special Return self as a plain tuple. Used by copy and pickle. Source code in elegy/model/model_core.py def __getnewargs__ ( self ): 'Return self as a plain tuple. Used by copy and pickle.' return _tuple ( self ) __new__ ( _cls , loss , logs , states ) special staticmethod Create new instance of TestStep(loss, logs, states) __repr__ ( self ) special Return a nicely formatted representation string Source code in elegy/model/model_core.py def __repr__ ( self ): 'Return a nicely formatted representation string' return self . __class__ . __name__ + repr_fmt % self","title":"TestStep"},{"location":"api/TestStep/#elegyteststep","text":"","title":"elegy.TestStep"},{"location":"api/TestStep/#elegy.model.model_core.TestStep","text":"TestStep(loss, logs, states)","title":"elegy.model.model_core.TestStep"},{"location":"api/TestStep/#elegy.model.model_core.TestStep.__getnewargs__","text":"Return self as a plain tuple. Used by copy and pickle. Source code in elegy/model/model_core.py def __getnewargs__ ( self ): 'Return self as a plain tuple. Used by copy and pickle.' return _tuple ( self )","title":"__getnewargs__()"},{"location":"api/TestStep/#elegy.model.model_core.TestStep.__new__","text":"Create new instance of TestStep(loss, logs, states)","title":"__new__()"},{"location":"api/TestStep/#elegy.model.model_core.TestStep.__repr__","text":"Return a nicely formatted representation string Source code in elegy/model/model_core.py def __repr__ ( self ): 'Return a nicely formatted representation string' return self . __class__ . __name__ + repr_fmt % self","title":"__repr__()"},{"location":"api/TrainStep/","text":"elegy.TrainStep TrainStep(logs, states) __getnewargs__ ( self ) special Return self as a plain tuple. Used by copy and pickle. Source code in elegy/model/model_core.py def __getnewargs__ ( self ): 'Return self as a plain tuple. Used by copy and pickle.' return _tuple ( self ) __new__ ( _cls , logs , states ) special staticmethod Create new instance of TrainStep(logs, states) __repr__ ( self ) special Return a nicely formatted representation string Source code in elegy/model/model_core.py def __repr__ ( self ): 'Return a nicely formatted representation string' return self . __class__ . __name__ + repr_fmt % self","title":"TrainStep"},{"location":"api/TrainStep/#elegytrainstep","text":"","title":"elegy.TrainStep"},{"location":"api/TrainStep/#elegy.model.model_core.TrainStep","text":"TrainStep(logs, states)","title":"elegy.model.model_core.TrainStep"},{"location":"api/TrainStep/#elegy.model.model_core.TrainStep.__getnewargs__","text":"Return self as a plain tuple. Used by copy and pickle. Source code in elegy/model/model_core.py def __getnewargs__ ( self ): 'Return self as a plain tuple. Used by copy and pickle.' return _tuple ( self )","title":"__getnewargs__()"},{"location":"api/TrainStep/#elegy.model.model_core.TrainStep.__new__","text":"Create new instance of TrainStep(logs, states)","title":"__new__()"},{"location":"api/TrainStep/#elegy.model.model_core.TrainStep.__repr__","text":"Return a nicely formatted representation string Source code in elegy/model/model_core.py def __repr__ ( self ): 'Return a nicely formatted representation string' return self . __class__ . __name__ + repr_fmt % self","title":"__repr__()"},{"location":"api/flax_summarize/","text":"elegy.flax_summarize Source code in elegy/generalized_module/linen_module.py def flax_summarize ( f ): @functools . wraps ( f ) def wrapper ( self : linen . Module , * args , ** kwargs ): outputs = f ( self , * args , ** kwargs ) if hooks . summaries_active (): path = self . scope . path hooks . add_summary ( path , self , outputs ) return outputs return wrapper","title":"flax_summarize"},{"location":"api/flax_summarize/#elegyflax_summarize","text":"","title":"elegy.flax_summarize"},{"location":"api/flax_summarize/#elegy.generalized_module.linen_module.flax_summarize","text":"Source code in elegy/generalized_module/linen_module.py def flax_summarize ( f ): @functools . wraps ( f ) def wrapper ( self : linen . Module , * args , ** kwargs ): outputs = f ( self , * args , ** kwargs ) if hooks . summaries_active (): path = self . scope . path hooks . add_summary ( path , self , outputs ) return outputs return wrapper","title":"elegy.generalized_module.linen_module.flax_summarize"},{"location":"api/flax_summary/","text":"elegy.flax_summary Source code in elegy/generalized_module/linen_module.py def flax_summary ( flax_module : linen . Module , name : str , f : tp . Any , value : types . Scalar , ): if hooks . summaries_active (): path = flax_module . scope . path + ( name ,) hooks . add_summary ( path , f , value )","title":"flax_summary"},{"location":"api/flax_summary/#elegyflax_summary","text":"","title":"elegy.flax_summary"},{"location":"api/flax_summary/#elegy.generalized_module.linen_module.flax_summary","text":"Source code in elegy/generalized_module/linen_module.py def flax_summary ( flax_module : linen . Module , name : str , f : tp . Any , value : types . Scalar , ): if hooks . summaries_active (): path = flax_module . scope . path + ( name ,) hooks . add_summary ( path , f , value )","title":"elegy.generalized_module.linen_module.flax_summary"},{"location":"api/inject_dependencies/","text":"elegy.inject_dependencies Source code in elegy/utils.py def inject_dependencies ( f : F , signature_f : tp . Optional [ tp . Callable ] = None , rename : tp . Optional [ tp . Dict [ str , str ]] = None , ) -> F : if signature_f is None : signature_f = f signature_f = get_signature_f_recursive ( signature_f ) f_params = get_function_args ( signature_f ) @functools . wraps ( signature_f ) def wrapper ( * args , ** kwargs ): n_args = len ( args ) arg_names = [ arg . name for arg in f_params [: n_args ]] kwarg_names = [ arg . name for arg in f_params [ n_args :]] if rename : for old , new in rename . items (): if old in kwargs : kwargs [ new ] = kwargs . pop ( old ) if not any ( arg . kind == inspect . Parameter . VAR_KEYWORD for arg in f_params ): # print(list(kwargs.keys())) # print(kwarg_names) kwargs = { arg : kwargs [ arg ] for arg in kwarg_names if arg not in arg_names and arg in kwargs } return f ( * args , ** kwargs ) return wrapper","title":"inject_dependencies"},{"location":"api/inject_dependencies/#elegyinject_dependencies","text":"","title":"elegy.inject_dependencies"},{"location":"api/inject_dependencies/#elegy.utils.inject_dependencies","text":"Source code in elegy/utils.py def inject_dependencies ( f : F , signature_f : tp . Optional [ tp . Callable ] = None , rename : tp . Optional [ tp . Dict [ str , str ]] = None , ) -> F : if signature_f is None : signature_f = f signature_f = get_signature_f_recursive ( signature_f ) f_params = get_function_args ( signature_f ) @functools . wraps ( signature_f ) def wrapper ( * args , ** kwargs ): n_args = len ( args ) arg_names = [ arg . name for arg in f_params [: n_args ]] kwarg_names = [ arg . name for arg in f_params [ n_args :]] if rename : for old , new in rename . items (): if old in kwargs : kwargs [ new ] = kwargs . pop ( old ) if not any ( arg . kind == inspect . Parameter . VAR_KEYWORD for arg in f_params ): # print(list(kwargs.keys())) # print(kwarg_names) kwargs = { arg : kwargs [ arg ] for arg in kwarg_names if arg not in arg_names and arg in kwargs } return f ( * args , ** kwargs ) return wrapper","title":"elegy.utils.inject_dependencies"},{"location":"api/to_module/","text":"elegy.to_module Source code in elegy/module.py def to_module ( f ): class ToModule ( Module ): def __init__ ( self , name : tp . Optional [ str ] = None ): super () . __init__ ( name = utils . lower_snake_case ( f . __name__ ) if name is None else name ) self . _signature_f = f self . f = f def call ( self , * args , ** kwargs ): return self . f ( * args , ** kwargs ) ToModule . __name__ = f . __name__ return ToModule","title":"to_module"},{"location":"api/to_module/#elegyto_module","text":"","title":"elegy.to_module"},{"location":"api/to_module/#elegy.module.to_module","text":"Source code in elegy/module.py def to_module ( f ): class ToModule ( Module ): def __init__ ( self , name : tp . Optional [ str ] = None ): super () . __init__ ( name = utils . lower_snake_case ( f . __name__ ) if name is None else name ) self . _signature_f = f self . f = f def call ( self , * args , ** kwargs ): return self . f ( * args , ** kwargs ) ToModule . __name__ = f . __name__ return ToModule","title":"elegy.module.to_module"},{"location":"api/callbacks/CSVLogger/","text":"elegy.callbacks.CSVLogger Callback that streams epoch results to a csv file. Supports all values that can be represented as a string, including 1D iterables such as np.ndarray . Examples: csv_logger = CSVLogger( 'training.log' ) model . fit(X_train, Y_train, callbacks = [csv_logger]) on_epoch_begin ( self , epoch , logs = None ) inherited Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/csv_logger.py def on_epoch_begin ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_epoch_end ( self , epoch , logs = None ) Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/csv_logger.py def on_epoch_end ( self , epoch , logs = None ): logs = logs or {} def handle_value ( k ): is_zero_dim_ndarray = isinstance ( k , np . ndarray ) and k . ndim == 0 if isinstance ( k , six . string_types ): return k elif isinstance ( k , tp . Iterable ) and not is_zero_dim_ndarray : return '\"[ %s ]\"' % ( \", \" . join ( map ( str , k ))) else : return k if self . keys is None : self . keys = sorted ( logs . keys ()) if self . model . stop_training : # We set NA so that csv parsers do not fail for this last epoch. logs = dict ([( k , logs [ k ]) if k in logs else ( k , \"NA\" ) for k in self . keys ]) if not self . writer : class CustomDialect ( csv . excel ): delimiter = self . sep fieldnames = [ \"epoch\" ] + self . keys self . writer = csv . DictWriter ( self . csv_file , fieldnames = fieldnames , dialect = CustomDialect ) if self . append_header : self . writer . writeheader () row_dict = collections . OrderedDict ({ \"epoch\" : epoch }) row_dict . update (( key , handle_value ( logs [ key ])) for key in self . keys ) self . writer . writerow ( row_dict ) self . csv_file . flush () on_predict_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/csv_logger.py @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_predict_batch_end ( self , batch , logs = None ) inherited Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/csv_logger.py @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_predict_begin ( self , logs = None ) inherited Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/csv_logger.py def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_predict_end ( self , logs = None ) inherited Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/csv_logger.py def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/csv_logger.py @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_test_batch_end ( self , batch , logs = None ) inherited Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/csv_logger.py @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_test_begin ( self , logs = None ) inherited Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/csv_logger.py def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_end ( self , logs = None ) inherited Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/csv_logger.py def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_train_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/csv_logger.py @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_train_batch_end ( self , batch , logs = None ) inherited Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/csv_logger.py @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_train_begin ( self , logs = None ) Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/csv_logger.py def on_train_begin ( self , logs = None ): if self . append : if os . path . exists ( self . filename ): with open ( self . filename , \"r\" + self . file_flags ) as f : self . append_header = not bool ( len ( f . readline ())) mode = \"a\" else : mode = \"w\" self . csv_file = io . open ( self . filename , mode + self . file_flags , ** self . _open_args ) on_train_end ( self , logs = None ) Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/csv_logger.py def on_train_end ( self , logs = None ): self . csv_file . close () self . writer = None","title":"CSVLogger"},{"location":"api/callbacks/CSVLogger/#elegycallbackscsvlogger","text":"","title":"elegy.callbacks.CSVLogger"},{"location":"api/callbacks/CSVLogger/#elegy.callbacks.csv_logger.CSVLogger","text":"Callback that streams epoch results to a csv file. Supports all values that can be represented as a string, including 1D iterables such as np.ndarray . Examples: csv_logger = CSVLogger( 'training.log' ) model . fit(X_train, Y_train, callbacks = [csv_logger])","title":"elegy.callbacks.csv_logger.CSVLogger"},{"location":"api/callbacks/CSVLogger/#elegy.callbacks.csv_logger.CSVLogger.on_epoch_begin","text":"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/csv_logger.py def on_epoch_begin ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_epoch_begin()"},{"location":"api/callbacks/CSVLogger/#elegy.callbacks.csv_logger.CSVLogger.on_epoch_end","text":"Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/csv_logger.py def on_epoch_end ( self , epoch , logs = None ): logs = logs or {} def handle_value ( k ): is_zero_dim_ndarray = isinstance ( k , np . ndarray ) and k . ndim == 0 if isinstance ( k , six . string_types ): return k elif isinstance ( k , tp . Iterable ) and not is_zero_dim_ndarray : return '\"[ %s ]\"' % ( \", \" . join ( map ( str , k ))) else : return k if self . keys is None : self . keys = sorted ( logs . keys ()) if self . model . stop_training : # We set NA so that csv parsers do not fail for this last epoch. logs = dict ([( k , logs [ k ]) if k in logs else ( k , \"NA\" ) for k in self . keys ]) if not self . writer : class CustomDialect ( csv . excel ): delimiter = self . sep fieldnames = [ \"epoch\" ] + self . keys self . writer = csv . DictWriter ( self . csv_file , fieldnames = fieldnames , dialect = CustomDialect ) if self . append_header : self . writer . writeheader () row_dict = collections . OrderedDict ({ \"epoch\" : epoch }) row_dict . update (( key , handle_value ( logs [ key ])) for key in self . keys ) self . writer . writerow ( row_dict ) self . csv_file . flush ()","title":"on_epoch_end()"},{"location":"api/callbacks/CSVLogger/#elegy.callbacks.csv_logger.CSVLogger.on_predict_batch_begin","text":"Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/csv_logger.py @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_predict_batch_begin()"},{"location":"api/callbacks/CSVLogger/#elegy.callbacks.csv_logger.CSVLogger.on_predict_batch_end","text":"Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/csv_logger.py @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_predict_batch_end()"},{"location":"api/callbacks/CSVLogger/#elegy.callbacks.csv_logger.CSVLogger.on_predict_begin","text":"Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/csv_logger.py def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_begin()"},{"location":"api/callbacks/CSVLogger/#elegy.callbacks.csv_logger.CSVLogger.on_predict_end","text":"Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/csv_logger.py def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_end()"},{"location":"api/callbacks/CSVLogger/#elegy.callbacks.csv_logger.CSVLogger.on_test_batch_begin","text":"Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/csv_logger.py @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_test_batch_begin()"},{"location":"api/callbacks/CSVLogger/#elegy.callbacks.csv_logger.CSVLogger.on_test_batch_end","text":"Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/csv_logger.py @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_test_batch_end()"},{"location":"api/callbacks/CSVLogger/#elegy.callbacks.csv_logger.CSVLogger.on_test_begin","text":"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/csv_logger.py def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_begin()"},{"location":"api/callbacks/CSVLogger/#elegy.callbacks.csv_logger.CSVLogger.on_test_end","text":"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/csv_logger.py def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_end()"},{"location":"api/callbacks/CSVLogger/#elegy.callbacks.csv_logger.CSVLogger.on_train_batch_begin","text":"Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/csv_logger.py @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_train_batch_begin()"},{"location":"api/callbacks/CSVLogger/#elegy.callbacks.csv_logger.CSVLogger.on_train_batch_end","text":"Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/csv_logger.py @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_train_batch_end()"},{"location":"api/callbacks/CSVLogger/#elegy.callbacks.csv_logger.CSVLogger.on_train_begin","text":"Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/csv_logger.py def on_train_begin ( self , logs = None ): if self . append : if os . path . exists ( self . filename ): with open ( self . filename , \"r\" + self . file_flags ) as f : self . append_header = not bool ( len ( f . readline ())) mode = \"a\" else : mode = \"w\" self . csv_file = io . open ( self . filename , mode + self . file_flags , ** self . _open_args )","title":"on_train_begin()"},{"location":"api/callbacks/CSVLogger/#elegy.callbacks.csv_logger.CSVLogger.on_train_end","text":"Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/csv_logger.py def on_train_end ( self , logs = None ): self . csv_file . close () self . writer = None","title":"on_train_end()"},{"location":"api/callbacks/Callback/","text":"elegy.callbacks.Callback Abstract base class used to build new callbacks. The logs dictionary that callback methods take as argument will contain keys for quantities relevant to the current batch or epoch. Currently, the .fit() method of the Model class will include the following quantities in the logs that it passes to its callbacks: on_epoch_end: logs include ` acc ` and ` loss ` , and optionally include ` val_loss ` ( if validation is enabled in ` fit ` ), and ` val_acc ` ( if validation and accuracy monitoring are enabled) . on_train_batch_begin: logs include ` size ` , the number of samples in the current batch . on_train_batch_end: logs include ` loss ` , and optionally ` acc ` ( if accuracy monitoring is enabled) . Attributes: Name Type Description params dict Training parameters (eg. verbosity, batch size, number of epochs...). model elegy.model.Model Reference of the model being trained. on_epoch_begin ( self , epoch , logs = None ) Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py def on_epoch_begin ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_epoch_end ( self , epoch , logs = None ) Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/callback.py def on_epoch_end ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with `val_`. \"\"\" pass on_predict_batch_begin ( self , batch , logs = None ) Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/callback.py @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_predict_batch_end ( self , batch , logs = None ) Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/callback.py @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_predict_begin ( self , logs = None ) Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_predict_end ( self , logs = None ) Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_batch_begin ( self , batch , logs = None ) Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/callback.py @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_test_batch_end ( self , batch , logs = None ) Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/callback.py @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_test_begin ( self , logs = None ) Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_end ( self , logs = None ) Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_train_batch_begin ( self , batch , logs = None ) Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/callback.py @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_train_batch_end ( self , batch , logs = None ) Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/callback.py @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_train_begin ( self , logs = None ) Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py def on_train_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_train_end ( self , logs = None ) Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py def on_train_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"Callback"},{"location":"api/callbacks/Callback/#elegycallbackscallback","text":"","title":"elegy.callbacks.Callback"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback","text":"Abstract base class used to build new callbacks. The logs dictionary that callback methods take as argument will contain keys for quantities relevant to the current batch or epoch. Currently, the .fit() method of the Model class will include the following quantities in the logs that it passes to its callbacks: on_epoch_end: logs include ` acc ` and ` loss ` , and optionally include ` val_loss ` ( if validation is enabled in ` fit ` ), and ` val_acc ` ( if validation and accuracy monitoring are enabled) . on_train_batch_begin: logs include ` size ` , the number of samples in the current batch . on_train_batch_end: logs include ` loss ` , and optionally ` acc ` ( if accuracy monitoring is enabled) . Attributes: Name Type Description params dict Training parameters (eg. verbosity, batch size, number of epochs...). model elegy.model.Model Reference of the model being trained.","title":"elegy.callbacks.callback.Callback"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_epoch_begin","text":"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py def on_epoch_begin ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_epoch_begin()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_epoch_end","text":"Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/callback.py def on_epoch_end ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with `val_`. \"\"\" pass","title":"on_epoch_end()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_predict_batch_begin","text":"Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/callback.py @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_predict_batch_begin()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_predict_batch_end","text":"Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/callback.py @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_predict_batch_end()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_predict_begin","text":"Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_begin()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_predict_end","text":"Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_end()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_test_batch_begin","text":"Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/callback.py @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_test_batch_begin()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_test_batch_end","text":"Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/callback.py @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_test_batch_end()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_test_begin","text":"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_begin()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_test_end","text":"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_end()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_train_batch_begin","text":"Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/callback.py @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_train_batch_begin()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_train_batch_end","text":"Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/callback.py @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_train_batch_end()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_train_begin","text":"Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py def on_train_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_train_begin()"},{"location":"api/callbacks/Callback/#elegy.callbacks.callback.Callback.on_train_end","text":"Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback.py def on_train_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_train_end()"},{"location":"api/callbacks/CallbackList/","text":"elegy.callbacks.CallbackList Container abstracting a list of callbacks. __init__ ( self , callbacks = None , add_history = False , add_progbar = False , model = None , ** params ) special Creates a container for Callbacks . Parameters: Name Type Description Default callbacks Optional[List[elegy.callbacks.callback.Callback]] List of Callback instances. None add_history bool Whether a History callback should be added, if one does not already exist in callback s. False add_progbar bool Whether a ProgbarLogger callback should be added, if one does not already exist in callback s. False model Optional[Any] The Model these Callback s are used with.` None **params If provided, parameters will be passed to each Callback via Callback.set_params . {} Source code in elegy/callbacks/callback_list.py def __init__ ( self , callbacks : tp . Optional [ tp . List [ Callback ]] = None , add_history : bool = False , add_progbar : bool = False , model : tp . Optional [ tp . Any ] = None , ** params ): \"\"\"Creates a container for `Callbacks`. Arguments: callbacks: List of `Callback` instances. add_history: Whether a `History` callback should be added, if one does not already exist in `callback`s. add_progbar: Whether a `ProgbarLogger` callback should be added, if one does not already exist in `callback`s. model: The `Model` these `Callback`s are used with.` **params: If provided, parameters will be passed to each `Callback` via `Callback.set_params`. \"\"\" self . callbacks = callbacks if callbacks else [] self . _add_default_callbacks ( add_history , add_progbar ) if model : self . set_model ( model ) if params : self . set_params ( params ) self . _queue_length = 10 self . _reset_batch_timing () # Determines if batch-level hooks need to be called. # This is important for performance, because processing batch-level logs # will cause async eager to block on each batch. # pylint: disable=protected-access self . _should_call_train_batch_hooks = any ( cb . _implements_train_batch_hooks () for cb in self . callbacks ) self . _should_call_test_batch_hooks = any ( cb . _implements_test_batch_hooks () for cb in self . callbacks ) self . _should_call_predict_batch_hooks = any ( cb . _implements_predict_batch_hooks () for cb in self . callbacks ) on_epoch_begin ( self , epoch , logs = None ) Calls the on_epoch_begin methods of its callbacks. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback_list.py def on_epoch_begin ( self , epoch , logs = None ): \"\"\"Calls the `on_epoch_begin` methods of its callbacks. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" logs = self . _process_logs ( logs ) for callback in self . callbacks : callback . on_epoch_begin ( epoch , logs ) self . _reset_batch_timing () on_epoch_end ( self , epoch , logs = None ) Calls the on_epoch_end methods of its callbacks. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/callback_list.py def on_epoch_end ( self , epoch , logs = None ): \"\"\"Calls the `on_epoch_end` methods of its callbacks. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with `val_`. \"\"\" logs = self . _process_logs ( logs ) for callback in self . callbacks : callback . on_epoch_end ( epoch , logs ) on_predict_batch_begin ( self , batch , logs = None ) Calls the on_predict_batch_begin methods of its callbacks. Parameters: Name Type Description Default batch integer, index of batch within the current epoch. required logs dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/callback_list.py def on_predict_batch_begin ( self , batch , logs = None ): \"\"\"Calls the `on_predict_batch_begin` methods of its callbacks. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" if self . _should_call_predict_batch_hooks : logs = self . _process_logs ( logs ) self . _call_batch_hook ( ModeKeys . PREDICT , \"begin\" , batch , logs = logs ) on_predict_batch_end ( self , batch , logs = None ) Calls the on_predict_batch_end methods of its callbacks. Parameters: Name Type Description Default batch integer, index of batch within the current epoch. required logs dict. Metric results for this batch. None Source code in elegy/callbacks/callback_list.py def on_predict_batch_end ( self , batch , logs = None ): \"\"\"Calls the `on_predict_batch_end` methods of its callbacks. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" if self . _should_call_predict_batch_hooks : logs = self . _process_logs ( logs ) self . _call_batch_hook ( ModeKeys . PREDICT , \"end\" , batch , logs = logs ) on_predict_begin ( self , logs = None ) Calls the 'on_predict_begin` methods of its callbacks. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback_list.py def on_predict_begin ( self , logs = None ): \"\"\"Calls the 'on_predict_begin` methods of its callbacks. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" logs = self . _process_logs ( logs ) for callback in self . callbacks : callback . on_predict_begin ( logs ) on_predict_end ( self , logs = None ) Calls the on_predict_end methods of its callbacks. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback_list.py def on_predict_end ( self , logs = None ): \"\"\"Calls the `on_predict_end` methods of its callbacks. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" logs = self . _process_logs ( logs ) for callback in self . callbacks : callback . on_predict_end ( logs ) on_test_batch_begin ( self , batch , logs = None ) Calls the on_test_batch_begin methods of its callbacks. Parameters: Name Type Description Default batch integer, index of batch within the current epoch. required logs dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/callback_list.py def on_test_batch_begin ( self , batch , logs = None ): \"\"\"Calls the `on_test_batch_begin` methods of its callbacks. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" if self . _should_call_test_batch_hooks : logs = self . _process_logs ( logs ) self . _call_batch_hook ( ModeKeys . TEST , \"begin\" , batch , logs = logs ) on_test_batch_end ( self , batch , logs = None ) Calls the on_test_batch_end methods of its callbacks. Parameters: Name Type Description Default batch integer, index of batch within the current epoch. required logs dict. Metric results for this batch. None Source code in elegy/callbacks/callback_list.py def on_test_batch_end ( self , batch , logs = None ): \"\"\"Calls the `on_test_batch_end` methods of its callbacks. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" if self . _should_call_test_batch_hooks : logs = self . _process_logs ( logs ) self . _call_batch_hook ( ModeKeys . TEST , \"end\" , batch , logs = logs ) on_test_begin ( self , logs = None ) Calls the on_test_begin methods of its callbacks. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback_list.py def on_test_begin ( self , logs = None ): \"\"\"Calls the `on_test_begin` methods of its callbacks. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" logs = self . _process_logs ( logs ) for callback in self . callbacks : callback . on_test_begin ( logs ) on_test_end ( self , logs = None ) Calls the on_test_end methods of its callbacks. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback_list.py def on_test_end ( self , logs = None ): \"\"\"Calls the `on_test_end` methods of its callbacks. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" logs = self . _process_logs ( logs ) for callback in self . callbacks : callback . on_test_end ( logs ) on_train_batch_begin ( self , batch , logs = None ) Calls the on_train_batch_begin methods of its callbacks. Parameters: Name Type Description Default batch integer, index of batch within the current epoch. required logs dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/callback_list.py def on_train_batch_begin ( self , batch , logs = None ): \"\"\"Calls the `on_train_batch_begin` methods of its callbacks. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" # TODO(b/150629188): Make ProgBarLogger callback not use batch hooks # when verbose != 1 if self . _should_call_train_batch_hooks : logs = self . _process_logs ( logs ) self . _call_batch_hook ( ModeKeys . TRAIN , \"begin\" , batch , logs = logs ) on_train_batch_end ( self , batch , logs = None ) Calls the on_train_batch_end methods of its callbacks. Parameters: Name Type Description Default batch integer, index of batch within the current epoch. required logs dict. Metric results for this batch. None Source code in elegy/callbacks/callback_list.py def on_train_batch_end ( self , batch , logs = None ): \"\"\"Calls the `on_train_batch_end` methods of its callbacks. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" if self . _should_call_train_batch_hooks : logs = self . _process_logs ( logs ) self . _call_batch_hook ( ModeKeys . TRAIN , \"end\" , batch , logs = logs ) on_train_begin ( self , logs = None ) Calls the on_train_begin methods of its callbacks. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback_list.py def on_train_begin ( self , logs = None ): \"\"\"Calls the `on_train_begin` methods of its callbacks. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" logs = self . _process_logs ( logs ) for callback in self . callbacks : callback . on_train_begin ( logs ) on_train_end ( self , logs = None ) Calls the on_train_end methods of its callbacks. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback_list.py def on_train_end ( self , logs = None ): \"\"\"Calls the `on_train_end` methods of its callbacks. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" logs = self . _process_logs ( logs ) for callback in self . callbacks : callback . on_train_end ( logs )","title":"CallbackList"},{"location":"api/callbacks/CallbackList/#elegycallbackscallbacklist","text":"","title":"elegy.callbacks.CallbackList"},{"location":"api/callbacks/CallbackList/#elegy.callbacks.callback_list.CallbackList","text":"Container abstracting a list of callbacks.","title":"elegy.callbacks.callback_list.CallbackList"},{"location":"api/callbacks/CallbackList/#elegy.callbacks.callback_list.CallbackList.__init__","text":"Creates a container for Callbacks . Parameters: Name Type Description Default callbacks Optional[List[elegy.callbacks.callback.Callback]] List of Callback instances. None add_history bool Whether a History callback should be added, if one does not already exist in callback s. False add_progbar bool Whether a ProgbarLogger callback should be added, if one does not already exist in callback s. False model Optional[Any] The Model these Callback s are used with.` None **params If provided, parameters will be passed to each Callback via Callback.set_params . {} Source code in elegy/callbacks/callback_list.py def __init__ ( self , callbacks : tp . Optional [ tp . List [ Callback ]] = None , add_history : bool = False , add_progbar : bool = False , model : tp . Optional [ tp . Any ] = None , ** params ): \"\"\"Creates a container for `Callbacks`. Arguments: callbacks: List of `Callback` instances. add_history: Whether a `History` callback should be added, if one does not already exist in `callback`s. add_progbar: Whether a `ProgbarLogger` callback should be added, if one does not already exist in `callback`s. model: The `Model` these `Callback`s are used with.` **params: If provided, parameters will be passed to each `Callback` via `Callback.set_params`. \"\"\" self . callbacks = callbacks if callbacks else [] self . _add_default_callbacks ( add_history , add_progbar ) if model : self . set_model ( model ) if params : self . set_params ( params ) self . _queue_length = 10 self . _reset_batch_timing () # Determines if batch-level hooks need to be called. # This is important for performance, because processing batch-level logs # will cause async eager to block on each batch. # pylint: disable=protected-access self . _should_call_train_batch_hooks = any ( cb . _implements_train_batch_hooks () for cb in self . callbacks ) self . _should_call_test_batch_hooks = any ( cb . _implements_test_batch_hooks () for cb in self . callbacks ) self . _should_call_predict_batch_hooks = any ( cb . _implements_predict_batch_hooks () for cb in self . callbacks )","title":"__init__()"},{"location":"api/callbacks/CallbackList/#elegy.callbacks.callback_list.CallbackList.on_epoch_begin","text":"Calls the on_epoch_begin methods of its callbacks. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback_list.py def on_epoch_begin ( self , epoch , logs = None ): \"\"\"Calls the `on_epoch_begin` methods of its callbacks. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" logs = self . _process_logs ( logs ) for callback in self . callbacks : callback . on_epoch_begin ( epoch , logs ) self . _reset_batch_timing ()","title":"on_epoch_begin()"},{"location":"api/callbacks/CallbackList/#elegy.callbacks.callback_list.CallbackList.on_epoch_end","text":"Calls the on_epoch_end methods of its callbacks. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/callback_list.py def on_epoch_end ( self , epoch , logs = None ): \"\"\"Calls the `on_epoch_end` methods of its callbacks. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with `val_`. \"\"\" logs = self . _process_logs ( logs ) for callback in self . callbacks : callback . on_epoch_end ( epoch , logs )","title":"on_epoch_end()"},{"location":"api/callbacks/CallbackList/#elegy.callbacks.callback_list.CallbackList.on_predict_batch_begin","text":"Calls the on_predict_batch_begin methods of its callbacks. Parameters: Name Type Description Default batch integer, index of batch within the current epoch. required logs dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/callback_list.py def on_predict_batch_begin ( self , batch , logs = None ): \"\"\"Calls the `on_predict_batch_begin` methods of its callbacks. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" if self . _should_call_predict_batch_hooks : logs = self . _process_logs ( logs ) self . _call_batch_hook ( ModeKeys . PREDICT , \"begin\" , batch , logs = logs )","title":"on_predict_batch_begin()"},{"location":"api/callbacks/CallbackList/#elegy.callbacks.callback_list.CallbackList.on_predict_batch_end","text":"Calls the on_predict_batch_end methods of its callbacks. Parameters: Name Type Description Default batch integer, index of batch within the current epoch. required logs dict. Metric results for this batch. None Source code in elegy/callbacks/callback_list.py def on_predict_batch_end ( self , batch , logs = None ): \"\"\"Calls the `on_predict_batch_end` methods of its callbacks. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" if self . _should_call_predict_batch_hooks : logs = self . _process_logs ( logs ) self . _call_batch_hook ( ModeKeys . PREDICT , \"end\" , batch , logs = logs )","title":"on_predict_batch_end()"},{"location":"api/callbacks/CallbackList/#elegy.callbacks.callback_list.CallbackList.on_predict_begin","text":"Calls the 'on_predict_begin` methods of its callbacks. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback_list.py def on_predict_begin ( self , logs = None ): \"\"\"Calls the 'on_predict_begin` methods of its callbacks. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" logs = self . _process_logs ( logs ) for callback in self . callbacks : callback . on_predict_begin ( logs )","title":"on_predict_begin()"},{"location":"api/callbacks/CallbackList/#elegy.callbacks.callback_list.CallbackList.on_predict_end","text":"Calls the on_predict_end methods of its callbacks. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback_list.py def on_predict_end ( self , logs = None ): \"\"\"Calls the `on_predict_end` methods of its callbacks. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" logs = self . _process_logs ( logs ) for callback in self . callbacks : callback . on_predict_end ( logs )","title":"on_predict_end()"},{"location":"api/callbacks/CallbackList/#elegy.callbacks.callback_list.CallbackList.on_test_batch_begin","text":"Calls the on_test_batch_begin methods of its callbacks. Parameters: Name Type Description Default batch integer, index of batch within the current epoch. required logs dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/callback_list.py def on_test_batch_begin ( self , batch , logs = None ): \"\"\"Calls the `on_test_batch_begin` methods of its callbacks. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" if self . _should_call_test_batch_hooks : logs = self . _process_logs ( logs ) self . _call_batch_hook ( ModeKeys . TEST , \"begin\" , batch , logs = logs )","title":"on_test_batch_begin()"},{"location":"api/callbacks/CallbackList/#elegy.callbacks.callback_list.CallbackList.on_test_batch_end","text":"Calls the on_test_batch_end methods of its callbacks. Parameters: Name Type Description Default batch integer, index of batch within the current epoch. required logs dict. Metric results for this batch. None Source code in elegy/callbacks/callback_list.py def on_test_batch_end ( self , batch , logs = None ): \"\"\"Calls the `on_test_batch_end` methods of its callbacks. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" if self . _should_call_test_batch_hooks : logs = self . _process_logs ( logs ) self . _call_batch_hook ( ModeKeys . TEST , \"end\" , batch , logs = logs )","title":"on_test_batch_end()"},{"location":"api/callbacks/CallbackList/#elegy.callbacks.callback_list.CallbackList.on_test_begin","text":"Calls the on_test_begin methods of its callbacks. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback_list.py def on_test_begin ( self , logs = None ): \"\"\"Calls the `on_test_begin` methods of its callbacks. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" logs = self . _process_logs ( logs ) for callback in self . callbacks : callback . on_test_begin ( logs )","title":"on_test_begin()"},{"location":"api/callbacks/CallbackList/#elegy.callbacks.callback_list.CallbackList.on_test_end","text":"Calls the on_test_end methods of its callbacks. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback_list.py def on_test_end ( self , logs = None ): \"\"\"Calls the `on_test_end` methods of its callbacks. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" logs = self . _process_logs ( logs ) for callback in self . callbacks : callback . on_test_end ( logs )","title":"on_test_end()"},{"location":"api/callbacks/CallbackList/#elegy.callbacks.callback_list.CallbackList.on_train_batch_begin","text":"Calls the on_train_batch_begin methods of its callbacks. Parameters: Name Type Description Default batch integer, index of batch within the current epoch. required logs dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/callback_list.py def on_train_batch_begin ( self , batch , logs = None ): \"\"\"Calls the `on_train_batch_begin` methods of its callbacks. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" # TODO(b/150629188): Make ProgBarLogger callback not use batch hooks # when verbose != 1 if self . _should_call_train_batch_hooks : logs = self . _process_logs ( logs ) self . _call_batch_hook ( ModeKeys . TRAIN , \"begin\" , batch , logs = logs )","title":"on_train_batch_begin()"},{"location":"api/callbacks/CallbackList/#elegy.callbacks.callback_list.CallbackList.on_train_batch_end","text":"Calls the on_train_batch_end methods of its callbacks. Parameters: Name Type Description Default batch integer, index of batch within the current epoch. required logs dict. Metric results for this batch. None Source code in elegy/callbacks/callback_list.py def on_train_batch_end ( self , batch , logs = None ): \"\"\"Calls the `on_train_batch_end` methods of its callbacks. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" if self . _should_call_train_batch_hooks : logs = self . _process_logs ( logs ) self . _call_batch_hook ( ModeKeys . TRAIN , \"end\" , batch , logs = logs )","title":"on_train_batch_end()"},{"location":"api/callbacks/CallbackList/#elegy.callbacks.callback_list.CallbackList.on_train_begin","text":"Calls the on_train_begin methods of its callbacks. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback_list.py def on_train_begin ( self , logs = None ): \"\"\"Calls the `on_train_begin` methods of its callbacks. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" logs = self . _process_logs ( logs ) for callback in self . callbacks : callback . on_train_begin ( logs )","title":"on_train_begin()"},{"location":"api/callbacks/CallbackList/#elegy.callbacks.callback_list.CallbackList.on_train_end","text":"Calls the on_train_end methods of its callbacks. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/callback_list.py def on_train_end ( self , logs = None ): \"\"\"Calls the `on_train_end` methods of its callbacks. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" logs = self . _process_logs ( logs ) for callback in self . callbacks : callback . on_train_end ( logs )","title":"on_train_end()"},{"location":"api/callbacks/EarlyStopping/","text":"elegy.callbacks.EarlyStopping Stop training when a monitored metric has stopped improving. Assuming the goal of a training is to minimize the loss. With this, the metric to be monitored would be 'loss', and mode would be 'min'. A model.fit() training loop will check at end of every epoch whether the loss is no longer decreasing, considering the min_delta and patience if applicable. Once it's found no longer decreasing, model.stop_training is marked True and the training terminates. The quantity to be monitored needs to be available in logs dict. To make it so, pass the loss or metrics at model.__init__() . Examples: np . random . seed( 42 ) class MLP (elegy . Module): def call ( self , input ): mlp = elegy . Sequential([elegy . nn . Linear( 10 ),]) return mlp( input ) callback = elegy . callbacks . EarlyStopping(monitor = \"loss\" , patience =3 ) # This callback will stop the training when there is no improvement in # the for three consecutive epochs. model = elegy . Model( module = MLP(), loss = elegy . losses . MeanSquaredError(), optimizer = optax . rmsprop( 0.01 ), ) history = model . fit( np . arange( 100 ) . reshape( 5 , 20 ) . astype(np . float32), np . zeros( 5 ), epochs =10 , batch_size =1 , callbacks = [callback], verbose =0 , ) assert len (history . history[ \"loss\" ]) == 7 # Only 7 epochs are run. on_epoch_begin ( self , epoch , logs = None ) inherited Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/early_stopping.py def on_epoch_begin ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_epoch_end ( self , epoch , logs = None ) Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/early_stopping.py def on_epoch_end ( self , epoch , logs = None ): current = self . get_monitor_value ( logs ) if current is None : return if self . monitor_op ( current - self . min_delta , self . best ): self . best = current self . wait = 0 if self . restore_best_weights : # This will also save optimizer state self . best_state = self . model . full_state else : self . wait += 1 if self . wait >= self . patience : self . stopped_epoch = epoch self . model . stop_training = True if self . restore_best_weights : if self . verbose > 0 : print ( \"Restoring model weights from the end of the best epoch.\" ) self . model . full_state = self . best_state on_predict_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/early_stopping.py @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_predict_batch_end ( self , batch , logs = None ) inherited Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/early_stopping.py @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_predict_begin ( self , logs = None ) inherited Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/early_stopping.py def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_predict_end ( self , logs = None ) inherited Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/early_stopping.py def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/early_stopping.py @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_test_batch_end ( self , batch , logs = None ) inherited Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/early_stopping.py @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_test_begin ( self , logs = None ) inherited Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/early_stopping.py def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_end ( self , logs = None ) inherited Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/early_stopping.py def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_train_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/early_stopping.py @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_train_batch_end ( self , batch , logs = None ) inherited Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/early_stopping.py @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_train_begin ( self , logs = None ) Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/early_stopping.py def on_train_begin ( self , logs = None ): # Allow instances to be re-used self . wait = 0 self . stopped_epoch = 0 if self . baseline is not None : self . best = self . baseline else : self . best = np . Inf if self . monitor_op == np . less else - np . Inf on_train_end ( self , logs = None ) Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/early_stopping.py def on_train_end ( self , logs = None ): if self . stopped_epoch > 0 and self . verbose > 0 : print ( \"Epoch %05d : early stopping\" % ( self . stopped_epoch + 1 ))","title":"EarlyStopping"},{"location":"api/callbacks/EarlyStopping/#elegycallbacksearlystopping","text":"","title":"elegy.callbacks.EarlyStopping"},{"location":"api/callbacks/EarlyStopping/#elegy.callbacks.early_stopping.EarlyStopping","text":"Stop training when a monitored metric has stopped improving. Assuming the goal of a training is to minimize the loss. With this, the metric to be monitored would be 'loss', and mode would be 'min'. A model.fit() training loop will check at end of every epoch whether the loss is no longer decreasing, considering the min_delta and patience if applicable. Once it's found no longer decreasing, model.stop_training is marked True and the training terminates. The quantity to be monitored needs to be available in logs dict. To make it so, pass the loss or metrics at model.__init__() . Examples: np . random . seed( 42 ) class MLP (elegy . Module): def call ( self , input ): mlp = elegy . Sequential([elegy . nn . Linear( 10 ),]) return mlp( input ) callback = elegy . callbacks . EarlyStopping(monitor = \"loss\" , patience =3 ) # This callback will stop the training when there is no improvement in # the for three consecutive epochs. model = elegy . Model( module = MLP(), loss = elegy . losses . MeanSquaredError(), optimizer = optax . rmsprop( 0.01 ), ) history = model . fit( np . arange( 100 ) . reshape( 5 , 20 ) . astype(np . float32), np . zeros( 5 ), epochs =10 , batch_size =1 , callbacks = [callback], verbose =0 , ) assert len (history . history[ \"loss\" ]) == 7 # Only 7 epochs are run.","title":"elegy.callbacks.early_stopping.EarlyStopping"},{"location":"api/callbacks/EarlyStopping/#elegy.callbacks.early_stopping.EarlyStopping.on_epoch_begin","text":"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/early_stopping.py def on_epoch_begin ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_epoch_begin()"},{"location":"api/callbacks/EarlyStopping/#elegy.callbacks.early_stopping.EarlyStopping.on_epoch_end","text":"Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/early_stopping.py def on_epoch_end ( self , epoch , logs = None ): current = self . get_monitor_value ( logs ) if current is None : return if self . monitor_op ( current - self . min_delta , self . best ): self . best = current self . wait = 0 if self . restore_best_weights : # This will also save optimizer state self . best_state = self . model . full_state else : self . wait += 1 if self . wait >= self . patience : self . stopped_epoch = epoch self . model . stop_training = True if self . restore_best_weights : if self . verbose > 0 : print ( \"Restoring model weights from the end of the best epoch.\" ) self . model . full_state = self . best_state","title":"on_epoch_end()"},{"location":"api/callbacks/EarlyStopping/#elegy.callbacks.early_stopping.EarlyStopping.on_predict_batch_begin","text":"Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/early_stopping.py @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_predict_batch_begin()"},{"location":"api/callbacks/EarlyStopping/#elegy.callbacks.early_stopping.EarlyStopping.on_predict_batch_end","text":"Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/early_stopping.py @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_predict_batch_end()"},{"location":"api/callbacks/EarlyStopping/#elegy.callbacks.early_stopping.EarlyStopping.on_predict_begin","text":"Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/early_stopping.py def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_begin()"},{"location":"api/callbacks/EarlyStopping/#elegy.callbacks.early_stopping.EarlyStopping.on_predict_end","text":"Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/early_stopping.py def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_end()"},{"location":"api/callbacks/EarlyStopping/#elegy.callbacks.early_stopping.EarlyStopping.on_test_batch_begin","text":"Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/early_stopping.py @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_test_batch_begin()"},{"location":"api/callbacks/EarlyStopping/#elegy.callbacks.early_stopping.EarlyStopping.on_test_batch_end","text":"Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/early_stopping.py @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_test_batch_end()"},{"location":"api/callbacks/EarlyStopping/#elegy.callbacks.early_stopping.EarlyStopping.on_test_begin","text":"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/early_stopping.py def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_begin()"},{"location":"api/callbacks/EarlyStopping/#elegy.callbacks.early_stopping.EarlyStopping.on_test_end","text":"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/early_stopping.py def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_end()"},{"location":"api/callbacks/EarlyStopping/#elegy.callbacks.early_stopping.EarlyStopping.on_train_batch_begin","text":"Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/early_stopping.py @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_train_batch_begin()"},{"location":"api/callbacks/EarlyStopping/#elegy.callbacks.early_stopping.EarlyStopping.on_train_batch_end","text":"Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/early_stopping.py @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_train_batch_end()"},{"location":"api/callbacks/EarlyStopping/#elegy.callbacks.early_stopping.EarlyStopping.on_train_begin","text":"Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/early_stopping.py def on_train_begin ( self , logs = None ): # Allow instances to be re-used self . wait = 0 self . stopped_epoch = 0 if self . baseline is not None : self . best = self . baseline else : self . best = np . Inf if self . monitor_op == np . less else - np . Inf","title":"on_train_begin()"},{"location":"api/callbacks/EarlyStopping/#elegy.callbacks.early_stopping.EarlyStopping.on_train_end","text":"Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/early_stopping.py def on_train_end ( self , logs = None ): if self . stopped_epoch > 0 and self . verbose > 0 : print ( \"Epoch %05d : early stopping\" % ( self . stopped_epoch + 1 ))","title":"on_train_end()"},{"location":"api/callbacks/History/","text":"elegy.callbacks.History Callback that records events into a History object. This callback is automatically applied to every Keras model. The History object gets returned by the fit method of models. on_epoch_begin ( self , epoch , logs = None ) inherited Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/history.py def on_epoch_begin ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_epoch_end ( self , epoch , logs = None ) Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/history.py def on_epoch_end ( self , epoch , logs = None ): logs = logs or {} self . epoch . append ( epoch ) for k , v in logs . items (): self . history . setdefault ( k , []) . append ( v ) # Set the history attribute on the model after the epoch ends. This will # make sure that the state which is set is the latest one. self . model . history = self on_predict_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/history.py @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_predict_batch_end ( self , batch , logs = None ) inherited Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/history.py @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_predict_begin ( self , logs = None ) inherited Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/history.py def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_predict_end ( self , logs = None ) inherited Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/history.py def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/history.py @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_test_batch_end ( self , batch , logs = None ) inherited Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/history.py @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_test_begin ( self , logs = None ) inherited Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/history.py def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_end ( self , logs = None ) inherited Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/history.py def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_train_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/history.py @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_train_batch_end ( self , batch , logs = None ) inherited Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/history.py @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_train_begin ( self , logs = None ) Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/history.py def on_train_begin ( self , logs = None ): self . epoch = [] on_train_end ( self , logs = None ) inherited Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/history.py def on_train_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"History"},{"location":"api/callbacks/History/#elegycallbackshistory","text":"","title":"elegy.callbacks.History"},{"location":"api/callbacks/History/#elegy.callbacks.history.History","text":"Callback that records events into a History object. This callback is automatically applied to every Keras model. The History object gets returned by the fit method of models.","title":"elegy.callbacks.history.History"},{"location":"api/callbacks/History/#elegy.callbacks.history.History.on_epoch_begin","text":"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/history.py def on_epoch_begin ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_epoch_begin()"},{"location":"api/callbacks/History/#elegy.callbacks.history.History.on_epoch_end","text":"Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/history.py def on_epoch_end ( self , epoch , logs = None ): logs = logs or {} self . epoch . append ( epoch ) for k , v in logs . items (): self . history . setdefault ( k , []) . append ( v ) # Set the history attribute on the model after the epoch ends. This will # make sure that the state which is set is the latest one. self . model . history = self","title":"on_epoch_end()"},{"location":"api/callbacks/History/#elegy.callbacks.history.History.on_predict_batch_begin","text":"Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/history.py @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_predict_batch_begin()"},{"location":"api/callbacks/History/#elegy.callbacks.history.History.on_predict_batch_end","text":"Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/history.py @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_predict_batch_end()"},{"location":"api/callbacks/History/#elegy.callbacks.history.History.on_predict_begin","text":"Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/history.py def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_begin()"},{"location":"api/callbacks/History/#elegy.callbacks.history.History.on_predict_end","text":"Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/history.py def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_end()"},{"location":"api/callbacks/History/#elegy.callbacks.history.History.on_test_batch_begin","text":"Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/history.py @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_test_batch_begin()"},{"location":"api/callbacks/History/#elegy.callbacks.history.History.on_test_batch_end","text":"Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/history.py @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_test_batch_end()"},{"location":"api/callbacks/History/#elegy.callbacks.history.History.on_test_begin","text":"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/history.py def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_begin()"},{"location":"api/callbacks/History/#elegy.callbacks.history.History.on_test_end","text":"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/history.py def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_end()"},{"location":"api/callbacks/History/#elegy.callbacks.history.History.on_train_batch_begin","text":"Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/history.py @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_train_batch_begin()"},{"location":"api/callbacks/History/#elegy.callbacks.history.History.on_train_batch_end","text":"Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/history.py @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_train_batch_end()"},{"location":"api/callbacks/History/#elegy.callbacks.history.History.on_train_begin","text":"Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/history.py def on_train_begin ( self , logs = None ): self . epoch = []","title":"on_train_begin()"},{"location":"api/callbacks/History/#elegy.callbacks.history.History.on_train_end","text":"Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/history.py def on_train_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_train_end()"},{"location":"api/callbacks/LambdaCallback/","text":"elegy.callbacks.LambdaCallback Callback for creating simple, custom callbacks on-the-fly. This callback is constructed with anonymous functions that will be called at the appropriate time. Note that the callbacks expects positional arguments, as: on_epoch_begin and on_epoch_end expect two positional arguments: epoch , logs on_train_batch_begin and on_train_batch_end expect two positional arguments: batch , logs on_train_begin and on_train_end expect one positional argument: logs Examples: # Print the batch number at the beginning of every batch. batch_print_callback = LambdaCallback( on_train_batch_begin = lambda batch,logs: print (batch)) # Stream the epoch loss to a file in JSON format. The file content # is not well-formed JSON but rather has a JSON object per line. import json json_log = open ( 'loss_log.json' , mode = 'wt' , buffering =1 ) json_logging_callback = LambdaCallback( on_epoch_end = lambda epoch, logs: json_log . write( json . dumps({ 'epoch' : epoch, 'loss' : logs[ 'loss' ]}) + ' \\n ' ), on_train_end = lambda logs: json_log . close() ) # Terminate some processes after having finished model training. processes = ... cleanup_callback = LambdaCallback( on_train_end = lambda logs: [ p . terminate() for p in processes if p . is_alive()]) model . fit( ... , callbacks = [batch_print_callback, json_logging_callback, cleanup_callback]) on_epoch_begin ( self , epoch , logs = None ) inherited Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/lambda_callback.py def on_epoch_begin ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_epoch_end ( self , epoch , logs = None ) inherited Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/lambda_callback.py def on_epoch_end ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with `val_`. \"\"\" pass on_predict_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/lambda_callback.py @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_predict_batch_end ( self , batch , logs = None ) inherited Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/lambda_callback.py @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_predict_begin ( self , logs = None ) inherited Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/lambda_callback.py def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_predict_end ( self , logs = None ) inherited Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/lambda_callback.py def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/lambda_callback.py @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_test_batch_end ( self , batch , logs = None ) inherited Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/lambda_callback.py @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_test_begin ( self , logs = None ) inherited Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/lambda_callback.py def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_end ( self , logs = None ) inherited Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/lambda_callback.py def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_train_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/lambda_callback.py @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_train_batch_end ( self , batch , logs = None ) inherited Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/lambda_callback.py @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_train_begin ( self , logs = None ) inherited Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/lambda_callback.py def on_train_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_train_end ( self , logs = None ) inherited Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/lambda_callback.py def on_train_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"LambdaCallback"},{"location":"api/callbacks/LambdaCallback/#elegycallbackslambdacallback","text":"","title":"elegy.callbacks.LambdaCallback"},{"location":"api/callbacks/LambdaCallback/#elegy.callbacks.lambda_callback.LambdaCallback","text":"Callback for creating simple, custom callbacks on-the-fly. This callback is constructed with anonymous functions that will be called at the appropriate time. Note that the callbacks expects positional arguments, as: on_epoch_begin and on_epoch_end expect two positional arguments: epoch , logs on_train_batch_begin and on_train_batch_end expect two positional arguments: batch , logs on_train_begin and on_train_end expect one positional argument: logs Examples: # Print the batch number at the beginning of every batch. batch_print_callback = LambdaCallback( on_train_batch_begin = lambda batch,logs: print (batch)) # Stream the epoch loss to a file in JSON format. The file content # is not well-formed JSON but rather has a JSON object per line. import json json_log = open ( 'loss_log.json' , mode = 'wt' , buffering =1 ) json_logging_callback = LambdaCallback( on_epoch_end = lambda epoch, logs: json_log . write( json . dumps({ 'epoch' : epoch, 'loss' : logs[ 'loss' ]}) + ' \\n ' ), on_train_end = lambda logs: json_log . close() ) # Terminate some processes after having finished model training. processes = ... cleanup_callback = LambdaCallback( on_train_end = lambda logs: [ p . terminate() for p in processes if p . is_alive()]) model . fit( ... , callbacks = [batch_print_callback, json_logging_callback, cleanup_callback])","title":"elegy.callbacks.lambda_callback.LambdaCallback"},{"location":"api/callbacks/LambdaCallback/#elegy.callbacks.lambda_callback.LambdaCallback.on_epoch_begin","text":"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/lambda_callback.py def on_epoch_begin ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_epoch_begin()"},{"location":"api/callbacks/LambdaCallback/#elegy.callbacks.lambda_callback.LambdaCallback.on_epoch_end","text":"Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/lambda_callback.py def on_epoch_end ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with `val_`. \"\"\" pass","title":"on_epoch_end()"},{"location":"api/callbacks/LambdaCallback/#elegy.callbacks.lambda_callback.LambdaCallback.on_predict_batch_begin","text":"Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/lambda_callback.py @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_predict_batch_begin()"},{"location":"api/callbacks/LambdaCallback/#elegy.callbacks.lambda_callback.LambdaCallback.on_predict_batch_end","text":"Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/lambda_callback.py @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_predict_batch_end()"},{"location":"api/callbacks/LambdaCallback/#elegy.callbacks.lambda_callback.LambdaCallback.on_predict_begin","text":"Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/lambda_callback.py def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_begin()"},{"location":"api/callbacks/LambdaCallback/#elegy.callbacks.lambda_callback.LambdaCallback.on_predict_end","text":"Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/lambda_callback.py def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_end()"},{"location":"api/callbacks/LambdaCallback/#elegy.callbacks.lambda_callback.LambdaCallback.on_test_batch_begin","text":"Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/lambda_callback.py @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_test_batch_begin()"},{"location":"api/callbacks/LambdaCallback/#elegy.callbacks.lambda_callback.LambdaCallback.on_test_batch_end","text":"Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/lambda_callback.py @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_test_batch_end()"},{"location":"api/callbacks/LambdaCallback/#elegy.callbacks.lambda_callback.LambdaCallback.on_test_begin","text":"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/lambda_callback.py def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_begin()"},{"location":"api/callbacks/LambdaCallback/#elegy.callbacks.lambda_callback.LambdaCallback.on_test_end","text":"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/lambda_callback.py def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_end()"},{"location":"api/callbacks/LambdaCallback/#elegy.callbacks.lambda_callback.LambdaCallback.on_train_batch_begin","text":"Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/lambda_callback.py @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_train_batch_begin()"},{"location":"api/callbacks/LambdaCallback/#elegy.callbacks.lambda_callback.LambdaCallback.on_train_batch_end","text":"Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/lambda_callback.py @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_train_batch_end()"},{"location":"api/callbacks/LambdaCallback/#elegy.callbacks.lambda_callback.LambdaCallback.on_train_begin","text":"Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/lambda_callback.py def on_train_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_train_begin()"},{"location":"api/callbacks/LambdaCallback/#elegy.callbacks.lambda_callback.LambdaCallback.on_train_end","text":"Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/lambda_callback.py def on_train_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_train_end()"},{"location":"api/callbacks/ModelCheckpoint/","text":"elegy.callbacks.ModelCheckpoint Callback to save the Elegy model or model weights at some frequency. ModelCheckpoint callback is used in conjunction with training using model.fit() to save a model or weights at some interval, so the model or weights can be loaded later to continue the training from the state saved. A few options this callback provides include: Whether to only keep the model that has achieved the \"best performance\" so far, or whether to save the model at the end of every epoch regardless of performance. Definition of 'best'; which quantity to monitor and whether it should be maximized or minimized. The frequency it should save at. Currently, the callback supports saving at the end of every epoch, or after a fixed number of training batches. Examples: EPOCHS = 10 checkpoint_path = '/tmp/checkpoint' model_checkpoint_callback = elegy . callbacks . ModelCheckpoint( path = checkpoint_path, monitor = 'val_acc' , mode = 'max' , save_best_only = True ) # Model is saved at the end of every epoch, if it's the best seen # so far. model . fit(epochs = EPOCHS, callbacks = [model_checkpoint_callback]) # The model status (that are considered the best) are loaded into the model. model . load(checkpoint_path) on_epoch_begin ( self , epoch , logs = None ) Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/model_checkpoint.py def on_epoch_begin ( self , epoch , logs = None ): self . _current_epoch = epoch on_epoch_end ( self , epoch , logs = None ) Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/model_checkpoint.py def on_epoch_end ( self , epoch , logs = None ): self . epochs_since_last_save += 1 # pylint: disable=protected-access if self . save_freq == \"epoch\" : self . _save_model ( epoch = epoch , logs = logs ) on_predict_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/model_checkpoint.py @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_predict_batch_end ( self , batch , logs = None ) inherited Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/model_checkpoint.py @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_predict_begin ( self , logs = None ) inherited Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/model_checkpoint.py def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_predict_end ( self , logs = None ) inherited Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/model_checkpoint.py def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/model_checkpoint.py @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_test_batch_end ( self , batch , logs = None ) inherited Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/model_checkpoint.py @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_test_begin ( self , logs = None ) inherited Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/model_checkpoint.py def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_end ( self , logs = None ) inherited Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/model_checkpoint.py def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_train_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/model_checkpoint.py @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_train_batch_end ( self , batch , logs = None ) inherited Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/model_checkpoint.py @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_train_begin ( self , logs = None ) inherited Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/model_checkpoint.py def on_train_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_train_end ( self , logs = None ) inherited Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/model_checkpoint.py def on_train_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"ModelCheckpoint"},{"location":"api/callbacks/ModelCheckpoint/#elegycallbacksmodelcheckpoint","text":"","title":"elegy.callbacks.ModelCheckpoint"},{"location":"api/callbacks/ModelCheckpoint/#elegy.callbacks.model_checkpoint.ModelCheckpoint","text":"Callback to save the Elegy model or model weights at some frequency. ModelCheckpoint callback is used in conjunction with training using model.fit() to save a model or weights at some interval, so the model or weights can be loaded later to continue the training from the state saved. A few options this callback provides include: Whether to only keep the model that has achieved the \"best performance\" so far, or whether to save the model at the end of every epoch regardless of performance. Definition of 'best'; which quantity to monitor and whether it should be maximized or minimized. The frequency it should save at. Currently, the callback supports saving at the end of every epoch, or after a fixed number of training batches. Examples: EPOCHS = 10 checkpoint_path = '/tmp/checkpoint' model_checkpoint_callback = elegy . callbacks . ModelCheckpoint( path = checkpoint_path, monitor = 'val_acc' , mode = 'max' , save_best_only = True ) # Model is saved at the end of every epoch, if it's the best seen # so far. model . fit(epochs = EPOCHS, callbacks = [model_checkpoint_callback]) # The model status (that are considered the best) are loaded into the model. model . load(checkpoint_path)","title":"elegy.callbacks.model_checkpoint.ModelCheckpoint"},{"location":"api/callbacks/ModelCheckpoint/#elegy.callbacks.model_checkpoint.ModelCheckpoint.on_epoch_begin","text":"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/model_checkpoint.py def on_epoch_begin ( self , epoch , logs = None ): self . _current_epoch = epoch","title":"on_epoch_begin()"},{"location":"api/callbacks/ModelCheckpoint/#elegy.callbacks.model_checkpoint.ModelCheckpoint.on_epoch_end","text":"Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/model_checkpoint.py def on_epoch_end ( self , epoch , logs = None ): self . epochs_since_last_save += 1 # pylint: disable=protected-access if self . save_freq == \"epoch\" : self . _save_model ( epoch = epoch , logs = logs )","title":"on_epoch_end()"},{"location":"api/callbacks/ModelCheckpoint/#elegy.callbacks.model_checkpoint.ModelCheckpoint.on_predict_batch_begin","text":"Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/model_checkpoint.py @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_predict_batch_begin()"},{"location":"api/callbacks/ModelCheckpoint/#elegy.callbacks.model_checkpoint.ModelCheckpoint.on_predict_batch_end","text":"Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/model_checkpoint.py @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_predict_batch_end()"},{"location":"api/callbacks/ModelCheckpoint/#elegy.callbacks.model_checkpoint.ModelCheckpoint.on_predict_begin","text":"Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/model_checkpoint.py def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_begin()"},{"location":"api/callbacks/ModelCheckpoint/#elegy.callbacks.model_checkpoint.ModelCheckpoint.on_predict_end","text":"Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/model_checkpoint.py def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_end()"},{"location":"api/callbacks/ModelCheckpoint/#elegy.callbacks.model_checkpoint.ModelCheckpoint.on_test_batch_begin","text":"Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/model_checkpoint.py @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_test_batch_begin()"},{"location":"api/callbacks/ModelCheckpoint/#elegy.callbacks.model_checkpoint.ModelCheckpoint.on_test_batch_end","text":"Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/model_checkpoint.py @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_test_batch_end()"},{"location":"api/callbacks/ModelCheckpoint/#elegy.callbacks.model_checkpoint.ModelCheckpoint.on_test_begin","text":"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/model_checkpoint.py def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_begin()"},{"location":"api/callbacks/ModelCheckpoint/#elegy.callbacks.model_checkpoint.ModelCheckpoint.on_test_end","text":"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/model_checkpoint.py def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_end()"},{"location":"api/callbacks/ModelCheckpoint/#elegy.callbacks.model_checkpoint.ModelCheckpoint.on_train_batch_begin","text":"Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/model_checkpoint.py @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_train_batch_begin()"},{"location":"api/callbacks/ModelCheckpoint/#elegy.callbacks.model_checkpoint.ModelCheckpoint.on_train_batch_end","text":"Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/model_checkpoint.py @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_train_batch_end()"},{"location":"api/callbacks/ModelCheckpoint/#elegy.callbacks.model_checkpoint.ModelCheckpoint.on_train_begin","text":"Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/model_checkpoint.py def on_train_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_train_begin()"},{"location":"api/callbacks/ModelCheckpoint/#elegy.callbacks.model_checkpoint.ModelCheckpoint.on_train_end","text":"Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/model_checkpoint.py def on_train_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_train_end()"},{"location":"api/callbacks/RemoteMonitor/","text":"elegy.callbacks.RemoteMonitor Callback used to stream events to a server. Requires the requests library. Events are sent to root + '/publish/epoch/end/' by default. Calls are HTTP POST, with a data argument which is a JSON-encoded dictionary of event data. If send_as_json is set to True, the content type of the request will be application/json. Otherwise the serialized JSON will be sent within a form. on_epoch_begin ( self , epoch , logs = None ) inherited Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/remote_monitor.py def on_epoch_begin ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_epoch_end ( self , epoch , logs = None ) Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/remote_monitor.py def on_epoch_end ( self , epoch , logs = None ): if requests is None : raise ImportError ( \"RemoteMonitor requires the `requests` library.\" ) logs = logs or {} send = {} send [ \"epoch\" ] = epoch for k , v in logs . items (): # np.ndarray and np.generic are not scalar types # therefore we must unwrap their scalar values and # pass to the json-serializable dict 'send' if isinstance ( v , ( np . ndarray , np . generic )): send [ k ] = v . item () else : send [ k ] = v try : if self . send_as_json : requests . post ( self . root + self . path , json = send , headers = self . headers ) else : requests . post ( self . root + self . path , { self . field : json . dumps ( send )}, headers = self . headers , ) except requests . exceptions . RequestException : logging . warning ( \"Warning: could not reach RemoteMonitor \" \"root server at \" + str ( self . root ) ) on_predict_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/remote_monitor.py @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_predict_batch_end ( self , batch , logs = None ) inherited Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/remote_monitor.py @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_predict_begin ( self , logs = None ) inherited Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/remote_monitor.py def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_predict_end ( self , logs = None ) inherited Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/remote_monitor.py def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/remote_monitor.py @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_test_batch_end ( self , batch , logs = None ) inherited Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/remote_monitor.py @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_test_begin ( self , logs = None ) inherited Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/remote_monitor.py def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_end ( self , logs = None ) inherited Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/remote_monitor.py def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_train_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/remote_monitor.py @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_train_batch_end ( self , batch , logs = None ) inherited Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/remote_monitor.py @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_train_begin ( self , logs = None ) inherited Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/remote_monitor.py def on_train_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_train_end ( self , logs = None ) inherited Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/remote_monitor.py def on_train_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"RemoteMonitor"},{"location":"api/callbacks/RemoteMonitor/#elegycallbacksremotemonitor","text":"","title":"elegy.callbacks.RemoteMonitor"},{"location":"api/callbacks/RemoteMonitor/#elegy.callbacks.remote_monitor.RemoteMonitor","text":"Callback used to stream events to a server. Requires the requests library. Events are sent to root + '/publish/epoch/end/' by default. Calls are HTTP POST, with a data argument which is a JSON-encoded dictionary of event data. If send_as_json is set to True, the content type of the request will be application/json. Otherwise the serialized JSON will be sent within a form.","title":"elegy.callbacks.remote_monitor.RemoteMonitor"},{"location":"api/callbacks/RemoteMonitor/#elegy.callbacks.remote_monitor.RemoteMonitor.on_epoch_begin","text":"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/remote_monitor.py def on_epoch_begin ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_epoch_begin()"},{"location":"api/callbacks/RemoteMonitor/#elegy.callbacks.remote_monitor.RemoteMonitor.on_epoch_end","text":"Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/remote_monitor.py def on_epoch_end ( self , epoch , logs = None ): if requests is None : raise ImportError ( \"RemoteMonitor requires the `requests` library.\" ) logs = logs or {} send = {} send [ \"epoch\" ] = epoch for k , v in logs . items (): # np.ndarray and np.generic are not scalar types # therefore we must unwrap their scalar values and # pass to the json-serializable dict 'send' if isinstance ( v , ( np . ndarray , np . generic )): send [ k ] = v . item () else : send [ k ] = v try : if self . send_as_json : requests . post ( self . root + self . path , json = send , headers = self . headers ) else : requests . post ( self . root + self . path , { self . field : json . dumps ( send )}, headers = self . headers , ) except requests . exceptions . RequestException : logging . warning ( \"Warning: could not reach RemoteMonitor \" \"root server at \" + str ( self . root ) )","title":"on_epoch_end()"},{"location":"api/callbacks/RemoteMonitor/#elegy.callbacks.remote_monitor.RemoteMonitor.on_predict_batch_begin","text":"Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/remote_monitor.py @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_predict_batch_begin()"},{"location":"api/callbacks/RemoteMonitor/#elegy.callbacks.remote_monitor.RemoteMonitor.on_predict_batch_end","text":"Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/remote_monitor.py @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_predict_batch_end()"},{"location":"api/callbacks/RemoteMonitor/#elegy.callbacks.remote_monitor.RemoteMonitor.on_predict_begin","text":"Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/remote_monitor.py def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_begin()"},{"location":"api/callbacks/RemoteMonitor/#elegy.callbacks.remote_monitor.RemoteMonitor.on_predict_end","text":"Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/remote_monitor.py def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_end()"},{"location":"api/callbacks/RemoteMonitor/#elegy.callbacks.remote_monitor.RemoteMonitor.on_test_batch_begin","text":"Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/remote_monitor.py @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_test_batch_begin()"},{"location":"api/callbacks/RemoteMonitor/#elegy.callbacks.remote_monitor.RemoteMonitor.on_test_batch_end","text":"Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/remote_monitor.py @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_test_batch_end()"},{"location":"api/callbacks/RemoteMonitor/#elegy.callbacks.remote_monitor.RemoteMonitor.on_test_begin","text":"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/remote_monitor.py def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_begin()"},{"location":"api/callbacks/RemoteMonitor/#elegy.callbacks.remote_monitor.RemoteMonitor.on_test_end","text":"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/remote_monitor.py def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_end()"},{"location":"api/callbacks/RemoteMonitor/#elegy.callbacks.remote_monitor.RemoteMonitor.on_train_batch_begin","text":"Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/remote_monitor.py @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_train_batch_begin()"},{"location":"api/callbacks/RemoteMonitor/#elegy.callbacks.remote_monitor.RemoteMonitor.on_train_batch_end","text":"Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/remote_monitor.py @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_train_batch_end()"},{"location":"api/callbacks/RemoteMonitor/#elegy.callbacks.remote_monitor.RemoteMonitor.on_train_begin","text":"Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/remote_monitor.py def on_train_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_train_begin()"},{"location":"api/callbacks/RemoteMonitor/#elegy.callbacks.remote_monitor.RemoteMonitor.on_train_end","text":"Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/remote_monitor.py def on_train_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_train_end()"},{"location":"api/callbacks/TensorBoard/","text":"elegy.callbacks.TensorBoard Callback that streams epoch results to tensorboard events folder. Supports all values that can be represented as a string, including 1D iterables such as np.ndarray . tensorboard_logger = TensorBoard( 'runs' ) model . fit(X_train, Y_train, callbacks = [tensorboard_logger]) on_epoch_begin ( self , epoch , logs = None ) Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/tensorboard.py def on_epoch_begin ( self , epoch : int , logs = None ): self . current_epoch = epoch on_epoch_end ( self , epoch , logs = None ) Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/tensorboard.py def on_epoch_end ( self , epoch , logs = None ): logs = logs or {} if self . keys is None : self . keys = logs . keys () # logs on on_{train, test}_batch_end do not have val metrics if self . write_per_batch : for key in logs : if \"val\" in key : self . val_writer . add_scalar ( key . replace ( \"val_\" , \"\" ), logs [ key ], self . global_step ) return elif epoch % self . update_freq == 0 : for key in self . keys : if \"val\" in key : self . val_writer . add_scalar ( key . replace ( \"val_\" , \"\" ), logs [ key ], epoch ) else : self . train_writer . add_scalar ( key , logs [ key ], epoch ) on_predict_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/tensorboard.py @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_predict_batch_end ( self , batch , logs = None ) inherited Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/tensorboard.py @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_predict_begin ( self , logs = None ) inherited Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/tensorboard.py def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_predict_end ( self , logs = None ) inherited Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/tensorboard.py def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/tensorboard.py @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_test_batch_end ( self , batch , logs = None ) inherited Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/tensorboard.py @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_test_begin ( self , logs = None ) inherited Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/tensorboard.py def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_end ( self , logs = None ) inherited Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/tensorboard.py def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_train_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/tensorboard.py @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_train_batch_end ( self , batch , logs = None ) Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs dict. Metric results for this batch. None Source code in elegy/callbacks/tensorboard.py def on_train_batch_end ( self , batch : int , logs = None ): if not self . write_per_batch : return logs = logs or {} self . global_step = batch + self . current_epoch * ( self . steps ) if self . global_step % self . update_freq == 0 : if self . keys is None : self . keys = logs . keys () for key in self . keys : self . train_writer . add_scalar ( key , logs [ key ], self . global_step ) on_train_begin ( self , logs = None ) Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/tensorboard.py def on_train_begin ( self , logs = None ): self . train_writer = SummaryWriter ( os . path . join ( self . logdir , \"train\" ), purge_step = self . purge_step ) self . val_writer = SummaryWriter ( os . path . join ( self . logdir , \"val\" ), purge_step = self . purge_step ) self . steps = self . params [ \"steps\" ] self . global_step = 0 on_train_end ( self , logs = None ) Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/tensorboard.py def on_train_end ( self , logs = None ): self . train_writer . close () self . val_writer . close ()","title":"TensorBoard"},{"location":"api/callbacks/TensorBoard/#elegycallbackstensorboard","text":"","title":"elegy.callbacks.TensorBoard"},{"location":"api/callbacks/TensorBoard/#elegy.callbacks.tensorboard.TensorBoard","text":"Callback that streams epoch results to tensorboard events folder. Supports all values that can be represented as a string, including 1D iterables such as np.ndarray . tensorboard_logger = TensorBoard( 'runs' ) model . fit(X_train, Y_train, callbacks = [tensorboard_logger])","title":"elegy.callbacks.tensorboard.TensorBoard"},{"location":"api/callbacks/TensorBoard/#elegy.callbacks.tensorboard.TensorBoard.on_epoch_begin","text":"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/tensorboard.py def on_epoch_begin ( self , epoch : int , logs = None ): self . current_epoch = epoch","title":"on_epoch_begin()"},{"location":"api/callbacks/TensorBoard/#elegy.callbacks.tensorboard.TensorBoard.on_epoch_end","text":"Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch integer, index of epoch. required logs dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/tensorboard.py def on_epoch_end ( self , epoch , logs = None ): logs = logs or {} if self . keys is None : self . keys = logs . keys () # logs on on_{train, test}_batch_end do not have val metrics if self . write_per_batch : for key in logs : if \"val\" in key : self . val_writer . add_scalar ( key . replace ( \"val_\" , \"\" ), logs [ key ], self . global_step ) return elif epoch % self . update_freq == 0 : for key in self . keys : if \"val\" in key : self . val_writer . add_scalar ( key . replace ( \"val_\" , \"\" ), logs [ key ], epoch ) else : self . train_writer . add_scalar ( key , logs [ key ], epoch )","title":"on_epoch_end()"},{"location":"api/callbacks/TensorBoard/#elegy.callbacks.tensorboard.TensorBoard.on_predict_batch_begin","text":"Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/tensorboard.py @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_predict_batch_begin()"},{"location":"api/callbacks/TensorBoard/#elegy.callbacks.tensorboard.TensorBoard.on_predict_batch_end","text":"Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/tensorboard.py @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_predict_batch_end()"},{"location":"api/callbacks/TensorBoard/#elegy.callbacks.tensorboard.TensorBoard.on_predict_begin","text":"Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/tensorboard.py def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_begin()"},{"location":"api/callbacks/TensorBoard/#elegy.callbacks.tensorboard.TensorBoard.on_predict_end","text":"Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/tensorboard.py def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_end()"},{"location":"api/callbacks/TensorBoard/#elegy.callbacks.tensorboard.TensorBoard.on_test_batch_begin","text":"Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/tensorboard.py @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_test_batch_begin()"},{"location":"api/callbacks/TensorBoard/#elegy.callbacks.tensorboard.TensorBoard.on_test_batch_end","text":"Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/tensorboard.py @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_test_batch_end()"},{"location":"api/callbacks/TensorBoard/#elegy.callbacks.tensorboard.TensorBoard.on_test_begin","text":"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/tensorboard.py def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_begin()"},{"location":"api/callbacks/TensorBoard/#elegy.callbacks.tensorboard.TensorBoard.on_test_end","text":"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/tensorboard.py def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_end()"},{"location":"api/callbacks/TensorBoard/#elegy.callbacks.tensorboard.TensorBoard.on_train_batch_begin","text":"Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/tensorboard.py @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_train_batch_begin()"},{"location":"api/callbacks/TensorBoard/#elegy.callbacks.tensorboard.TensorBoard.on_train_batch_end","text":"Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs dict. Metric results for this batch. None Source code in elegy/callbacks/tensorboard.py def on_train_batch_end ( self , batch : int , logs = None ): if not self . write_per_batch : return logs = logs or {} self . global_step = batch + self . current_epoch * ( self . steps ) if self . global_step % self . update_freq == 0 : if self . keys is None : self . keys = logs . keys () for key in self . keys : self . train_writer . add_scalar ( key , logs [ key ], self . global_step )","title":"on_train_batch_end()"},{"location":"api/callbacks/TensorBoard/#elegy.callbacks.tensorboard.TensorBoard.on_train_begin","text":"Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/tensorboard.py def on_train_begin ( self , logs = None ): self . train_writer = SummaryWriter ( os . path . join ( self . logdir , \"train\" ), purge_step = self . purge_step ) self . val_writer = SummaryWriter ( os . path . join ( self . logdir , \"val\" ), purge_step = self . purge_step ) self . steps = self . params [ \"steps\" ] self . global_step = 0","title":"on_train_begin()"},{"location":"api/callbacks/TensorBoard/#elegy.callbacks.tensorboard.TensorBoard.on_train_end","text":"Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/tensorboard.py def on_train_end ( self , logs = None ): self . train_writer . close () self . val_writer . close ()","title":"on_train_end()"},{"location":"api/callbacks/TerminateOnNaN/","text":"elegy.callbacks.TerminateOnNaN Callback that terminates training when a NaN loss is encountered. on_epoch_begin ( self , epoch , logs = None ) inherited Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/terminate_nan.py def on_epoch_begin ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_epoch_end ( self , epoch , logs = None ) inherited Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/terminate_nan.py def on_epoch_end ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with `val_`. \"\"\" pass on_predict_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/terminate_nan.py @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_predict_batch_end ( self , batch , logs = None ) inherited Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/terminate_nan.py @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_predict_begin ( self , logs = None ) inherited Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/terminate_nan.py def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_predict_end ( self , logs = None ) inherited Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/terminate_nan.py def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/terminate_nan.py @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_test_batch_end ( self , batch , logs = None ) inherited Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/terminate_nan.py @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_test_begin ( self , logs = None ) inherited Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/terminate_nan.py def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_test_end ( self , logs = None ) inherited Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/terminate_nan.py def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_train_batch_begin ( self , batch , logs = None ) inherited Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/terminate_nan.py @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass on_train_batch_end ( self , batch , logs = None ) inherited Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/terminate_nan.py @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass on_train_begin ( self , logs = None ) inherited Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/terminate_nan.py def on_train_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass on_train_end ( self , logs = None ) inherited Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/terminate_nan.py def on_train_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"TerminateOnNaN"},{"location":"api/callbacks/TerminateOnNaN/#elegycallbacksterminateonnan","text":"","title":"elegy.callbacks.TerminateOnNaN"},{"location":"api/callbacks/TerminateOnNaN/#elegy.callbacks.terminate_nan.TerminateOnNaN","text":"Callback that terminates training when a NaN loss is encountered.","title":"elegy.callbacks.terminate_nan.TerminateOnNaN"},{"location":"api/callbacks/TerminateOnNaN/#elegy.callbacks.terminate_nan.TerminateOnNaN.on_epoch_begin","text":"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/terminate_nan.py def on_epoch_begin ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the start of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_epoch_begin()"},{"location":"api/callbacks/TerminateOnNaN/#elegy.callbacks.terminate_nan.TerminateOnNaN.on_epoch_end","text":"Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Type Description Default epoch int integer, index of epoch. required logs Optional[Dict[str, numpy.ndarray]] dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . None Source code in elegy/callbacks/terminate_nan.py def on_epoch_end ( self , epoch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: integer, index of epoch. logs: dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with `val_`. \"\"\" pass","title":"on_epoch_end()"},{"location":"api/callbacks/TerminateOnNaN/#elegy.callbacks.terminate_nan.TerminateOnNaN.on_predict_batch_begin","text":"Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/terminate_nan.py @default def on_predict_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_predict_batch_begin()"},{"location":"api/callbacks/TerminateOnNaN/#elegy.callbacks.terminate_nan.TerminateOnNaN.on_predict_batch_end","text":"Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/terminate_nan.py @default def on_predict_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_predict_batch_end()"},{"location":"api/callbacks/TerminateOnNaN/#elegy.callbacks.terminate_nan.TerminateOnNaN.on_predict_begin","text":"Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/terminate_nan.py def on_predict_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_begin()"},{"location":"api/callbacks/TerminateOnNaN/#elegy.callbacks.terminate_nan.TerminateOnNaN.on_predict_end","text":"Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/terminate_nan.py def on_predict_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_predict_end()"},{"location":"api/callbacks/TerminateOnNaN/#elegy.callbacks.terminate_nan.TerminateOnNaN.on_test_batch_begin","text":"Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/terminate_nan.py @default def on_test_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_test_batch_begin()"},{"location":"api/callbacks/TerminateOnNaN/#elegy.callbacks.terminate_nan.TerminateOnNaN.on_test_batch_end","text":"Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/terminate_nan.py @default def on_test_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_test_batch_end()"},{"location":"api/callbacks/TerminateOnNaN/#elegy.callbacks.terminate_nan.TerminateOnNaN.on_test_begin","text":"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/terminate_nan.py def on_test_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_begin()"},{"location":"api/callbacks/TerminateOnNaN/#elegy.callbacks.terminate_nan.TerminateOnNaN.on_test_end","text":"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/terminate_nan.py def on_test_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_test_end()"},{"location":"api/callbacks/TerminateOnNaN/#elegy.callbacks.terminate_nan.TerminateOnNaN.on_train_batch_begin","text":"Called at the beginning of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Has keys batch and size representing the current batch number and the size of the batch. None Source code in elegy/callbacks/terminate_nan.py @default def on_train_batch_begin ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Has keys `batch` and `size` representing the current batch number and the size of the batch. \"\"\" pass","title":"on_train_batch_begin()"},{"location":"api/callbacks/TerminateOnNaN/#elegy.callbacks.terminate_nan.TerminateOnNaN.on_train_batch_end","text":"Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Parameters: Name Type Description Default batch int integer, index of batch within the current epoch. required logs Optional[Dict[str, numpy.ndarray]] dict. Metric results for this batch. None Source code in elegy/callbacks/terminate_nan.py @default def on_train_batch_end ( self , batch : int , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Arguments: batch: integer, index of batch within the current epoch. logs: dict. Metric results for this batch. \"\"\" pass","title":"on_train_batch_end()"},{"location":"api/callbacks/TerminateOnNaN/#elegy.callbacks.terminate_nan.TerminateOnNaN.on_train_begin","text":"Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/terminate_nan.py def on_train_begin ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the beginning of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_train_begin()"},{"location":"api/callbacks/TerminateOnNaN/#elegy.callbacks.terminate_nan.TerminateOnNaN.on_train_end","text":"Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Type Description Default logs Optional[Dict[str, numpy.ndarray]] dict. Currently no data is passed to this argument for this method but that may change in the future. None Source code in elegy/callbacks/terminate_nan.py def on_train_end ( self , logs : tp . Optional [ tp . Dict [ str , np . ndarray ]] = None ): \"\"\"Called at the end of training. Subclasses should override for any actions to run. Arguments: logs: dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" pass","title":"on_train_end()"},{"location":"api/data/DataLoader/","text":"elegy.data.DataLoader Loads samples from a dataset and combines them into batches. Can be directly passed to Model.fit() Example Usage: class MyDataset(elegy.data.Dataset): def __len__(self): return 128 def __getitem__(self, i): #dummy data return np.random.random([224, 224, 3]), np.random.randint(10) ds = MyDataset() loader = elegy.data.DataLoader(ds, batch_size=8, n_workers=8, worker_type='thread', shuffle=True) model.fit(loader, epochs=10) __init__ ( self , dataset , batch_size , n_workers = 0 , shuffle = False , worker_type = 'thread' , prefetch = 1 ) special Parameters: Name Type Description Default dataset Dataset The dataset from which to load samples. A subclass of elegy.data.Dataset or an iterable which implements __getitem__ and __len__ . required batch_size int A positive integer specifying how many samples a batch should have. required n_workers Optional[int] The number of parallel worker threads or processes which load data from the dataset. A value of 0 (default) means to load data from the main thread. 0 shuffle Optional[bool] Whether to load the samples in random order or not. Reshuffles on every epoch if True. Default: False False worker_type Optional[str] One of 'thread' (default), 'process', 'spawn', 'fork' or 'forkserver'. Only used if n_workers >0. Threads are light-weight but underly the limitations of Python's global interpreter lock. 'process' uses the default process type as defined in the multiprocessing module. 'spawn', 'fork' and 'forkserver' can be used to select a specific process type. For more information consult the Python multiprocessing documentation. 'thread' prefetch Optional[int] Number of batches to prefetch for pipelined execution (Default: 2) 1 Source code in elegy/data/dataset.py def __init__ ( self , dataset : Dataset , batch_size : int , n_workers : tp . Optional [ int ] = 0 , shuffle : tp . Optional [ bool ] = False , worker_type : tp . Optional [ str ] = \"thread\" , prefetch : tp . Optional [ int ] = 1 , ): \"\"\" Arguments: dataset: The dataset from which to load samples. A subclass of elegy.data.Dataset or an iterable which implements `__getitem__` and `__len__`. batch_size: A positive integer specifying how many samples a batch should have. n_workers: The number of parallel worker threads or processes which load data from the dataset. A value of 0 (default) means to load data from the main thread. shuffle: Whether to load the samples in random order or not. Reshuffles on every epoch if True. Default: False worker_type: One of 'thread' (default), 'process', 'spawn', 'fork' or 'forkserver'. Only used if `n_workers`>0. Threads are light-weight but underly the limitations of Python's global interpreter lock. 'process' uses the default process type as defined in the `multiprocessing` module. 'spawn', 'fork' and 'forkserver' can be used to select a specific process type. For more information consult the Python `multiprocessing` documentation. prefetch: Number of batches to prefetch for pipelined execution (Default: 2) \"\"\" assert ( batch_size > 0 and type ( batch_size ) == int ), \"batch_size must be a positive integer\" assert worker_type in [ \"thread\" , \"process\" , \"spawn\" , \"fork\" , \"forkserver\" ] assert ( prefetch >= 0 and type ( prefetch ) == int ), \"prefetch must be a non-negative integer\" self . dataset = dataset self . batch_size = batch_size self . n_workers = n_workers self . shuffle = shuffle self . worker_type = worker_type self . prefetch = prefetch __iter__ ( self ) special Returns a generator which generates batches of loaded data samples Source code in elegy/data/dataset.py def __iter__ ( self ) -> tp . Generator [ tp . Any , None , None ]: \"\"\"Returns a generator which generates batches of loaded data samples\"\"\" indices = np . arange ( len ( self . dataset )) if self . shuffle : np . random . shuffle ( indices ) batched_indices = [ indices [ i :][: self . batch_size ] for i in range ( 0 , len ( indices ), self . batch_size ) ] if self . n_workers == 0 : return mainthread_data_iterator ( self . dataset , batched_indices ) else : return MultiProcessIterator ( self . dataset , batched_indices , self . n_workers , prefetch = self . prefetch , worker_type = self . worker_type , ) __len__ ( self ) special Returns the number of batches per epoch Source code in elegy/data/dataset.py def __len__ ( self ) -> int : \"\"\"Returns the number of batches per epoch\"\"\" return int ( np . ceil ( len ( self . dataset ) / self . batch_size ))","title":"DataLoader"},{"location":"api/data/DataLoader/#elegydatadataloader","text":"","title":"elegy.data.DataLoader"},{"location":"api/data/DataLoader/#elegy.data.dataset.DataLoader","text":"Loads samples from a dataset and combines them into batches. Can be directly passed to Model.fit() Example Usage: class MyDataset(elegy.data.Dataset): def __len__(self): return 128 def __getitem__(self, i): #dummy data return np.random.random([224, 224, 3]), np.random.randint(10) ds = MyDataset() loader = elegy.data.DataLoader(ds, batch_size=8, n_workers=8, worker_type='thread', shuffle=True) model.fit(loader, epochs=10)","title":"elegy.data.dataset.DataLoader"},{"location":"api/data/DataLoader/#elegy.data.dataset.DataLoader.__init__","text":"Parameters: Name Type Description Default dataset Dataset The dataset from which to load samples. A subclass of elegy.data.Dataset or an iterable which implements __getitem__ and __len__ . required batch_size int A positive integer specifying how many samples a batch should have. required n_workers Optional[int] The number of parallel worker threads or processes which load data from the dataset. A value of 0 (default) means to load data from the main thread. 0 shuffle Optional[bool] Whether to load the samples in random order or not. Reshuffles on every epoch if True. Default: False False worker_type Optional[str] One of 'thread' (default), 'process', 'spawn', 'fork' or 'forkserver'. Only used if n_workers >0. Threads are light-weight but underly the limitations of Python's global interpreter lock. 'process' uses the default process type as defined in the multiprocessing module. 'spawn', 'fork' and 'forkserver' can be used to select a specific process type. For more information consult the Python multiprocessing documentation. 'thread' prefetch Optional[int] Number of batches to prefetch for pipelined execution (Default: 2) 1 Source code in elegy/data/dataset.py def __init__ ( self , dataset : Dataset , batch_size : int , n_workers : tp . Optional [ int ] = 0 , shuffle : tp . Optional [ bool ] = False , worker_type : tp . Optional [ str ] = \"thread\" , prefetch : tp . Optional [ int ] = 1 , ): \"\"\" Arguments: dataset: The dataset from which to load samples. A subclass of elegy.data.Dataset or an iterable which implements `__getitem__` and `__len__`. batch_size: A positive integer specifying how many samples a batch should have. n_workers: The number of parallel worker threads or processes which load data from the dataset. A value of 0 (default) means to load data from the main thread. shuffle: Whether to load the samples in random order or not. Reshuffles on every epoch if True. Default: False worker_type: One of 'thread' (default), 'process', 'spawn', 'fork' or 'forkserver'. Only used if `n_workers`>0. Threads are light-weight but underly the limitations of Python's global interpreter lock. 'process' uses the default process type as defined in the `multiprocessing` module. 'spawn', 'fork' and 'forkserver' can be used to select a specific process type. For more information consult the Python `multiprocessing` documentation. prefetch: Number of batches to prefetch for pipelined execution (Default: 2) \"\"\" assert ( batch_size > 0 and type ( batch_size ) == int ), \"batch_size must be a positive integer\" assert worker_type in [ \"thread\" , \"process\" , \"spawn\" , \"fork\" , \"forkserver\" ] assert ( prefetch >= 0 and type ( prefetch ) == int ), \"prefetch must be a non-negative integer\" self . dataset = dataset self . batch_size = batch_size self . n_workers = n_workers self . shuffle = shuffle self . worker_type = worker_type self . prefetch = prefetch","title":"__init__()"},{"location":"api/data/DataLoader/#elegy.data.dataset.DataLoader.__iter__","text":"Returns a generator which generates batches of loaded data samples Source code in elegy/data/dataset.py def __iter__ ( self ) -> tp . Generator [ tp . Any , None , None ]: \"\"\"Returns a generator which generates batches of loaded data samples\"\"\" indices = np . arange ( len ( self . dataset )) if self . shuffle : np . random . shuffle ( indices ) batched_indices = [ indices [ i :][: self . batch_size ] for i in range ( 0 , len ( indices ), self . batch_size ) ] if self . n_workers == 0 : return mainthread_data_iterator ( self . dataset , batched_indices ) else : return MultiProcessIterator ( self . dataset , batched_indices , self . n_workers , prefetch = self . prefetch , worker_type = self . worker_type , )","title":"__iter__()"},{"location":"api/data/DataLoader/#elegy.data.dataset.DataLoader.__len__","text":"Returns the number of batches per epoch Source code in elegy/data/dataset.py def __len__ ( self ) -> int : \"\"\"Returns the number of batches per epoch\"\"\" return int ( np . ceil ( len ( self . dataset ) / self . batch_size ))","title":"__len__()"},{"location":"api/data/Dataset/","text":"elegy.data.Dataset Abstract base class for datasets. Subclasses should implement the __getitem__ and __len__ methods. Example Usage: class MyDataset(elegy.data.Dataset): def __len__(self): return 128 def __getitem__(self, i): #dummy data return np.random.random([224, 224, 3]), np.random.randint(10) ds = MyDataset() loader = elegy.data.DataLoader(ds, batch_size=8, n_workers=8, worker_type='thread', shuffle=True) model.fit(loader, epochs=10) __getitem__ ( self , i ) special Abstract method. In a subclass this should return the i -th data sample Source code in elegy/data/dataset.py def __getitem__ ( self , i : int ) -> tp . Any : \"\"\"Abstract method. In a subclass this should return the `i`-th data sample\"\"\" raise NotImplementedError __len__ ( self ) special Abstract method. In a subclass this should return the number of data samples in the dataset. Source code in elegy/data/dataset.py def __len__ ( self ) -> int : \"\"\"Abstract method. In a subclass this should return the number of data samples in the dataset.\"\"\" raise NotImplementedError","title":"Dataset"},{"location":"api/data/Dataset/#elegydatadataset","text":"","title":"elegy.data.Dataset"},{"location":"api/data/Dataset/#elegy.data.dataset.Dataset","text":"Abstract base class for datasets. Subclasses should implement the __getitem__ and __len__ methods. Example Usage: class MyDataset(elegy.data.Dataset): def __len__(self): return 128 def __getitem__(self, i): #dummy data return np.random.random([224, 224, 3]), np.random.randint(10) ds = MyDataset() loader = elegy.data.DataLoader(ds, batch_size=8, n_workers=8, worker_type='thread', shuffle=True) model.fit(loader, epochs=10)","title":"elegy.data.dataset.Dataset"},{"location":"api/data/Dataset/#elegy.data.dataset.Dataset.__getitem__","text":"Abstract method. In a subclass this should return the i -th data sample Source code in elegy/data/dataset.py def __getitem__ ( self , i : int ) -> tp . Any : \"\"\"Abstract method. In a subclass this should return the `i`-th data sample\"\"\" raise NotImplementedError","title":"__getitem__()"},{"location":"api/data/Dataset/#elegy.data.dataset.Dataset.__len__","text":"Abstract method. In a subclass this should return the number of data samples in the dataset. Source code in elegy/data/dataset.py def __len__ ( self ) -> int : \"\"\"Abstract method. In a subclass this should return the number of data samples in the dataset.\"\"\" raise NotImplementedError","title":"__len__()"},{"location":"api/hooks/add_loss/","text":"elegy.hooks.add_loss A hook that lets you define a loss within a module . w = self . add_parameter( \"w\" , [ 3 , 5 ], initializer = jnp . ones) # L2 regularization penalty elegy . hooks . add_loss( \"l2_regularization\" , 0.01 * jnp . mean(w ** 2 )) Parameters: Name Type Description Default name str The name of the loss. If a name is repeated on different calls values will be added together. required value Union[numpy.ndarray, float, int] The value for the loss. required Source code in elegy/hooks.py def add_loss ( name : str , value : types . Scalar ) -> None : \"\"\" A hook that lets you define a loss within a [`module`][elegy.module.Module]. ```python w = self.add_parameter(\"w\", [3, 5], initializer=jnp.ones) # L2 regularization penalty elegy.hooks.add_loss(\"l2_regularization\", 0.01 * jnp.mean(w ** 2)) ``` Arguments: name: The name of the loss. If a `name` is repeated on different calls values will be added together. value: The value for the loss. \"\"\" if LOCAL . losses is None : return if not name . endswith ( \"loss\" ): name += \"_loss\" if name in LOCAL . losses : LOCAL . losses [ name ] = LOCAL . losses [ name ] + value else : LOCAL . losses [ name ] = value","title":"add_loss"},{"location":"api/hooks/add_loss/#elegyhooksadd_loss","text":"","title":"elegy.hooks.add_loss"},{"location":"api/hooks/add_loss/#elegy.hooks.add_loss","text":"A hook that lets you define a loss within a module . w = self . add_parameter( \"w\" , [ 3 , 5 ], initializer = jnp . ones) # L2 regularization penalty elegy . hooks . add_loss( \"l2_regularization\" , 0.01 * jnp . mean(w ** 2 )) Parameters: Name Type Description Default name str The name of the loss. If a name is repeated on different calls values will be added together. required value Union[numpy.ndarray, float, int] The value for the loss. required Source code in elegy/hooks.py def add_loss ( name : str , value : types . Scalar ) -> None : \"\"\" A hook that lets you define a loss within a [`module`][elegy.module.Module]. ```python w = self.add_parameter(\"w\", [3, 5], initializer=jnp.ones) # L2 regularization penalty elegy.hooks.add_loss(\"l2_regularization\", 0.01 * jnp.mean(w ** 2)) ``` Arguments: name: The name of the loss. If a `name` is repeated on different calls values will be added together. value: The value for the loss. \"\"\" if LOCAL . losses is None : return if not name . endswith ( \"loss\" ): name += \"_loss\" if name in LOCAL . losses : LOCAL . losses [ name ] = LOCAL . losses [ name ] + value else : LOCAL . losses [ name ] = value","title":"elegy.hooks.add_loss"},{"location":"api/hooks/add_metric/","text":"elegy.hooks.add_metric A hook that lets you define a metric within a module . y = jax . nn . relu(x) elegy . hooks . add_metric( \"activation_mean\" , jnp . mean(y)) Parameters: Name Type Description Default name str The name of the loss. If a metric with the same name already exists a unique identifier will be generated. required value Union[numpy.ndarray, float, int] The value for the metric. required Source code in elegy/hooks.py def add_metric ( name : str , value : types . Scalar ) -> None : \"\"\" A hook that lets you define a metric within a [`module`][elegy.module.Module]. ```python y = jax.nn.relu(x) elegy.hooks.add_metric(\"activation_mean\", jnp.mean(y)) ``` Arguments: name: The name of the loss. If a metric with the same `name` already exists a unique identifier will be generated. value: The value for the metric. \"\"\" if LOCAL . metrics is None : return # name = f\"{base_name()}/{name}\" name = utils . get_unique_name ( set ( LOCAL . metrics ), name ) LOCAL . metrics [ name ] = value","title":"add_metric"},{"location":"api/hooks/add_metric/#elegyhooksadd_metric","text":"","title":"elegy.hooks.add_metric"},{"location":"api/hooks/add_metric/#elegy.hooks.add_metric","text":"A hook that lets you define a metric within a module . y = jax . nn . relu(x) elegy . hooks . add_metric( \"activation_mean\" , jnp . mean(y)) Parameters: Name Type Description Default name str The name of the loss. If a metric with the same name already exists a unique identifier will be generated. required value Union[numpy.ndarray, float, int] The value for the metric. required Source code in elegy/hooks.py def add_metric ( name : str , value : types . Scalar ) -> None : \"\"\" A hook that lets you define a metric within a [`module`][elegy.module.Module]. ```python y = jax.nn.relu(x) elegy.hooks.add_metric(\"activation_mean\", jnp.mean(y)) ``` Arguments: name: The name of the loss. If a metric with the same `name` already exists a unique identifier will be generated. value: The value for the metric. \"\"\" if LOCAL . metrics is None : return # name = f\"{base_name()}/{name}\" name = utils . get_unique_name ( set ( LOCAL . metrics ), name ) LOCAL . metrics [ name ] = value","title":"elegy.hooks.add_metric"},{"location":"api/hooks/add_summary/","text":"elegy.hooks.add_summary A hook that lets you define a summary in the current module. Its primary use is to keep track of certain values as they flow through the network so Model.summary can show a representation of architecture. def call ( self , x): ... y = jax . nn . relu(x) elegy . hooks . add_summary( \"relu\" , y) ... Parameters: Name Type Description Default module_or_name The name of the summary or alternatively the module that this summary will represent. If a summary with the same name already exists a unique identifier will be generated. required value Any The value for the summary. required Source code in elegy/hooks.py def add_summary ( path : types . Path , module : tp . Any , value : tp . Any , ) -> None : \"\"\" A hook that lets you define a summary in the current module. Its primary use is to keep track of certain values as they flow through the network so [`Model.summary`][elegy.model.model.Model.summary] can show a representation of architecture. ```python def call(self, x): ... y = jax.nn.relu(x) elegy.hooks.add_summary(\"relu\", y) ... ``` Arguments: module_or_name: The name of the summary or alternatively the module that this summary will represent. If a summary with the same name already exists a unique identifier will be generated. value: The value for the summary. \"\"\" if not summaries_active (): return LOCAL . summaries . append ( types . Summary ( path , module , value ))","title":"add_summary"},{"location":"api/hooks/add_summary/#elegyhooksadd_summary","text":"","title":"elegy.hooks.add_summary"},{"location":"api/hooks/add_summary/#elegy.hooks.add_summary","text":"A hook that lets you define a summary in the current module. Its primary use is to keep track of certain values as they flow through the network so Model.summary can show a representation of architecture. def call ( self , x): ... y = jax . nn . relu(x) elegy . hooks . add_summary( \"relu\" , y) ... Parameters: Name Type Description Default module_or_name The name of the summary or alternatively the module that this summary will represent. If a summary with the same name already exists a unique identifier will be generated. required value Any The value for the summary. required Source code in elegy/hooks.py def add_summary ( path : types . Path , module : tp . Any , value : tp . Any , ) -> None : \"\"\" A hook that lets you define a summary in the current module. Its primary use is to keep track of certain values as they flow through the network so [`Model.summary`][elegy.model.model.Model.summary] can show a representation of architecture. ```python def call(self, x): ... y = jax.nn.relu(x) elegy.hooks.add_summary(\"relu\", y) ... ``` Arguments: module_or_name: The name of the summary or alternatively the module that this summary will represent. If a summary with the same name already exists a unique identifier will be generated. value: The value for the summary. \"\"\" if not summaries_active (): return LOCAL . summaries . append ( types . Summary ( path , module , value ))","title":"elegy.hooks.add_summary"},{"location":"api/hooks/context/","text":"elegy.hooks.context Source code in elegy/hooks.py def context ( * , losses : tp . Union [ types . Logs , bool , None ] = None , metrics : tp . Union [ types . Logs , bool , None ] = None , summaries : tp . Union [ types . Summaries , bool , None ] = None , set_all : bool = False , ) -> tp . ContextManager [ None ]: if set_all : if losses is None : losses = True if metrics is None : metrics = True if summaries is None : summaries = True if isinstance ( losses , bool ): losses = {} if losses else None if isinstance ( metrics , bool ): metrics = {} if metrics else None if isinstance ( summaries , bool ): summaries = [] if summaries else None return _context ( losses = losses , metrics = metrics , summaries = summaries , )","title":"context"},{"location":"api/hooks/context/#elegyhookscontext","text":"","title":"elegy.hooks.context"},{"location":"api/hooks/context/#elegy.hooks.context","text":"Source code in elegy/hooks.py def context ( * , losses : tp . Union [ types . Logs , bool , None ] = None , metrics : tp . Union [ types . Logs , bool , None ] = None , summaries : tp . Union [ types . Summaries , bool , None ] = None , set_all : bool = False , ) -> tp . ContextManager [ None ]: if set_all : if losses is None : losses = True if metrics is None : metrics = True if summaries is None : summaries = True if isinstance ( losses , bool ): losses = {} if losses else None if isinstance ( metrics , bool ): metrics = {} if metrics else None if isinstance ( summaries , bool ): summaries = [] if summaries else None return _context ( losses = losses , metrics = metrics , summaries = summaries , )","title":"elegy.hooks.context"},{"location":"api/hooks/get_losses/","text":"elegy.hooks.get_losses Source code in elegy/hooks.py def get_losses () -> types . Logs : if LOCAL . losses is None : return {} return LOCAL . losses . copy ()","title":"get_losses"},{"location":"api/hooks/get_losses/#elegyhooksget_losses","text":"","title":"elegy.hooks.get_losses"},{"location":"api/hooks/get_losses/#elegy.hooks.get_losses","text":"Source code in elegy/hooks.py def get_losses () -> types . Logs : if LOCAL . losses is None : return {} return LOCAL . losses . copy ()","title":"elegy.hooks.get_losses"},{"location":"api/hooks/get_metrics/","text":"elegy.hooks.get_metrics Source code in elegy/hooks.py def get_metrics () -> types . Logs : if LOCAL . metrics is None : return {} return LOCAL . metrics . copy ()","title":"get_metrics"},{"location":"api/hooks/get_metrics/#elegyhooksget_metrics","text":"","title":"elegy.hooks.get_metrics"},{"location":"api/hooks/get_metrics/#elegy.hooks.get_metrics","text":"Source code in elegy/hooks.py def get_metrics () -> types . Logs : if LOCAL . metrics is None : return {} return LOCAL . metrics . copy ()","title":"elegy.hooks.get_metrics"},{"location":"api/hooks/get_summaries/","text":"elegy.hooks.get_summaries Source code in elegy/hooks.py def get_summaries () -> types . Summaries : if LOCAL . summaries is None : return [] return LOCAL . summaries . copy ()","title":"get_summaries"},{"location":"api/hooks/get_summaries/#elegyhooksget_summaries","text":"","title":"elegy.hooks.get_summaries"},{"location":"api/hooks/get_summaries/#elegy.hooks.get_summaries","text":"Source code in elegy/hooks.py def get_summaries () -> types . Summaries : if LOCAL . summaries is None : return [] return LOCAL . summaries . copy ()","title":"elegy.hooks.get_summaries"},{"location":"api/hooks/jit/","text":"elegy.hooks.jit Source code in elegy/hooks.py def jit ( f , ** kwargs , ): def _transform_fn ( * args , ) -> TransformtOutput : # extract input context dynamic_args : DynamicArgs static_args : StaticArgs # static_args is unused because they dont need to be set back static_args , dynamic_args = args [: 2 ] # get from beginning args = args [ 2 :] ( losses , metrics , summaries ) = dynamic_args # perform updates _update_local_context ( losses , metrics , summaries ) # call output = f ( * args ) # add outputs context return TransformtOutput ( output = output , losses = LOCAL . losses , metrics = LOCAL . metrics , summaries = LOCAL . summaries , ) # transform kwargs static_argnums = kwargs . pop ( \"static_argnums\" , []) if isinstance ( static_argnums , int ): static_argnums = [ static_argnums ] static_argnums = [ 0 ] + [ i + 2 for i in static_argnums ] # make fn transform_fn = jax . jit ( _transform_fn , static_argnums = static_argnums , ** kwargs , ) @functools . wraps ( f ) def wrapper ( * args ): # add input context dynamic_args = DynamicArgs ( losses = LOCAL . losses , metrics = LOCAL . metrics , summaries = LOCAL . summaries , ) static_args = StaticArgs () # put them first because of static_args args = ( static_args , dynamic_args ) + args # call and patch ( output , losses , metrics , summaries , ) = transform_fn ( * args ) # perform updates _update_local_context ( losses , metrics , summaries ) return output return wrapper","title":"jit"},{"location":"api/hooks/jit/#elegyhooksjit","text":"","title":"elegy.hooks.jit"},{"location":"api/hooks/jit/#elegy.hooks.jit","text":"Source code in elegy/hooks.py def jit ( f , ** kwargs , ): def _transform_fn ( * args , ) -> TransformtOutput : # extract input context dynamic_args : DynamicArgs static_args : StaticArgs # static_args is unused because they dont need to be set back static_args , dynamic_args = args [: 2 ] # get from beginning args = args [ 2 :] ( losses , metrics , summaries ) = dynamic_args # perform updates _update_local_context ( losses , metrics , summaries ) # call output = f ( * args ) # add outputs context return TransformtOutput ( output = output , losses = LOCAL . losses , metrics = LOCAL . metrics , summaries = LOCAL . summaries , ) # transform kwargs static_argnums = kwargs . pop ( \"static_argnums\" , []) if isinstance ( static_argnums , int ): static_argnums = [ static_argnums ] static_argnums = [ 0 ] + [ i + 2 for i in static_argnums ] # make fn transform_fn = jax . jit ( _transform_fn , static_argnums = static_argnums , ** kwargs , ) @functools . wraps ( f ) def wrapper ( * args ): # add input context dynamic_args = DynamicArgs ( losses = LOCAL . losses , metrics = LOCAL . metrics , summaries = LOCAL . summaries , ) static_args = StaticArgs () # put them first because of static_args args = ( static_args , dynamic_args ) + args # call and patch ( output , losses , metrics , summaries , ) = transform_fn ( * args ) # perform updates _update_local_context ( losses , metrics , summaries ) return output return wrapper","title":"elegy.hooks.jit"},{"location":"api/hooks/losses_active/","text":"elegy.hooks.losses_active Source code in elegy/hooks.py def losses_active () -> bool : return LOCAL . losses is not None","title":"losses_active"},{"location":"api/hooks/losses_active/#elegyhookslosses_active","text":"","title":"elegy.hooks.losses_active"},{"location":"api/hooks/losses_active/#elegy.hooks.losses_active","text":"Source code in elegy/hooks.py def losses_active () -> bool : return LOCAL . losses is not None","title":"elegy.hooks.losses_active"},{"location":"api/hooks/metrics_active/","text":"elegy.hooks.metrics_active Source code in elegy/hooks.py def metrics_active () -> bool : return LOCAL . metrics is not None","title":"metrics_active"},{"location":"api/hooks/metrics_active/#elegyhooksmetrics_active","text":"","title":"elegy.hooks.metrics_active"},{"location":"api/hooks/metrics_active/#elegy.hooks.metrics_active","text":"Source code in elegy/hooks.py def metrics_active () -> bool : return LOCAL . metrics is not None","title":"elegy.hooks.metrics_active"},{"location":"api/hooks/summaries_active/","text":"elegy.hooks.summaries_active Source code in elegy/hooks.py def summaries_active () -> bool : return LOCAL . summaries is not None","title":"summaries_active"},{"location":"api/hooks/summaries_active/#elegyhookssummaries_active","text":"","title":"elegy.hooks.summaries_active"},{"location":"api/hooks/summaries_active/#elegy.hooks.summaries_active","text":"Source code in elegy/hooks.py def summaries_active () -> bool : return LOCAL . summaries is not None","title":"elegy.hooks.summaries_active"},{"location":"api/initializers/Constant/","text":"elegy.initializers.Constant Initializes with a constant. __class__ inherited __base__ inherited Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). __instancecheck__ ( cls , instance ) special Override for isinstance(instance, cls). Source code in elegy/initializers.py def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) __new__ ( mcls , name , bases , namespace , ** kwargs ) special staticmethod Create and return a new object. See help(type) for accurate signature. Source code in elegy/initializers.py def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls __subclasscheck__ ( cls , subclass ) special Override for issubclass(subclass, cls). Source code in elegy/initializers.py def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) register ( cls , subclass ) Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) register ( cls , subclass ) inherited Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) __init__ ( self , constant ) special Constructs a Constant initializer. Parameters: Name Type Description Default constant Constant to initialize with. required Source code in elegy/initializers.py def __init__ ( self , constant ): \"\"\" Constructs a Constant initializer. Args: constant: Constant to initialize with. \"\"\" self . constant = constant","title":"Constant"},{"location":"api/initializers/Constant/#elegyinitializersconstant","text":"","title":"elegy.initializers.Constant"},{"location":"api/initializers/Constant/#elegy.initializers.Constant","text":"Initializes with a constant.","title":"elegy.initializers.Constant"},{"location":"api/initializers/Constant/#elegy.initializers.Constant.__class__","text":"","title":"__class__"},{"location":"api/initializers/Constant/#elegy.initializers.Constant.__class__.__base__","text":"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()).","title":"__base__"},{"location":"api/initializers/Constant/#elegy.initializers.Constant.__class__.__base__.__instancecheck__","text":"Override for isinstance(instance, cls). Source code in elegy/initializers.py def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance )","title":"__instancecheck__()"},{"location":"api/initializers/Constant/#elegy.initializers.Constant.__class__.__base__.__new__","text":"Create and return a new object. See help(type) for accurate signature. Source code in elegy/initializers.py def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls","title":"__new__()"},{"location":"api/initializers/Constant/#elegy.initializers.Constant.__class__.__base__.__subclasscheck__","text":"Override for issubclass(subclass, cls). Source code in elegy/initializers.py def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass )","title":"__subclasscheck__()"},{"location":"api/initializers/Constant/#elegy.initializers.Constant.__class__.__base__.register","text":"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"register()"},{"location":"api/initializers/Constant/#elegy.initializers.Constant.__class__.register","text":"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"register()"},{"location":"api/initializers/Constant/#elegy.initializers.Constant.__init__","text":"Constructs a Constant initializer. Parameters: Name Type Description Default constant Constant to initialize with. required Source code in elegy/initializers.py def __init__ ( self , constant ): \"\"\" Constructs a Constant initializer. Args: constant: Constant to initialize with. \"\"\" self . constant = constant","title":"__init__()"},{"location":"api/initializers/Orthogonal/","text":"elegy.initializers.Orthogonal Uniform scaling initializer. __class__ inherited __base__ inherited Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). __instancecheck__ ( cls , instance ) special Override for isinstance(instance, cls). Source code in elegy/initializers.py def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) __new__ ( mcls , name , bases , namespace , ** kwargs ) special staticmethod Create and return a new object. See help(type) for accurate signature. Source code in elegy/initializers.py def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls __subclasscheck__ ( cls , subclass ) special Override for issubclass(subclass, cls). Source code in elegy/initializers.py def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) register ( cls , subclass ) Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) register ( cls , subclass ) inherited Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) __init__ ( self , scale = 1.0 , axis =- 1 ) special Construct an initializer for uniformly distributed orthogonal matrices. These matrices will be row-orthonormal along the access specified by axis . If the rank of the weight is greater than 2, the shape will be flattened in all other dimensions and then will be row-orthonormal along the final dimension. Note that this only works if the axis dimension is larger, otherwise the matrix will be transposed (equivalently, it will be column orthonormal instead of row orthonormal). Parameters: Name Type Description Default scale Scale factor. 1.0 axis int Which axis corresponds to the \"output dimension\" of the tensor. -1 If the shape is not square, the matrices will have orthonormal rows or columns depending on which side is smaller. Source code in elegy/initializers.py def __init__ ( self , scale = 1.0 , axis =- 1 ): \"\"\" Construct an initializer for uniformly distributed orthogonal matrices. These matrices will be row-orthonormal along the access specified by `axis`. If the rank of the weight is greater than 2, the shape will be flattened in all other dimensions and then will be row-orthonormal along the final dimension. Note that this only works if the `axis` dimension is larger, otherwise the matrix will be transposed (equivalently, it will be column orthonormal instead of row orthonormal). Args: scale: Scale factor. axis (int): Which axis corresponds to the \"output dimension\" of the tensor. If the shape is not square, the matrices will have orthonormal rows or columns depending on which side is smaller. \"\"\" self . scale = scale self . axis = axis","title":"Orthogonal"},{"location":"api/initializers/Orthogonal/#elegyinitializersorthogonal","text":"","title":"elegy.initializers.Orthogonal"},{"location":"api/initializers/Orthogonal/#elegy.initializers.Orthogonal","text":"Uniform scaling initializer.","title":"elegy.initializers.Orthogonal"},{"location":"api/initializers/Orthogonal/#elegy.initializers.Orthogonal.__class__","text":"","title":"__class__"},{"location":"api/initializers/Orthogonal/#elegy.initializers.Orthogonal.__class__.__base__","text":"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()).","title":"__base__"},{"location":"api/initializers/Orthogonal/#elegy.initializers.Orthogonal.__class__.__base__.__instancecheck__","text":"Override for isinstance(instance, cls). Source code in elegy/initializers.py def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance )","title":"__instancecheck__()"},{"location":"api/initializers/Orthogonal/#elegy.initializers.Orthogonal.__class__.__base__.__new__","text":"Create and return a new object. See help(type) for accurate signature. Source code in elegy/initializers.py def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls","title":"__new__()"},{"location":"api/initializers/Orthogonal/#elegy.initializers.Orthogonal.__class__.__base__.__subclasscheck__","text":"Override for issubclass(subclass, cls). Source code in elegy/initializers.py def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass )","title":"__subclasscheck__()"},{"location":"api/initializers/Orthogonal/#elegy.initializers.Orthogonal.__class__.__base__.register","text":"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"register()"},{"location":"api/initializers/Orthogonal/#elegy.initializers.Orthogonal.__class__.register","text":"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"register()"},{"location":"api/initializers/Orthogonal/#elegy.initializers.Orthogonal.__init__","text":"Construct an initializer for uniformly distributed orthogonal matrices. These matrices will be row-orthonormal along the access specified by axis . If the rank of the weight is greater than 2, the shape will be flattened in all other dimensions and then will be row-orthonormal along the final dimension. Note that this only works if the axis dimension is larger, otherwise the matrix will be transposed (equivalently, it will be column orthonormal instead of row orthonormal). Parameters: Name Type Description Default scale Scale factor. 1.0 axis int Which axis corresponds to the \"output dimension\" of the tensor. -1 If the shape is not square, the matrices will have orthonormal rows or columns depending on which side is smaller. Source code in elegy/initializers.py def __init__ ( self , scale = 1.0 , axis =- 1 ): \"\"\" Construct an initializer for uniformly distributed orthogonal matrices. These matrices will be row-orthonormal along the access specified by `axis`. If the rank of the weight is greater than 2, the shape will be flattened in all other dimensions and then will be row-orthonormal along the final dimension. Note that this only works if the `axis` dimension is larger, otherwise the matrix will be transposed (equivalently, it will be column orthonormal instead of row orthonormal). Args: scale: Scale factor. axis (int): Which axis corresponds to the \"output dimension\" of the tensor. If the shape is not square, the matrices will have orthonormal rows or columns depending on which side is smaller. \"\"\" self . scale = scale self . axis = axis","title":"__init__()"},{"location":"api/initializers/RandomNormal/","text":"elegy.initializers.RandomNormal Initializes by sampling from a normal distribution. __class__ inherited __base__ inherited Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). __instancecheck__ ( cls , instance ) special Override for isinstance(instance, cls). Source code in elegy/initializers.py def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) __new__ ( mcls , name , bases , namespace , ** kwargs ) special staticmethod Create and return a new object. See help(type) for accurate signature. Source code in elegy/initializers.py def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls __subclasscheck__ ( cls , subclass ) special Override for issubclass(subclass, cls). Source code in elegy/initializers.py def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) register ( cls , subclass ) Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) register ( cls , subclass ) inherited Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) __init__ ( self , stddev = 1.0 , mean = 0.0 ) special Constructs a RandomNormal initializer. Parameters: Name Type Description Default stddev The standard deviation of the normal distribution to sample from. 1.0 mean The mean of the normal distribution to sample from. 0.0 Source code in elegy/initializers.py def __init__ ( self , stddev = 1.0 , mean = 0.0 ): \"\"\" Constructs a RandomNormal initializer. Args: stddev: The standard deviation of the normal distribution to sample from. mean: The mean of the normal distribution to sample from. \"\"\" self . stddev = stddev self . mean = mean","title":"RandomNormal"},{"location":"api/initializers/RandomNormal/#elegyinitializersrandomnormal","text":"","title":"elegy.initializers.RandomNormal"},{"location":"api/initializers/RandomNormal/#elegy.initializers.RandomNormal","text":"Initializes by sampling from a normal distribution.","title":"elegy.initializers.RandomNormal"},{"location":"api/initializers/RandomNormal/#elegy.initializers.RandomNormal.__class__","text":"","title":"__class__"},{"location":"api/initializers/RandomNormal/#elegy.initializers.RandomNormal.__class__.__base__","text":"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()).","title":"__base__"},{"location":"api/initializers/RandomNormal/#elegy.initializers.RandomNormal.__class__.__base__.__instancecheck__","text":"Override for isinstance(instance, cls). Source code in elegy/initializers.py def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance )","title":"__instancecheck__()"},{"location":"api/initializers/RandomNormal/#elegy.initializers.RandomNormal.__class__.__base__.__new__","text":"Create and return a new object. See help(type) for accurate signature. Source code in elegy/initializers.py def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls","title":"__new__()"},{"location":"api/initializers/RandomNormal/#elegy.initializers.RandomNormal.__class__.__base__.__subclasscheck__","text":"Override for issubclass(subclass, cls). Source code in elegy/initializers.py def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass )","title":"__subclasscheck__()"},{"location":"api/initializers/RandomNormal/#elegy.initializers.RandomNormal.__class__.__base__.register","text":"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"register()"},{"location":"api/initializers/RandomNormal/#elegy.initializers.RandomNormal.__class__.register","text":"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"register()"},{"location":"api/initializers/RandomNormal/#elegy.initializers.RandomNormal.__init__","text":"Constructs a RandomNormal initializer. Parameters: Name Type Description Default stddev The standard deviation of the normal distribution to sample from. 1.0 mean The mean of the normal distribution to sample from. 0.0 Source code in elegy/initializers.py def __init__ ( self , stddev = 1.0 , mean = 0.0 ): \"\"\" Constructs a RandomNormal initializer. Args: stddev: The standard deviation of the normal distribution to sample from. mean: The mean of the normal distribution to sample from. \"\"\" self . stddev = stddev self . mean = mean","title":"__init__()"},{"location":"api/initializers/RandomUniform/","text":"elegy.initializers.RandomUniform Initializes by sampling from a uniform distribution. __class__ inherited __base__ inherited Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). __instancecheck__ ( cls , instance ) special Override for isinstance(instance, cls). Source code in elegy/initializers.py def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) __new__ ( mcls , name , bases , namespace , ** kwargs ) special staticmethod Create and return a new object. See help(type) for accurate signature. Source code in elegy/initializers.py def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls __subclasscheck__ ( cls , subclass ) special Override for issubclass(subclass, cls). Source code in elegy/initializers.py def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) register ( cls , subclass ) Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) register ( cls , subclass ) inherited Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) __init__ ( self , minval = 0.0 , maxval = 1.0 ) special Constructs a RandomUniform initializer. Parameters: Name Type Description Default minval The lower limit of the uniform distribution. 0.0 maxval The upper limit of the uniform distribution. 1.0 Source code in elegy/initializers.py def __init__ ( self , minval = 0.0 , maxval = 1.0 ): \"\"\" Constructs a RandomUniform initializer. Args: minval: The lower limit of the uniform distribution. maxval: The upper limit of the uniform distribution. \"\"\" self . minval = minval self . maxval = maxval","title":"RandomUniform"},{"location":"api/initializers/RandomUniform/#elegyinitializersrandomuniform","text":"","title":"elegy.initializers.RandomUniform"},{"location":"api/initializers/RandomUniform/#elegy.initializers.RandomUniform","text":"Initializes by sampling from a uniform distribution.","title":"elegy.initializers.RandomUniform"},{"location":"api/initializers/RandomUniform/#elegy.initializers.RandomUniform.__class__","text":"","title":"__class__"},{"location":"api/initializers/RandomUniform/#elegy.initializers.RandomUniform.__class__.__base__","text":"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()).","title":"__base__"},{"location":"api/initializers/RandomUniform/#elegy.initializers.RandomUniform.__class__.__base__.__instancecheck__","text":"Override for isinstance(instance, cls). Source code in elegy/initializers.py def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance )","title":"__instancecheck__()"},{"location":"api/initializers/RandomUniform/#elegy.initializers.RandomUniform.__class__.__base__.__new__","text":"Create and return a new object. See help(type) for accurate signature. Source code in elegy/initializers.py def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls","title":"__new__()"},{"location":"api/initializers/RandomUniform/#elegy.initializers.RandomUniform.__class__.__base__.__subclasscheck__","text":"Override for issubclass(subclass, cls). Source code in elegy/initializers.py def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass )","title":"__subclasscheck__()"},{"location":"api/initializers/RandomUniform/#elegy.initializers.RandomUniform.__class__.__base__.register","text":"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"register()"},{"location":"api/initializers/RandomUniform/#elegy.initializers.RandomUniform.__class__.register","text":"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"register()"},{"location":"api/initializers/RandomUniform/#elegy.initializers.RandomUniform.__init__","text":"Constructs a RandomUniform initializer. Parameters: Name Type Description Default minval The lower limit of the uniform distribution. 0.0 maxval The upper limit of the uniform distribution. 1.0 Source code in elegy/initializers.py def __init__ ( self , minval = 0.0 , maxval = 1.0 ): \"\"\" Constructs a RandomUniform initializer. Args: minval: The lower limit of the uniform distribution. maxval: The upper limit of the uniform distribution. \"\"\" self . minval = minval self . maxval = maxval","title":"__init__()"},{"location":"api/initializers/TruncatedNormal/","text":"elegy.initializers.TruncatedNormal Initializes by sampling from a truncated normal distribution. __class__ inherited __base__ inherited Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). __instancecheck__ ( cls , instance ) special Override for isinstance(instance, cls). Source code in elegy/initializers.py def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) __new__ ( mcls , name , bases , namespace , ** kwargs ) special staticmethod Create and return a new object. See help(type) for accurate signature. Source code in elegy/initializers.py def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls __subclasscheck__ ( cls , subclass ) special Override for issubclass(subclass, cls). Source code in elegy/initializers.py def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) register ( cls , subclass ) Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) register ( cls , subclass ) inherited Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) __init__ ( self , stddev = 1.0 , mean = 0.0 ) special Constructs a RandomNormal initializer. Parameters: Name Type Description Default stddev The standard deviation parameter of the truncated normal distribution. 1.0 mean The mean of the truncated normal distribution. 0.0 Source code in elegy/initializers.py def __init__ ( self , stddev = 1.0 , mean = 0.0 ): \"\"\" Constructs a RandomNormal initializer. Args: stddev: The standard deviation parameter of the truncated normal distribution. mean: The mean of the truncated normal distribution. \"\"\" self . stddev = stddev self . mean = mean","title":"TruncatedNormal"},{"location":"api/initializers/TruncatedNormal/#elegyinitializerstruncatednormal","text":"","title":"elegy.initializers.TruncatedNormal"},{"location":"api/initializers/TruncatedNormal/#elegy.initializers.TruncatedNormal","text":"Initializes by sampling from a truncated normal distribution.","title":"elegy.initializers.TruncatedNormal"},{"location":"api/initializers/TruncatedNormal/#elegy.initializers.TruncatedNormal.__class__","text":"","title":"__class__"},{"location":"api/initializers/TruncatedNormal/#elegy.initializers.TruncatedNormal.__class__.__base__","text":"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()).","title":"__base__"},{"location":"api/initializers/TruncatedNormal/#elegy.initializers.TruncatedNormal.__class__.__base__.__instancecheck__","text":"Override for isinstance(instance, cls). Source code in elegy/initializers.py def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance )","title":"__instancecheck__()"},{"location":"api/initializers/TruncatedNormal/#elegy.initializers.TruncatedNormal.__class__.__base__.__new__","text":"Create and return a new object. See help(type) for accurate signature. Source code in elegy/initializers.py def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls","title":"__new__()"},{"location":"api/initializers/TruncatedNormal/#elegy.initializers.TruncatedNormal.__class__.__base__.__subclasscheck__","text":"Override for issubclass(subclass, cls). Source code in elegy/initializers.py def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass )","title":"__subclasscheck__()"},{"location":"api/initializers/TruncatedNormal/#elegy.initializers.TruncatedNormal.__class__.__base__.register","text":"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"register()"},{"location":"api/initializers/TruncatedNormal/#elegy.initializers.TruncatedNormal.__class__.register","text":"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"register()"},{"location":"api/initializers/TruncatedNormal/#elegy.initializers.TruncatedNormal.__init__","text":"Constructs a RandomNormal initializer. Parameters: Name Type Description Default stddev The standard deviation parameter of the truncated normal distribution. 1.0 mean The mean of the truncated normal distribution. 0.0 Source code in elegy/initializers.py def __init__ ( self , stddev = 1.0 , mean = 0.0 ): \"\"\" Constructs a RandomNormal initializer. Args: stddev: The standard deviation parameter of the truncated normal distribution. mean: The mean of the truncated normal distribution. \"\"\" self . stddev = stddev self . mean = mean","title":"__init__()"},{"location":"api/initializers/UniformScaling/","text":"elegy.initializers.UniformScaling Uniform scaling initializer. Initializes by sampling from a uniform distribution, but with the variance scaled by the inverse square root of the number of input units, multiplied by the scale. __class__ inherited __base__ inherited Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). __instancecheck__ ( cls , instance ) special Override for isinstance(instance, cls). Source code in elegy/initializers.py def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) __new__ ( mcls , name , bases , namespace , ** kwargs ) special staticmethod Create and return a new object. See help(type) for accurate signature. Source code in elegy/initializers.py def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls __subclasscheck__ ( cls , subclass ) special Override for issubclass(subclass, cls). Source code in elegy/initializers.py def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) register ( cls , subclass ) Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) register ( cls , subclass ) inherited Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) __init__ ( self , scale = 1.0 ) special Constructs the UniformScaling initializer. Parameters: Name Type Description Default scale Scale to multiply the upper limit of the uniform distribution by. 1.0 Source code in elegy/initializers.py def __init__ ( self , scale = 1.0 ): \"\"\"Constructs the UniformScaling initializer. Args: scale: Scale to multiply the upper limit of the uniform distribution by. \"\"\" self . scale = scale","title":"UniformScaling"},{"location":"api/initializers/UniformScaling/#elegyinitializersuniformscaling","text":"","title":"elegy.initializers.UniformScaling"},{"location":"api/initializers/UniformScaling/#elegy.initializers.UniformScaling","text":"Uniform scaling initializer. Initializes by sampling from a uniform distribution, but with the variance scaled by the inverse square root of the number of input units, multiplied by the scale.","title":"elegy.initializers.UniformScaling"},{"location":"api/initializers/UniformScaling/#elegy.initializers.UniformScaling.__class__","text":"","title":"__class__"},{"location":"api/initializers/UniformScaling/#elegy.initializers.UniformScaling.__class__.__base__","text":"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()).","title":"__base__"},{"location":"api/initializers/UniformScaling/#elegy.initializers.UniformScaling.__class__.__base__.__instancecheck__","text":"Override for isinstance(instance, cls). Source code in elegy/initializers.py def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance )","title":"__instancecheck__()"},{"location":"api/initializers/UniformScaling/#elegy.initializers.UniformScaling.__class__.__base__.__new__","text":"Create and return a new object. See help(type) for accurate signature. Source code in elegy/initializers.py def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls","title":"__new__()"},{"location":"api/initializers/UniformScaling/#elegy.initializers.UniformScaling.__class__.__base__.__subclasscheck__","text":"Override for issubclass(subclass, cls). Source code in elegy/initializers.py def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass )","title":"__subclasscheck__()"},{"location":"api/initializers/UniformScaling/#elegy.initializers.UniformScaling.__class__.__base__.register","text":"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"register()"},{"location":"api/initializers/UniformScaling/#elegy.initializers.UniformScaling.__class__.register","text":"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"register()"},{"location":"api/initializers/UniformScaling/#elegy.initializers.UniformScaling.__init__","text":"Constructs the UniformScaling initializer. Parameters: Name Type Description Default scale Scale to multiply the upper limit of the uniform distribution by. 1.0 Source code in elegy/initializers.py def __init__ ( self , scale = 1.0 ): \"\"\"Constructs the UniformScaling initializer. Args: scale: Scale to multiply the upper limit of the uniform distribution by. \"\"\" self . scale = scale","title":"__init__()"},{"location":"api/initializers/VarianceScaling/","text":"elegy.initializers.VarianceScaling types.Initializer which adapts its scale to the shape of the initialized array. The initializer first computes the scaling factor s = scale / n , where n is: Number of input units in the weight tensor, if mode = fan_in . Number of output units, if mode = fan_out . Average of the numbers of input and output units, if mode = fan_avg . Then, with distribution=\"truncated_normal\" or \"normal\" , samples are drawn from a distribution with a mean of zero and a standard deviation (after truncation, if used) stddev = sqrt(s) . With distribution=uniform , samples are drawn from a uniform distribution within [-limit, limit] , with limit = sqrt(3 * s) . The variance scaling initializer can be configured to generate other standard initializers using the scale, mode and distribution arguments. Here are some example configurations: ============== ============================================================== Name Parameters ============== ============================================================== glorot_uniform scale=1.0, mode= fan_avg , distribution= uniform glorot_normal scale=1.0, mode= fan_avg , distribution= truncated_normal lecun_uniform scale=1.0, mode= fan_in , distribution= uniform lecun_normal scale=1.0, mode= fan_in , distribution= truncated_normal he_uniform scale=2.0, mode= fan_in , distribution= uniform he_normal scale=2.0, mode= fan_in , distribution= truncated_normal ============== ============================================================== __class__ inherited __base__ inherited Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). __instancecheck__ ( cls , instance ) special Override for isinstance(instance, cls). Source code in elegy/initializers.py def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) __new__ ( mcls , name , bases , namespace , ** kwargs ) special staticmethod Create and return a new object. See help(type) for accurate signature. Source code in elegy/initializers.py def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls __subclasscheck__ ( cls , subclass ) special Override for issubclass(subclass, cls). Source code in elegy/initializers.py def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) register ( cls , subclass ) Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) register ( cls , subclass ) inherited Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) __init__ ( self , scale = 1.0 , mode = 'fan_in' , distribution = 'truncated_normal' ) special Constructs the VarianceScaling initializer. Parameters: Name Type Description Default scale Scale to multiply the variance by. 1.0 mode One of fan_in , fan_out , fan_avg 'fan_in' distribution Random distribution to use. One of truncated_normal , normal or uniform . 'truncated_normal' Source code in elegy/initializers.py def __init__ ( self , scale = 1.0 , mode = \"fan_in\" , distribution = \"truncated_normal\" ): \"\"\" Constructs the VarianceScaling initializer. Args: scale: Scale to multiply the variance by. mode: One of ``fan_in``, ``fan_out``, ``fan_avg`` distribution: Random distribution to use. One of ``truncated_normal``, ``normal`` or ``uniform``. \"\"\" if scale <= 0.0 : raise ValueError ( \"`scale` must be a positive float.\" ) if mode not in { \"fan_in\" , \"fan_out\" , \"fan_avg\" }: raise ValueError ( \"Invalid `mode` argument:\" , mode ) distribution = distribution . lower () if distribution not in { \"normal\" , \"truncated_normal\" , \"uniform\" }: raise ValueError ( \"Invalid `distribution` argument:\" , distribution ) self . scale = scale self . mode = mode self . distribution = distribution","title":"VarianceScaling"},{"location":"api/initializers/VarianceScaling/#elegyinitializersvariancescaling","text":"","title":"elegy.initializers.VarianceScaling"},{"location":"api/initializers/VarianceScaling/#elegy.initializers.VarianceScaling","text":"types.Initializer which adapts its scale to the shape of the initialized array. The initializer first computes the scaling factor s = scale / n , where n is: Number of input units in the weight tensor, if mode = fan_in . Number of output units, if mode = fan_out . Average of the numbers of input and output units, if mode = fan_avg . Then, with distribution=\"truncated_normal\" or \"normal\" , samples are drawn from a distribution with a mean of zero and a standard deviation (after truncation, if used) stddev = sqrt(s) . With distribution=uniform , samples are drawn from a uniform distribution within [-limit, limit] , with limit = sqrt(3 * s) . The variance scaling initializer can be configured to generate other standard initializers using the scale, mode and distribution arguments. Here are some example configurations: ============== ============================================================== Name Parameters ============== ============================================================== glorot_uniform scale=1.0, mode= fan_avg , distribution= uniform glorot_normal scale=1.0, mode= fan_avg , distribution= truncated_normal lecun_uniform scale=1.0, mode= fan_in , distribution= uniform lecun_normal scale=1.0, mode= fan_in , distribution= truncated_normal he_uniform scale=2.0, mode= fan_in , distribution= uniform he_normal scale=2.0, mode= fan_in , distribution= truncated_normal ============== ==============================================================","title":"elegy.initializers.VarianceScaling"},{"location":"api/initializers/VarianceScaling/#elegy.initializers.VarianceScaling.__class__","text":"","title":"__class__"},{"location":"api/initializers/VarianceScaling/#elegy.initializers.VarianceScaling.__class__.__base__","text":"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()).","title":"__base__"},{"location":"api/initializers/VarianceScaling/#elegy.initializers.VarianceScaling.__class__.__base__.__instancecheck__","text":"Override for isinstance(instance, cls). Source code in elegy/initializers.py def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance )","title":"__instancecheck__()"},{"location":"api/initializers/VarianceScaling/#elegy.initializers.VarianceScaling.__class__.__base__.__new__","text":"Create and return a new object. See help(type) for accurate signature. Source code in elegy/initializers.py def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls","title":"__new__()"},{"location":"api/initializers/VarianceScaling/#elegy.initializers.VarianceScaling.__class__.__base__.__subclasscheck__","text":"Override for issubclass(subclass, cls). Source code in elegy/initializers.py def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass )","title":"__subclasscheck__()"},{"location":"api/initializers/VarianceScaling/#elegy.initializers.VarianceScaling.__class__.__base__.register","text":"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"register()"},{"location":"api/initializers/VarianceScaling/#elegy.initializers.VarianceScaling.__class__.register","text":"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in elegy/initializers.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"register()"},{"location":"api/initializers/VarianceScaling/#elegy.initializers.VarianceScaling.__init__","text":"Constructs the VarianceScaling initializer. Parameters: Name Type Description Default scale Scale to multiply the variance by. 1.0 mode One of fan_in , fan_out , fan_avg 'fan_in' distribution Random distribution to use. One of truncated_normal , normal or uniform . 'truncated_normal' Source code in elegy/initializers.py def __init__ ( self , scale = 1.0 , mode = \"fan_in\" , distribution = \"truncated_normal\" ): \"\"\" Constructs the VarianceScaling initializer. Args: scale: Scale to multiply the variance by. mode: One of ``fan_in``, ``fan_out``, ``fan_avg`` distribution: Random distribution to use. One of ``truncated_normal``, ``normal`` or ``uniform``. \"\"\" if scale <= 0.0 : raise ValueError ( \"`scale` must be a positive float.\" ) if mode not in { \"fan_in\" , \"fan_out\" , \"fan_avg\" }: raise ValueError ( \"Invalid `mode` argument:\" , mode ) distribution = distribution . lower () if distribution not in { \"normal\" , \"truncated_normal\" , \"uniform\" }: raise ValueError ( \"Invalid `distribution` argument:\" , distribution ) self . scale = scale self . mode = mode self . distribution = distribution","title":"__init__()"},{"location":"api/losses/BinaryCrossentropy/","text":"elegy.losses.BinaryCrossentropy Computes the cross-entropy loss between true labels and predicted labels. Use this cross-entropy loss when there are only two label classes (assumed to be 0 and 1). For each example, there should be a single floating-point value per prediction. In the snippet below, each of the four examples has only a single floating-pointing value, and both y_pred and y_true have the shape [batch_size] . Usage: y_true = jnp . array([[ 0. , 1. ], [ 0. , 0. ]]) y_pred = jnp . array[[ 0.6 , 0.4 ], [ 0.4 , 0.6 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. bce = elegy . losses . BinaryCrossentropy() result = bce(y_true, y_pred) assert jnp . isclose(result, 0.815 , rtol =0.01 ) # Calling with 'sample_weight'. bce = elegy . losses . BinaryCrossentropy() result = bce(y_true, y_pred, sample_weight = jnp . array([ 1 , 0 ])) assert jnp . isclose(result, 0.458 , rtol =0.01 ) # Using 'sum' reduction type. bce = elegy . losses . BinaryCrossentropy(reduction = elegy . losses . Reduction . SUM) result = bce(y_true, y_pred) assert jnp . isclose(result, 1.630 , rtol =0.01 ) # Using 'none' reduction type. bce = elegy . losses . BinaryCrossentropy(reduction = elegy . losses . Reduction . NONE) result = bce(y_true, y_pred) assert jnp . all(jnp . isclose(result, [ 0.916 , 0.713 ], rtol =0.01 )) Usage with the Elegy API: model = elegy . Model( module_fn, loss = elegy . losses . BinaryCrossentropy(), metrics = elegy . metrics . Accuracy(), optimizer = optax . adam( 1e-3 ), ) __init__ ( self , from_logits = False , label_smoothing = 0 , reduction = None , weight = None , on = None , ** kwargs ) special Initializes CategoricalCrossentropy instance. Parameters: Name Type Description Default from_logits bool Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution. Note - Using from_logits=True is more numerically stable. False label_smoothing float Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. label_smoothing=0.2 means that we will use a value of 0.1 for label 0 and 0.9 for label 1 \" 0 reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . Indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to SUM_OVER_BATCH_SIZE . When used with tf.distribute.Strategy , outside of built-in training loops such as elegy compile and fit , or SUM_OVER_BATCH_SIZE` will raise an error. None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/binary_crossentropy.py def __init__ ( self , from_logits : bool = False , label_smoothing : float = 0 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `CategoricalCrossentropy` instance. Arguments: from_logits: Whether `y_pred` is expected to be a logits tensor. By default, we assume that `y_pred` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** label_smoothing: Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. `label_smoothing=0.2` means that we will use a value of `0.1` for label `0` and `0.9` for label `1`\" reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. Indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `elegy` `compile` and `fit`, ` or `SUM_OVER_BATCH_SIZE` will raise an error. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs ) self . _from_logits = from_logits self . _label_smoothing = label_smoothing call ( self , y_true , y_pred , sample_weight = None ) Invokes the BinaryCrossentropy instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. required y_pred ndarray The predicted values. required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Loss values per sample. Source code in elegy/losses/binary_crossentropy.py def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Invokes the `BinaryCrossentropy` instance. Arguments: y_true: Ground truth values. y_pred: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return binary_crossentropy ( y_true , y_pred , from_logits = self . _from_logits , label_smoothing = self . _label_smoothing , )","title":"BinaryCrossentropy"},{"location":"api/losses/BinaryCrossentropy/#elegylossesbinarycrossentropy","text":"","title":"elegy.losses.BinaryCrossentropy"},{"location":"api/losses/BinaryCrossentropy/#elegy.losses.binary_crossentropy.BinaryCrossentropy","text":"Computes the cross-entropy loss between true labels and predicted labels. Use this cross-entropy loss when there are only two label classes (assumed to be 0 and 1). For each example, there should be a single floating-point value per prediction. In the snippet below, each of the four examples has only a single floating-pointing value, and both y_pred and y_true have the shape [batch_size] . Usage: y_true = jnp . array([[ 0. , 1. ], [ 0. , 0. ]]) y_pred = jnp . array[[ 0.6 , 0.4 ], [ 0.4 , 0.6 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. bce = elegy . losses . BinaryCrossentropy() result = bce(y_true, y_pred) assert jnp . isclose(result, 0.815 , rtol =0.01 ) # Calling with 'sample_weight'. bce = elegy . losses . BinaryCrossentropy() result = bce(y_true, y_pred, sample_weight = jnp . array([ 1 , 0 ])) assert jnp . isclose(result, 0.458 , rtol =0.01 ) # Using 'sum' reduction type. bce = elegy . losses . BinaryCrossentropy(reduction = elegy . losses . Reduction . SUM) result = bce(y_true, y_pred) assert jnp . isclose(result, 1.630 , rtol =0.01 ) # Using 'none' reduction type. bce = elegy . losses . BinaryCrossentropy(reduction = elegy . losses . Reduction . NONE) result = bce(y_true, y_pred) assert jnp . all(jnp . isclose(result, [ 0.916 , 0.713 ], rtol =0.01 )) Usage with the Elegy API: model = elegy . Model( module_fn, loss = elegy . losses . BinaryCrossentropy(), metrics = elegy . metrics . Accuracy(), optimizer = optax . adam( 1e-3 ), )","title":"elegy.losses.binary_crossentropy.BinaryCrossentropy"},{"location":"api/losses/BinaryCrossentropy/#elegy.losses.binary_crossentropy.BinaryCrossentropy.__init__","text":"Initializes CategoricalCrossentropy instance. Parameters: Name Type Description Default from_logits bool Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution. Note - Using from_logits=True is more numerically stable. False label_smoothing float Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. label_smoothing=0.2 means that we will use a value of 0.1 for label 0 and 0.9 for label 1 \" 0 reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . Indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to SUM_OVER_BATCH_SIZE . When used with tf.distribute.Strategy , outside of built-in training loops such as elegy compile and fit , or SUM_OVER_BATCH_SIZE` will raise an error. None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/binary_crossentropy.py def __init__ ( self , from_logits : bool = False , label_smoothing : float = 0 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `CategoricalCrossentropy` instance. Arguments: from_logits: Whether `y_pred` is expected to be a logits tensor. By default, we assume that `y_pred` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** label_smoothing: Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. `label_smoothing=0.2` means that we will use a value of `0.1` for label `0` and `0.9` for label `1`\" reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. Indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `elegy` `compile` and `fit`, ` or `SUM_OVER_BATCH_SIZE` will raise an error. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs ) self . _from_logits = from_logits self . _label_smoothing = label_smoothing","title":"__init__()"},{"location":"api/losses/BinaryCrossentropy/#elegy.losses.binary_crossentropy.BinaryCrossentropy.call","text":"Invokes the BinaryCrossentropy instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. required y_pred ndarray The predicted values. required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Loss values per sample. Source code in elegy/losses/binary_crossentropy.py def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Invokes the `BinaryCrossentropy` instance. Arguments: y_true: Ground truth values. y_pred: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return binary_crossentropy ( y_true , y_pred , from_logits = self . _from_logits , label_smoothing = self . _label_smoothing , )","title":"call()"},{"location":"api/losses/CategoricalCrossentropy/","text":"elegy.losses.CategoricalCrossentropy Computes the crossentropy loss between the labels and predictions. Use this crossentropy loss function when there are two or more label classes. We expect labels to be provided in a one_hot representation. If you want to provide labels as integers, please use SparseCategoricalCrossentropy loss. There should be # classes floating point values per feature. In the snippet below, there is # classes floating pointing values per example. The shape of both y_pred and y_true are [batch_size, num_classes] . Usage: y_true = jnp . array([[ 0 , 1 , 0 ], [ 0 , 0 , 1 ]]) y_pred = jnp . array([[ 0.05 , 0.95 , 0 ], [ 0.1 , 0.8 , 0.1 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. cce = elegy . losses . CategoricalCrossentropy() assert cce(y_true, y_pred) == 1.177 # Calling with 'sample_weight'. assert cce(y_true, y_pred, sample_weight = tf . constant([ 0.3 , 0.7 ])) == 0.814 # Using 'sum' reduction type. cce = elegy . losses . CategoricalCrossentropy( reduction = elegy . losses . Reduction . SUM ) assert cce(y_true, y_pred) == 2.354 # Using 'none' reduction type. cce = elegy . losses . CategoricalCrossentropy( reduction = elegy . losses . Reduction . NONE ) assert list (cce(y_true, y_pred)) == [ 0.0513 , 2.303 ] Usage with the Elegy API: model = elegy . Model( module_fn, loss = elegy . losses . CategoricalCrossentropy(), metrics = elegy . metrics . Accuracy(), optimizer = optax . adam( 1e-3 ), ) __init__ ( self , from_logits = False , label_smoothing = 0 , reduction = None , weight = None , on = None , ** kwargs ) special Initializes CategoricalCrossentropy instance. Parameters: Name Type Description Default from_logits bool Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution. Note - Using from_logits=True is more numerically stable. False label_smoothing float Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. label_smoothing=0.2 means that we will use a value of 0.1 for label 0 and 0.9 for label 1 \" 0 reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . Indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/categorical_crossentropy.py def __init__ ( self , from_logits : bool = False , label_smoothing : float = 0 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs , ): \"\"\" Initializes `CategoricalCrossentropy` instance. Arguments: from_logits: Whether `y_pred` is expected to be a logits tensor. By default, we assume that `y_pred` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** label_smoothing: Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. `label_smoothing=0.2` means that we will use a value of `0.1` for label `0` and `0.9` for label `1`\" reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. Indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs ) self . _from_logits = from_logits self . _label_smoothing = label_smoothing call ( self , y_true , y_pred , sample_weight = None ) Invokes the CategoricalCrossentropy instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. required y_pred ndarray The predicted values. required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Loss values per sample. Source code in elegy/losses/categorical_crossentropy.py def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Invokes the `CategoricalCrossentropy` instance. Arguments: y_true: Ground truth values. y_pred: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return categorical_crossentropy ( y_true , y_pred , from_logits = self . _from_logits , label_smoothing = self . _label_smoothing , )","title":"CategoricalCrossentropy"},{"location":"api/losses/CategoricalCrossentropy/#elegylossescategoricalcrossentropy","text":"","title":"elegy.losses.CategoricalCrossentropy"},{"location":"api/losses/CategoricalCrossentropy/#elegy.losses.categorical_crossentropy.CategoricalCrossentropy","text":"Computes the crossentropy loss between the labels and predictions. Use this crossentropy loss function when there are two or more label classes. We expect labels to be provided in a one_hot representation. If you want to provide labels as integers, please use SparseCategoricalCrossentropy loss. There should be # classes floating point values per feature. In the snippet below, there is # classes floating pointing values per example. The shape of both y_pred and y_true are [batch_size, num_classes] . Usage: y_true = jnp . array([[ 0 , 1 , 0 ], [ 0 , 0 , 1 ]]) y_pred = jnp . array([[ 0.05 , 0.95 , 0 ], [ 0.1 , 0.8 , 0.1 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. cce = elegy . losses . CategoricalCrossentropy() assert cce(y_true, y_pred) == 1.177 # Calling with 'sample_weight'. assert cce(y_true, y_pred, sample_weight = tf . constant([ 0.3 , 0.7 ])) == 0.814 # Using 'sum' reduction type. cce = elegy . losses . CategoricalCrossentropy( reduction = elegy . losses . Reduction . SUM ) assert cce(y_true, y_pred) == 2.354 # Using 'none' reduction type. cce = elegy . losses . CategoricalCrossentropy( reduction = elegy . losses . Reduction . NONE ) assert list (cce(y_true, y_pred)) == [ 0.0513 , 2.303 ] Usage with the Elegy API: model = elegy . Model( module_fn, loss = elegy . losses . CategoricalCrossentropy(), metrics = elegy . metrics . Accuracy(), optimizer = optax . adam( 1e-3 ), )","title":"elegy.losses.categorical_crossentropy.CategoricalCrossentropy"},{"location":"api/losses/CategoricalCrossentropy/#elegy.losses.categorical_crossentropy.CategoricalCrossentropy.__init__","text":"Initializes CategoricalCrossentropy instance. Parameters: Name Type Description Default from_logits bool Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution. Note - Using from_logits=True is more numerically stable. False label_smoothing float Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. label_smoothing=0.2 means that we will use a value of 0.1 for label 0 and 0.9 for label 1 \" 0 reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . Indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/categorical_crossentropy.py def __init__ ( self , from_logits : bool = False , label_smoothing : float = 0 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs , ): \"\"\" Initializes `CategoricalCrossentropy` instance. Arguments: from_logits: Whether `y_pred` is expected to be a logits tensor. By default, we assume that `y_pred` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** label_smoothing: Float in [0, 1]. When > 0, label values are smoothed, meaning the confidence on label values are relaxed. e.g. `label_smoothing=0.2` means that we will use a value of `0.1` for label `0` and `0.9` for label `1`\" reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. Indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs ) self . _from_logits = from_logits self . _label_smoothing = label_smoothing","title":"__init__()"},{"location":"api/losses/CategoricalCrossentropy/#elegy.losses.categorical_crossentropy.CategoricalCrossentropy.call","text":"Invokes the CategoricalCrossentropy instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. required y_pred ndarray The predicted values. required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Loss values per sample. Source code in elegy/losses/categorical_crossentropy.py def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Invokes the `CategoricalCrossentropy` instance. Arguments: y_true: Ground truth values. y_pred: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return categorical_crossentropy ( y_true , y_pred , from_logits = self . _from_logits , label_smoothing = self . _label_smoothing , )","title":"call()"},{"location":"api/losses/CosineSimilarity/","text":"elegy.losses.CosineSimilarity Computes the mean squared logarithmic errors between labels and predictions. loss = -sum(l2_norm(y_true) * l2_norm(y_pred)) Usage: y_true = jnp . array([[ 0. , 1. ], [ 1. , 1. ]]) y_pred = jnp . array([[ 1. , 0. ], [ 1. , 1. ]]) # Using 'auto'/'sum_over_batch_size' reduction type. cosine_loss = elegy . losses . CosineSimilarity(axis =1 ) assert cosine_loss(y_true, y_pred) == -0.49999997 # Calling with 'sample_weight'. assert cosine_loss(y_true, y_pred, sample_weight = jnp . array([ 0.8 , 0.2 ])) == -0.099999994 # Using 'sum' reduction type. cosine_loss = elegy . losses . CosineSimilarity(axis =1 , reduction = elegy . losses . Reduction . SUM ) assert cosine_loss(y_true, y_pred) == -0.99999994 # Using 'none' reduction type. cosine_loss = elegy . losses . CosineSimilarity(axis =1 , reduction = elegy . losses . Reduction . NONE ) assert jnp . equal(cosine_loss(y_true, y_pred), jnp . array([ -0. , -0.99999994 ])) . all() Usage with the Elegy API: model = elegy . Model( module_fn, loss = elegy . losses . CosineSimilarity(axis =1 ), metrics = elegy . metrics . Mean(), ) __init__ ( self , axis =- 1 , reduction = None , weight = None , on = None , ** kwargs ) special Initializes Mean class. Parameters: Name Type Description Default axis int (Optional) Defaults to -1. The dimension along which the cosine similarity is computed. -1 reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/cosine_similarity.py def __init__ ( self , axis : int = - 1 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: axis: (Optional) Defaults to -1. The dimension along which the cosine similarity is computed. reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" self . axis = axis return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs ) call ( self , y_true , y_pred , sample_weight = None ) Invokes the CosineSimilarity instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in elegy/losses/cosine_similarity.py def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `CosineSimilarity` instance. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` y_pred: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return cosine_similarity ( y_true , y_pred , self . axis )","title":"CosineSimilarity"},{"location":"api/losses/CosineSimilarity/#elegylossescosinesimilarity","text":"","title":"elegy.losses.CosineSimilarity"},{"location":"api/losses/CosineSimilarity/#elegy.losses.cosine_similarity.CosineSimilarity","text":"Computes the mean squared logarithmic errors between labels and predictions. loss = -sum(l2_norm(y_true) * l2_norm(y_pred)) Usage: y_true = jnp . array([[ 0. , 1. ], [ 1. , 1. ]]) y_pred = jnp . array([[ 1. , 0. ], [ 1. , 1. ]]) # Using 'auto'/'sum_over_batch_size' reduction type. cosine_loss = elegy . losses . CosineSimilarity(axis =1 ) assert cosine_loss(y_true, y_pred) == -0.49999997 # Calling with 'sample_weight'. assert cosine_loss(y_true, y_pred, sample_weight = jnp . array([ 0.8 , 0.2 ])) == -0.099999994 # Using 'sum' reduction type. cosine_loss = elegy . losses . CosineSimilarity(axis =1 , reduction = elegy . losses . Reduction . SUM ) assert cosine_loss(y_true, y_pred) == -0.99999994 # Using 'none' reduction type. cosine_loss = elegy . losses . CosineSimilarity(axis =1 , reduction = elegy . losses . Reduction . NONE ) assert jnp . equal(cosine_loss(y_true, y_pred), jnp . array([ -0. , -0.99999994 ])) . all() Usage with the Elegy API: model = elegy . Model( module_fn, loss = elegy . losses . CosineSimilarity(axis =1 ), metrics = elegy . metrics . Mean(), )","title":"elegy.losses.cosine_similarity.CosineSimilarity"},{"location":"api/losses/CosineSimilarity/#elegy.losses.cosine_similarity.CosineSimilarity.__init__","text":"Initializes Mean class. Parameters: Name Type Description Default axis int (Optional) Defaults to -1. The dimension along which the cosine similarity is computed. -1 reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/cosine_similarity.py def __init__ ( self , axis : int = - 1 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: axis: (Optional) Defaults to -1. The dimension along which the cosine similarity is computed. reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" self . axis = axis return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs )","title":"__init__()"},{"location":"api/losses/CosineSimilarity/#elegy.losses.cosine_similarity.CosineSimilarity.call","text":"Invokes the CosineSimilarity instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in elegy/losses/cosine_similarity.py def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `CosineSimilarity` instance. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` y_pred: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return cosine_similarity ( y_true , y_pred , self . axis )","title":"call()"},{"location":"api/losses/Huber/","text":"elegy.losses.Huber Computes the Huber loss between labels and predictions. For each value x in error = y_true - y_pred: loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} where d is delta. See: https://en.wikipedia.org/wiki/Huber_loss Usage: y_true = jnp . array([[ 0 , 1 ], [ 0 , 0 ]]) y_pred = jnp . array([[ 0.6 , 0.4 ], [ 0.4 , 0.6 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. huber_loss = elegy . losses . Huber() assert huber_loss(y_true, y_pred) == 0.155 # Calling with 'sample_weight'. assert ( huber_loss(y_true, y_pred, sample_weight = jnp . array([ 0.8 , 0.2 ])) == 0.08500001 ) # Using 'sum' reduction type. huber_loss = elegy . losses . Huber( reduction = elegy . losses . Reduction . SUM ) assert huber_loss(y_true, y_pred) == 0.31 # Using 'none' reduction type. huber_loss = elegy . losses . Huber( reduction = elegy . losses . Reduction . NONE ) assert jnp . equal(huber_loss(y_true, y_pred), jnp . array([ 0.18 , 0.13000001 ])) . all() Usage with the Elegy API: model = elegy . Model( module_fn, loss = elegy . losses . Huber(delta =1.0 ), metrics = elegy . metrics . Mean(), ) __init__ ( self , delta = 1.0 , reduction = None , weight = None , on = None , ** kwargs ) special Initializes Mean class. Parameters: Name Type Description Default delta float (Optional) Defaults to 1.0. A float, the point where the Huber loss function changes from a quadratic to linear. 1.0 reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/huber.py def __init__ ( self , delta : float = 1.0 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: delta: (Optional) Defaults to 1.0. A float, the point where the Huber loss function changes from a quadratic to linear. reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" self . delta = delta return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs ) call ( self , y_true , y_pred , sample_weight = None ) Invokes the Huber instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in elegy/losses/huber.py def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `Huber` instance. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` y_pred: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return huber ( y_true , y_pred , self . delta )","title":"Huber"},{"location":"api/losses/Huber/#elegylosseshuber","text":"","title":"elegy.losses.Huber"},{"location":"api/losses/Huber/#elegy.losses.huber.Huber","text":"Computes the Huber loss between labels and predictions. For each value x in error = y_true - y_pred: loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} where d is delta. See: https://en.wikipedia.org/wiki/Huber_loss Usage: y_true = jnp . array([[ 0 , 1 ], [ 0 , 0 ]]) y_pred = jnp . array([[ 0.6 , 0.4 ], [ 0.4 , 0.6 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. huber_loss = elegy . losses . Huber() assert huber_loss(y_true, y_pred) == 0.155 # Calling with 'sample_weight'. assert ( huber_loss(y_true, y_pred, sample_weight = jnp . array([ 0.8 , 0.2 ])) == 0.08500001 ) # Using 'sum' reduction type. huber_loss = elegy . losses . Huber( reduction = elegy . losses . Reduction . SUM ) assert huber_loss(y_true, y_pred) == 0.31 # Using 'none' reduction type. huber_loss = elegy . losses . Huber( reduction = elegy . losses . Reduction . NONE ) assert jnp . equal(huber_loss(y_true, y_pred), jnp . array([ 0.18 , 0.13000001 ])) . all() Usage with the Elegy API: model = elegy . Model( module_fn, loss = elegy . losses . Huber(delta =1.0 ), metrics = elegy . metrics . Mean(), )","title":"elegy.losses.huber.Huber"},{"location":"api/losses/Huber/#elegy.losses.huber.Huber.__init__","text":"Initializes Mean class. Parameters: Name Type Description Default delta float (Optional) Defaults to 1.0. A float, the point where the Huber loss function changes from a quadratic to linear. 1.0 reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/huber.py def __init__ ( self , delta : float = 1.0 , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: delta: (Optional) Defaults to 1.0. A float, the point where the Huber loss function changes from a quadratic to linear. reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" self . delta = delta return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs )","title":"__init__()"},{"location":"api/losses/Huber/#elegy.losses.huber.Huber.call","text":"Invokes the Huber instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in elegy/losses/huber.py def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `Huber` instance. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` y_pred: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return huber ( y_true , y_pred , self . delta )","title":"call()"},{"location":"api/losses/Loss/","text":"elegy.losses.Loss Loss base class. To be implemented by subclasses: call() : Contains the logic for loss calculation. Example subclass implementation: class MeanSquaredError (Loss): def call ( self , y_true, y_pred): return jnp . mean(jnp . square(y_pred - y_true), axis =-1 ) Please see the [Modules, Losses, and Metrics Guide] (https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#losses) for more details on this. __init__ ( self , reduction = None , weight = None , on = None , name = None ) special Initializes Loss class. Parameters: Name Type Description Default reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None name Optional[str] Optional name for the instance, if not provided lower snake_case version of the name of the class is used instead. None Source code in elegy/losses/loss.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , name : tp . Optional [ str ] = None , ): \"\"\" Initializes `Loss` class. Arguments: reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). name: Optional name for the instance, if not provided lower snake_case version of the name of the class is used instead. \"\"\" self . name = name if name is not None else utils . get_name ( self ) self . weight = weight if weight is not None else 1.0 self . _reduction = ( reduction if reduction is not None else Reduction . SUM_OVER_BATCH_SIZE ) self . _labels_filter = ( on ,) if isinstance ( on , ( str , int )) else on self . _signature_f = self . call","title":"Loss"},{"location":"api/losses/Loss/#elegylossesloss","text":"","title":"elegy.losses.Loss"},{"location":"api/losses/Loss/#elegy.losses.loss.Loss","text":"Loss base class. To be implemented by subclasses: call() : Contains the logic for loss calculation. Example subclass implementation: class MeanSquaredError (Loss): def call ( self , y_true, y_pred): return jnp . mean(jnp . square(y_pred - y_true), axis =-1 ) Please see the [Modules, Losses, and Metrics Guide] (https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#losses) for more details on this.","title":"elegy.losses.loss.Loss"},{"location":"api/losses/Loss/#elegy.losses.loss.Loss.__init__","text":"Initializes Loss class. Parameters: Name Type Description Default reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None name Optional[str] Optional name for the instance, if not provided lower snake_case version of the name of the class is used instead. None Source code in elegy/losses/loss.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , name : tp . Optional [ str ] = None , ): \"\"\" Initializes `Loss` class. Arguments: reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). name: Optional name for the instance, if not provided lower snake_case version of the name of the class is used instead. \"\"\" self . name = name if name is not None else utils . get_name ( self ) self . weight = weight if weight is not None else 1.0 self . _reduction = ( reduction if reduction is not None else Reduction . SUM_OVER_BATCH_SIZE ) self . _labels_filter = ( on ,) if isinstance ( on , ( str , int )) else on self . _signature_f = self . call","title":"__init__()"},{"location":"api/losses/MeanAbsoluteError/","text":"elegy.losses.MeanAbsoluteError Computes the mean absolute errors between labels and predictions. loss = mean(abs(y_true - y_pred)) Usage: y_true = jnp . array([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) y_pred = jnp . array([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mae = elegy . losses . MeanAbsoluteError() assert mae(y_true, y_pred) == 0.5 # Calling with 'sample_weight'. assert mae(y_true, y_pred, sample_weight = jnp . array([ 0.7 , 0.3 ])) == 0.25 # Using 'sum' reduction type. mae = elegy . losses . MeanAbsoluteError(reduction = elegy . losses . Reduction . SUM) assert mae(y_true, y_pred) == 1.0 # Using 'none' reduction type. mae = elegy . losses . MeanAbsoluteError(reduction = elegy . losses . Reduction . NONE) assert list (mae(y_true, y_pred)) == [ 0.5 , 0.5 ] Usage with the Elegy API: model = elegy . Model( module_fn, loss = elegy . losses . MeanAbsoluteError(), metrics = elegy . metrics . Mean(), ) __init__ ( self , reduction = None , weight = None , on = None , ** kwargs ) special Initializes Mean class. Parameters: Name Type Description Default reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/mean_absolute_error.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs ) call ( self , y_true , y_pred , sample_weight = None ) Invokes the MeanAbsoluteError instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in elegy/losses/mean_absolute_error.py def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `MeanAbsoluteError` instance. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` y_pred: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_error ( y_true , y_pred )","title":"MeanAbsoluteError"},{"location":"api/losses/MeanAbsoluteError/#elegylossesmeanabsoluteerror","text":"","title":"elegy.losses.MeanAbsoluteError"},{"location":"api/losses/MeanAbsoluteError/#elegy.losses.mean_absolute_error.MeanAbsoluteError","text":"Computes the mean absolute errors between labels and predictions. loss = mean(abs(y_true - y_pred)) Usage: y_true = jnp . array([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) y_pred = jnp . array([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mae = elegy . losses . MeanAbsoluteError() assert mae(y_true, y_pred) == 0.5 # Calling with 'sample_weight'. assert mae(y_true, y_pred, sample_weight = jnp . array([ 0.7 , 0.3 ])) == 0.25 # Using 'sum' reduction type. mae = elegy . losses . MeanAbsoluteError(reduction = elegy . losses . Reduction . SUM) assert mae(y_true, y_pred) == 1.0 # Using 'none' reduction type. mae = elegy . losses . MeanAbsoluteError(reduction = elegy . losses . Reduction . NONE) assert list (mae(y_true, y_pred)) == [ 0.5 , 0.5 ] Usage with the Elegy API: model = elegy . Model( module_fn, loss = elegy . losses . MeanAbsoluteError(), metrics = elegy . metrics . Mean(), )","title":"elegy.losses.mean_absolute_error.MeanAbsoluteError"},{"location":"api/losses/MeanAbsoluteError/#elegy.losses.mean_absolute_error.MeanAbsoluteError.__init__","text":"Initializes Mean class. Parameters: Name Type Description Default reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/mean_absolute_error.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs )","title":"__init__()"},{"location":"api/losses/MeanAbsoluteError/#elegy.losses.mean_absolute_error.MeanAbsoluteError.call","text":"Invokes the MeanAbsoluteError instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in elegy/losses/mean_absolute_error.py def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `MeanAbsoluteError` instance. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` y_pred: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_error ( y_true , y_pred )","title":"call()"},{"location":"api/losses/MeanAbsolutePercentageError/","text":"elegy.losses.MeanAbsolutePercentageError Computes the mean absolute errors between labels and predictions. loss = mean(abs((y_true - y_pred) / y_true)) Usage: y_true = jnp . array([[ 1.0 , 1.0 ], [ 0.9 , 0.0 ]]) y_pred = jnp . array([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mape = elegy . losses . MeanAbsolutePercentageError() result = mape(y_true, y_pred) assert jnp . isclose(result, 2.78 , rtol =0.01 ) # Calling with 'sample_weight'. assert jnp . isclose(mape(y_true, y_pred, sample_weight = jnp . array([ 0.1 , 0.9 ])), 2.5 , rtol =0.01 ) # Using 'sum' reduction type. mape = elegy . losses . MeanAbsolutePercentageError(reduction = elegy . losses . Reduction . SUM) assert jnp . isclose(mape(y_true, y_pred), 5.6 , rtol =0.01 ) # Using 'none' reduction type. mape = elegy . losses . MeanAbsolutePercentageError(reduction = elegy . losses . Reduction . NONE) assert jnp . all(jnp . isclose(result, [ 0. , 5.6 ], rtol =0.01 )) Usage with the Elegy API: model = elegy . Model( module_fn, loss = elegy . losses . MeanAbsolutePercentageError(), metrics = elegy . metrics . Mean(), ) __init__ ( self , reduction = None , weight = None , on = None , ** kwargs ) special Initializes Mean class. Parameters: Name Type Description Default reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . When used with tf.distribute.Strategy , outside of built-in training loops such as elegy compile and fit , or SUM_OVER_BATCH_SIZE will raise an error. for more details. None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/mean_absolute_percentage_error.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `elegy` `compile` and `fit`, or `SUM_OVER_BATCH_SIZE` will raise an error. for more details. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs ) call ( self , y_true , y_pred , sample_weight = None ) Invokes the MeanAbsolutePercentageError instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in elegy/losses/mean_absolute_percentage_error.py def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `MeanAbsolutePercentageError` instance. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` y_pred: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_percentage_error ( y_true , y_pred )","title":"MeanAbsolutePercentageError"},{"location":"api/losses/MeanAbsolutePercentageError/#elegylossesmeanabsolutepercentageerror","text":"","title":"elegy.losses.MeanAbsolutePercentageError"},{"location":"api/losses/MeanAbsolutePercentageError/#elegy.losses.mean_absolute_percentage_error.MeanAbsolutePercentageError","text":"Computes the mean absolute errors between labels and predictions. loss = mean(abs((y_true - y_pred) / y_true)) Usage: y_true = jnp . array([[ 1.0 , 1.0 ], [ 0.9 , 0.0 ]]) y_pred = jnp . array([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mape = elegy . losses . MeanAbsolutePercentageError() result = mape(y_true, y_pred) assert jnp . isclose(result, 2.78 , rtol =0.01 ) # Calling with 'sample_weight'. assert jnp . isclose(mape(y_true, y_pred, sample_weight = jnp . array([ 0.1 , 0.9 ])), 2.5 , rtol =0.01 ) # Using 'sum' reduction type. mape = elegy . losses . MeanAbsolutePercentageError(reduction = elegy . losses . Reduction . SUM) assert jnp . isclose(mape(y_true, y_pred), 5.6 , rtol =0.01 ) # Using 'none' reduction type. mape = elegy . losses . MeanAbsolutePercentageError(reduction = elegy . losses . Reduction . NONE) assert jnp . all(jnp . isclose(result, [ 0. , 5.6 ], rtol =0.01 )) Usage with the Elegy API: model = elegy . Model( module_fn, loss = elegy . losses . MeanAbsolutePercentageError(), metrics = elegy . metrics . Mean(), )","title":"elegy.losses.mean_absolute_percentage_error.MeanAbsolutePercentageError"},{"location":"api/losses/MeanAbsolutePercentageError/#elegy.losses.mean_absolute_percentage_error.MeanAbsolutePercentageError.__init__","text":"Initializes Mean class. Parameters: Name Type Description Default reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . When used with tf.distribute.Strategy , outside of built-in training loops such as elegy compile and fit , or SUM_OVER_BATCH_SIZE will raise an error. for more details. None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/mean_absolute_percentage_error.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`, outside of built-in training loops such as `elegy` `compile` and `fit`, or `SUM_OVER_BATCH_SIZE` will raise an error. for more details. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs )","title":"__init__()"},{"location":"api/losses/MeanAbsolutePercentageError/#elegy.losses.mean_absolute_percentage_error.MeanAbsolutePercentageError.call","text":"Invokes the MeanAbsolutePercentageError instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in elegy/losses/mean_absolute_percentage_error.py def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `MeanAbsolutePercentageError` instance. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` y_pred: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_absolute_percentage_error ( y_true , y_pred )","title":"call()"},{"location":"api/losses/MeanSquaredError/","text":"elegy.losses.MeanSquaredError Computes the mean of squares of errors between labels and predictions. loss = square(y_true - y_pred) Usage: y_true = jnp . array([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) y_pred = jnp . array([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mse = elegy . losses . MeanSquaredError() assert mse(y_true, y_pred) == 0.5 # Calling with 'sample_weight'. assert mse(y_true, y_pred, sample_weight = jnp . array([ 0.7 , 0.3 ])) == 0.25 # Using 'sum' reduction type. mse = elegy . losses . MeanSquaredError(reduction = elegy . losses . Reduction . SUM) assert mse(y_true, y_pred) == 1.0 # Using 'none' reduction type. mse = elegy . losses . MeanSquaredError(reduction = elegy . losses . Reduction . NONE) assert list (mse(y_true, y_pred)) == [ 0.5 , 0.5 ] Usage with the Elegy API: model = elegy . Model( module_fn, loss = elegy . losses . MeanSquaredError(), metrics = elegy . metrics . Mean(), ) __init__ ( self , reduction = None , weight = None , on = None , ** kwargs ) special Initializes Mean class. Parameters: Name Type Description Default reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/mean_squared_error.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs ) call ( self , y_true , y_pred , sample_weight = None ) Invokes the MeanSquaredError instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in elegy/losses/mean_squared_error.py def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `MeanSquaredError` instance. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` y_pred: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_error ( y_true , y_pred )","title":"MeanSquaredError"},{"location":"api/losses/MeanSquaredError/#elegylossesmeansquarederror","text":"","title":"elegy.losses.MeanSquaredError"},{"location":"api/losses/MeanSquaredError/#elegy.losses.mean_squared_error.MeanSquaredError","text":"Computes the mean of squares of errors between labels and predictions. loss = square(y_true - y_pred) Usage: y_true = jnp . array([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) y_pred = jnp . array([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. mse = elegy . losses . MeanSquaredError() assert mse(y_true, y_pred) == 0.5 # Calling with 'sample_weight'. assert mse(y_true, y_pred, sample_weight = jnp . array([ 0.7 , 0.3 ])) == 0.25 # Using 'sum' reduction type. mse = elegy . losses . MeanSquaredError(reduction = elegy . losses . Reduction . SUM) assert mse(y_true, y_pred) == 1.0 # Using 'none' reduction type. mse = elegy . losses . MeanSquaredError(reduction = elegy . losses . Reduction . NONE) assert list (mse(y_true, y_pred)) == [ 0.5 , 0.5 ] Usage with the Elegy API: model = elegy . Model( module_fn, loss = elegy . losses . MeanSquaredError(), metrics = elegy . metrics . Mean(), )","title":"elegy.losses.mean_squared_error.MeanSquaredError"},{"location":"api/losses/MeanSquaredError/#elegy.losses.mean_squared_error.MeanSquaredError.__init__","text":"Initializes Mean class. Parameters: Name Type Description Default reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/mean_squared_error.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs )","title":"__init__()"},{"location":"api/losses/MeanSquaredError/#elegy.losses.mean_squared_error.MeanSquaredError.call","text":"Invokes the MeanSquaredError instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in elegy/losses/mean_squared_error.py def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `MeanSquaredError` instance. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` y_pred: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_error ( y_true , y_pred )","title":"call()"},{"location":"api/losses/MeanSquaredLogarithmicError/","text":"elegy.losses.MeanSquaredLogarithmicError Computes the mean squared logarithmic errors between labels and predictions. loss = mean(square(log(y_true + 1) - log(y_pred + 1)), axis=-1) Usage: y_true = jnp . array([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) y_pred = jnp . array([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. msle = elegy . losses . MeanSquaredLogarithmicError() assert msle(y_true, y_pred) == 0.24022643 # Calling with 'sample_weight'. assert msle(y_true, y_pred, sample_weight = jnp . array([ 0.7 , 0.3 ])) = 0.12011322 # Using 'sum' reduction type. msle = elegy . losses . MeanSquaredLogarithmicError(reduction = elegy . losses . Reduction . SUM) assert msle(y_true, y_pred) == 0.48045287 # Using 'none' reduction type. msle = elegy . losses . MeanSquaredLogarithmicError(reduction = elegy . losses . Reduction . NONE) assert jnp . equal(msle(y_true, y_pred), jnp . array([ 0.24022643 , 0.24022643 ])) . all() Usage with the Elegy API: model = elegy . Model( module_fn, loss = elegy . losses . MeanSquaredLogarithmicError(), metrics = elegy . metrics . Mean(), ) __init__ ( self , reduction = None , weight = None , on = None , ** kwargs ) special Initializes Mean class. Parameters: Name Type Description Default reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/mean_squared_logarithmic_error.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs ) call ( self , y_true , y_pred , sample_weight = None ) Invokes the MeanSquaredLogarithmicError instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in elegy/losses/mean_squared_logarithmic_error.py def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `MeanSquaredLogarithmicError` instance. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` y_pred: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_logarithmic_error ( y_true , y_pred )","title":"MeanSquaredLogarithmicError"},{"location":"api/losses/MeanSquaredLogarithmicError/#elegylossesmeansquaredlogarithmicerror","text":"","title":"elegy.losses.MeanSquaredLogarithmicError"},{"location":"api/losses/MeanSquaredLogarithmicError/#elegy.losses.mean_squared_logarithmic_error.MeanSquaredLogarithmicError","text":"Computes the mean squared logarithmic errors between labels and predictions. loss = mean(square(log(y_true + 1) - log(y_pred + 1)), axis=-1) Usage: y_true = jnp . array([[ 0.0 , 1.0 ], [ 0.0 , 0.0 ]]) y_pred = jnp . array([[ 1.0 , 1.0 ], [ 1.0 , 0.0 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. msle = elegy . losses . MeanSquaredLogarithmicError() assert msle(y_true, y_pred) == 0.24022643 # Calling with 'sample_weight'. assert msle(y_true, y_pred, sample_weight = jnp . array([ 0.7 , 0.3 ])) = 0.12011322 # Using 'sum' reduction type. msle = elegy . losses . MeanSquaredLogarithmicError(reduction = elegy . losses . Reduction . SUM) assert msle(y_true, y_pred) == 0.48045287 # Using 'none' reduction type. msle = elegy . losses . MeanSquaredLogarithmicError(reduction = elegy . losses . Reduction . NONE) assert jnp . equal(msle(y_true, y_pred), jnp . array([ 0.24022643 , 0.24022643 ])) . all() Usage with the Elegy API: model = elegy . Model( module_fn, loss = elegy . losses . MeanSquaredLogarithmicError(), metrics = elegy . metrics . Mean(), )","title":"elegy.losses.mean_squared_logarithmic_error.MeanSquaredLogarithmicError"},{"location":"api/losses/MeanSquaredLogarithmicError/#elegy.losses.mean_squared_logarithmic_error.MeanSquaredLogarithmicError.__init__","text":"Initializes Mean class. Parameters: Name Type Description Default reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/losses/mean_squared_logarithmic_error.py def __init__ ( self , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Initializes `Mean` class. Arguments: reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" return super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs )","title":"__init__()"},{"location":"api/losses/MeanSquaredLogarithmicError/#elegy.losses.mean_squared_logarithmic_error.MeanSquaredLogarithmicError.call","text":"Invokes the MeanSquaredLogarithmicError instance. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] , except sparse loss functions such as sparse categorical crossentropy where shape = [batch_size, d0, .. dN-1] required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Weighted loss float Tensor . If reduction is NONE , this has shape [batch_size, d0, .. dN-1] ; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Exceptions: Type Description ValueError If the shape of sample_weight is invalid. Source code in elegy/losses/mean_squared_logarithmic_error.py def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , # not used, __call__ handles it, left for documentation purposes. ) -> jnp . ndarray : \"\"\" Invokes the `MeanSquaredLogarithmicError` instance. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` y_pred: The predicted values. shape = `[batch_size, d0, .. dN]` sample_weight: Optional `sample_weight` acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Weighted loss float `Tensor`. If `reduction` is `NONE`, this has shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note `dN-1` because all loss functions reduce by 1 dimension, usually axis=-1.) Raises: ValueError: If the shape of `sample_weight` is invalid. \"\"\" return mean_squared_logarithmic_error ( y_true , y_pred )","title":"call()"},{"location":"api/losses/Reduction/","text":"elegy.losses.Reduction Types of loss reduction. Contains the following values: * NONE : Weighted losses with one dimension reduced (axis=-1, or axis specified by loss function). When this reduction type used with built-in Keras training loops like fit / evaluate , the unreduced vector loss is passed to the optimizer but the reported loss will be a scalar value. * SUM : Scalar sum of weighted losses. * SUM_OVER_BATCH_SIZE : Scalar SUM divided by number of elements in losses. __class__ inherited Metaclass for Enum __members__ property readonly special Returns a mapping of member name->value. This mapping lists all enum members, including aliases. Note that this is a read-only view of the internal mapping. __bool__ ( self ) special classes/types should always be True. Source code in elegy/losses/loss.py def __bool__ ( self ): \"\"\" classes/types should always be True. \"\"\" return True __call__ ( cls , value , names = None , * , module = None , qualname = None , type = None , start = 1 ) special Either returns an existing member, or creates a new enum class. This method is used both when an enum class is given a value to match to an enumeration member (i.e. Color(3)) and for the functional API (i.e. Color = Enum('Color', names='RED GREEN BLUE')). When used for the functional API: value will be the name of the new class. names should be either a string of white-space/comma delimited names (values will start at start ), or an iterator/mapping of name, value pairs. module should be set to the module this class is being created in; if it is not set, an attempt to find that module will be made, but if it fails the class will not be picklable. qualname should be set to the actual location this class can be found at in its module; by default it is set to the global scope. If this is not correct, unpickling will fail in some circumstances. type , if set, will be mixed in as the first base class. Source code in elegy/losses/loss.py def __call__ ( cls , value , names = None , * , module = None , qualname = None , type = None , start = 1 ): \"\"\"Either returns an existing member, or creates a new enum class. This method is used both when an enum class is given a value to match to an enumeration member (i.e. Color(3)) and for the functional API (i.e. Color = Enum('Color', names='RED GREEN BLUE')). When used for the functional API: `value` will be the name of the new class. `names` should be either a string of white-space/comma delimited names (values will start at `start`), or an iterator/mapping of name, value pairs. `module` should be set to the module this class is being created in; if it is not set, an attempt to find that module will be made, but if it fails the class will not be picklable. `qualname` should be set to the actual location this class can be found at in its module; by default it is set to the global scope. If this is not correct, unpickling will fail in some circumstances. `type`, if set, will be mixed in as the first base class. \"\"\" if names is None : # simple value lookup return cls . __new__ ( cls , value ) # otherwise, functional API: we're creating a new Enum type return cls . _create_ ( value , names , module = module , qualname = qualname , type = type , start = start ) __getattr__ ( cls , name ) special Return the enum member matching name We use getattr instead of descriptors or inserting into the enum class' dict in order to support name and value being both properties for enum members (which live in the class' dict ) and enum members themselves. Source code in elegy/losses/loss.py def __getattr__ ( cls , name ): \"\"\"Return the enum member matching `name` We use __getattr__ instead of descriptors or inserting into the enum class' __dict__ in order to support `name` and `value` being both properties for enum members (which live in the class' __dict__) and enum members themselves. \"\"\" if _is_dunder ( name ): raise AttributeError ( name ) try : return cls . _member_map_ [ name ] except KeyError : raise AttributeError ( name ) from None __new__ ( metacls , cls , bases , classdict ) special staticmethod Create and return a new object. See help(type) for accurate signature. Source code in elegy/losses/loss.py def __new__ ( metacls , cls , bases , classdict ): # an Enum class is final once enumeration items have been defined; it # cannot be mixed with other types (int, float, etc.) if it has an # inherited __new__ unless a new __new__ is defined (or the resulting # class will fail). # # remove any keys listed in _ignore_ classdict . setdefault ( '_ignore_' , []) . append ( '_ignore_' ) ignore = classdict [ '_ignore_' ] for key in ignore : classdict . pop ( key , None ) member_type , first_enum = metacls . _get_mixins_ ( bases ) __new__ , save_new , use_args = metacls . _find_new_ ( classdict , member_type , first_enum ) # save enum items into separate mapping so they don't get baked into # the new class enum_members = { k : classdict [ k ] for k in classdict . _member_names } for name in classdict . _member_names : del classdict [ name ] # adjust the sunders _order_ = classdict . pop ( '_order_' , None ) # check for illegal enum names (any others?) invalid_names = set ( enum_members ) & { 'mro' , '' } if invalid_names : raise ValueError ( 'Invalid enum member name: {0} ' . format ( ',' . join ( invalid_names ))) # create a default docstring if one has not been provided if '__doc__' not in classdict : classdict [ '__doc__' ] = 'An enumeration.' # create our new Enum type enum_class = super () . __new__ ( metacls , cls , bases , classdict ) enum_class . _member_names_ = [] # names in definition order enum_class . _member_map_ = {} # name->value map enum_class . _member_type_ = member_type # save DynamicClassAttribute attributes from super classes so we know # if we can take the shortcut of storing members in the class dict dynamic_attributes = { k for c in enum_class . mro () for k , v in c . __dict__ . items () if isinstance ( v , DynamicClassAttribute )} # Reverse value->name map for hashable values. enum_class . _value2member_map_ = {} # If a custom type is mixed into the Enum, and it does not know how # to pickle itself, pickle.dumps will succeed but pickle.loads will # fail. Rather than have the error show up later and possibly far # from the source, sabotage the pickle protocol for this class so # that pickle.dumps also fails. # # However, if the new class implements its own __reduce_ex__, do not # sabotage -- it's on them to make sure it works correctly. We use # __reduce_ex__ instead of any of the others as it is preferred by # pickle over __reduce__, and it handles all pickle protocols. if '__reduce_ex__' not in classdict : if member_type is not object : methods = ( '__getnewargs_ex__' , '__getnewargs__' , '__reduce_ex__' , '__reduce__' ) if not any ( m in member_type . __dict__ for m in methods ): _make_class_unpicklable ( enum_class ) # instantiate them, checking for duplicates as we go # we instantiate first instead of checking for duplicates first in case # a custom __new__ is doing something funky with the values -- such as # auto-numbering ;) for member_name in classdict . _member_names : value = enum_members [ member_name ] if not isinstance ( value , tuple ): args = ( value , ) else : args = value if member_type is tuple : # special case for tuple enums args = ( args , ) # wrap it one more time if not use_args : enum_member = __new__ ( enum_class ) if not hasattr ( enum_member , '_value_' ): enum_member . _value_ = value else : enum_member = __new__ ( enum_class , * args ) if not hasattr ( enum_member , '_value_' ): if member_type is object : enum_member . _value_ = value else : enum_member . _value_ = member_type ( * args ) value = enum_member . _value_ enum_member . _name_ = member_name enum_member . __objclass__ = enum_class enum_member . __init__ ( * args ) # If another member with the same value was already defined, the # new member becomes an alias to the existing one. for name , canonical_member in enum_class . _member_map_ . items (): if canonical_member . _value_ == enum_member . _value_ : enum_member = canonical_member break else : # Aliases don't appear in member names (only in __members__). enum_class . _member_names_ . append ( member_name ) # performance boost for any member that would not shadow # a DynamicClassAttribute if member_name not in dynamic_attributes : setattr ( enum_class , member_name , enum_member ) # now add to _member_map_ enum_class . _member_map_ [ member_name ] = enum_member try : # This may fail if value is not hashable. We can't add the value # to the map, and by-value lookups for this value will be # linear. enum_class . _value2member_map_ [ value ] = enum_member except TypeError : pass # double check that repr and friends are not the mixin's or various # things break (such as pickle) for name in ( '__repr__' , '__str__' , '__format__' , '__reduce_ex__' ): class_method = getattr ( enum_class , name ) obj_method = getattr ( member_type , name , None ) enum_method = getattr ( first_enum , name , None ) if obj_method is not None and obj_method is class_method : setattr ( enum_class , name , enum_method ) # replace any other __new__ with our own (as long as Enum is not None, # anyway) -- again, this is to support pickle if Enum is not None : # if the user defined their own __new__, save it before it gets # clobbered in case they subclass later if save_new : enum_class . __new_member__ = __new__ enum_class . __new__ = Enum . __new__ # py3 support for definition order (helps keep py2/py3 code in sync) if _order_ is not None : if isinstance ( _order_ , str ): _order_ = _order_ . replace ( ',' , ' ' ) . split () if _order_ != enum_class . _member_names_ : raise TypeError ( 'member order does not match _order_' ) return enum_class __prepare__ ( cls , bases ) classmethod special prepare () -> dict used to create the namespace for the class statement Source code in elegy/losses/loss.py @classmethod def __prepare__ ( metacls , cls , bases ): # create the namespace dict enum_dict = _EnumDict () # inherit previous flags and _generate_next_value_ function member_type , first_enum = metacls . _get_mixins_ ( bases ) if first_enum is not None : enum_dict [ '_generate_next_value_' ] = getattr ( first_enum , '_generate_next_value_' , None ) return enum_dict __setattr__ ( cls , name , value ) special Block attempts to reassign Enum members. A simple assignment to the class namespace only changes one of the several possible ways to get an Enum member from the Enum class, resulting in an inconsistent Enumeration. Source code in elegy/losses/loss.py def __setattr__ ( cls , name , value ): \"\"\"Block attempts to reassign Enum members. A simple assignment to the class namespace only changes one of the several possible ways to get an Enum member from the Enum class, resulting in an inconsistent Enumeration. \"\"\" member_map = cls . __dict__ . get ( '_member_map_' , {}) if name in member_map : raise AttributeError ( 'Cannot reassign members.' ) super () . __setattr__ ( name , value )","title":"Reduction"},{"location":"api/losses/Reduction/#elegylossesreduction","text":"","title":"elegy.losses.Reduction"},{"location":"api/losses/Reduction/#elegy.losses.loss.Reduction","text":"Types of loss reduction. Contains the following values: * NONE : Weighted losses with one dimension reduced (axis=-1, or axis specified by loss function). When this reduction type used with built-in Keras training loops like fit / evaluate , the unreduced vector loss is passed to the optimizer but the reported loss will be a scalar value. * SUM : Scalar sum of weighted losses. * SUM_OVER_BATCH_SIZE : Scalar SUM divided by number of elements in losses.","title":"elegy.losses.loss.Reduction"},{"location":"api/losses/Reduction/#elegy.losses.loss.Reduction.__class__","text":"Metaclass for Enum","title":"__class__"},{"location":"api/losses/Reduction/#elegy.losses.loss.Reduction.__class__.__members__","text":"Returns a mapping of member name->value. This mapping lists all enum members, including aliases. Note that this is a read-only view of the internal mapping.","title":"__members__"},{"location":"api/losses/Reduction/#elegy.losses.loss.Reduction.__class__.__bool__","text":"classes/types should always be True. Source code in elegy/losses/loss.py def __bool__ ( self ): \"\"\" classes/types should always be True. \"\"\" return True","title":"__bool__()"},{"location":"api/losses/Reduction/#elegy.losses.loss.Reduction.__class__.__call__","text":"Either returns an existing member, or creates a new enum class. This method is used both when an enum class is given a value to match to an enumeration member (i.e. Color(3)) and for the functional API (i.e. Color = Enum('Color', names='RED GREEN BLUE')). When used for the functional API: value will be the name of the new class. names should be either a string of white-space/comma delimited names (values will start at start ), or an iterator/mapping of name, value pairs. module should be set to the module this class is being created in; if it is not set, an attempt to find that module will be made, but if it fails the class will not be picklable. qualname should be set to the actual location this class can be found at in its module; by default it is set to the global scope. If this is not correct, unpickling will fail in some circumstances. type , if set, will be mixed in as the first base class. Source code in elegy/losses/loss.py def __call__ ( cls , value , names = None , * , module = None , qualname = None , type = None , start = 1 ): \"\"\"Either returns an existing member, or creates a new enum class. This method is used both when an enum class is given a value to match to an enumeration member (i.e. Color(3)) and for the functional API (i.e. Color = Enum('Color', names='RED GREEN BLUE')). When used for the functional API: `value` will be the name of the new class. `names` should be either a string of white-space/comma delimited names (values will start at `start`), or an iterator/mapping of name, value pairs. `module` should be set to the module this class is being created in; if it is not set, an attempt to find that module will be made, but if it fails the class will not be picklable. `qualname` should be set to the actual location this class can be found at in its module; by default it is set to the global scope. If this is not correct, unpickling will fail in some circumstances. `type`, if set, will be mixed in as the first base class. \"\"\" if names is None : # simple value lookup return cls . __new__ ( cls , value ) # otherwise, functional API: we're creating a new Enum type return cls . _create_ ( value , names , module = module , qualname = qualname , type = type , start = start )","title":"__call__()"},{"location":"api/losses/Reduction/#elegy.losses.loss.Reduction.__class__.__getattr__","text":"Return the enum member matching name We use getattr instead of descriptors or inserting into the enum class' dict in order to support name and value being both properties for enum members (which live in the class' dict ) and enum members themselves. Source code in elegy/losses/loss.py def __getattr__ ( cls , name ): \"\"\"Return the enum member matching `name` We use __getattr__ instead of descriptors or inserting into the enum class' __dict__ in order to support `name` and `value` being both properties for enum members (which live in the class' __dict__) and enum members themselves. \"\"\" if _is_dunder ( name ): raise AttributeError ( name ) try : return cls . _member_map_ [ name ] except KeyError : raise AttributeError ( name ) from None","title":"__getattr__()"},{"location":"api/losses/Reduction/#elegy.losses.loss.Reduction.__class__.__new__","text":"Create and return a new object. See help(type) for accurate signature. Source code in elegy/losses/loss.py def __new__ ( metacls , cls , bases , classdict ): # an Enum class is final once enumeration items have been defined; it # cannot be mixed with other types (int, float, etc.) if it has an # inherited __new__ unless a new __new__ is defined (or the resulting # class will fail). # # remove any keys listed in _ignore_ classdict . setdefault ( '_ignore_' , []) . append ( '_ignore_' ) ignore = classdict [ '_ignore_' ] for key in ignore : classdict . pop ( key , None ) member_type , first_enum = metacls . _get_mixins_ ( bases ) __new__ , save_new , use_args = metacls . _find_new_ ( classdict , member_type , first_enum ) # save enum items into separate mapping so they don't get baked into # the new class enum_members = { k : classdict [ k ] for k in classdict . _member_names } for name in classdict . _member_names : del classdict [ name ] # adjust the sunders _order_ = classdict . pop ( '_order_' , None ) # check for illegal enum names (any others?) invalid_names = set ( enum_members ) & { 'mro' , '' } if invalid_names : raise ValueError ( 'Invalid enum member name: {0} ' . format ( ',' . join ( invalid_names ))) # create a default docstring if one has not been provided if '__doc__' not in classdict : classdict [ '__doc__' ] = 'An enumeration.' # create our new Enum type enum_class = super () . __new__ ( metacls , cls , bases , classdict ) enum_class . _member_names_ = [] # names in definition order enum_class . _member_map_ = {} # name->value map enum_class . _member_type_ = member_type # save DynamicClassAttribute attributes from super classes so we know # if we can take the shortcut of storing members in the class dict dynamic_attributes = { k for c in enum_class . mro () for k , v in c . __dict__ . items () if isinstance ( v , DynamicClassAttribute )} # Reverse value->name map for hashable values. enum_class . _value2member_map_ = {} # If a custom type is mixed into the Enum, and it does not know how # to pickle itself, pickle.dumps will succeed but pickle.loads will # fail. Rather than have the error show up later and possibly far # from the source, sabotage the pickle protocol for this class so # that pickle.dumps also fails. # # However, if the new class implements its own __reduce_ex__, do not # sabotage -- it's on them to make sure it works correctly. We use # __reduce_ex__ instead of any of the others as it is preferred by # pickle over __reduce__, and it handles all pickle protocols. if '__reduce_ex__' not in classdict : if member_type is not object : methods = ( '__getnewargs_ex__' , '__getnewargs__' , '__reduce_ex__' , '__reduce__' ) if not any ( m in member_type . __dict__ for m in methods ): _make_class_unpicklable ( enum_class ) # instantiate them, checking for duplicates as we go # we instantiate first instead of checking for duplicates first in case # a custom __new__ is doing something funky with the values -- such as # auto-numbering ;) for member_name in classdict . _member_names : value = enum_members [ member_name ] if not isinstance ( value , tuple ): args = ( value , ) else : args = value if member_type is tuple : # special case for tuple enums args = ( args , ) # wrap it one more time if not use_args : enum_member = __new__ ( enum_class ) if not hasattr ( enum_member , '_value_' ): enum_member . _value_ = value else : enum_member = __new__ ( enum_class , * args ) if not hasattr ( enum_member , '_value_' ): if member_type is object : enum_member . _value_ = value else : enum_member . _value_ = member_type ( * args ) value = enum_member . _value_ enum_member . _name_ = member_name enum_member . __objclass__ = enum_class enum_member . __init__ ( * args ) # If another member with the same value was already defined, the # new member becomes an alias to the existing one. for name , canonical_member in enum_class . _member_map_ . items (): if canonical_member . _value_ == enum_member . _value_ : enum_member = canonical_member break else : # Aliases don't appear in member names (only in __members__). enum_class . _member_names_ . append ( member_name ) # performance boost for any member that would not shadow # a DynamicClassAttribute if member_name not in dynamic_attributes : setattr ( enum_class , member_name , enum_member ) # now add to _member_map_ enum_class . _member_map_ [ member_name ] = enum_member try : # This may fail if value is not hashable. We can't add the value # to the map, and by-value lookups for this value will be # linear. enum_class . _value2member_map_ [ value ] = enum_member except TypeError : pass # double check that repr and friends are not the mixin's or various # things break (such as pickle) for name in ( '__repr__' , '__str__' , '__format__' , '__reduce_ex__' ): class_method = getattr ( enum_class , name ) obj_method = getattr ( member_type , name , None ) enum_method = getattr ( first_enum , name , None ) if obj_method is not None and obj_method is class_method : setattr ( enum_class , name , enum_method ) # replace any other __new__ with our own (as long as Enum is not None, # anyway) -- again, this is to support pickle if Enum is not None : # if the user defined their own __new__, save it before it gets # clobbered in case they subclass later if save_new : enum_class . __new_member__ = __new__ enum_class . __new__ = Enum . __new__ # py3 support for definition order (helps keep py2/py3 code in sync) if _order_ is not None : if isinstance ( _order_ , str ): _order_ = _order_ . replace ( ',' , ' ' ) . split () if _order_ != enum_class . _member_names_ : raise TypeError ( 'member order does not match _order_' ) return enum_class","title":"__new__()"},{"location":"api/losses/Reduction/#elegy.losses.loss.Reduction.__class__.__prepare__","text":"prepare () -> dict used to create the namespace for the class statement Source code in elegy/losses/loss.py @classmethod def __prepare__ ( metacls , cls , bases ): # create the namespace dict enum_dict = _EnumDict () # inherit previous flags and _generate_next_value_ function member_type , first_enum = metacls . _get_mixins_ ( bases ) if first_enum is not None : enum_dict [ '_generate_next_value_' ] = getattr ( first_enum , '_generate_next_value_' , None ) return enum_dict","title":"__prepare__()"},{"location":"api/losses/Reduction/#elegy.losses.loss.Reduction.__class__.__setattr__","text":"Block attempts to reassign Enum members. A simple assignment to the class namespace only changes one of the several possible ways to get an Enum member from the Enum class, resulting in an inconsistent Enumeration. Source code in elegy/losses/loss.py def __setattr__ ( cls , name , value ): \"\"\"Block attempts to reassign Enum members. A simple assignment to the class namespace only changes one of the several possible ways to get an Enum member from the Enum class, resulting in an inconsistent Enumeration. \"\"\" member_map = cls . __dict__ . get ( '_member_map_' , {}) if name in member_map : raise AttributeError ( 'Cannot reassign members.' ) super () . __setattr__ ( name , value )","title":"__setattr__()"},{"location":"api/losses/SparseCategoricalCrossentropy/","text":"elegy.losses.SparseCategoricalCrossentropy Computes the crossentropy loss between the labels and predictions. Use this crossentropy loss function when there are two or more label classes. We expect labels to be provided as integers. If you want to provide labels using one-hot representation, please use CategoricalCrossentropy loss. There should be # classes floating point values per feature for y_pred and a single floating point value per feature for y_true . In the snippet below, there is a single floating point value per example for y_true and # classes floating pointing values per example for y_pred . The shape of y_true is [batch_size] and the shape of y_pred is [batch_size, num_classes] . Usage: y_true = jnp . array([ 1 , 2 ]) y_pred = jnp . array([[ 0.05 , 0.95 , 0 ], [ 0.1 , 0.8 , 0.1 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. scce = elegy . losses . SparseCategoricalCrossentropy() result = scce(y_true, y_pred) # 1.177 assert jnp . isclose(result, 1.177 , rtol =0.01 ) # Calling with 'sample_weight'. result = scce(y_true, y_pred, sample_weight = jnp . array([ 0.3 , 0.7 ])) # 0.814 assert jnp . isclose(result, 0.814 , rtol =0.01 ) # Using 'sum' reduction type. scce = elegy . losses . SparseCategoricalCrossentropy( reduction = elegy . losses . Reduction . SUM ) result = scce(y_true, y_pred) # 2.354 assert jnp . isclose(result, 2.354 , rtol =0.01 ) # Using 'none' reduction type. scce = elegy . losses . SparseCategoricalCrossentropy( reduction = elegy . losses . Reduction . NONE ) result = scce(y_true, y_pred) # [0.0513, 2.303] assert jnp . all(jnp . isclose(result, [ 0.0513 , 2.303 ], rtol =0.01 )) Usage with the Elegy API: model = elegy . Model( module_fn, loss = elegy . losses . SparseCategoricalCrossentropy(), metrics = elegy . metrics . Accuracy(), optimizer = optax . adam( 1e-3 ), ) __init__ ( self , from_logits = False , reduction = None , weight = None , on = None , check_bounds = True , ** kwargs ) special Initializes SparseCategoricalCrossentropy instance. Parameters: Name Type Description Default from_logits bool Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution. Note - Using from_logits=True is more numerically stable. False reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None check_bounds Optional[bool] If True (default), checks y_true for negative values and values larger or equal than the number of channels in y_pred . Sets loss to NaN if this is the case. If False , the check is disabled and the loss may contain incorrect values. True Source code in elegy/losses/sparse_categorical_crossentropy.py def __init__ ( self , from_logits : bool = False , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , check_bounds : tp . Optional [ bool ] = True , ** kwargs ): \"\"\" Initializes `SparseCategoricalCrossentropy` instance. Arguments: from_logits: Whether `y_pred` is expected to be a logits tensor. By default, we assume that `y_pred` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). check_bounds: If `True` (default), checks `y_true` for negative values and values larger or equal than the number of channels in `y_pred`. Sets loss to NaN if this is the case. If `False`, the check is disabled and the loss may contain incorrect values. \"\"\" super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs ) self . _from_logits = from_logits self . _check_bounds = check_bounds call ( self , y_true , y_pred , sample_weight = None ) Invokes the SparseCategoricalCrossentropy instance. Parameters: Name Type Description Default y_true Ground truth values. required y_pred The predicted values. required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Loss values per sample. Source code in elegy/losses/sparse_categorical_crossentropy.py def call ( self , y_true , y_pred , sample_weight : tp . Optional [ jnp . ndarray ] = None ) -> jnp . ndarray : \"\"\" Invokes the `SparseCategoricalCrossentropy` instance. Arguments: y_true: Ground truth values. y_pred: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return sparse_categorical_crossentropy ( y_true , y_pred , from_logits = self . _from_logits , check_bounds = self . _check_bounds , )","title":"SparseCategoricalCrossentropy"},{"location":"api/losses/SparseCategoricalCrossentropy/#elegylossessparsecategoricalcrossentropy","text":"","title":"elegy.losses.SparseCategoricalCrossentropy"},{"location":"api/losses/SparseCategoricalCrossentropy/#elegy.losses.sparse_categorical_crossentropy.SparseCategoricalCrossentropy","text":"Computes the crossentropy loss between the labels and predictions. Use this crossentropy loss function when there are two or more label classes. We expect labels to be provided as integers. If you want to provide labels using one-hot representation, please use CategoricalCrossentropy loss. There should be # classes floating point values per feature for y_pred and a single floating point value per feature for y_true . In the snippet below, there is a single floating point value per example for y_true and # classes floating pointing values per example for y_pred . The shape of y_true is [batch_size] and the shape of y_pred is [batch_size, num_classes] . Usage: y_true = jnp . array([ 1 , 2 ]) y_pred = jnp . array([[ 0.05 , 0.95 , 0 ], [ 0.1 , 0.8 , 0.1 ]]) # Using 'auto'/'sum_over_batch_size' reduction type. scce = elegy . losses . SparseCategoricalCrossentropy() result = scce(y_true, y_pred) # 1.177 assert jnp . isclose(result, 1.177 , rtol =0.01 ) # Calling with 'sample_weight'. result = scce(y_true, y_pred, sample_weight = jnp . array([ 0.3 , 0.7 ])) # 0.814 assert jnp . isclose(result, 0.814 , rtol =0.01 ) # Using 'sum' reduction type. scce = elegy . losses . SparseCategoricalCrossentropy( reduction = elegy . losses . Reduction . SUM ) result = scce(y_true, y_pred) # 2.354 assert jnp . isclose(result, 2.354 , rtol =0.01 ) # Using 'none' reduction type. scce = elegy . losses . SparseCategoricalCrossentropy( reduction = elegy . losses . Reduction . NONE ) result = scce(y_true, y_pred) # [0.0513, 2.303] assert jnp . all(jnp . isclose(result, [ 0.0513 , 2.303 ], rtol =0.01 )) Usage with the Elegy API: model = elegy . Model( module_fn, loss = elegy . losses . SparseCategoricalCrossentropy(), metrics = elegy . metrics . Accuracy(), optimizer = optax . adam( 1e-3 ), )","title":"elegy.losses.sparse_categorical_crossentropy.SparseCategoricalCrossentropy"},{"location":"api/losses/SparseCategoricalCrossentropy/#elegy.losses.sparse_categorical_crossentropy.SparseCategoricalCrossentropy.__init__","text":"Initializes SparseCategoricalCrossentropy instance. Parameters: Name Type Description Default from_logits bool Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution. Note - Using from_logits=True is more numerically stable. False reduction Optional[elegy.losses.loss.Reduction] (Optional) Type of elegy.losses.Reduction to apply to loss. Default value is SUM_OVER_BATCH_SIZE . For almost all cases this defaults to SUM_OVER_BATCH_SIZE . None weight Optional[float] Optional weight contribution for the total loss. Defaults to 1 . None on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None check_bounds Optional[bool] If True (default), checks y_true for negative values and values larger or equal than the number of channels in y_pred . Sets loss to NaN if this is the case. If False , the check is disabled and the loss may contain incorrect values. True Source code in elegy/losses/sparse_categorical_crossentropy.py def __init__ ( self , from_logits : bool = False , reduction : tp . Optional [ Reduction ] = None , weight : tp . Optional [ float ] = None , on : tp . Optional [ types . IndexLike ] = None , check_bounds : tp . Optional [ bool ] = True , ** kwargs ): \"\"\" Initializes `SparseCategoricalCrossentropy` instance. Arguments: from_logits: Whether `y_pred` is expected to be a logits tensor. By default, we assume that `y_pred` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** reduction: (Optional) Type of `elegy.losses.Reduction` to apply to loss. Default value is `SUM_OVER_BATCH_SIZE`. For almost all cases this defaults to `SUM_OVER_BATCH_SIZE`. weight: Optional weight contribution for the total loss. Defaults to `1`. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). check_bounds: If `True` (default), checks `y_true` for negative values and values larger or equal than the number of channels in `y_pred`. Sets loss to NaN if this is the case. If `False`, the check is disabled and the loss may contain incorrect values. \"\"\" super () . __init__ ( reduction = reduction , weight = weight , on = on , ** kwargs ) self . _from_logits = from_logits self . _check_bounds = check_bounds","title":"__init__()"},{"location":"api/losses/SparseCategoricalCrossentropy/#elegy.losses.sparse_categorical_crossentropy.SparseCategoricalCrossentropy.call","text":"Invokes the SparseCategoricalCrossentropy instance. Parameters: Name Type Description Default y_true Ground truth values. required y_pred The predicted values. required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the total loss for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each loss element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all loss functions reduce by 1 dimension, usually axis=-1.) None Returns: Type Description ndarray Loss values per sample. Source code in elegy/losses/sparse_categorical_crossentropy.py def call ( self , y_true , y_pred , sample_weight : tp . Optional [ jnp . ndarray ] = None ) -> jnp . ndarray : \"\"\" Invokes the `SparseCategoricalCrossentropy` instance. Arguments: y_true: Ground truth values. y_pred: The predicted values. sample_weight: Acts as a coefficient for the loss. If a scalar is provided, then the loss is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the total loss for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each loss element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on`dN-1`: all loss functions reduce by 1 dimension, usually axis=-1.) Returns: Loss values per sample. \"\"\" return sparse_categorical_crossentropy ( y_true , y_pred , from_logits = self . _from_logits , check_bounds = self . _check_bounds , )","title":"call()"},{"location":"api/losses/binary_crossentropy/","text":"elegy.losses.binary_crossentropy Source code in elegy/losses/binary_crossentropy.py def binary_crossentropy ( y_true : jnp . ndarray , y_pred : jnp . ndarray , from_logits : bool = False , label_smoothing : float = 0 , ) -> jnp . ndarray : assert abs ( y_pred . ndim - y_true . ndim ) <= 1 y_true , y_pred = utils . maybe_expand_dims ( y_true , y_pred ) if label_smoothing : y_true = y_true * ( 1.0 - label_smoothing ) + 0.5 * label_smoothing if from_logits : return - jnp . mean ( y_true * y_pred - jnp . logaddexp ( 0.0 , y_pred ), axis =- 1 ) y_pred = jnp . clip ( y_pred , types . EPSILON , 1.0 - types . EPSILON ) return - jnp . mean ( y_true * jnp . log ( y_pred ) + ( 1 - y_true ) * jnp . log ( 1 - y_pred ), axis =- 1 )","title":"binary_crossentropy"},{"location":"api/losses/binary_crossentropy/#elegylossesbinary_crossentropy","text":"","title":"elegy.losses.binary_crossentropy"},{"location":"api/losses/binary_crossentropy/#elegy.losses.binary_crossentropy.binary_crossentropy","text":"Source code in elegy/losses/binary_crossentropy.py def binary_crossentropy ( y_true : jnp . ndarray , y_pred : jnp . ndarray , from_logits : bool = False , label_smoothing : float = 0 , ) -> jnp . ndarray : assert abs ( y_pred . ndim - y_true . ndim ) <= 1 y_true , y_pred = utils . maybe_expand_dims ( y_true , y_pred ) if label_smoothing : y_true = y_true * ( 1.0 - label_smoothing ) + 0.5 * label_smoothing if from_logits : return - jnp . mean ( y_true * y_pred - jnp . logaddexp ( 0.0 , y_pred ), axis =- 1 ) y_pred = jnp . clip ( y_pred , types . EPSILON , 1.0 - types . EPSILON ) return - jnp . mean ( y_true * jnp . log ( y_pred ) + ( 1 - y_true ) * jnp . log ( 1 - y_pred ), axis =- 1 )","title":"elegy.losses.binary_crossentropy.binary_crossentropy"},{"location":"api/losses/cosine_similarity/","text":"elegy.losses.cosine_similarity Computes the cosine similarity between labels and predictions. loss = - sum (l2_norm(y_true) * l2_norm(y_pred)) Usage: rng = jax . random . PRNGKey( 42 ) y_true = jax . random . randint(rng, shape = ( 2 , 3 ), minval =0 , maxval =2 ) y_pred = jax . random . uniform(rng, shape = ( 2 , 3 )) loss = elegy . losses . cosine_similarity(y_true, y_pred, axis =1 ) assert loss . shape == ( 2 ,) y_true = y_true / jnp . maximum(jnp . linalg . norm(y_true, axis =1 , keepdims = True ), jnp . sqrt(types . EPSILON)) y_pred = y_pred / jnp . maximum(jnp . linalg . norm(y_pred, axis =1 , keepdims = True ), jnp . sqrt(types . EPSILON)) assert jnp . array_equal(loss, - jnp . sum(y_true * y_pred, axis =1 )) Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required axis int The dimension along which the cosine similarity is computed. required Returns: Type Description ndarray cosine similarity Values. If reduction is NONE, this has shape [batch_size, d0, .. dN-1]; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Source code in elegy/losses/cosine_similarity.py def cosine_similarity ( y_true : jnp . ndarray , y_pred : jnp . ndarray , axis : int ) -> jnp . ndarray : \"\"\" Computes the cosine similarity between labels and predictions. ```python loss = -sum(l2_norm(y_true) * l2_norm(y_pred)) ``` Usage: ```python rng = jax.random.PRNGKey(42) y_true = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) y_pred = jax.random.uniform(rng, shape=(2, 3)) loss = elegy.losses.cosine_similarity(y_true, y_pred, axis=1) assert loss.shape == (2,) y_true = y_true / jnp.maximum(jnp.linalg.norm(y_true, axis=1, keepdims=True), jnp.sqrt(types.EPSILON)) y_pred = y_pred / jnp.maximum(jnp.linalg.norm(y_pred, axis=1, keepdims=True), jnp.sqrt(types.EPSILON)) assert jnp.array_equal(loss, -jnp.sum(y_true * y_pred, axis=1)) ``` Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. axis: The dimension along which the cosine similarity is computed. Returns: cosine similarity Values. If reduction is NONE, this has shape [batch_size, d0, .. dN-1]; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) \"\"\" y_true = y_true / jnp . maximum ( jnp . linalg . norm ( y_true , axis = axis , keepdims = True ), jnp . sqrt ( types . EPSILON ) ) y_pred = y_pred / jnp . maximum ( jnp . linalg . norm ( y_pred , axis = axis , keepdims = True ), jnp . sqrt ( types . EPSILON ) ) return - jnp . sum ( y_true * y_pred , axis = axis )","title":"cosine_similarity"},{"location":"api/losses/cosine_similarity/#elegylossescosine_similarity","text":"","title":"elegy.losses.cosine_similarity"},{"location":"api/losses/cosine_similarity/#elegy.losses.cosine_similarity.cosine_similarity","text":"Computes the cosine similarity between labels and predictions. loss = - sum (l2_norm(y_true) * l2_norm(y_pred)) Usage: rng = jax . random . PRNGKey( 42 ) y_true = jax . random . randint(rng, shape = ( 2 , 3 ), minval =0 , maxval =2 ) y_pred = jax . random . uniform(rng, shape = ( 2 , 3 )) loss = elegy . losses . cosine_similarity(y_true, y_pred, axis =1 ) assert loss . shape == ( 2 ,) y_true = y_true / jnp . maximum(jnp . linalg . norm(y_true, axis =1 , keepdims = True ), jnp . sqrt(types . EPSILON)) y_pred = y_pred / jnp . maximum(jnp . linalg . norm(y_pred, axis =1 , keepdims = True ), jnp . sqrt(types . EPSILON)) assert jnp . array_equal(loss, - jnp . sum(y_true * y_pred, axis =1 )) Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required axis int The dimension along which the cosine similarity is computed. required Returns: Type Description ndarray cosine similarity Values. If reduction is NONE, this has shape [batch_size, d0, .. dN-1]; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Source code in elegy/losses/cosine_similarity.py def cosine_similarity ( y_true : jnp . ndarray , y_pred : jnp . ndarray , axis : int ) -> jnp . ndarray : \"\"\" Computes the cosine similarity between labels and predictions. ```python loss = -sum(l2_norm(y_true) * l2_norm(y_pred)) ``` Usage: ```python rng = jax.random.PRNGKey(42) y_true = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) y_pred = jax.random.uniform(rng, shape=(2, 3)) loss = elegy.losses.cosine_similarity(y_true, y_pred, axis=1) assert loss.shape == (2,) y_true = y_true / jnp.maximum(jnp.linalg.norm(y_true, axis=1, keepdims=True), jnp.sqrt(types.EPSILON)) y_pred = y_pred / jnp.maximum(jnp.linalg.norm(y_pred, axis=1, keepdims=True), jnp.sqrt(types.EPSILON)) assert jnp.array_equal(loss, -jnp.sum(y_true * y_pred, axis=1)) ``` Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. axis: The dimension along which the cosine similarity is computed. Returns: cosine similarity Values. If reduction is NONE, this has shape [batch_size, d0, .. dN-1]; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) \"\"\" y_true = y_true / jnp . maximum ( jnp . linalg . norm ( y_true , axis = axis , keepdims = True ), jnp . sqrt ( types . EPSILON ) ) y_pred = y_pred / jnp . maximum ( jnp . linalg . norm ( y_pred , axis = axis , keepdims = True ), jnp . sqrt ( types . EPSILON ) ) return - jnp . sum ( y_true * y_pred , axis = axis )","title":"elegy.losses.cosine_similarity.cosine_similarity"},{"location":"api/losses/huber/","text":"elegy.losses.huber Computes the Huber loss between labels and predictions. For each value x in error = y_true - y_pred: loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} where d is delta. See: https://en.wikipedia.org/wiki/Huber_loss Usage: rng = jax . random . PRNGKey( 42 ) y_true = jax . random . randint(rng, shape = ( 2 , 3 ), minval =0 , maxval =2 ) y_pred = jax . random . uniform(rng, shape = ( 2 , 3 )) loss = elegy . losses . huber(y_true, y_pred, delta =1.0 ) assert loss . shape == ( 2 ,) y_pred = y_pred . astype( float ) y_true = y_true . astype( float ) delta = 1.0 error = jnp . subtract(y_pred, y_true) abs_error = jnp . abs(error) quadratic = jnp . minimum(abs_error, delta) linear = jnp . subtract(abs_error, quadratic) assert jnp . array_equal(loss, jnp . mean( jnp . add( jnp . multiply( 0.5 , jnp . multiply(quadratic, quadratic) ), jnp . multiply(delta, linear)), axis =-1 )) Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required delta float A float, the point where the Huber loss function changes from a quadratic to linear. required Returns: Type Description ndarray huber loss Values. If reduction is NONE, this has shape [batch_size, d0, .. dN-1]; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Source code in elegy/losses/huber.py def huber ( y_true : jnp . ndarray , y_pred : jnp . ndarray , delta : float ) -> jnp . ndarray : r \"\"\" Computes the Huber loss between labels and predictions. For each value x in error = y_true - y_pred: $$ loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} $$ where d is delta. See: https://en.wikipedia.org/wiki/Huber_loss Usage: ```python rng = jax.random.PRNGKey(42) y_true = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) y_pred = jax.random.uniform(rng, shape=(2, 3)) loss = elegy.losses.huber(y_true, y_pred, delta=1.0) assert loss.shape == (2,) y_pred = y_pred.astype(float) y_true = y_true.astype(float) delta = 1.0 error = jnp.subtract(y_pred, y_true) abs_error = jnp.abs(error) quadratic = jnp.minimum(abs_error, delta) linear = jnp.subtract(abs_error, quadratic) assert jnp.array_equal(loss, jnp.mean( jnp.add( jnp.multiply( 0.5, jnp.multiply(quadratic, quadratic) ), jnp.multiply(delta, linear)), axis=-1 )) ``` Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. delta: A float, the point where the Huber loss function changes from a quadratic to linear. Returns: huber loss Values. If reduction is NONE, this has shape [batch_size, d0, .. dN-1]; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) \"\"\" y_pred = y_pred . astype ( float ) y_true = y_true . astype ( float ) delta = float ( delta ) error = jnp . subtract ( y_pred , y_true ) abs_error = jnp . abs ( error ) quadratic = jnp . minimum ( abs_error , delta ) linear = jnp . subtract ( abs_error , quadratic ) return jnp . mean ( jnp . add ( jnp . multiply ( 0.5 , jnp . multiply ( quadratic , quadratic )), jnp . multiply ( delta , linear ), ), axis =- 1 , )","title":"huber"},{"location":"api/losses/huber/#elegylosseshuber","text":"","title":"elegy.losses.huber"},{"location":"api/losses/huber/#elegy.losses.huber.huber","text":"Computes the Huber loss between labels and predictions. For each value x in error = y_true - y_pred: loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} where d is delta. See: https://en.wikipedia.org/wiki/Huber_loss Usage: rng = jax . random . PRNGKey( 42 ) y_true = jax . random . randint(rng, shape = ( 2 , 3 ), minval =0 , maxval =2 ) y_pred = jax . random . uniform(rng, shape = ( 2 , 3 )) loss = elegy . losses . huber(y_true, y_pred, delta =1.0 ) assert loss . shape == ( 2 ,) y_pred = y_pred . astype( float ) y_true = y_true . astype( float ) delta = 1.0 error = jnp . subtract(y_pred, y_true) abs_error = jnp . abs(error) quadratic = jnp . minimum(abs_error, delta) linear = jnp . subtract(abs_error, quadratic) assert jnp . array_equal(loss, jnp . mean( jnp . add( jnp . multiply( 0.5 , jnp . multiply(quadratic, quadratic) ), jnp . multiply(delta, linear)), axis =-1 )) Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required delta float A float, the point where the Huber loss function changes from a quadratic to linear. required Returns: Type Description ndarray huber loss Values. If reduction is NONE, this has shape [batch_size, d0, .. dN-1]; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) Source code in elegy/losses/huber.py def huber ( y_true : jnp . ndarray , y_pred : jnp . ndarray , delta : float ) -> jnp . ndarray : r \"\"\" Computes the Huber loss between labels and predictions. For each value x in error = y_true - y_pred: $$ loss = \\begin{cases} \\ 0.5 \\times x^2,\\hskip8em\\text{if } |x|\\leq d\\\\ 0.5 \\times d^2 + d \\times (|x| - d),\\hskip1.7em \\text{otherwise} \\end{cases} $$ where d is delta. See: https://en.wikipedia.org/wiki/Huber_loss Usage: ```python rng = jax.random.PRNGKey(42) y_true = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) y_pred = jax.random.uniform(rng, shape=(2, 3)) loss = elegy.losses.huber(y_true, y_pred, delta=1.0) assert loss.shape == (2,) y_pred = y_pred.astype(float) y_true = y_true.astype(float) delta = 1.0 error = jnp.subtract(y_pred, y_true) abs_error = jnp.abs(error) quadratic = jnp.minimum(abs_error, delta) linear = jnp.subtract(abs_error, quadratic) assert jnp.array_equal(loss, jnp.mean( jnp.add( jnp.multiply( 0.5, jnp.multiply(quadratic, quadratic) ), jnp.multiply(delta, linear)), axis=-1 )) ``` Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. delta: A float, the point where the Huber loss function changes from a quadratic to linear. Returns: huber loss Values. If reduction is NONE, this has shape [batch_size, d0, .. dN-1]; otherwise, it is scalar. (Note dN-1 because all loss functions reduce by 1 dimension, usually axis=-1.) \"\"\" y_pred = y_pred . astype ( float ) y_true = y_true . astype ( float ) delta = float ( delta ) error = jnp . subtract ( y_pred , y_true ) abs_error = jnp . abs ( error ) quadratic = jnp . minimum ( abs_error , delta ) linear = jnp . subtract ( abs_error , quadratic ) return jnp . mean ( jnp . add ( jnp . multiply ( 0.5 , jnp . multiply ( quadratic , quadratic )), jnp . multiply ( delta , linear ), ), axis =- 1 , )","title":"elegy.losses.huber.huber"},{"location":"api/losses/mean_absolute_error/","text":"elegy.losses.mean_absolute_error Computes the mean absolute error between labels and predictions. After computing the absolute distance between the inputs, the mean value over the last dimension is returned. loss = mean( abs (y_true - y_pred), axis =-1 ) Usage: rng = jax . random . PRNGKey( 42 ) y_true = jax . random . randint(rng, shape = ( 2 , 3 ), minval =0 , maxval =2 ) y_pred = jax . random . uniform(rng, shape = ( 2 , 3 )) loss = elegy . losses . mean_absolute_error(y_true, y_pred) assert loss . shape == ( 2 ,) assert jnp . array_equal(loss, jnp . mean(jnp . abs(y_true - y_pred), axis =-1 )) Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean absolute error values. shape = [batch_size, d0, .. dN-1] . Source code in elegy/losses/mean_absolute_error.py def mean_absolute_error ( y_true : jnp . ndarray , y_pred : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean absolute error between labels and predictions. After computing the absolute distance between the inputs, the mean value over the last dimension is returned. ```python loss = mean(abs(y_true - y_pred), axis=-1) ``` Usage: ```python rng = jax.random.PRNGKey(42) y_true = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) y_pred = jax.random.uniform(rng, shape=(2, 3)) loss = elegy.losses.mean_absolute_error(y_true, y_pred) assert loss.shape == (2,) assert jnp.array_equal(loss, jnp.mean(jnp.abs(y_true - y_pred), axis=-1)) ``` Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean absolute error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" y_true = y_true . astype ( y_pred . dtype ) return jnp . mean ( jnp . abs ( y_pred - y_true ), axis =- 1 )","title":"mean_absolute_error"},{"location":"api/losses/mean_absolute_error/#elegylossesmean_absolute_error","text":"","title":"elegy.losses.mean_absolute_error"},{"location":"api/losses/mean_absolute_error/#elegy.losses.mean_absolute_error.mean_absolute_error","text":"Computes the mean absolute error between labels and predictions. After computing the absolute distance between the inputs, the mean value over the last dimension is returned. loss = mean( abs (y_true - y_pred), axis =-1 ) Usage: rng = jax . random . PRNGKey( 42 ) y_true = jax . random . randint(rng, shape = ( 2 , 3 ), minval =0 , maxval =2 ) y_pred = jax . random . uniform(rng, shape = ( 2 , 3 )) loss = elegy . losses . mean_absolute_error(y_true, y_pred) assert loss . shape == ( 2 ,) assert jnp . array_equal(loss, jnp . mean(jnp . abs(y_true - y_pred), axis =-1 )) Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean absolute error values. shape = [batch_size, d0, .. dN-1] . Source code in elegy/losses/mean_absolute_error.py def mean_absolute_error ( y_true : jnp . ndarray , y_pred : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean absolute error between labels and predictions. After computing the absolute distance between the inputs, the mean value over the last dimension is returned. ```python loss = mean(abs(y_true - y_pred), axis=-1) ``` Usage: ```python rng = jax.random.PRNGKey(42) y_true = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) y_pred = jax.random.uniform(rng, shape=(2, 3)) loss = elegy.losses.mean_absolute_error(y_true, y_pred) assert loss.shape == (2,) assert jnp.array_equal(loss, jnp.mean(jnp.abs(y_true - y_pred), axis=-1)) ``` Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean absolute error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" y_true = y_true . astype ( y_pred . dtype ) return jnp . mean ( jnp . abs ( y_pred - y_true ), axis =- 1 )","title":"elegy.losses.mean_absolute_error.mean_absolute_error"},{"location":"api/losses/mean_absolute_percentage_error/","text":"elegy.losses.mean_absolute_percentage_error Computes the mean absolute percentage error (MAPE) between labels and predictions. After computing the absolute distance between the true value and the prediction value and divide by the true value, the mean value over the last dimension is returned. Usage: rng = jax . random . PRNGKey( 42 ) y_true = jax . random . randint(rng, shape = ( 2 , 3 ), minval =0 , maxval =2 ) y_pred = jax . random . uniform(rng, shape = ( 2 , 3 )) loss = elegy . losses . mean_absolute_percentage_error(y_true, y_pred) assert loss . shape == ( 2 ,) assert jnp . array_equal(loss, 100. * jnp . mean(jnp . abs((y_pred - y_true) / jnp . clip(y_true, types . EPSILON, None )))) Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean absolute percentage error values. shape = [batch_size, d0, .. dN-1] . Source code in elegy/losses/mean_absolute_percentage_error.py def mean_absolute_percentage_error ( y_true : jnp . ndarray , y_pred : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean absolute percentage error (MAPE) between labels and predictions. After computing the absolute distance between the true value and the prediction value and divide by the true value, the mean value over the last dimension is returned. Usage: ```python rng = jax.random.PRNGKey(42) y_true = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) y_pred = jax.random.uniform(rng, shape=(2, 3)) loss = elegy.losses.mean_absolute_percentage_error(y_true, y_pred) assert loss.shape == (2,) assert jnp.array_equal(loss, 100. * jnp.mean(jnp.abs((y_pred - y_true) / jnp.clip(y_true, types.EPSILON, None)))) ``` Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean absolute percentage error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" y_true = y_true . astype ( y_pred . dtype ) diff = jnp . abs (( y_pred - y_true ) / jnp . maximum ( jnp . abs ( y_true ), types . EPSILON )) return 100.0 * jnp . mean ( diff , axis =- 1 )","title":"mean_absolute_percentage_error"},{"location":"api/losses/mean_absolute_percentage_error/#elegylossesmean_absolute_percentage_error","text":"","title":"elegy.losses.mean_absolute_percentage_error"},{"location":"api/losses/mean_absolute_percentage_error/#elegy.losses.mean_absolute_percentage_error.mean_absolute_percentage_error","text":"Computes the mean absolute percentage error (MAPE) between labels and predictions. After computing the absolute distance between the true value and the prediction value and divide by the true value, the mean value over the last dimension is returned. Usage: rng = jax . random . PRNGKey( 42 ) y_true = jax . random . randint(rng, shape = ( 2 , 3 ), minval =0 , maxval =2 ) y_pred = jax . random . uniform(rng, shape = ( 2 , 3 )) loss = elegy . losses . mean_absolute_percentage_error(y_true, y_pred) assert loss . shape == ( 2 ,) assert jnp . array_equal(loss, 100. * jnp . mean(jnp . abs((y_pred - y_true) / jnp . clip(y_true, types . EPSILON, None )))) Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean absolute percentage error values. shape = [batch_size, d0, .. dN-1] . Source code in elegy/losses/mean_absolute_percentage_error.py def mean_absolute_percentage_error ( y_true : jnp . ndarray , y_pred : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean absolute percentage error (MAPE) between labels and predictions. After computing the absolute distance between the true value and the prediction value and divide by the true value, the mean value over the last dimension is returned. Usage: ```python rng = jax.random.PRNGKey(42) y_true = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) y_pred = jax.random.uniform(rng, shape=(2, 3)) loss = elegy.losses.mean_absolute_percentage_error(y_true, y_pred) assert loss.shape == (2,) assert jnp.array_equal(loss, 100. * jnp.mean(jnp.abs((y_pred - y_true) / jnp.clip(y_true, types.EPSILON, None)))) ``` Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean absolute percentage error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" y_true = y_true . astype ( y_pred . dtype ) diff = jnp . abs (( y_pred - y_true ) / jnp . maximum ( jnp . abs ( y_true ), types . EPSILON )) return 100.0 * jnp . mean ( diff , axis =- 1 )","title":"elegy.losses.mean_absolute_percentage_error.mean_absolute_percentage_error"},{"location":"api/losses/mean_squared_error/","text":"elegy.losses.mean_squared_error Computes the mean squared error between labels and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. loss = mean(square(y_true - y_pred), axis =-1 ) Usage: rng = jax . random . PRNGKey( 42 ) y_true = jax . random . randint(rng, shape = ( 2 , 3 ), minval =0 , maxval =2 ) y_pred = jax . random . uniform(rng, shape = ( 2 , 3 )) loss = elegy . losses . mean_squared_error(y_true, y_pred) assert loss . shape == ( 2 ,) assert jnp . array_equal(loss, jnp . mean(jnp . square(y_true - y_pred), axis =-1 )) Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean squared error values. shape = [batch_size, d0, .. dN-1] . Source code in elegy/losses/mean_squared_error.py def mean_squared_error ( y_true : jnp . ndarray , y_pred : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean squared error between labels and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. ```python loss = mean(square(y_true - y_pred), axis=-1) ``` Usage: ```python rng = jax.random.PRNGKey(42) y_true = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) y_pred = jax.random.uniform(rng, shape=(2, 3)) loss = elegy.losses.mean_squared_error(y_true, y_pred) assert loss.shape == (2,) assert jnp.array_equal(loss, jnp.mean(jnp.square(y_true - y_pred), axis=-1)) ``` Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean squared error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" y_true = y_true . astype ( y_pred . dtype ) return jnp . mean ( jnp . square ( y_pred - y_true ), axis =- 1 )","title":"mean_squared_error"},{"location":"api/losses/mean_squared_error/#elegylossesmean_squared_error","text":"","title":"elegy.losses.mean_squared_error"},{"location":"api/losses/mean_squared_error/#elegy.losses.mean_squared_error.mean_squared_error","text":"Computes the mean squared error between labels and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. loss = mean(square(y_true - y_pred), axis =-1 ) Usage: rng = jax . random . PRNGKey( 42 ) y_true = jax . random . randint(rng, shape = ( 2 , 3 ), minval =0 , maxval =2 ) y_pred = jax . random . uniform(rng, shape = ( 2 , 3 )) loss = elegy . losses . mean_squared_error(y_true, y_pred) assert loss . shape == ( 2 ,) assert jnp . array_equal(loss, jnp . mean(jnp . square(y_true - y_pred), axis =-1 )) Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean squared error values. shape = [batch_size, d0, .. dN-1] . Source code in elegy/losses/mean_squared_error.py def mean_squared_error ( y_true : jnp . ndarray , y_pred : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean squared error between labels and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. ```python loss = mean(square(y_true - y_pred), axis=-1) ``` Usage: ```python rng = jax.random.PRNGKey(42) y_true = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) y_pred = jax.random.uniform(rng, shape=(2, 3)) loss = elegy.losses.mean_squared_error(y_true, y_pred) assert loss.shape == (2,) assert jnp.array_equal(loss, jnp.mean(jnp.square(y_true - y_pred), axis=-1)) ``` Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean squared error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" y_true = y_true . astype ( y_pred . dtype ) return jnp . mean ( jnp . square ( y_pred - y_true ), axis =- 1 )","title":"elegy.losses.mean_squared_error.mean_squared_error"},{"location":"api/losses/mean_squared_logarithmic_error/","text":"elegy.losses.mean_squared_logarithmic_error Computes the mean squared logarithmic error between labels and predictions. loss = mean(square(log(y_true + 1 ) - log(y_pred + 1 )), axis =-1 ) Usage: rng = jax . random . PRNGKey( 42 ) y_true = jax . random . randint(rng, shape = ( 2 , 3 ), minval =0 , maxval =2 ) y_pred = jax . random . uniform(rng, shape = ( 2 , 3 )) loss = elegy . losses . mean_squared_logarithmic_error(y_true, y_pred) assert loss . shape == ( 2 ,) first_log = jnp . log(jnp . maximum(y_true, types . EPSILON) + 1.0 ) second_log = jnp . log(jnp . maximum(y_pred, types . EPSILON) + 1.0 ) assert jnp . array_equal(loss, jnp . mean(jnp . square(first_log - second_log), axis =-1 )) Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean squared logarithmic error values. shape = [batch_size, d0, .. dN-1] . Source code in elegy/losses/mean_squared_logarithmic_error.py def mean_squared_logarithmic_error ( y_true : jnp . ndarray , y_pred : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean squared logarithmic error between labels and predictions. ```python loss = mean(square(log(y_true + 1) - log(y_pred + 1)), axis=-1) ``` Usage: ```python rng = jax.random.PRNGKey(42) y_true = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) y_pred = jax.random.uniform(rng, shape=(2, 3)) loss = elegy.losses.mean_squared_logarithmic_error(y_true, y_pred) assert loss.shape == (2,) first_log = jnp.log(jnp.maximum(y_true, types.EPSILON) + 1.0) second_log = jnp.log(jnp.maximum(y_pred, types.EPSILON) + 1.0) assert jnp.array_equal(loss, jnp.mean(jnp.square(first_log - second_log), axis=-1)) ``` Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean squared logarithmic error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" y_true = y_true . astype ( y_pred . dtype ) first_log = jnp . log ( jnp . maximum ( y_true , types . EPSILON ) + 1.0 ) second_log = jnp . log ( jnp . maximum ( y_pred , types . EPSILON ) + 1.0 ) return jnp . mean ( jnp . square ( first_log - second_log ), axis =- 1 )","title":"mean_squared_logarithmic_error"},{"location":"api/losses/mean_squared_logarithmic_error/#elegylossesmean_squared_logarithmic_error","text":"","title":"elegy.losses.mean_squared_logarithmic_error"},{"location":"api/losses/mean_squared_logarithmic_error/#elegy.losses.mean_squared_logarithmic_error.mean_squared_logarithmic_error","text":"Computes the mean squared logarithmic error between labels and predictions. loss = mean(square(log(y_true + 1 ) - log(y_pred + 1 )), axis =-1 ) Usage: rng = jax . random . PRNGKey( 42 ) y_true = jax . random . randint(rng, shape = ( 2 , 3 ), minval =0 , maxval =2 ) y_pred = jax . random . uniform(rng, shape = ( 2 , 3 )) loss = elegy . losses . mean_squared_logarithmic_error(y_true, y_pred) assert loss . shape == ( 2 ,) first_log = jnp . log(jnp . maximum(y_true, types . EPSILON) + 1.0 ) second_log = jnp . log(jnp . maximum(y_pred, types . EPSILON) + 1.0 ) assert jnp . array_equal(loss, jnp . mean(jnp . square(first_log - second_log), axis =-1 )) Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean squared logarithmic error values. shape = [batch_size, d0, .. dN-1] . Source code in elegy/losses/mean_squared_logarithmic_error.py def mean_squared_logarithmic_error ( y_true : jnp . ndarray , y_pred : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean squared logarithmic error between labels and predictions. ```python loss = mean(square(log(y_true + 1) - log(y_pred + 1)), axis=-1) ``` Usage: ```python rng = jax.random.PRNGKey(42) y_true = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) y_pred = jax.random.uniform(rng, shape=(2, 3)) loss = elegy.losses.mean_squared_logarithmic_error(y_true, y_pred) assert loss.shape == (2,) first_log = jnp.log(jnp.maximum(y_true, types.EPSILON) + 1.0) second_log = jnp.log(jnp.maximum(y_pred, types.EPSILON) + 1.0) assert jnp.array_equal(loss, jnp.mean(jnp.square(first_log - second_log), axis=-1)) ``` Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean squared logarithmic error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" y_true = y_true . astype ( y_pred . dtype ) first_log = jnp . log ( jnp . maximum ( y_true , types . EPSILON ) + 1.0 ) second_log = jnp . log ( jnp . maximum ( y_pred , types . EPSILON ) + 1.0 ) return jnp . mean ( jnp . square ( first_log - second_log ), axis =- 1 )","title":"elegy.losses.mean_squared_logarithmic_error.mean_squared_logarithmic_error"},{"location":"api/losses/sparse_categorical_crossentropy/","text":"elegy.losses.sparse_categorical_crossentropy Source code in elegy/losses/sparse_categorical_crossentropy.py def sparse_categorical_crossentropy ( y_true : jnp . ndarray , y_pred : jnp . ndarray , from_logits : bool = False , check_bounds : bool = True , ) -> jnp . ndarray : n_classes = y_pred . shape [ - 1 ] if from_logits : y_pred = jax . nn . log_softmax ( y_pred ) loss = - jnp . take_along_axis ( y_pred , y_true [ ... , None ], axis =- 1 )[ ... , 0 ] else : # select output value y_pred = jnp . take_along_axis ( y_pred , y_true [ ... , None ], axis =- 1 )[ ... , 0 ] # calculate log y_pred = jnp . maximum ( y_pred , types . EPSILON ) y_pred = jnp . log ( y_pred ) loss = - y_pred if check_bounds : # set NaN where y_true is negative or larger/equal to the number of y_pred channels loss = jnp . where ( y_true < 0 , jnp . nan , loss ) loss = jnp . where ( y_true >= n_classes , jnp . nan , loss ) return loss","title":"sparse_categorical_crossentropy"},{"location":"api/losses/sparse_categorical_crossentropy/#elegylossessparse_categorical_crossentropy","text":"","title":"elegy.losses.sparse_categorical_crossentropy"},{"location":"api/losses/sparse_categorical_crossentropy/#elegy.losses.sparse_categorical_crossentropy.sparse_categorical_crossentropy","text":"Source code in elegy/losses/sparse_categorical_crossentropy.py def sparse_categorical_crossentropy ( y_true : jnp . ndarray , y_pred : jnp . ndarray , from_logits : bool = False , check_bounds : bool = True , ) -> jnp . ndarray : n_classes = y_pred . shape [ - 1 ] if from_logits : y_pred = jax . nn . log_softmax ( y_pred ) loss = - jnp . take_along_axis ( y_pred , y_true [ ... , None ], axis =- 1 )[ ... , 0 ] else : # select output value y_pred = jnp . take_along_axis ( y_pred , y_true [ ... , None ], axis =- 1 )[ ... , 0 ] # calculate log y_pred = jnp . maximum ( y_pred , types . EPSILON ) y_pred = jnp . log ( y_pred ) loss = - y_pred if check_bounds : # set NaN where y_true is negative or larger/equal to the number of y_pred channels loss = jnp . where ( y_true < 0 , jnp . nan , loss ) loss = jnp . where ( y_true >= n_classes , jnp . nan , loss ) return loss","title":"elegy.losses.sparse_categorical_crossentropy.sparse_categorical_crossentropy"},{"location":"api/metrics/Accuracy/","text":"elegy.metrics.Accuracy Calculates how often predictions equals labels. This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true . This frequency is ultimately returned as binary accuracy : an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. accuracy = elegy . metrics . Accuracy() result = accuracy( y_true = jnp . array([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array([ 0 , 1 , 1 , 1 ]) ) assert result == 0.75 # 3 / 4 result = accuracy( y_true = jnp . array([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array([ 1 , 0 , 0 , 0 ]) ) assert result == 0.5 # 4 / 8 Usage with elegy API: model = elegy . Model( module_fn, loss = elegy . losses . CategoricalCrossentropy(), metrics = elegy . metrics . Accuracy(), optimizer = optax . adam( 1e-3 ), ) __init__ ( self , on = None , ** kwargs ) special Creates a Accuracy instance. Parameters: Name Type Description Default on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/accuracy.py def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Creates a `Accuracy` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs ) call ( self , y_true , y_pred , sample_weight = None ) Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/accuracy.py def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = accuracy ( y_true = y_true , y_pred = y_pred ), sample_weight = sample_weight , )","title":"Accuracy"},{"location":"api/metrics/Accuracy/#elegymetricsaccuracy","text":"","title":"elegy.metrics.Accuracy"},{"location":"api/metrics/Accuracy/#elegy.metrics.accuracy.Accuracy","text":"Calculates how often predictions equals labels. This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true . This frequency is ultimately returned as binary accuracy : an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. accuracy = elegy . metrics . Accuracy() result = accuracy( y_true = jnp . array([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array([ 0 , 1 , 1 , 1 ]) ) assert result == 0.75 # 3 / 4 result = accuracy( y_true = jnp . array([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array([ 1 , 0 , 0 , 0 ]) ) assert result == 0.5 # 4 / 8 Usage with elegy API: model = elegy . Model( module_fn, loss = elegy . losses . CategoricalCrossentropy(), metrics = elegy . metrics . Accuracy(), optimizer = optax . adam( 1e-3 ), )","title":"elegy.metrics.accuracy.Accuracy"},{"location":"api/metrics/Accuracy/#elegy.metrics.accuracy.Accuracy.__init__","text":"Creates a Accuracy instance. Parameters: Name Type Description Default on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/accuracy.py def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Creates a `Accuracy` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs )","title":"__init__()"},{"location":"api/metrics/Accuracy/#elegy.metrics.accuracy.Accuracy.call","text":"Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/accuracy.py def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = accuracy ( y_true = y_true , y_pred = y_pred ), sample_weight = sample_weight , )","title":"call()"},{"location":"api/metrics/BinaryAccuracy/","text":"elegy.metrics.BinaryAccuracy Calculates how often predictions matches binary labels. Standalone usage: m = elegy . metrics . BinaryAccuracy() result = m( y_true = np . array([[ 1 ], [ 1 ], [ 0 ], [ 0 ]]), y_pred = np . array([[ 0.98 ], [ 1 ], [ 0 ], [ 0.6 ]]), ) assert result == 0.75 m = elegy . metrics . BinaryAccuracy() result = m( y_true = np . array([[ 1 ], [ 1 ], [ 0 ], [ 0 ]]), y_pred = np . array([[ 0.98 ], [ 1 ], [ 0 ], [ 0.6 ]]), sample_weight = np . array([ 1 , 0 , 0 , 1 ]), ) assert result == 0.5 Usage with Model API: model = elegy . Model( ... metrics = [tf . keras . metrics . BinaryAccuracy()], ) __init__ ( self , threshold = 0.5 , on = None , ** kwargs ) special Creates a BinaryAccuracy instance. Parameters: Name Type Description Default threshold float Float representing the threshold for deciding whether prediction values are 1 or 0. 0.5 on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/binary_accuracy.py def __init__ ( self , threshold : float = 0.5 , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Creates a `BinaryAccuracy` instance. Arguments: threshold: Float representing the threshold for deciding whether prediction values are 1 or 0. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs ) self . threshold = threshold call ( self , y_true , y_pred , sample_weight = None ) Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/binary_accuracy.py def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = binary_accuracy ( y_true = y_true , y_pred = y_pred , threshold = self . threshold ), sample_weight = sample_weight , )","title":"BinaryAccuracy"},{"location":"api/metrics/BinaryAccuracy/#elegymetricsbinaryaccuracy","text":"","title":"elegy.metrics.BinaryAccuracy"},{"location":"api/metrics/BinaryAccuracy/#elegy.metrics.binary_accuracy.BinaryAccuracy","text":"Calculates how often predictions matches binary labels. Standalone usage: m = elegy . metrics . BinaryAccuracy() result = m( y_true = np . array([[ 1 ], [ 1 ], [ 0 ], [ 0 ]]), y_pred = np . array([[ 0.98 ], [ 1 ], [ 0 ], [ 0.6 ]]), ) assert result == 0.75 m = elegy . metrics . BinaryAccuracy() result = m( y_true = np . array([[ 1 ], [ 1 ], [ 0 ], [ 0 ]]), y_pred = np . array([[ 0.98 ], [ 1 ], [ 0 ], [ 0.6 ]]), sample_weight = np . array([ 1 , 0 , 0 , 1 ]), ) assert result == 0.5 Usage with Model API: model = elegy . Model( ... metrics = [tf . keras . metrics . BinaryAccuracy()], )","title":"elegy.metrics.binary_accuracy.BinaryAccuracy"},{"location":"api/metrics/BinaryAccuracy/#elegy.metrics.binary_accuracy.BinaryAccuracy.__init__","text":"Creates a BinaryAccuracy instance. Parameters: Name Type Description Default threshold float Float representing the threshold for deciding whether prediction values are 1 or 0. 0.5 on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/binary_accuracy.py def __init__ ( self , threshold : float = 0.5 , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Creates a `BinaryAccuracy` instance. Arguments: threshold: Float representing the threshold for deciding whether prediction values are 1 or 0. on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs ) self . threshold = threshold","title":"__init__()"},{"location":"api/metrics/BinaryAccuracy/#elegy.metrics.binary_accuracy.BinaryAccuracy.call","text":"Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/binary_accuracy.py def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = binary_accuracy ( y_true = y_true , y_pred = y_pred , threshold = self . threshold ), sample_weight = sample_weight , )","title":"call()"},{"location":"api/metrics/BinaryCrossentropy/","text":"elegy.metrics.BinaryCrossentropy Computes the crossentropy metric between the labels and predictions. This is the crossentropy metric class to be used when there are only two label classes (0 and 1). Usage: y_true = jnp . array([[ 0. , 1. ], [ 0. , 0. ]]), y_pred = jnp . array([[ 0.6 , 0.4 ], [ 0.4 , 0.6 ]]) bce = elegy . metrics . BinaryCrossentropy() result = bce( y_true = y_true, y_pred = y_pred, ) assert jnp . isclose(result, 0.815 , rtol =0.01 ) # BCE using sample_weight bce = elegy . metrics . BinaryCrossentropy() result = bce(y_true, y_pred, sample_weight = jnp . array([ 1. , 0. ])) assert jnp . isclose(result, 0.916 , rtol =0.01 ) Usage with elegy API: model = elegy . Model( module_fn, loss = elegy . losses . CategoricalCrossentropy(), metrics = elegy . metrics . BinaryCrossentropy(), ) __init__ ( self , from_logits = False , on = None , ** kwargs ) special Creates a BinaryCrossentropy instance. Parameters: Name Type Description Default from_logits bool Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution. Note - Using from_logits=True is more numerically stable. False on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/binary_crossentropy.py def __init__ ( self , from_logits : bool = False , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Creates a `BinaryCrossentropy` instance. Arguments: from_logits: Whether `y_pred` is expected to be a logits tensor. By default, we assume that `y_pred` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs ) self . _from_logits = from_logits call ( self , y_true , y_pred , sample_weight = None ) Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/binary_crossentropy.py def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = binary_crossentropy ( y_true = y_true , y_pred = y_pred , from_logits = self . _from_logits ), sample_weight = sample_weight , )","title":"BinaryCrossentropy"},{"location":"api/metrics/BinaryCrossentropy/#elegymetricsbinarycrossentropy","text":"","title":"elegy.metrics.BinaryCrossentropy"},{"location":"api/metrics/BinaryCrossentropy/#elegy.metrics.binary_crossentropy.BinaryCrossentropy","text":"Computes the crossentropy metric between the labels and predictions. This is the crossentropy metric class to be used when there are only two label classes (0 and 1). Usage: y_true = jnp . array([[ 0. , 1. ], [ 0. , 0. ]]), y_pred = jnp . array([[ 0.6 , 0.4 ], [ 0.4 , 0.6 ]]) bce = elegy . metrics . BinaryCrossentropy() result = bce( y_true = y_true, y_pred = y_pred, ) assert jnp . isclose(result, 0.815 , rtol =0.01 ) # BCE using sample_weight bce = elegy . metrics . BinaryCrossentropy() result = bce(y_true, y_pred, sample_weight = jnp . array([ 1. , 0. ])) assert jnp . isclose(result, 0.916 , rtol =0.01 ) Usage with elegy API: model = elegy . Model( module_fn, loss = elegy . losses . CategoricalCrossentropy(), metrics = elegy . metrics . BinaryCrossentropy(), )","title":"elegy.metrics.binary_crossentropy.BinaryCrossentropy"},{"location":"api/metrics/BinaryCrossentropy/#elegy.metrics.binary_crossentropy.BinaryCrossentropy.__init__","text":"Creates a BinaryCrossentropy instance. Parameters: Name Type Description Default from_logits bool Whether y_pred is expected to be a logits tensor. By default, we assume that y_pred encodes a probability distribution. Note - Using from_logits=True is more numerically stable. False on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/binary_crossentropy.py def __init__ ( self , from_logits : bool = False , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Creates a `BinaryCrossentropy` instance. Arguments: from_logits: Whether `y_pred` is expected to be a logits tensor. By default, we assume that `y_pred` encodes a probability distribution. **Note - Using from_logits=True is more numerically stable.** on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs ) self . _from_logits = from_logits","title":"__init__()"},{"location":"api/metrics/BinaryCrossentropy/#elegy.metrics.binary_crossentropy.BinaryCrossentropy.call","text":"Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/binary_crossentropy.py def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = binary_crossentropy ( y_true = y_true , y_pred = y_pred , from_logits = self . _from_logits ), sample_weight = sample_weight , )","title":"call()"},{"location":"api/metrics/CategoricalAccuracy/","text":"elegy.metrics.CategoricalAccuracy Calculates how often predictions matches one-hot labels. You can provide logits of classes as y_pred , since argmax of logits and probabilities are same. This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true . This frequency is ultimately returned as categorical accuracy : an idempotent operation that simply divides total by count . y_pred and y_true should be passed in as vectors of probabilities, rather than as labels. If necessary, use tf.one_hot to expand y_true as a vector. If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Usage: accuracy = elegy . metrics . CategoricalAccuracy() result = accuracy( y_true = jnp . array([[ 0 , 0 , 1 ], [ 0 , 1 , 0 ]]), y_pred = jnp . array([[ 0.1 , 0.9 , 0.8 ], [ 0.05 , 0.95 , 0 ]]), ) assert result == 0.5 # 1/2 result = accuracy( y_true = jnp . array([[ 0 , 1 , 0 ], [ 0 , 1 , 0 ]]), y_pred = jnp . array([[ 0.1 , 0.9 , 0.8 ], [ 0.05 , 0.95 , 0 ]]), ) assert result == 0.75 # 3/4 Usage with elegy API: model = elegy . Model( module_fn, loss = elegy . losses . CategoricalCrossentropy(), metrics = elegy . metrics . CategoricalAccuracy(), optimizer = optax . adam( 1e-3 ), ) __init__ ( self , on = None , ** kwargs ) special Creates a CategoricalAccuracy instance. Parameters: Name Type Description Default on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/categorical_accuracy.py def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Creates a `CategoricalAccuracy` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs ) call ( self , y_true , y_pred , sample_weight = None ) Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/categorical_accuracy.py def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = categorical_accuracy ( y_true = y_true , y_pred = y_pred ), sample_weight = sample_weight , )","title":"CategoricalAccuracy"},{"location":"api/metrics/CategoricalAccuracy/#elegymetricscategoricalaccuracy","text":"","title":"elegy.metrics.CategoricalAccuracy"},{"location":"api/metrics/CategoricalAccuracy/#elegy.metrics.categorical_accuracy.CategoricalAccuracy","text":"Calculates how often predictions matches one-hot labels. You can provide logits of classes as y_pred , since argmax of logits and probabilities are same. This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true . This frequency is ultimately returned as categorical accuracy : an idempotent operation that simply divides total by count . y_pred and y_true should be passed in as vectors of probabilities, rather than as labels. If necessary, use tf.one_hot to expand y_true as a vector. If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Usage: accuracy = elegy . metrics . CategoricalAccuracy() result = accuracy( y_true = jnp . array([[ 0 , 0 , 1 ], [ 0 , 1 , 0 ]]), y_pred = jnp . array([[ 0.1 , 0.9 , 0.8 ], [ 0.05 , 0.95 , 0 ]]), ) assert result == 0.5 # 1/2 result = accuracy( y_true = jnp . array([[ 0 , 1 , 0 ], [ 0 , 1 , 0 ]]), y_pred = jnp . array([[ 0.1 , 0.9 , 0.8 ], [ 0.05 , 0.95 , 0 ]]), ) assert result == 0.75 # 3/4 Usage with elegy API: model = elegy . Model( module_fn, loss = elegy . losses . CategoricalCrossentropy(), metrics = elegy . metrics . CategoricalAccuracy(), optimizer = optax . adam( 1e-3 ), )","title":"elegy.metrics.categorical_accuracy.CategoricalAccuracy"},{"location":"api/metrics/CategoricalAccuracy/#elegy.metrics.categorical_accuracy.CategoricalAccuracy.__init__","text":"Creates a CategoricalAccuracy instance. Parameters: Name Type Description Default on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/categorical_accuracy.py def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Creates a `CategoricalAccuracy` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs )","title":"__init__()"},{"location":"api/metrics/CategoricalAccuracy/#elegy.metrics.categorical_accuracy.CategoricalAccuracy.call","text":"Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/categorical_accuracy.py def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = categorical_accuracy ( y_true = y_true , y_pred = y_pred ), sample_weight = sample_weight , )","title":"call()"},{"location":"api/metrics/F1/","text":"elegy.metrics.F1 The metric calculates the Harmonic mean between precision and recall. This value is ultimately returned as f1 . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. If sample_weight is None, weights default to 1. Use sample_weight of 0 to mask values. If class_id is specified, we calculate precision by considering only the entries in the batch for which class_id is above the threshold and computing the fraction of them for which class_id is indeed a correct label. f1 = elegy . metrics . F1() result = f1( y_true = jnp . array([ 0 , 1 , 1 , 1 ]), y_pred = jnp . array([ 1 , 0 , 1 , 1 ]) ) assert result == 0.6666667 # 2 * (0.44445 / 1.33334) result = f1( y_true = jnp . array([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array([ 1 , 1 , 0 , 0 ]) ) assert result == 666667 # 2 * (0.5 / 1.5) Usage with elegy API: model = elegy . Model( module_fn, loss = elegy . losses . CategoricalCrossentropy(), metrics = elegy . metrics . F1(), optimizer = optix . adam( 1e-3 ), ) __init__ ( self , on = None , threshold = None , class_id = None , ** kwargs ) special Creates a F1 instance. Parameters: Name Type Description Default on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None threshold (Optional) A float value or a python list/tuple of float threshold values in [0, 1]. A threshold is compared with prediction values to determine the truth value of predictions (i.e., above the threshold is true, below is false). One metric value is generated for each threshold value. If neither threshold is set the default is to calculate precision with threshold=0.5. None class_id (Optional) Integer class ID for which we want binary metrics. This must be in the half-open interval [0, num_classes) , where num_classes is the last dimension of predictions. None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/f1.py def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , threshold = None , class_id = None , ** kwargs ): \"\"\" Creates a `F1` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). threshold: (Optional) A float value or a python list/tuple of float threshold values in [0, 1]. A threshold is compared with prediction values to determine the truth value of predictions (i.e., above the threshold is true, below is false). One metric value is generated for each threshold value. If neither threshold is set the default is to calculate precision with threshold=0.5. class_id: (Optional) Integer class ID for which we want binary metrics. This must be in the half-open interval `[0, num_classes)`, where `num_classes` is the last dimension of predictions. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs ) self . threshold = 0.5 if threshold is None else threshold self . class_id = 1 if class_id is None else class_id self . precision = Precision ( threshold = self . threshold ) self . recall = Recall ( threshold = self . threshold ) call ( self , y_true , y_pred , sample_weight = None ) Accumulates f1 values (armonic mean between precision and recall). Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional weighting of each example. Defaults to 1. Can be a Tensor whose rank is either 0, or the same rank as y_true , and must be broadcastable to y_true . None Returns: Type Description ndarray Array with the cumulative f1. Source code in elegy/metrics/f1.py def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates f1 values (armonic mean between precision and recall). Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional weighting of each example. Defaults to 1. Can be a `Tensor` whose rank is either 0, or the same rank as `y_true`, and must be broadcastable to `y_true`. Returns: Array with the cumulative f1. \"\"\" return f1 ( y_true = y_true , y_pred = y_pred , sample_weight = sample_weight , precision = self . precision , recall = self . recall , )","title":"F1"},{"location":"api/metrics/F1/#elegymetricsf1","text":"","title":"elegy.metrics.F1"},{"location":"api/metrics/F1/#elegy.metrics.f1.F1","text":"The metric calculates the Harmonic mean between precision and recall. This value is ultimately returned as f1 . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. If sample_weight is None, weights default to 1. Use sample_weight of 0 to mask values. If class_id is specified, we calculate precision by considering only the entries in the batch for which class_id is above the threshold and computing the fraction of them for which class_id is indeed a correct label. f1 = elegy . metrics . F1() result = f1( y_true = jnp . array([ 0 , 1 , 1 , 1 ]), y_pred = jnp . array([ 1 , 0 , 1 , 1 ]) ) assert result == 0.6666667 # 2 * (0.44445 / 1.33334) result = f1( y_true = jnp . array([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array([ 1 , 1 , 0 , 0 ]) ) assert result == 666667 # 2 * (0.5 / 1.5) Usage with elegy API: model = elegy . Model( module_fn, loss = elegy . losses . CategoricalCrossentropy(), metrics = elegy . metrics . F1(), optimizer = optix . adam( 1e-3 ), )","title":"elegy.metrics.f1.F1"},{"location":"api/metrics/F1/#elegy.metrics.f1.F1.__init__","text":"Creates a F1 instance. Parameters: Name Type Description Default on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None threshold (Optional) A float value or a python list/tuple of float threshold values in [0, 1]. A threshold is compared with prediction values to determine the truth value of predictions (i.e., above the threshold is true, below is false). One metric value is generated for each threshold value. If neither threshold is set the default is to calculate precision with threshold=0.5. None class_id (Optional) Integer class ID for which we want binary metrics. This must be in the half-open interval [0, num_classes) , where num_classes is the last dimension of predictions. None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/f1.py def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , threshold = None , class_id = None , ** kwargs ): \"\"\" Creates a `F1` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). threshold: (Optional) A float value or a python list/tuple of float threshold values in [0, 1]. A threshold is compared with prediction values to determine the truth value of predictions (i.e., above the threshold is true, below is false). One metric value is generated for each threshold value. If neither threshold is set the default is to calculate precision with threshold=0.5. class_id: (Optional) Integer class ID for which we want binary metrics. This must be in the half-open interval `[0, num_classes)`, where `num_classes` is the last dimension of predictions. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs ) self . threshold = 0.5 if threshold is None else threshold self . class_id = 1 if class_id is None else class_id self . precision = Precision ( threshold = self . threshold ) self . recall = Recall ( threshold = self . threshold )","title":"__init__()"},{"location":"api/metrics/F1/#elegy.metrics.f1.F1.call","text":"Accumulates f1 values (armonic mean between precision and recall). Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional weighting of each example. Defaults to 1. Can be a Tensor whose rank is either 0, or the same rank as y_true , and must be broadcastable to y_true . None Returns: Type Description ndarray Array with the cumulative f1. Source code in elegy/metrics/f1.py def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates f1 values (armonic mean between precision and recall). Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional weighting of each example. Defaults to 1. Can be a `Tensor` whose rank is either 0, or the same rank as `y_true`, and must be broadcastable to `y_true`. Returns: Array with the cumulative f1. \"\"\" return f1 ( y_true = y_true , y_pred = y_pred , sample_weight = sample_weight , precision = self . precision , recall = self . recall , )","title":"call()"},{"location":"api/metrics/Mean/","text":"elegy.metrics.Mean Computes the (weighted) mean of the given values. For example, if values is [1, 3, 5, 7] then the mean is 4 . If the weights were specified as [1, 1, 0, 0] then the mean would be 2 . This metric creates two variables, total and count that are used to compute the average of values . This average is ultimately returned as mean which is an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Usage: mean = elegy . metrics . Mean() result = mean([ 1 , 3 , 5 , 7 ]) # 16 / 4 assert result == 4.0 result = mean([ 4 , 10 ]) # 30 / 6 assert result == 5.0 Usage with elegy API: model = elegy . Model( module_fn, loss = elegy . losses . MeanSquaredError(), metrics = elegy . metrics . Mean(), ) __init__ ( self , on = None , ** kwargs ) special Creates a Mean instance. Parameters: Name Type Description Default on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/mean.py def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\"Creates a `Mean` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( reduction = Reduction . WEIGHTED_MEAN , on = on , ** kwargs ) call ( self , values , sample_weight = None ) Accumulates the mean statistic over various batches. Parameters: Name Type Description Default values ndarray Per-example value. required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional weighting of each example. None Returns: Type Description ndarray Array with the cumulative mean. Source code in elegy/metrics/mean.py def call ( self , values : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None ) -> jnp . ndarray : \"\"\" Accumulates the mean statistic over various batches. Arguments: values: Per-example value. sample_weight: Optional weighting of each example. Returns: Array with the cumulative mean. \"\"\" return super () . call ( values = values , sample_weight = sample_weight )","title":"Mean"},{"location":"api/metrics/Mean/#elegymetricsmean","text":"","title":"elegy.metrics.Mean"},{"location":"api/metrics/Mean/#elegy.metrics.mean.Mean","text":"Computes the (weighted) mean of the given values. For example, if values is [1, 3, 5, 7] then the mean is 4 . If the weights were specified as [1, 1, 0, 0] then the mean would be 2 . This metric creates two variables, total and count that are used to compute the average of values . This average is ultimately returned as mean which is an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Usage: mean = elegy . metrics . Mean() result = mean([ 1 , 3 , 5 , 7 ]) # 16 / 4 assert result == 4.0 result = mean([ 4 , 10 ]) # 30 / 6 assert result == 5.0 Usage with elegy API: model = elegy . Model( module_fn, loss = elegy . losses . MeanSquaredError(), metrics = elegy . metrics . Mean(), )","title":"elegy.metrics.mean.Mean"},{"location":"api/metrics/Mean/#elegy.metrics.mean.Mean.__init__","text":"Creates a Mean instance. Parameters: Name Type Description Default on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/mean.py def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\"Creates a `Mean` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( reduction = Reduction . WEIGHTED_MEAN , on = on , ** kwargs )","title":"__init__()"},{"location":"api/metrics/Mean/#elegy.metrics.mean.Mean.call","text":"Accumulates the mean statistic over various batches. Parameters: Name Type Description Default values ndarray Per-example value. required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional weighting of each example. None Returns: Type Description ndarray Array with the cumulative mean. Source code in elegy/metrics/mean.py def call ( self , values : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None ) -> jnp . ndarray : \"\"\" Accumulates the mean statistic over various batches. Arguments: values: Per-example value. sample_weight: Optional weighting of each example. Returns: Array with the cumulative mean. \"\"\" return super () . call ( values = values , sample_weight = sample_weight )","title":"call()"},{"location":"api/metrics/MeanAbsoluteError/","text":"elegy.metrics.MeanAbsoluteError Computes the cumulative mean absolute error between y_true and y_pred . Usage: mae = elegy . metrics . MeanAbsoluteError() result = mae(y_true = jnp . array([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array([ 0 , 1 , 1 , 1 ])) assert result == 0.25 result = mae(y_true = jnp . array([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array([ 1 , 0 , 0 , 0 ])) assert result == 0.5 Usage with elegy API: model = elegy . Model( module_fn, loss = elegy . losses . CategoricalCrossentropy(), metrics = elegy . metrics . MeanAbsoluteError(), ) __init__ ( self , on = None , ** kwargs ) special Creates a MeanAbsoluteError instance. Parameters: Name Type Description Default on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/mean_absolute_error.py def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Creates a `MeanAbsoluteError` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs ) call ( self , y_true , y_pred , sample_weight = None ) Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/mean_absolute_error.py def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = mean_absolute_error ( y_true = y_true , y_pred = y_pred ))","title":"MeanAbsoluteError"},{"location":"api/metrics/MeanAbsoluteError/#elegymetricsmeanabsoluteerror","text":"","title":"elegy.metrics.MeanAbsoluteError"},{"location":"api/metrics/MeanAbsoluteError/#elegy.metrics.mean_absolute_error.MeanAbsoluteError","text":"Computes the cumulative mean absolute error between y_true and y_pred . Usage: mae = elegy . metrics . MeanAbsoluteError() result = mae(y_true = jnp . array([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array([ 0 , 1 , 1 , 1 ])) assert result == 0.25 result = mae(y_true = jnp . array([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array([ 1 , 0 , 0 , 0 ])) assert result == 0.5 Usage with elegy API: model = elegy . Model( module_fn, loss = elegy . losses . CategoricalCrossentropy(), metrics = elegy . metrics . MeanAbsoluteError(), )","title":"elegy.metrics.mean_absolute_error.MeanAbsoluteError"},{"location":"api/metrics/MeanAbsoluteError/#elegy.metrics.mean_absolute_error.MeanAbsoluteError.__init__","text":"Creates a MeanAbsoluteError instance. Parameters: Name Type Description Default on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/mean_absolute_error.py def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Creates a `MeanAbsoluteError` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs )","title":"__init__()"},{"location":"api/metrics/MeanAbsoluteError/#elegy.metrics.mean_absolute_error.MeanAbsoluteError.call","text":"Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/mean_absolute_error.py def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = mean_absolute_error ( y_true = y_true , y_pred = y_pred ))","title":"call()"},{"location":"api/metrics/MeanAbsolutePercentageError/","text":"elegy.metrics.MeanAbsolutePercentageError Computes the cumulative mean absoluted percentage error between y_true and y_pred . Usage: mape = elegy . metrics . MeanAbsolutePercentageError() result = mape(y_true = jnp . array([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array([ 0 , 1 , 1 , 1 ])) assert result == 25. result = mape(y_true = jnp . array([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array([ 1 , 0 , 0 , 0 ])) assert result == 50. Usage with elegy API: model = elegy . Model( module_fn, loss = elegy . losses . CategoricalCrossentropy(), metrics = elegy . metrics . MeanAbsolutePercentageError(), ) __init__ ( self , on = None , ** kwargs ) special Creates a MeanAbsolutePercentageError instance. Parameters: Name Type Description Default on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/mean_absolute_percentage_error.py def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Creates a `MeanAbsolutePercentageError` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs ) call ( self , y_true , y_pred , sample_weight = None ) Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/mean_absolute_percentage_error.py def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = mean_absolute_percentage_error ( y_true = y_true , y_pred = y_pred ) )","title":"MeanAbsolutePercentageError"},{"location":"api/metrics/MeanAbsolutePercentageError/#elegymetricsmeanabsolutepercentageerror","text":"","title":"elegy.metrics.MeanAbsolutePercentageError"},{"location":"api/metrics/MeanAbsolutePercentageError/#elegy.metrics.mean_absolute_percentage_error.MeanAbsolutePercentageError","text":"Computes the cumulative mean absoluted percentage error between y_true and y_pred . Usage: mape = elegy . metrics . MeanAbsolutePercentageError() result = mape(y_true = jnp . array([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array([ 0 , 1 , 1 , 1 ])) assert result == 25. result = mape(y_true = jnp . array([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array([ 1 , 0 , 0 , 0 ])) assert result == 50. Usage with elegy API: model = elegy . Model( module_fn, loss = elegy . losses . CategoricalCrossentropy(), metrics = elegy . metrics . MeanAbsolutePercentageError(), )","title":"elegy.metrics.mean_absolute_percentage_error.MeanAbsolutePercentageError"},{"location":"api/metrics/MeanAbsolutePercentageError/#elegy.metrics.mean_absolute_percentage_error.MeanAbsolutePercentageError.__init__","text":"Creates a MeanAbsolutePercentageError instance. Parameters: Name Type Description Default on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/mean_absolute_percentage_error.py def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Creates a `MeanAbsolutePercentageError` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs )","title":"__init__()"},{"location":"api/metrics/MeanAbsolutePercentageError/#elegy.metrics.mean_absolute_percentage_error.MeanAbsolutePercentageError.call","text":"Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/mean_absolute_percentage_error.py def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = mean_absolute_percentage_error ( y_true = y_true , y_pred = y_pred ) )","title":"call()"},{"location":"api/metrics/MeanSquaredError/","text":"elegy.metrics.MeanSquaredError Computes the cumulative mean squared error between y_true and y_pred . Usage: mse = elegy . metrics . MeanSquaredError() result = mse(y_true = jnp . array([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array([ 0 , 1 , 1 , 1 ])) assert result == 0.25 result = mse(y_true = jnp . array([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array([ 1 , 0 , 0 , 0 ])) assert result == 0.5 Usage with elegy API: model = elegy . Model( module_fn, loss = elegy . losses . CategoricalCrossentropy(), metrics = elegy . metrics . MeanSquaredError(), ) __init__ ( self , on = None , ** kwargs ) special Creates a MeanSquaredError instance. Parameters: Name Type Description Default on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/mean_squared_error.py def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Creates a `MeanSquaredError` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs ) call ( self , y_true , y_pred , sample_weight = None ) Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/mean_squared_error.py def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = mean_squared_error ( y_true = y_true , y_pred = y_pred ))","title":"MeanSquaredError"},{"location":"api/metrics/MeanSquaredError/#elegymetricsmeansquarederror","text":"","title":"elegy.metrics.MeanSquaredError"},{"location":"api/metrics/MeanSquaredError/#elegy.metrics.mean_squared_error.MeanSquaredError","text":"Computes the cumulative mean squared error between y_true and y_pred . Usage: mse = elegy . metrics . MeanSquaredError() result = mse(y_true = jnp . array([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array([ 0 , 1 , 1 , 1 ])) assert result == 0.25 result = mse(y_true = jnp . array([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array([ 1 , 0 , 0 , 0 ])) assert result == 0.5 Usage with elegy API: model = elegy . Model( module_fn, loss = elegy . losses . CategoricalCrossentropy(), metrics = elegy . metrics . MeanSquaredError(), )","title":"elegy.metrics.mean_squared_error.MeanSquaredError"},{"location":"api/metrics/MeanSquaredError/#elegy.metrics.mean_squared_error.MeanSquaredError.__init__","text":"Creates a MeanSquaredError instance. Parameters: Name Type Description Default on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/mean_squared_error.py def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Creates a `MeanSquaredError` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs )","title":"__init__()"},{"location":"api/metrics/MeanSquaredError/#elegy.metrics.mean_squared_error.MeanSquaredError.call","text":"Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/mean_squared_error.py def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = mean_squared_error ( y_true = y_true , y_pred = y_pred ))","title":"call()"},{"location":"api/metrics/Metric/","text":"elegy.metrics.Metric Encapsulates metric logic and state. Metrics accumulate state between apply s such that their output value reflect the metric as if calculated on the whole data given up to that point. Usage: m = SomeMetric() _, state = m . init()(x) for x in batch: result = m . apply(state)(x) print ( 'Final result: ' , result) Usage with the Model API: >>> import elegy , jax , optax >>> model = elegy . Model( ... module = elegy . nn . Sequential( ... lambda : [ ... elegy . nn . Flatten(), ... elegy . nn . Linear( 300 ), ... jax . nn . relu, ... elegy . nn . Linear( 10 ), ... ] ... ), ... loss = [ ... elegy . losses . SparseCategoricalCrossentropy(from_logits = True ), ... ], ... metrics = [ ... elegy . metrics . SparseCategoricalAccuracy() ... ], ... optimizer = optax . rmsprop( 1e-3 ), ... ) To be implemented by subclasses: call() : All state variables should be created in this method by calling self.add_parameter(..., trainable=False) , update this state by calling self.update_parameter(...) , and return a result based on these states. Example subclass implementation: >>> class Accuracy (elegy . Metric): ... def call ( self , y_true, y_pred): ... ... total = self . add_parameter( \"total\" , lambda : jnp . array( 0 ), trainable = False ) ... count = self . add_parameter( \"count\" , lambda : jnp . array( 0 ), trainable = False ) ... ... total += jnp . sum(y_true == y_pred) ... count += jnp . prod(y_true . shape) ... ... self . update_parameter( \"total\" , total) ... self . update_parameter( \"count\" , count) ... ... return total / count __init__ ( self , on = None , ** kwargs ) special Base Metric constructor. Parameters: Name Type Description Default on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/metrics/metric.py def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Base Metric constructor. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" super () . __init__ ( ** kwargs ) self . _labels_filter = ( on ,) if isinstance ( on , ( str , int )) else on","title":"Metric"},{"location":"api/metrics/Metric/#elegymetricsmetric","text":"","title":"elegy.metrics.Metric"},{"location":"api/metrics/Metric/#elegy.metrics.metric.Metric","text":"Encapsulates metric logic and state. Metrics accumulate state between apply s such that their output value reflect the metric as if calculated on the whole data given up to that point. Usage: m = SomeMetric() _, state = m . init()(x) for x in batch: result = m . apply(state)(x) print ( 'Final result: ' , result) Usage with the Model API: >>> import elegy , jax , optax >>> model = elegy . Model( ... module = elegy . nn . Sequential( ... lambda : [ ... elegy . nn . Flatten(), ... elegy . nn . Linear( 300 ), ... jax . nn . relu, ... elegy . nn . Linear( 10 ), ... ] ... ), ... loss = [ ... elegy . losses . SparseCategoricalCrossentropy(from_logits = True ), ... ], ... metrics = [ ... elegy . metrics . SparseCategoricalAccuracy() ... ], ... optimizer = optax . rmsprop( 1e-3 ), ... ) To be implemented by subclasses: call() : All state variables should be created in this method by calling self.add_parameter(..., trainable=False) , update this state by calling self.update_parameter(...) , and return a result based on these states. Example subclass implementation: >>> class Accuracy (elegy . Metric): ... def call ( self , y_true, y_pred): ... ... total = self . add_parameter( \"total\" , lambda : jnp . array( 0 ), trainable = False ) ... count = self . add_parameter( \"count\" , lambda : jnp . array( 0 ), trainable = False ) ... ... total += jnp . sum(y_true == y_pred) ... count += jnp . prod(y_true . shape) ... ... self . update_parameter( \"total\" , total) ... self . update_parameter( \"count\" , count) ... ... return total / count","title":"elegy.metrics.metric.Metric"},{"location":"api/metrics/Metric/#elegy.metrics.metric.Metric.__init__","text":"Base Metric constructor. Parameters: Name Type Description Default on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None Source code in elegy/metrics/metric.py def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Base Metric constructor. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). \"\"\" super () . __init__ ( ** kwargs ) self . _labels_filter = ( on ,) if isinstance ( on , ( str , int )) else on","title":"__init__()"},{"location":"api/metrics/Precision/","text":"elegy.metrics.Precision The metric creates two local variables, true_positives and false_positives that are used to compute the precision. This value is ultimately returned as precision , an idempotent operation that simply divides true_positives by the sum of true_positives and false_positives . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. If sample_weight is None, weights default to 1. Use sample_weight of 0 to mask values. If class_id is specified, we calculate precision by considering only the entries in the batch for which class_id is above the threshold and computing the fraction of them for which class_id is indeed a correct label. precision = elegy . metrics . Precision() result = precision( y_true = jnp . array([ 0 , 1 , 1 , 1 ]), y_pred = jnp . array([ 1 , 0 , 1 , 1 ]) ) assert result == 0.6666667 # 2 / 3 result = precision( y_true = jnp . array([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array([ 1 , 1 , 0 , 0 ]) ) assert result == 0.8 # 4 / 5 Usage with elegy API: model = elegy . Model( module_fn, loss = elegy . losses . CategoricalCrossentropy(), metrics = elegy . metrics . Precision(), optimizer = optix . adam( 1e-3 ), ) __init__ ( self , on = None , threshold = None , class_id = None , ** kwargs ) special Creates a Precision instance. Parameters: Name Type Description Default on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None threshold (Optional) A float value or a python list/tuple of float threshold values in [0, 1]. A threshold is compared with prediction values to determine the truth value of predictions (i.e., above the threshold is true, below is false). One metric value is generated for each threshold value. If neither threshold is set the default is to calculate precision with threshold=0.5. None class_id (Optional) Integer class ID for which we want binary metrics. This must be in the half-open interval [0, num_classes) , where num_classes is the last dimension of predictions. None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/precision.py def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , threshold = None , class_id = None , ** kwargs ): \"\"\" Creates a `Precision` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). threshold: (Optional) A float value or a python list/tuple of float threshold values in [0, 1]. A threshold is compared with prediction values to determine the truth value of predictions (i.e., above the threshold is true, below is false). One metric value is generated for each threshold value. If neither threshold is set the default is to calculate precision with threshold=0.5. class_id: (Optional) Integer class ID for which we want binary metrics. This must be in the half-open interval `[0, num_classes)`, where `num_classes` is the last dimension of predictions. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs ) self . threshold = 0.5 if threshold is None else threshold self . class_id = 1 if class_id is None else class_id self . true_positives = ReduceConfusionMatrix ( reduction = Reduction . TRUE_POSITIVES ) self . false_positives = ReduceConfusionMatrix ( reduction = Reduction . FALSE_POSITIVES ) call ( self , y_true , y_pred , sample_weight = None ) Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional weighting of each example. Defaults to 1. Can be a Tensor whose rank is either 0, or the same rank as y_true , and must be broadcastable to y_true . None Returns: Type Description ndarray Array with the cumulative precision. Source code in elegy/metrics/precision.py def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional weighting of each example. Defaults to 1. Can be a `Tensor` whose rank is either 0, or the same rank as `y_true`, and must be broadcastable to `y_true`. Returns: Array with the cumulative precision. \"\"\" return precision ( y_true = y_true , y_pred = y_pred , sample_weight = sample_weight , threshold = self . threshold , class_id = self . class_id , true_positives = self . true_positives , false_positives = self . false_positives , )","title":"Precision"},{"location":"api/metrics/Precision/#elegymetricsprecision","text":"","title":"elegy.metrics.Precision"},{"location":"api/metrics/Precision/#elegy.metrics.precision.Precision","text":"The metric creates two local variables, true_positives and false_positives that are used to compute the precision. This value is ultimately returned as precision , an idempotent operation that simply divides true_positives by the sum of true_positives and false_positives . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. If sample_weight is None, weights default to 1. Use sample_weight of 0 to mask values. If class_id is specified, we calculate precision by considering only the entries in the batch for which class_id is above the threshold and computing the fraction of them for which class_id is indeed a correct label. precision = elegy . metrics . Precision() result = precision( y_true = jnp . array([ 0 , 1 , 1 , 1 ]), y_pred = jnp . array([ 1 , 0 , 1 , 1 ]) ) assert result == 0.6666667 # 2 / 3 result = precision( y_true = jnp . array([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array([ 1 , 1 , 0 , 0 ]) ) assert result == 0.8 # 4 / 5 Usage with elegy API: model = elegy . Model( module_fn, loss = elegy . losses . CategoricalCrossentropy(), metrics = elegy . metrics . Precision(), optimizer = optix . adam( 1e-3 ), )","title":"elegy.metrics.precision.Precision"},{"location":"api/metrics/Precision/#elegy.metrics.precision.Precision.__init__","text":"Creates a Precision instance. Parameters: Name Type Description Default on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None threshold (Optional) A float value or a python list/tuple of float threshold values in [0, 1]. A threshold is compared with prediction values to determine the truth value of predictions (i.e., above the threshold is true, below is false). One metric value is generated for each threshold value. If neither threshold is set the default is to calculate precision with threshold=0.5. None class_id (Optional) Integer class ID for which we want binary metrics. This must be in the half-open interval [0, num_classes) , where num_classes is the last dimension of predictions. None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/precision.py def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , threshold = None , class_id = None , ** kwargs ): \"\"\" Creates a `Precision` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). threshold: (Optional) A float value or a python list/tuple of float threshold values in [0, 1]. A threshold is compared with prediction values to determine the truth value of predictions (i.e., above the threshold is true, below is false). One metric value is generated for each threshold value. If neither threshold is set the default is to calculate precision with threshold=0.5. class_id: (Optional) Integer class ID for which we want binary metrics. This must be in the half-open interval `[0, num_classes)`, where `num_classes` is the last dimension of predictions. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs ) self . threshold = 0.5 if threshold is None else threshold self . class_id = 1 if class_id is None else class_id self . true_positives = ReduceConfusionMatrix ( reduction = Reduction . TRUE_POSITIVES ) self . false_positives = ReduceConfusionMatrix ( reduction = Reduction . FALSE_POSITIVES )","title":"__init__()"},{"location":"api/metrics/Precision/#elegy.metrics.precision.Precision.call","text":"Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional weighting of each example. Defaults to 1. Can be a Tensor whose rank is either 0, or the same rank as y_true , and must be broadcastable to y_true . None Returns: Type Description ndarray Array with the cumulative precision. Source code in elegy/metrics/precision.py def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional weighting of each example. Defaults to 1. Can be a `Tensor` whose rank is either 0, or the same rank as `y_true`, and must be broadcastable to `y_true`. Returns: Array with the cumulative precision. \"\"\" return precision ( y_true = y_true , y_pred = y_pred , sample_weight = sample_weight , threshold = self . threshold , class_id = self . class_id , true_positives = self . true_positives , false_positives = self . false_positives , )","title":"call()"},{"location":"api/metrics/Recall/","text":"elegy.metrics.Recall This metric creates two local variables, true_positives and false_negatives , that are used to compute the recall. This value is ultimately returned as recall , an idempotent operation that simply divides true_positives by the sum of true_positives and false_negatives . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. If sample_weight is None, weights default to 1. Use sample_weight of 0 to mask values. If class_id is specified, we calculate recall by considering only the entries in the batch for which class_id is above the threshold and computing the fraction of them for which class_id is indeed a correct label. recall = elegy . metrics . Recall() result = recall( y_true = jnp . array([ 0 , 1 , 1 , 1 ]), y_pred = jnp . array([ 1 , 0 , 1 , 1 ]) ) assert result == 0.6666667 # 2 / 3 result = recall( y_true = jnp . array([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array([ 1 , 0 , 0 , 0 ]) ) assert result == 0.42857143 # 3 / 7 Usage with elegy API: model = elegy . Model( module_fn, loss = elegy . losses . CategoricalCrossentropy(), metrics = elegy . metrics . Recall(), optimizer = optix . adam( 1e-3 ), ) __init__ ( self , on = None , threshold = None , class_id = None , ** kwargs ) special Creates a Recall instance. Parameters: Name Type Description Default on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None threshold (Optional) A float value or a python list/tuple of float threshold values in [0, 1]. A threshold is compared with prediction values to determine the truth value of predictions (i.e., above the threshold is true, below is false). One metric value is generated for each threshold value. If neither threshold is set the default is to calculate recall with threshold=0.5. None class_id (Optional) Integer class ID for which we want binary metrics. This must be in the half-open interval [0, num_classes) , where num_classes is the last dimension of predictions. None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/recall.py def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , threshold = None , class_id = None , ** kwargs ): \"\"\" Creates a `Recall` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). threshold: (Optional) A float value or a python list/tuple of float threshold values in [0, 1]. A threshold is compared with prediction values to determine the truth value of predictions (i.e., above the threshold is true, below is false). One metric value is generated for each threshold value. If neither threshold is set the default is to calculate recall with threshold=0.5. class_id: (Optional) Integer class ID for which we want binary metrics. This must be in the half-open interval `[0, num_classes)`, where `num_classes` is the last dimension of predictions. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs ) self . threshold = 0.5 if threshold is None else threshold self . class_id = 1 if class_id is None else class_id self . true_positives = ReduceConfusionMatrix ( reduction = Reduction . TRUE_POSITIVES ) self . false_negatives = ReduceConfusionMatrix ( reduction = Reduction . FALSE_NEGATIVES ) call ( self , y_true , y_pred , sample_weight = None ) Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional weighting of each example. Defaults to 1. Can be a Tensor whose rank is either 0, or the same rank as y_true , and must be broadcastable to y_true . None Returns: Type Description ndarray Array with the cumulative recall. Source code in elegy/metrics/recall.py def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional weighting of each example. Defaults to 1. Can be a `Tensor` whose rank is either 0, or the same rank as `y_true`, and must be broadcastable to `y_true`. Returns: Array with the cumulative recall. \"\"\" return recall ( y_true = y_true , y_pred = y_pred , sample_weight = sample_weight , threshold = self . threshold , class_id = self . class_id , true_positives = self . true_positives , false_negatives = self . false_negatives , )","title":"Recall"},{"location":"api/metrics/Recall/#elegymetricsrecall","text":"","title":"elegy.metrics.Recall"},{"location":"api/metrics/Recall/#elegy.metrics.recall.Recall","text":"This metric creates two local variables, true_positives and false_negatives , that are used to compute the recall. This value is ultimately returned as recall , an idempotent operation that simply divides true_positives by the sum of true_positives and false_negatives . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. If sample_weight is None, weights default to 1. Use sample_weight of 0 to mask values. If class_id is specified, we calculate recall by considering only the entries in the batch for which class_id is above the threshold and computing the fraction of them for which class_id is indeed a correct label. recall = elegy . metrics . Recall() result = recall( y_true = jnp . array([ 0 , 1 , 1 , 1 ]), y_pred = jnp . array([ 1 , 0 , 1 , 1 ]) ) assert result == 0.6666667 # 2 / 3 result = recall( y_true = jnp . array([ 1 , 1 , 1 , 1 ]), y_pred = jnp . array([ 1 , 0 , 0 , 0 ]) ) assert result == 0.42857143 # 3 / 7 Usage with elegy API: model = elegy . Model( module_fn, loss = elegy . losses . CategoricalCrossentropy(), metrics = elegy . metrics . Recall(), optimizer = optix . adam( 1e-3 ), )","title":"elegy.metrics.recall.Recall"},{"location":"api/metrics/Recall/#elegy.metrics.recall.Recall.__init__","text":"Creates a Recall instance. Parameters: Name Type Description Default on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None threshold (Optional) A float value or a python list/tuple of float threshold values in [0, 1]. A threshold is compared with prediction values to determine the truth value of predictions (i.e., above the threshold is true, below is false). One metric value is generated for each threshold value. If neither threshold is set the default is to calculate recall with threshold=0.5. None class_id (Optional) Integer class ID for which we want binary metrics. This must be in the half-open interval [0, num_classes) , where num_classes is the last dimension of predictions. None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/recall.py def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , threshold = None , class_id = None , ** kwargs ): \"\"\" Creates a `Recall` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). threshold: (Optional) A float value or a python list/tuple of float threshold values in [0, 1]. A threshold is compared with prediction values to determine the truth value of predictions (i.e., above the threshold is true, below is false). One metric value is generated for each threshold value. If neither threshold is set the default is to calculate recall with threshold=0.5. class_id: (Optional) Integer class ID for which we want binary metrics. This must be in the half-open interval `[0, num_classes)`, where `num_classes` is the last dimension of predictions. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs ) self . threshold = 0.5 if threshold is None else threshold self . class_id = 1 if class_id is None else class_id self . true_positives = ReduceConfusionMatrix ( reduction = Reduction . TRUE_POSITIVES ) self . false_negatives = ReduceConfusionMatrix ( reduction = Reduction . FALSE_NEGATIVES )","title":"__init__()"},{"location":"api/metrics/Recall/#elegy.metrics.recall.Recall.call","text":"Accumulates metric statistics. y_true and y_pred should have the same shape. Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional weighting of each example. Defaults to 1. Can be a Tensor whose rank is either 0, or the same rank as y_true , and must be broadcastable to y_true . None Returns: Type Description ndarray Array with the cumulative recall. Source code in elegy/metrics/recall.py def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape. Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. sample_weight: Optional weighting of each example. Defaults to 1. Can be a `Tensor` whose rank is either 0, or the same rank as `y_true`, and must be broadcastable to `y_true`. Returns: Array with the cumulative recall. \"\"\" return recall ( y_true = y_true , y_pred = y_pred , sample_weight = sample_weight , threshold = self . threshold , class_id = self . class_id , true_positives = self . true_positives , false_negatives = self . false_negatives , )","title":"call()"},{"location":"api/metrics/Reduce/","text":"elegy.metrics.Reduce Encapsulates metrics that perform a reduce operation on the values. call ( self , values , sample_weight = None ) Accumulates statistics for computing the reduction metric. For example, if values is [1, 3, 5, 7] and reduction=SUM_OVER_BATCH_SIZE, then the value of result() is 4. If the sample_weight is specified as [1, 1, 0, 0] then value of result() would be 2. Parameters: Name Type Description Default values ndarray Per-example value. required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional weighting of each example. Defaults to 1. None Returns: Type Description ndarray Array with the cumulative reduce. Source code in elegy/metrics/reduce.py def call ( self , values : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None ) -> jnp . ndarray : \"\"\" Accumulates statistics for computing the reduction metric. For example, if `values` is [1, 3, 5, 7] and reduction=SUM_OVER_BATCH_SIZE, then the value of `result()` is 4. If the `sample_weight` is specified as [1, 1, 0, 0] then value of `result()` would be 2. Arguments: values: Per-example value. sample_weight: Optional weighting of each example. Defaults to 1. Returns: Array with the cumulative reduce. \"\"\" total = self . add_parameter ( \"total\" , lambda : jnp . array ( 0 , dtype = jnp . int32 ), trainable = False , ) if self . _reduction in ( Reduction . SUM_OVER_BATCH_SIZE , Reduction . WEIGHTED_MEAN , ): count = self . add_parameter ( \"count\" , lambda : jnp . array ( 0 , dtype = jnp . int32 ), trainable = False , ) else : count = None value , total , count = reduce ( total = total , count = count , values = values , reduction = self . _reduction , sample_weight = sample_weight , dtype = self . dtype , ) self . update_parameter ( \"total\" , total ) if count is not None : self . update_parameter ( \"count\" , count ) return value","title":"Reduce"},{"location":"api/metrics/Reduce/#elegymetricsreduce","text":"","title":"elegy.metrics.Reduce"},{"location":"api/metrics/Reduce/#elegy.metrics.reduce.Reduce","text":"Encapsulates metrics that perform a reduce operation on the values.","title":"elegy.metrics.reduce.Reduce"},{"location":"api/metrics/Reduce/#elegy.metrics.reduce.Reduce.call","text":"Accumulates statistics for computing the reduction metric. For example, if values is [1, 3, 5, 7] and reduction=SUM_OVER_BATCH_SIZE, then the value of result() is 4. If the sample_weight is specified as [1, 1, 0, 0] then value of result() would be 2. Parameters: Name Type Description Default values ndarray Per-example value. required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional weighting of each example. Defaults to 1. None Returns: Type Description ndarray Array with the cumulative reduce. Source code in elegy/metrics/reduce.py def call ( self , values : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None ) -> jnp . ndarray : \"\"\" Accumulates statistics for computing the reduction metric. For example, if `values` is [1, 3, 5, 7] and reduction=SUM_OVER_BATCH_SIZE, then the value of `result()` is 4. If the `sample_weight` is specified as [1, 1, 0, 0] then value of `result()` would be 2. Arguments: values: Per-example value. sample_weight: Optional weighting of each example. Defaults to 1. Returns: Array with the cumulative reduce. \"\"\" total = self . add_parameter ( \"total\" , lambda : jnp . array ( 0 , dtype = jnp . int32 ), trainable = False , ) if self . _reduction in ( Reduction . SUM_OVER_BATCH_SIZE , Reduction . WEIGHTED_MEAN , ): count = self . add_parameter ( \"count\" , lambda : jnp . array ( 0 , dtype = jnp . int32 ), trainable = False , ) else : count = None value , total , count = reduce ( total = total , count = count , values = values , reduction = self . _reduction , sample_weight = sample_weight , dtype = self . dtype , ) self . update_parameter ( \"total\" , total ) if count is not None : self . update_parameter ( \"count\" , count ) return value","title":"call()"},{"location":"api/metrics/Reduction/","text":"elegy.metrics.Reduction An enumeration. __class__ inherited Metaclass for Enum __members__ property readonly special Returns a mapping of member name->value. This mapping lists all enum members, including aliases. Note that this is a read-only view of the internal mapping. __bool__ ( self ) special classes/types should always be True. Source code in elegy/metrics/reduce.py def __bool__ ( self ): \"\"\" classes/types should always be True. \"\"\" return True __call__ ( cls , value , names = None , * , module = None , qualname = None , type = None , start = 1 ) special Either returns an existing member, or creates a new enum class. This method is used both when an enum class is given a value to match to an enumeration member (i.e. Color(3)) and for the functional API (i.e. Color = Enum('Color', names='RED GREEN BLUE')). When used for the functional API: value will be the name of the new class. names should be either a string of white-space/comma delimited names (values will start at start ), or an iterator/mapping of name, value pairs. module should be set to the module this class is being created in; if it is not set, an attempt to find that module will be made, but if it fails the class will not be picklable. qualname should be set to the actual location this class can be found at in its module; by default it is set to the global scope. If this is not correct, unpickling will fail in some circumstances. type , if set, will be mixed in as the first base class. Source code in elegy/metrics/reduce.py def __call__ ( cls , value , names = None , * , module = None , qualname = None , type = None , start = 1 ): \"\"\"Either returns an existing member, or creates a new enum class. This method is used both when an enum class is given a value to match to an enumeration member (i.e. Color(3)) and for the functional API (i.e. Color = Enum('Color', names='RED GREEN BLUE')). When used for the functional API: `value` will be the name of the new class. `names` should be either a string of white-space/comma delimited names (values will start at `start`), or an iterator/mapping of name, value pairs. `module` should be set to the module this class is being created in; if it is not set, an attempt to find that module will be made, but if it fails the class will not be picklable. `qualname` should be set to the actual location this class can be found at in its module; by default it is set to the global scope. If this is not correct, unpickling will fail in some circumstances. `type`, if set, will be mixed in as the first base class. \"\"\" if names is None : # simple value lookup return cls . __new__ ( cls , value ) # otherwise, functional API: we're creating a new Enum type return cls . _create_ ( value , names , module = module , qualname = qualname , type = type , start = start ) __getattr__ ( cls , name ) special Return the enum member matching name We use getattr instead of descriptors or inserting into the enum class' dict in order to support name and value being both properties for enum members (which live in the class' dict ) and enum members themselves. Source code in elegy/metrics/reduce.py def __getattr__ ( cls , name ): \"\"\"Return the enum member matching `name` We use __getattr__ instead of descriptors or inserting into the enum class' __dict__ in order to support `name` and `value` being both properties for enum members (which live in the class' __dict__) and enum members themselves. \"\"\" if _is_dunder ( name ): raise AttributeError ( name ) try : return cls . _member_map_ [ name ] except KeyError : raise AttributeError ( name ) from None __new__ ( metacls , cls , bases , classdict ) special staticmethod Create and return a new object. See help(type) for accurate signature. Source code in elegy/metrics/reduce.py def __new__ ( metacls , cls , bases , classdict ): # an Enum class is final once enumeration items have been defined; it # cannot be mixed with other types (int, float, etc.) if it has an # inherited __new__ unless a new __new__ is defined (or the resulting # class will fail). # # remove any keys listed in _ignore_ classdict . setdefault ( '_ignore_' , []) . append ( '_ignore_' ) ignore = classdict [ '_ignore_' ] for key in ignore : classdict . pop ( key , None ) member_type , first_enum = metacls . _get_mixins_ ( bases ) __new__ , save_new , use_args = metacls . _find_new_ ( classdict , member_type , first_enum ) # save enum items into separate mapping so they don't get baked into # the new class enum_members = { k : classdict [ k ] for k in classdict . _member_names } for name in classdict . _member_names : del classdict [ name ] # adjust the sunders _order_ = classdict . pop ( '_order_' , None ) # check for illegal enum names (any others?) invalid_names = set ( enum_members ) & { 'mro' , '' } if invalid_names : raise ValueError ( 'Invalid enum member name: {0} ' . format ( ',' . join ( invalid_names ))) # create a default docstring if one has not been provided if '__doc__' not in classdict : classdict [ '__doc__' ] = 'An enumeration.' # create our new Enum type enum_class = super () . __new__ ( metacls , cls , bases , classdict ) enum_class . _member_names_ = [] # names in definition order enum_class . _member_map_ = {} # name->value map enum_class . _member_type_ = member_type # save DynamicClassAttribute attributes from super classes so we know # if we can take the shortcut of storing members in the class dict dynamic_attributes = { k for c in enum_class . mro () for k , v in c . __dict__ . items () if isinstance ( v , DynamicClassAttribute )} # Reverse value->name map for hashable values. enum_class . _value2member_map_ = {} # If a custom type is mixed into the Enum, and it does not know how # to pickle itself, pickle.dumps will succeed but pickle.loads will # fail. Rather than have the error show up later and possibly far # from the source, sabotage the pickle protocol for this class so # that pickle.dumps also fails. # # However, if the new class implements its own __reduce_ex__, do not # sabotage -- it's on them to make sure it works correctly. We use # __reduce_ex__ instead of any of the others as it is preferred by # pickle over __reduce__, and it handles all pickle protocols. if '__reduce_ex__' not in classdict : if member_type is not object : methods = ( '__getnewargs_ex__' , '__getnewargs__' , '__reduce_ex__' , '__reduce__' ) if not any ( m in member_type . __dict__ for m in methods ): _make_class_unpicklable ( enum_class ) # instantiate them, checking for duplicates as we go # we instantiate first instead of checking for duplicates first in case # a custom __new__ is doing something funky with the values -- such as # auto-numbering ;) for member_name in classdict . _member_names : value = enum_members [ member_name ] if not isinstance ( value , tuple ): args = ( value , ) else : args = value if member_type is tuple : # special case for tuple enums args = ( args , ) # wrap it one more time if not use_args : enum_member = __new__ ( enum_class ) if not hasattr ( enum_member , '_value_' ): enum_member . _value_ = value else : enum_member = __new__ ( enum_class , * args ) if not hasattr ( enum_member , '_value_' ): if member_type is object : enum_member . _value_ = value else : enum_member . _value_ = member_type ( * args ) value = enum_member . _value_ enum_member . _name_ = member_name enum_member . __objclass__ = enum_class enum_member . __init__ ( * args ) # If another member with the same value was already defined, the # new member becomes an alias to the existing one. for name , canonical_member in enum_class . _member_map_ . items (): if canonical_member . _value_ == enum_member . _value_ : enum_member = canonical_member break else : # Aliases don't appear in member names (only in __members__). enum_class . _member_names_ . append ( member_name ) # performance boost for any member that would not shadow # a DynamicClassAttribute if member_name not in dynamic_attributes : setattr ( enum_class , member_name , enum_member ) # now add to _member_map_ enum_class . _member_map_ [ member_name ] = enum_member try : # This may fail if value is not hashable. We can't add the value # to the map, and by-value lookups for this value will be # linear. enum_class . _value2member_map_ [ value ] = enum_member except TypeError : pass # double check that repr and friends are not the mixin's or various # things break (such as pickle) for name in ( '__repr__' , '__str__' , '__format__' , '__reduce_ex__' ): class_method = getattr ( enum_class , name ) obj_method = getattr ( member_type , name , None ) enum_method = getattr ( first_enum , name , None ) if obj_method is not None and obj_method is class_method : setattr ( enum_class , name , enum_method ) # replace any other __new__ with our own (as long as Enum is not None, # anyway) -- again, this is to support pickle if Enum is not None : # if the user defined their own __new__, save it before it gets # clobbered in case they subclass later if save_new : enum_class . __new_member__ = __new__ enum_class . __new__ = Enum . __new__ # py3 support for definition order (helps keep py2/py3 code in sync) if _order_ is not None : if isinstance ( _order_ , str ): _order_ = _order_ . replace ( ',' , ' ' ) . split () if _order_ != enum_class . _member_names_ : raise TypeError ( 'member order does not match _order_' ) return enum_class __prepare__ ( cls , bases ) classmethod special prepare () -> dict used to create the namespace for the class statement Source code in elegy/metrics/reduce.py @classmethod def __prepare__ ( metacls , cls , bases ): # create the namespace dict enum_dict = _EnumDict () # inherit previous flags and _generate_next_value_ function member_type , first_enum = metacls . _get_mixins_ ( bases ) if first_enum is not None : enum_dict [ '_generate_next_value_' ] = getattr ( first_enum , '_generate_next_value_' , None ) return enum_dict __setattr__ ( cls , name , value ) special Block attempts to reassign Enum members. A simple assignment to the class namespace only changes one of the several possible ways to get an Enum member from the Enum class, resulting in an inconsistent Enumeration. Source code in elegy/metrics/reduce.py def __setattr__ ( cls , name , value ): \"\"\"Block attempts to reassign Enum members. A simple assignment to the class namespace only changes one of the several possible ways to get an Enum member from the Enum class, resulting in an inconsistent Enumeration. \"\"\" member_map = cls . __dict__ . get ( '_member_map_' , {}) if name in member_map : raise AttributeError ( 'Cannot reassign members.' ) super () . __setattr__ ( name , value )","title":"Reduction"},{"location":"api/metrics/Reduction/#elegymetricsreduction","text":"","title":"elegy.metrics.Reduction"},{"location":"api/metrics/Reduction/#elegy.metrics.reduce.Reduction","text":"An enumeration.","title":"elegy.metrics.reduce.Reduction"},{"location":"api/metrics/Reduction/#elegy.metrics.reduce.Reduction.__class__","text":"Metaclass for Enum","title":"__class__"},{"location":"api/metrics/Reduction/#elegy.metrics.reduce.Reduction.__class__.__members__","text":"Returns a mapping of member name->value. This mapping lists all enum members, including aliases. Note that this is a read-only view of the internal mapping.","title":"__members__"},{"location":"api/metrics/Reduction/#elegy.metrics.reduce.Reduction.__class__.__bool__","text":"classes/types should always be True. Source code in elegy/metrics/reduce.py def __bool__ ( self ): \"\"\" classes/types should always be True. \"\"\" return True","title":"__bool__()"},{"location":"api/metrics/Reduction/#elegy.metrics.reduce.Reduction.__class__.__call__","text":"Either returns an existing member, or creates a new enum class. This method is used both when an enum class is given a value to match to an enumeration member (i.e. Color(3)) and for the functional API (i.e. Color = Enum('Color', names='RED GREEN BLUE')). When used for the functional API: value will be the name of the new class. names should be either a string of white-space/comma delimited names (values will start at start ), or an iterator/mapping of name, value pairs. module should be set to the module this class is being created in; if it is not set, an attempt to find that module will be made, but if it fails the class will not be picklable. qualname should be set to the actual location this class can be found at in its module; by default it is set to the global scope. If this is not correct, unpickling will fail in some circumstances. type , if set, will be mixed in as the first base class. Source code in elegy/metrics/reduce.py def __call__ ( cls , value , names = None , * , module = None , qualname = None , type = None , start = 1 ): \"\"\"Either returns an existing member, or creates a new enum class. This method is used both when an enum class is given a value to match to an enumeration member (i.e. Color(3)) and for the functional API (i.e. Color = Enum('Color', names='RED GREEN BLUE')). When used for the functional API: `value` will be the name of the new class. `names` should be either a string of white-space/comma delimited names (values will start at `start`), or an iterator/mapping of name, value pairs. `module` should be set to the module this class is being created in; if it is not set, an attempt to find that module will be made, but if it fails the class will not be picklable. `qualname` should be set to the actual location this class can be found at in its module; by default it is set to the global scope. If this is not correct, unpickling will fail in some circumstances. `type`, if set, will be mixed in as the first base class. \"\"\" if names is None : # simple value lookup return cls . __new__ ( cls , value ) # otherwise, functional API: we're creating a new Enum type return cls . _create_ ( value , names , module = module , qualname = qualname , type = type , start = start )","title":"__call__()"},{"location":"api/metrics/Reduction/#elegy.metrics.reduce.Reduction.__class__.__getattr__","text":"Return the enum member matching name We use getattr instead of descriptors or inserting into the enum class' dict in order to support name and value being both properties for enum members (which live in the class' dict ) and enum members themselves. Source code in elegy/metrics/reduce.py def __getattr__ ( cls , name ): \"\"\"Return the enum member matching `name` We use __getattr__ instead of descriptors or inserting into the enum class' __dict__ in order to support `name` and `value` being both properties for enum members (which live in the class' __dict__) and enum members themselves. \"\"\" if _is_dunder ( name ): raise AttributeError ( name ) try : return cls . _member_map_ [ name ] except KeyError : raise AttributeError ( name ) from None","title":"__getattr__()"},{"location":"api/metrics/Reduction/#elegy.metrics.reduce.Reduction.__class__.__new__","text":"Create and return a new object. See help(type) for accurate signature. Source code in elegy/metrics/reduce.py def __new__ ( metacls , cls , bases , classdict ): # an Enum class is final once enumeration items have been defined; it # cannot be mixed with other types (int, float, etc.) if it has an # inherited __new__ unless a new __new__ is defined (or the resulting # class will fail). # # remove any keys listed in _ignore_ classdict . setdefault ( '_ignore_' , []) . append ( '_ignore_' ) ignore = classdict [ '_ignore_' ] for key in ignore : classdict . pop ( key , None ) member_type , first_enum = metacls . _get_mixins_ ( bases ) __new__ , save_new , use_args = metacls . _find_new_ ( classdict , member_type , first_enum ) # save enum items into separate mapping so they don't get baked into # the new class enum_members = { k : classdict [ k ] for k in classdict . _member_names } for name in classdict . _member_names : del classdict [ name ] # adjust the sunders _order_ = classdict . pop ( '_order_' , None ) # check for illegal enum names (any others?) invalid_names = set ( enum_members ) & { 'mro' , '' } if invalid_names : raise ValueError ( 'Invalid enum member name: {0} ' . format ( ',' . join ( invalid_names ))) # create a default docstring if one has not been provided if '__doc__' not in classdict : classdict [ '__doc__' ] = 'An enumeration.' # create our new Enum type enum_class = super () . __new__ ( metacls , cls , bases , classdict ) enum_class . _member_names_ = [] # names in definition order enum_class . _member_map_ = {} # name->value map enum_class . _member_type_ = member_type # save DynamicClassAttribute attributes from super classes so we know # if we can take the shortcut of storing members in the class dict dynamic_attributes = { k for c in enum_class . mro () for k , v in c . __dict__ . items () if isinstance ( v , DynamicClassAttribute )} # Reverse value->name map for hashable values. enum_class . _value2member_map_ = {} # If a custom type is mixed into the Enum, and it does not know how # to pickle itself, pickle.dumps will succeed but pickle.loads will # fail. Rather than have the error show up later and possibly far # from the source, sabotage the pickle protocol for this class so # that pickle.dumps also fails. # # However, if the new class implements its own __reduce_ex__, do not # sabotage -- it's on them to make sure it works correctly. We use # __reduce_ex__ instead of any of the others as it is preferred by # pickle over __reduce__, and it handles all pickle protocols. if '__reduce_ex__' not in classdict : if member_type is not object : methods = ( '__getnewargs_ex__' , '__getnewargs__' , '__reduce_ex__' , '__reduce__' ) if not any ( m in member_type . __dict__ for m in methods ): _make_class_unpicklable ( enum_class ) # instantiate them, checking for duplicates as we go # we instantiate first instead of checking for duplicates first in case # a custom __new__ is doing something funky with the values -- such as # auto-numbering ;) for member_name in classdict . _member_names : value = enum_members [ member_name ] if not isinstance ( value , tuple ): args = ( value , ) else : args = value if member_type is tuple : # special case for tuple enums args = ( args , ) # wrap it one more time if not use_args : enum_member = __new__ ( enum_class ) if not hasattr ( enum_member , '_value_' ): enum_member . _value_ = value else : enum_member = __new__ ( enum_class , * args ) if not hasattr ( enum_member , '_value_' ): if member_type is object : enum_member . _value_ = value else : enum_member . _value_ = member_type ( * args ) value = enum_member . _value_ enum_member . _name_ = member_name enum_member . __objclass__ = enum_class enum_member . __init__ ( * args ) # If another member with the same value was already defined, the # new member becomes an alias to the existing one. for name , canonical_member in enum_class . _member_map_ . items (): if canonical_member . _value_ == enum_member . _value_ : enum_member = canonical_member break else : # Aliases don't appear in member names (only in __members__). enum_class . _member_names_ . append ( member_name ) # performance boost for any member that would not shadow # a DynamicClassAttribute if member_name not in dynamic_attributes : setattr ( enum_class , member_name , enum_member ) # now add to _member_map_ enum_class . _member_map_ [ member_name ] = enum_member try : # This may fail if value is not hashable. We can't add the value # to the map, and by-value lookups for this value will be # linear. enum_class . _value2member_map_ [ value ] = enum_member except TypeError : pass # double check that repr and friends are not the mixin's or various # things break (such as pickle) for name in ( '__repr__' , '__str__' , '__format__' , '__reduce_ex__' ): class_method = getattr ( enum_class , name ) obj_method = getattr ( member_type , name , None ) enum_method = getattr ( first_enum , name , None ) if obj_method is not None and obj_method is class_method : setattr ( enum_class , name , enum_method ) # replace any other __new__ with our own (as long as Enum is not None, # anyway) -- again, this is to support pickle if Enum is not None : # if the user defined their own __new__, save it before it gets # clobbered in case they subclass later if save_new : enum_class . __new_member__ = __new__ enum_class . __new__ = Enum . __new__ # py3 support for definition order (helps keep py2/py3 code in sync) if _order_ is not None : if isinstance ( _order_ , str ): _order_ = _order_ . replace ( ',' , ' ' ) . split () if _order_ != enum_class . _member_names_ : raise TypeError ( 'member order does not match _order_' ) return enum_class","title":"__new__()"},{"location":"api/metrics/Reduction/#elegy.metrics.reduce.Reduction.__class__.__prepare__","text":"prepare () -> dict used to create the namespace for the class statement Source code in elegy/metrics/reduce.py @classmethod def __prepare__ ( metacls , cls , bases ): # create the namespace dict enum_dict = _EnumDict () # inherit previous flags and _generate_next_value_ function member_type , first_enum = metacls . _get_mixins_ ( bases ) if first_enum is not None : enum_dict [ '_generate_next_value_' ] = getattr ( first_enum , '_generate_next_value_' , None ) return enum_dict","title":"__prepare__()"},{"location":"api/metrics/Reduction/#elegy.metrics.reduce.Reduction.__class__.__setattr__","text":"Block attempts to reassign Enum members. A simple assignment to the class namespace only changes one of the several possible ways to get an Enum member from the Enum class, resulting in an inconsistent Enumeration. Source code in elegy/metrics/reduce.py def __setattr__ ( cls , name , value ): \"\"\"Block attempts to reassign Enum members. A simple assignment to the class namespace only changes one of the several possible ways to get an Enum member from the Enum class, resulting in an inconsistent Enumeration. \"\"\" member_map = cls . __dict__ . get ( '_member_map_' , {}) if name in member_map : raise AttributeError ( 'Cannot reassign members.' ) super () . __setattr__ ( name , value )","title":"__setattr__()"},{"location":"api/metrics/SparseCategoricalAccuracy/","text":"elegy.metrics.SparseCategoricalAccuracy Calculates how often predictions matches integer labels. You can provide logits of classes as y_pred , since argmax of logits and probabilities are same. This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true . This frequency is ultimately returned as sparse categorical accuracy : an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Usage: accuracy = elegy . metrics . SparseCategoricalAccuracy() result = accuracy( y_true = jnp . array([ 2 , 1 ]), y_pred = jnp . array([[ 0.1 , 0.9 , 0.8 ], [ 0.05 , 0.95 , 0 ]]) ) assert result == 0.5 # 1/2 result = accuracy( y_true = jnp . array([ 1 , 1 ]), y_pred = jnp . array([[ 0.1 , 0.9 , 0.8 ], [ 0.05 , 0.95 , 0 ]]), ) assert result == 0.75 # 3/4 Usage with elegy API: model = elegy . Model( module_fn, loss = elegy . losses . CategoricalCrossentropy(), metrics = elegy . metrics . SparseCategoricalAccuracy(), optimizer = optax . adam( 1e-3 ), ) __init__ ( self , on = None , ** kwargs ) special Creates a SparseCategoricalAccuracy instance. Parameters: Name Type Description Default on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/sparse_categorical_accuracy.py def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Creates a `SparseCategoricalAccuracy` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs ) call ( self , y_true , y_pred , sample_weight = None ) Accumulates metric statistics. y_true and y_pred should have the same shape except y_true should not have the last dimension of y_pred . Parameters: Name Type Description Default y_true ndarray Sparse ground truth values. shape = [batch_size, d0, .. dN-1] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN-1, dN] . required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/sparse_categorical_accuracy.py def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape except `y_true` should not have the last dimension of `y_pred`. Arguments: y_true: Sparse ground truth values. shape = `[batch_size, d0, .. dN-1]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN-1, dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = sparse_categorical_accuracy ( y_true = y_true , y_pred = y_pred ), sample_weight = sample_weight , )","title":"SparseCategoricalAccuracy"},{"location":"api/metrics/SparseCategoricalAccuracy/#elegymetricssparsecategoricalaccuracy","text":"","title":"elegy.metrics.SparseCategoricalAccuracy"},{"location":"api/metrics/SparseCategoricalAccuracy/#elegy.metrics.sparse_categorical_accuracy.SparseCategoricalAccuracy","text":"Calculates how often predictions matches integer labels. You can provide logits of classes as y_pred , since argmax of logits and probabilities are same. This metric creates two local variables, total and count that are used to compute the frequency with which y_pred matches y_true . This frequency is ultimately returned as sparse categorical accuracy : an idempotent operation that simply divides total by count . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Usage: accuracy = elegy . metrics . SparseCategoricalAccuracy() result = accuracy( y_true = jnp . array([ 2 , 1 ]), y_pred = jnp . array([[ 0.1 , 0.9 , 0.8 ], [ 0.05 , 0.95 , 0 ]]) ) assert result == 0.5 # 1/2 result = accuracy( y_true = jnp . array([ 1 , 1 ]), y_pred = jnp . array([[ 0.1 , 0.9 , 0.8 ], [ 0.05 , 0.95 , 0 ]]), ) assert result == 0.75 # 3/4 Usage with elegy API: model = elegy . Model( module_fn, loss = elegy . losses . CategoricalCrossentropy(), metrics = elegy . metrics . SparseCategoricalAccuracy(), optimizer = optax . adam( 1e-3 ), )","title":"elegy.metrics.sparse_categorical_accuracy.SparseCategoricalAccuracy"},{"location":"api/metrics/SparseCategoricalAccuracy/#elegy.metrics.sparse_categorical_accuracy.SparseCategoricalAccuracy.__init__","text":"Creates a SparseCategoricalAccuracy instance. Parameters: Name Type Description Default on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/sparse_categorical_accuracy.py def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\" Creates a `SparseCategoricalAccuracy` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( on = on , ** kwargs )","title":"__init__()"},{"location":"api/metrics/SparseCategoricalAccuracy/#elegy.metrics.sparse_categorical_accuracy.SparseCategoricalAccuracy.call","text":"Accumulates metric statistics. y_true and y_pred should have the same shape except y_true should not have the last dimension of y_pred . Parameters: Name Type Description Default y_true ndarray Sparse ground truth values. shape = [batch_size, d0, .. dN-1] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN-1, dN] . required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional sample_weight acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If sample_weight is a tensor of size [batch_size] , then the metric for each sample of the batch is rescaled by the corresponding element in the sample_weight vector. If the shape of sample_weight is [batch_size, d0, .. dN-1] (or can be broadcasted to this shape), then each metric element of y_pred is scaled by the corresponding value of sample_weight . (Note on dN-1 : all metric functions reduce by 1 dimension, usually the last axis (-1)). None Returns: Type Description ndarray Array with the cumulative accuracy. Source code in elegy/metrics/sparse_categorical_accuracy.py def call ( self , y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Accumulates metric statistics. `y_true` and `y_pred` should have the same shape except `y_true` should not have the last dimension of `y_pred`. Arguments: y_true: Sparse ground truth values. shape = `[batch_size, d0, .. dN-1]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN-1, dN]`. sample_weight: Optional `sample_weight` acts as a coefficient for the metric. If a scalar is provided, then the metric is simply scaled by the given value. If `sample_weight` is a tensor of size `[batch_size]`, then the metric for each sample of the batch is rescaled by the corresponding element in the `sample_weight` vector. If the shape of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted to this shape), then each metric element of `y_pred` is scaled by the corresponding value of `sample_weight`. (Note on `dN-1`: all metric functions reduce by 1 dimension, usually the last axis (-1)). Returns: Array with the cumulative accuracy. \"\"\" return super () . call ( values = sparse_categorical_accuracy ( y_true = y_true , y_pred = y_pred ), sample_weight = sample_weight , )","title":"call()"},{"location":"api/metrics/Sum/","text":"elegy.metrics.Sum Computes the (weighted) sum of the given values. For example, if values is [1, 3, 5, 7] then the sum is 16. If the weights were specified as [1, 1, 0, 0] then the sum would be 4. This metric creates one variable, total , that is used to compute the sum of values . This is ultimately returned as sum . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Usage: m = elegy . metrics . Sum() assert 16.0 == m([ 1 , 3 , 5 , 7 ]) Usage with Elegy API: model = elegy . Model( module_fn, loss = elegy . losses . CategoricalCrossentropy(), metrics = elegy . metrics . Sum(name = 'sum_1' ), ) model = elegy . Model(inputs, outputs) __init__ ( self , on = None , ** kwargs ) special Creates a Sum instance. Parameters: Name Type Description Default on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/sum.py def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\"Creates a `Sum` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( reduction = Reduction . SUM , on = on , ** kwargs ) call ( self , values , sample_weight = None ) inherited Accumulates statistics for computing the reduction metric. For example, if values is [1, 3, 5, 7] and reduction=SUM_OVER_BATCH_SIZE, then the value of result() is 4. If the sample_weight is specified as [1, 1, 0, 0] then value of result() would be 2. Parameters: Name Type Description Default values ndarray Per-example value. required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional weighting of each example. Defaults to 1. None Returns: Type Description ndarray Array with the cumulative reduce. Source code in elegy/metrics/sum.py def call ( self , values : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None ) -> jnp . ndarray : \"\"\" Accumulates statistics for computing the reduction metric. For example, if `values` is [1, 3, 5, 7] and reduction=SUM_OVER_BATCH_SIZE, then the value of `result()` is 4. If the `sample_weight` is specified as [1, 1, 0, 0] then value of `result()` would be 2. Arguments: values: Per-example value. sample_weight: Optional weighting of each example. Defaults to 1. Returns: Array with the cumulative reduce. \"\"\" total = self . add_parameter ( \"total\" , lambda : jnp . array ( 0 , dtype = jnp . int32 ), trainable = False , ) if self . _reduction in ( Reduction . SUM_OVER_BATCH_SIZE , Reduction . WEIGHTED_MEAN , ): count = self . add_parameter ( \"count\" , lambda : jnp . array ( 0 , dtype = jnp . int32 ), trainable = False , ) else : count = None value , total , count = reduce ( total = total , count = count , values = values , reduction = self . _reduction , sample_weight = sample_weight , dtype = self . dtype , ) self . update_parameter ( \"total\" , total ) if count is not None : self . update_parameter ( \"count\" , count ) return value","title":"Sum"},{"location":"api/metrics/Sum/#elegymetricssum","text":"","title":"elegy.metrics.Sum"},{"location":"api/metrics/Sum/#elegy.metrics.sum.Sum","text":"Computes the (weighted) sum of the given values. For example, if values is [1, 3, 5, 7] then the sum is 16. If the weights were specified as [1, 1, 0, 0] then the sum would be 4. This metric creates one variable, total , that is used to compute the sum of values . This is ultimately returned as sum . If sample_weight is None , weights default to 1. Use sample_weight of 0 to mask values. Usage: m = elegy . metrics . Sum() assert 16.0 == m([ 1 , 3 , 5 , 7 ]) Usage with Elegy API: model = elegy . Model( module_fn, loss = elegy . losses . CategoricalCrossentropy(), metrics = elegy . metrics . Sum(name = 'sum_1' ), ) model = elegy . Model(inputs, outputs)","title":"elegy.metrics.sum.Sum"},{"location":"api/metrics/Sum/#elegy.metrics.sum.Sum.__init__","text":"Creates a Sum instance. Parameters: Name Type Description Default on Union[str, int, Iterable[Union[str, int]]] A string or integer, or iterable of string or integers, that indicate how to index/filter the y_true and y_pred arguments before passing them to call . For example if on = \"a\" then y_true = y_true[\"a\"] . If on is an iterable the structures will be indexed iteratively, for example if on = [\"a\", 0, \"b\"] then y_true = y_true[\"a\"][0][\"b\"] , same for y_pred . For more information check out Keras-like behavior . None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/metrics/sum.py def __init__ ( self , on : tp . Optional [ types . IndexLike ] = None , ** kwargs ): \"\"\"Creates a `Sum` instance. Arguments: on: A string or integer, or iterable of string or integers, that indicate how to index/filter the `y_true` and `y_pred` arguments before passing them to `call`. For example if `on = \"a\"` then `y_true = y_true[\"a\"]`. If `on` is an iterable the structures will be indexed iteratively, for example if `on = [\"a\", 0, \"b\"]` then `y_true = y_true[\"a\"][0][\"b\"]`, same for `y_pred`. For more information check out [Keras-like behavior](https://poets-ai.github.io/elegy/guides/modules-losses-metrics/#keras-like-behavior). kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( reduction = Reduction . SUM , on = on , ** kwargs )","title":"__init__()"},{"location":"api/metrics/Sum/#elegy.metrics.sum.Sum.call","text":"Accumulates statistics for computing the reduction metric. For example, if values is [1, 3, 5, 7] and reduction=SUM_OVER_BATCH_SIZE, then the value of result() is 4. If the sample_weight is specified as [1, 1, 0, 0] then value of result() would be 2. Parameters: Name Type Description Default values ndarray Per-example value. required sample_weight Optional[jax._src.numpy.lax_numpy.ndarray] Optional weighting of each example. Defaults to 1. None Returns: Type Description ndarray Array with the cumulative reduce. Source code in elegy/metrics/sum.py def call ( self , values : jnp . ndarray , sample_weight : tp . Optional [ jnp . ndarray ] = None ) -> jnp . ndarray : \"\"\" Accumulates statistics for computing the reduction metric. For example, if `values` is [1, 3, 5, 7] and reduction=SUM_OVER_BATCH_SIZE, then the value of `result()` is 4. If the `sample_weight` is specified as [1, 1, 0, 0] then value of `result()` would be 2. Arguments: values: Per-example value. sample_weight: Optional weighting of each example. Defaults to 1. Returns: Array with the cumulative reduce. \"\"\" total = self . add_parameter ( \"total\" , lambda : jnp . array ( 0 , dtype = jnp . int32 ), trainable = False , ) if self . _reduction in ( Reduction . SUM_OVER_BATCH_SIZE , Reduction . WEIGHTED_MEAN , ): count = self . add_parameter ( \"count\" , lambda : jnp . array ( 0 , dtype = jnp . int32 ), trainable = False , ) else : count = None value , total , count = reduce ( total = total , count = count , values = values , reduction = self . _reduction , sample_weight = sample_weight , dtype = self . dtype , ) self . update_parameter ( \"total\" , total ) if count is not None : self . update_parameter ( \"count\" , count ) return value","title":"call()"},{"location":"api/metrics/accuracy/","text":"elegy.metrics.accuracy Source code in elegy/metrics/accuracy.py def accuracy ( y_true : jnp . ndarray , y_pred : jnp . ndarray ) -> jnp . ndarray : # [y_pred, y_true], _ = metrics_utils.ragged_assert_compatible_and_get_flat_values( # [y_pred, y_true] # ) # y_pred.shape.assert_is_compatible_with(y_true.shape) if y_true . dtype != y_pred . dtype : y_pred = y_pred . astype ( y_true . dtype ) return ( y_true == y_pred ) . astype ( jnp . float32 )","title":"accuracy"},{"location":"api/metrics/accuracy/#elegymetricsaccuracy","text":"","title":"elegy.metrics.accuracy"},{"location":"api/metrics/accuracy/#elegy.metrics.accuracy.accuracy","text":"Source code in elegy/metrics/accuracy.py def accuracy ( y_true : jnp . ndarray , y_pred : jnp . ndarray ) -> jnp . ndarray : # [y_pred, y_true], _ = metrics_utils.ragged_assert_compatible_and_get_flat_values( # [y_pred, y_true] # ) # y_pred.shape.assert_is_compatible_with(y_true.shape) if y_true . dtype != y_pred . dtype : y_pred = y_pred . astype ( y_true . dtype ) return ( y_true == y_pred ) . astype ( jnp . float32 )","title":"elegy.metrics.accuracy.accuracy"},{"location":"api/metrics/binary_accuracy/","text":"elegy.metrics.binary_accuracy Calculates how often predictions matches binary labels. Standalone usage: y_true = np . array([[ 1 ], [ 1 ], [ 0 ], [ 0 ]]) y_pred = np . array([[ 1 ], [ 1 ], [ 0 ], [ 0 ]]) m = elegy . metrics . binary_accuracy(y_true, y_pred) assert m . shape == ( 4 ,) Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required threshold float Float representing the threshold for deciding whether prediction values are 1 or 0. 0.5 Returns: Type Description ndarray Binary accuracy values. shape = [batch_size, d0, .. dN-1] Source code in elegy/metrics/binary_accuracy.py def binary_accuracy ( y_true : np . ndarray , y_pred : np . ndarray , threshold : float = 0.5 ) -> np . ndarray : \"\"\" Calculates how often predictions matches binary labels. Standalone usage: ```python y_true = np.array([[1], [1], [0], [0]]) y_pred = np.array([[1], [1], [0], [0]]) m = elegy.metrics.binary_accuracy(y_true, y_pred) assert m.shape == (4,) ``` Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. threshold: Float representing the threshold for deciding whether prediction values are 1 or 0. Returns: Binary accuracy values. shape = `[batch_size, d0, .. dN-1]` \"\"\" assert abs ( y_pred . ndim - y_true . ndim ) <= 1 y_true , y_pred = utils . maybe_expand_dims ( y_true , y_pred ) y_pred = y_pred > threshold return jnp . mean ( y_true == y_pred , axis =- 1 )","title":"binary_accuracy"},{"location":"api/metrics/binary_accuracy/#elegymetricsbinary_accuracy","text":"","title":"elegy.metrics.binary_accuracy"},{"location":"api/metrics/binary_accuracy/#elegy.metrics.binary_accuracy.binary_accuracy","text":"Calculates how often predictions matches binary labels. Standalone usage: y_true = np . array([[ 1 ], [ 1 ], [ 0 ], [ 0 ]]) y_pred = np . array([[ 1 ], [ 1 ], [ 0 ], [ 0 ]]) m = elegy . metrics . binary_accuracy(y_true, y_pred) assert m . shape == ( 4 ,) Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required threshold float Float representing the threshold for deciding whether prediction values are 1 or 0. 0.5 Returns: Type Description ndarray Binary accuracy values. shape = [batch_size, d0, .. dN-1] Source code in elegy/metrics/binary_accuracy.py def binary_accuracy ( y_true : np . ndarray , y_pred : np . ndarray , threshold : float = 0.5 ) -> np . ndarray : \"\"\" Calculates how often predictions matches binary labels. Standalone usage: ```python y_true = np.array([[1], [1], [0], [0]]) y_pred = np.array([[1], [1], [0], [0]]) m = elegy.metrics.binary_accuracy(y_true, y_pred) assert m.shape == (4,) ``` Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. threshold: Float representing the threshold for deciding whether prediction values are 1 or 0. Returns: Binary accuracy values. shape = `[batch_size, d0, .. dN-1]` \"\"\" assert abs ( y_pred . ndim - y_true . ndim ) <= 1 y_true , y_pred = utils . maybe_expand_dims ( y_true , y_pred ) y_pred = y_pred > threshold return jnp . mean ( y_true == y_pred , axis =- 1 )","title":"elegy.metrics.binary_accuracy.binary_accuracy"},{"location":"api/metrics/binary_crossentropy/","text":"elegy.metrics.binary_crossentropy Source code in elegy/losses/binary_crossentropy.py def binary_crossentropy ( y_true : jnp . ndarray , y_pred : jnp . ndarray , from_logits : bool = False , label_smoothing : float = 0 , ) -> jnp . ndarray : assert abs ( y_pred . ndim - y_true . ndim ) <= 1 y_true , y_pred = utils . maybe_expand_dims ( y_true , y_pred ) if label_smoothing : y_true = y_true * ( 1.0 - label_smoothing ) + 0.5 * label_smoothing if from_logits : return - jnp . mean ( y_true * y_pred - jnp . logaddexp ( 0.0 , y_pred ), axis =- 1 ) y_pred = jnp . clip ( y_pred , types . EPSILON , 1.0 - types . EPSILON ) return - jnp . mean ( y_true * jnp . log ( y_pred ) + ( 1 - y_true ) * jnp . log ( 1 - y_pred ), axis =- 1 )","title":"binary_crossentropy"},{"location":"api/metrics/binary_crossentropy/#elegymetricsbinary_crossentropy","text":"","title":"elegy.metrics.binary_crossentropy"},{"location":"api/metrics/binary_crossentropy/#elegy.losses.binary_crossentropy.binary_crossentropy","text":"Source code in elegy/losses/binary_crossentropy.py def binary_crossentropy ( y_true : jnp . ndarray , y_pred : jnp . ndarray , from_logits : bool = False , label_smoothing : float = 0 , ) -> jnp . ndarray : assert abs ( y_pred . ndim - y_true . ndim ) <= 1 y_true , y_pred = utils . maybe_expand_dims ( y_true , y_pred ) if label_smoothing : y_true = y_true * ( 1.0 - label_smoothing ) + 0.5 * label_smoothing if from_logits : return - jnp . mean ( y_true * y_pred - jnp . logaddexp ( 0.0 , y_pred ), axis =- 1 ) y_pred = jnp . clip ( y_pred , types . EPSILON , 1.0 - types . EPSILON ) return - jnp . mean ( y_true * jnp . log ( y_pred ) + ( 1 - y_true ) * jnp . log ( 1 - y_pred ), axis =- 1 )","title":"elegy.losses.binary_crossentropy.binary_crossentropy"},{"location":"api/metrics/categorical_accuracy/","text":"elegy.metrics.categorical_accuracy Source code in elegy/metrics/categorical_accuracy.py def categorical_accuracy ( y_true : jnp . ndarray , y_pred : jnp . ndarray ) -> jnp . ndarray : y_true = jnp . argmax ( y_true , axis =- 1 ) y_pred = jnp . argmax ( y_pred , axis =- 1 ) return accuracy ( y_true , y_pred )","title":"categorical_accuracy"},{"location":"api/metrics/categorical_accuracy/#elegymetricscategorical_accuracy","text":"","title":"elegy.metrics.categorical_accuracy"},{"location":"api/metrics/categorical_accuracy/#elegy.metrics.categorical_accuracy.categorical_accuracy","text":"Source code in elegy/metrics/categorical_accuracy.py def categorical_accuracy ( y_true : jnp . ndarray , y_pred : jnp . ndarray ) -> jnp . ndarray : y_true = jnp . argmax ( y_true , axis =- 1 ) y_pred = jnp . argmax ( y_pred , axis =- 1 ) return accuracy ( y_true , y_pred )","title":"elegy.metrics.categorical_accuracy.categorical_accuracy"},{"location":"api/metrics/f1/","text":"elegy.metrics.f1 Source code in elegy/metrics/f1.py def f1 ( y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : jnp . ndarray , precision : Precision , recall : Recall , ) -> jnp . ndarray : precision_values = precision ( y_true = y_true , y_pred = y_pred , sample_weight = sample_weight ) recall_values = recall ( y_true = y_true , y_pred = y_pred , sample_weight = sample_weight ) return 2 * jnp . divide ( ( precision_values * recall_values ), ( precision_values + recall_values ) )","title":"f1"},{"location":"api/metrics/f1/#elegymetricsf1","text":"","title":"elegy.metrics.f1"},{"location":"api/metrics/f1/#elegy.metrics.f1.f1","text":"Source code in elegy/metrics/f1.py def f1 ( y_true : jnp . ndarray , y_pred : jnp . ndarray , sample_weight : jnp . ndarray , precision : Precision , recall : Recall , ) -> jnp . ndarray : precision_values = precision ( y_true = y_true , y_pred = y_pred , sample_weight = sample_weight ) recall_values = recall ( y_true = y_true , y_pred = y_pred , sample_weight = sample_weight ) return 2 * jnp . divide ( ( precision_values * recall_values ), ( precision_values + recall_values ) )","title":"elegy.metrics.f1.f1"},{"location":"api/metrics/mean_absolute_error/","text":"elegy.metrics.mean_absolute_error Computes the mean absolute error between labels and predictions. After computing the absolute distance between the inputs, the mean value over the last dimension is returned. loss = mean( abs (y_true - y_pred), axis =-1 ) Usage: rng = jax . random . PRNGKey( 42 ) y_true = jax . random . randint(rng, shape = ( 2 , 3 ), minval =0 , maxval =2 ) y_pred = jax . random . uniform(rng, shape = ( 2 , 3 )) loss = elegy . losses . mean_absolute_error(y_true, y_pred) assert loss . shape == ( 2 ,) assert jnp . array_equal(loss, jnp . mean(jnp . abs(y_true - y_pred), axis =-1 )) Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean absolute error values. shape = [batch_size, d0, .. dN-1] . Source code in elegy/losses/mean_absolute_error.py def mean_absolute_error ( y_true : jnp . ndarray , y_pred : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean absolute error between labels and predictions. After computing the absolute distance between the inputs, the mean value over the last dimension is returned. ```python loss = mean(abs(y_true - y_pred), axis=-1) ``` Usage: ```python rng = jax.random.PRNGKey(42) y_true = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) y_pred = jax.random.uniform(rng, shape=(2, 3)) loss = elegy.losses.mean_absolute_error(y_true, y_pred) assert loss.shape == (2,) assert jnp.array_equal(loss, jnp.mean(jnp.abs(y_true - y_pred), axis=-1)) ``` Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean absolute error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" y_true = y_true . astype ( y_pred . dtype ) return jnp . mean ( jnp . abs ( y_pred - y_true ), axis =- 1 )","title":"mean_absolute_error"},{"location":"api/metrics/mean_absolute_error/#elegymetricsmean_absolute_error","text":"","title":"elegy.metrics.mean_absolute_error"},{"location":"api/metrics/mean_absolute_error/#elegy.losses.mean_absolute_error.mean_absolute_error","text":"Computes the mean absolute error between labels and predictions. After computing the absolute distance between the inputs, the mean value over the last dimension is returned. loss = mean( abs (y_true - y_pred), axis =-1 ) Usage: rng = jax . random . PRNGKey( 42 ) y_true = jax . random . randint(rng, shape = ( 2 , 3 ), minval =0 , maxval =2 ) y_pred = jax . random . uniform(rng, shape = ( 2 , 3 )) loss = elegy . losses . mean_absolute_error(y_true, y_pred) assert loss . shape == ( 2 ,) assert jnp . array_equal(loss, jnp . mean(jnp . abs(y_true - y_pred), axis =-1 )) Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean absolute error values. shape = [batch_size, d0, .. dN-1] . Source code in elegy/losses/mean_absolute_error.py def mean_absolute_error ( y_true : jnp . ndarray , y_pred : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean absolute error between labels and predictions. After computing the absolute distance between the inputs, the mean value over the last dimension is returned. ```python loss = mean(abs(y_true - y_pred), axis=-1) ``` Usage: ```python rng = jax.random.PRNGKey(42) y_true = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) y_pred = jax.random.uniform(rng, shape=(2, 3)) loss = elegy.losses.mean_absolute_error(y_true, y_pred) assert loss.shape == (2,) assert jnp.array_equal(loss, jnp.mean(jnp.abs(y_true - y_pred), axis=-1)) ``` Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean absolute error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" y_true = y_true . astype ( y_pred . dtype ) return jnp . mean ( jnp . abs ( y_pred - y_true ), axis =- 1 )","title":"elegy.losses.mean_absolute_error.mean_absolute_error"},{"location":"api/metrics/mean_absolute_percentage_error/","text":"elegy.metrics.mean_absolute_percentage_error Computes the mean absolute percentage error (MAPE) between labels and predictions. After computing the absolute distance between the true value and the prediction value and divide by the true value, the mean value over the last dimension is returned. Usage: rng = jax . random . PRNGKey( 42 ) y_true = jax . random . randint(rng, shape = ( 2 , 3 ), minval =0 , maxval =2 ) y_pred = jax . random . uniform(rng, shape = ( 2 , 3 )) loss = elegy . losses . mean_absolute_percentage_error(y_true, y_pred) assert loss . shape == ( 2 ,) assert jnp . array_equal(loss, 100. * jnp . mean(jnp . abs((y_pred - y_true) / jnp . clip(y_true, types . EPSILON, None )))) Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean absolute percentage error values. shape = [batch_size, d0, .. dN-1] . Source code in elegy/losses/mean_absolute_percentage_error.py def mean_absolute_percentage_error ( y_true : jnp . ndarray , y_pred : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean absolute percentage error (MAPE) between labels and predictions. After computing the absolute distance between the true value and the prediction value and divide by the true value, the mean value over the last dimension is returned. Usage: ```python rng = jax.random.PRNGKey(42) y_true = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) y_pred = jax.random.uniform(rng, shape=(2, 3)) loss = elegy.losses.mean_absolute_percentage_error(y_true, y_pred) assert loss.shape == (2,) assert jnp.array_equal(loss, 100. * jnp.mean(jnp.abs((y_pred - y_true) / jnp.clip(y_true, types.EPSILON, None)))) ``` Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean absolute percentage error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" y_true = y_true . astype ( y_pred . dtype ) diff = jnp . abs (( y_pred - y_true ) / jnp . maximum ( jnp . abs ( y_true ), types . EPSILON )) return 100.0 * jnp . mean ( diff , axis =- 1 )","title":"mean_absolute_percentage_error"},{"location":"api/metrics/mean_absolute_percentage_error/#elegymetricsmean_absolute_percentage_error","text":"","title":"elegy.metrics.mean_absolute_percentage_error"},{"location":"api/metrics/mean_absolute_percentage_error/#elegy.losses.mean_absolute_percentage_error.mean_absolute_percentage_error","text":"Computes the mean absolute percentage error (MAPE) between labels and predictions. After computing the absolute distance between the true value and the prediction value and divide by the true value, the mean value over the last dimension is returned. Usage: rng = jax . random . PRNGKey( 42 ) y_true = jax . random . randint(rng, shape = ( 2 , 3 ), minval =0 , maxval =2 ) y_pred = jax . random . uniform(rng, shape = ( 2 , 3 )) loss = elegy . losses . mean_absolute_percentage_error(y_true, y_pred) assert loss . shape == ( 2 ,) assert jnp . array_equal(loss, 100. * jnp . mean(jnp . abs((y_pred - y_true) / jnp . clip(y_true, types . EPSILON, None )))) Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean absolute percentage error values. shape = [batch_size, d0, .. dN-1] . Source code in elegy/losses/mean_absolute_percentage_error.py def mean_absolute_percentage_error ( y_true : jnp . ndarray , y_pred : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean absolute percentage error (MAPE) between labels and predictions. After computing the absolute distance between the true value and the prediction value and divide by the true value, the mean value over the last dimension is returned. Usage: ```python rng = jax.random.PRNGKey(42) y_true = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) y_pred = jax.random.uniform(rng, shape=(2, 3)) loss = elegy.losses.mean_absolute_percentage_error(y_true, y_pred) assert loss.shape == (2,) assert jnp.array_equal(loss, 100. * jnp.mean(jnp.abs((y_pred - y_true) / jnp.clip(y_true, types.EPSILON, None)))) ``` Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean absolute percentage error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" y_true = y_true . astype ( y_pred . dtype ) diff = jnp . abs (( y_pred - y_true ) / jnp . maximum ( jnp . abs ( y_true ), types . EPSILON )) return 100.0 * jnp . mean ( diff , axis =- 1 )","title":"elegy.losses.mean_absolute_percentage_error.mean_absolute_percentage_error"},{"location":"api/metrics/mean_squared_error/","text":"elegy.metrics.mean_squared_error Computes the mean squared error between labels and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. loss = mean(square(y_true - y_pred), axis =-1 ) Usage: rng = jax . random . PRNGKey( 42 ) y_true = jax . random . randint(rng, shape = ( 2 , 3 ), minval =0 , maxval =2 ) y_pred = jax . random . uniform(rng, shape = ( 2 , 3 )) loss = elegy . losses . mean_squared_error(y_true, y_pred) assert loss . shape == ( 2 ,) assert jnp . array_equal(loss, jnp . mean(jnp . square(y_true - y_pred), axis =-1 )) Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean squared error values. shape = [batch_size, d0, .. dN-1] . Source code in elegy/losses/mean_squared_error.py def mean_squared_error ( y_true : jnp . ndarray , y_pred : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean squared error between labels and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. ```python loss = mean(square(y_true - y_pred), axis=-1) ``` Usage: ```python rng = jax.random.PRNGKey(42) y_true = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) y_pred = jax.random.uniform(rng, shape=(2, 3)) loss = elegy.losses.mean_squared_error(y_true, y_pred) assert loss.shape == (2,) assert jnp.array_equal(loss, jnp.mean(jnp.square(y_true - y_pred), axis=-1)) ``` Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean squared error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" y_true = y_true . astype ( y_pred . dtype ) return jnp . mean ( jnp . square ( y_pred - y_true ), axis =- 1 )","title":"mean_squared_error"},{"location":"api/metrics/mean_squared_error/#elegymetricsmean_squared_error","text":"","title":"elegy.metrics.mean_squared_error"},{"location":"api/metrics/mean_squared_error/#elegy.losses.mean_squared_error.mean_squared_error","text":"Computes the mean squared error between labels and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. loss = mean(square(y_true - y_pred), axis =-1 ) Usage: rng = jax . random . PRNGKey( 42 ) y_true = jax . random . randint(rng, shape = ( 2 , 3 ), minval =0 , maxval =2 ) y_pred = jax . random . uniform(rng, shape = ( 2 , 3 )) loss = elegy . losses . mean_squared_error(y_true, y_pred) assert loss . shape == ( 2 ,) assert jnp . array_equal(loss, jnp . mean(jnp . square(y_true - y_pred), axis =-1 )) Parameters: Name Type Description Default y_true ndarray Ground truth values. shape = [batch_size, d0, .. dN] . required y_pred ndarray The predicted values. shape = [batch_size, d0, .. dN] . required Returns: Type Description ndarray Mean squared error values. shape = [batch_size, d0, .. dN-1] . Source code in elegy/losses/mean_squared_error.py def mean_squared_error ( y_true : jnp . ndarray , y_pred : jnp . ndarray ) -> jnp . ndarray : \"\"\" Computes the mean squared error between labels and predictions. After computing the squared distance between the inputs, the mean value over the last dimension is returned. ```python loss = mean(square(y_true - y_pred), axis=-1) ``` Usage: ```python rng = jax.random.PRNGKey(42) y_true = jax.random.randint(rng, shape=(2, 3), minval=0, maxval=2) y_pred = jax.random.uniform(rng, shape=(2, 3)) loss = elegy.losses.mean_squared_error(y_true, y_pred) assert loss.shape == (2,) assert jnp.array_equal(loss, jnp.mean(jnp.square(y_true - y_pred), axis=-1)) ``` Arguments: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`. y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`. Returns: Mean squared error values. shape = `[batch_size, d0, .. dN-1]`. \"\"\" y_true = y_true . astype ( y_pred . dtype ) return jnp . mean ( jnp . square ( y_pred - y_true ), axis =- 1 )","title":"elegy.losses.mean_squared_error.mean_squared_error"},{"location":"api/metrics/precision/","text":"elegy.metrics.precision Source code in elegy/metrics/precision.py def precision ( y_true : jnp . ndarray , y_pred : jnp . ndarray , threshold : jnp . ndarray , class_id : jnp . ndarray , sample_weight : jnp . ndarray , true_positives : ReduceConfusionMatrix , false_positives : ReduceConfusionMatrix , ) -> jnp . ndarray : # TODO: class_id behavior y_pred = ( y_pred > threshold ) . astype ( jnp . float32 ) if y_true . dtype != y_pred . dtype : y_pred = y_pred . astype ( y_true . dtype ) true_positives = true_positives ( y_true = y_true , y_pred = y_pred , sample_weight = sample_weight ) false_positives = false_positives ( y_true = y_true , y_pred = y_pred , sample_weight = sample_weight ) return jnp . nan_to_num ( jnp . divide ( true_positives , true_positives + false_positives ))","title":"precision"},{"location":"api/metrics/precision/#elegymetricsprecision","text":"","title":"elegy.metrics.precision"},{"location":"api/metrics/precision/#elegy.metrics.precision.precision","text":"Source code in elegy/metrics/precision.py def precision ( y_true : jnp . ndarray , y_pred : jnp . ndarray , threshold : jnp . ndarray , class_id : jnp . ndarray , sample_weight : jnp . ndarray , true_positives : ReduceConfusionMatrix , false_positives : ReduceConfusionMatrix , ) -> jnp . ndarray : # TODO: class_id behavior y_pred = ( y_pred > threshold ) . astype ( jnp . float32 ) if y_true . dtype != y_pred . dtype : y_pred = y_pred . astype ( y_true . dtype ) true_positives = true_positives ( y_true = y_true , y_pred = y_pred , sample_weight = sample_weight ) false_positives = false_positives ( y_true = y_true , y_pred = y_pred , sample_weight = sample_weight ) return jnp . nan_to_num ( jnp . divide ( true_positives , true_positives + false_positives ))","title":"elegy.metrics.precision.precision"},{"location":"api/metrics/recall/","text":"elegy.metrics.recall Source code in elegy/metrics/recall.py def recall ( y_true : jnp . ndarray , y_pred : jnp . ndarray , threshold : jnp . ndarray , class_id : jnp . ndarray , sample_weight : jnp . ndarray , true_positives : ReduceConfusionMatrix , false_negatives : ReduceConfusionMatrix , ) -> jnp . ndarray : # TODO: class_id behavior y_pred = ( y_pred > threshold ) . astype ( jnp . float32 ) if y_true . dtype != y_pred . dtype : y_pred = y_pred . astype ( y_true . dtype ) true_positives = true_positives ( y_true = y_true , y_pred = y_pred , sample_weight = sample_weight ) false_negatives = false_negatives ( y_true = y_true , y_pred = y_pred , sample_weight = sample_weight ) return jnp . nan_to_num ( jnp . divide ( true_positives , true_positives + false_negatives ))","title":"recall"},{"location":"api/metrics/recall/#elegymetricsrecall","text":"","title":"elegy.metrics.recall"},{"location":"api/metrics/recall/#elegy.metrics.recall.recall","text":"Source code in elegy/metrics/recall.py def recall ( y_true : jnp . ndarray , y_pred : jnp . ndarray , threshold : jnp . ndarray , class_id : jnp . ndarray , sample_weight : jnp . ndarray , true_positives : ReduceConfusionMatrix , false_negatives : ReduceConfusionMatrix , ) -> jnp . ndarray : # TODO: class_id behavior y_pred = ( y_pred > threshold ) . astype ( jnp . float32 ) if y_true . dtype != y_pred . dtype : y_pred = y_pred . astype ( y_true . dtype ) true_positives = true_positives ( y_true = y_true , y_pred = y_pred , sample_weight = sample_weight ) false_negatives = false_negatives ( y_true = y_true , y_pred = y_pred , sample_weight = sample_weight ) return jnp . nan_to_num ( jnp . divide ( true_positives , true_positives + false_negatives ))","title":"elegy.metrics.recall.recall"},{"location":"api/metrics/reduce/","text":"elegy.metrics.reduce Source code in elegy/metrics/reduce.py def reduce ( total : jnp . ndarray , count : tp . Optional [ jnp . ndarray ], values : jnp . ndarray , reduction : Reduction , sample_weight : tp . Optional [ np . ndarray ], dtype : jnp . dtype , ) -> tp . Tuple [ jnp . ndarray , jnp . ndarray , tp . Optional [ jnp . ndarray ]]: if sample_weight is not None : sample_weight = sample_weight . astype ( dtype ) # Update dimensions of weights to match with values if possible. # values, _, sample_weight = tf_losses_utils.squeeze_or_expand_dimensions( # values, sample_weight=sample_weight # ) try : # Broadcast weights if possible. sample_weight = jnp . broadcast_to ( sample_weight , values . shape ) except ValueError : # Reduce values to same ndim as weight array ndim = values . ndim weight_ndim = sample_weight . ndim if reduction == Reduction . SUM : values = jnp . sum ( values , axis = list ( range ( weight_ndim , ndim ))) else : values = jnp . mean ( values , axis = list ( range ( weight_ndim , ndim ))) values = values * sample_weight value_sum = jnp . sum ( values ) total += value_sum # Exit early if the reduction doesn't have a denominator. if reduction == Reduction . SUM : num_values = None # Update `count` for reductions that require a denominator. elif reduction == Reduction . SUM_OVER_BATCH_SIZE : num_values = jnp . prod ( values . shape ) . astype ( dtype ) else : if sample_weight is None : num_values = jnp . prod ( jnp . array ( values . shape )) . astype ( dtype ) else : num_values = jnp . sum ( sample_weight ) if count is not None and num_values is not None : count += num_values if reduction == Reduction . SUM : value = total else : value = total / count return value , total , count","title":"reduce"},{"location":"api/metrics/reduce/#elegymetricsreduce","text":"","title":"elegy.metrics.reduce"},{"location":"api/metrics/reduce/#elegy.metrics.reduce.reduce","text":"Source code in elegy/metrics/reduce.py def reduce ( total : jnp . ndarray , count : tp . Optional [ jnp . ndarray ], values : jnp . ndarray , reduction : Reduction , sample_weight : tp . Optional [ np . ndarray ], dtype : jnp . dtype , ) -> tp . Tuple [ jnp . ndarray , jnp . ndarray , tp . Optional [ jnp . ndarray ]]: if sample_weight is not None : sample_weight = sample_weight . astype ( dtype ) # Update dimensions of weights to match with values if possible. # values, _, sample_weight = tf_losses_utils.squeeze_or_expand_dimensions( # values, sample_weight=sample_weight # ) try : # Broadcast weights if possible. sample_weight = jnp . broadcast_to ( sample_weight , values . shape ) except ValueError : # Reduce values to same ndim as weight array ndim = values . ndim weight_ndim = sample_weight . ndim if reduction == Reduction . SUM : values = jnp . sum ( values , axis = list ( range ( weight_ndim , ndim ))) else : values = jnp . mean ( values , axis = list ( range ( weight_ndim , ndim ))) values = values * sample_weight value_sum = jnp . sum ( values ) total += value_sum # Exit early if the reduction doesn't have a denominator. if reduction == Reduction . SUM : num_values = None # Update `count` for reductions that require a denominator. elif reduction == Reduction . SUM_OVER_BATCH_SIZE : num_values = jnp . prod ( values . shape ) . astype ( dtype ) else : if sample_weight is None : num_values = jnp . prod ( jnp . array ( values . shape )) . astype ( dtype ) else : num_values = jnp . sum ( sample_weight ) if count is not None and num_values is not None : count += num_values if reduction == Reduction . SUM : value = total else : value = total / count return value , total , count","title":"elegy.metrics.reduce.reduce"},{"location":"api/metrics/sparse_categorical_accuracy/","text":"elegy.metrics.sparse_categorical_accuracy Source code in elegy/metrics/sparse_categorical_accuracy.py def sparse_categorical_accuracy ( y_true : jnp . ndarray , y_pred : jnp . ndarray ) -> jnp . ndarray : y_pred = jnp . argmax ( y_pred , axis =- 1 ) return accuracy ( y_true , y_pred )","title":"sparse_categorical_accuracy"},{"location":"api/metrics/sparse_categorical_accuracy/#elegymetricssparse_categorical_accuracy","text":"","title":"elegy.metrics.sparse_categorical_accuracy"},{"location":"api/metrics/sparse_categorical_accuracy/#elegy.metrics.sparse_categorical_accuracy.sparse_categorical_accuracy","text":"Source code in elegy/metrics/sparse_categorical_accuracy.py def sparse_categorical_accuracy ( y_true : jnp . ndarray , y_pred : jnp . ndarray ) -> jnp . ndarray : y_pred = jnp . argmax ( y_pred , axis =- 1 ) return accuracy ( y_true , y_pred )","title":"elegy.metrics.sparse_categorical_accuracy.sparse_categorical_accuracy"},{"location":"api/nets/ResNet/","text":"elegy.nets.ResNet A generic ResNet V1 architecture that can be customized for non-standard configurations Original Paper: Deep Residual Learning for Image Recognition __init__ ( self , stages , block_type , lowres = False , weights = None , dtype =< class ' jax . _src . numpy . lax_numpy . float32 '>, * args , ** kwargs ) special Parameters: Name Type Description Default stages List[int] A list of integers representing the number of blocks in each stage. e.g: [3, 4, 6, 3] for a ResNet50 required block_type Union[Type[elegy.nets.resnet.ResNetBlock], Type[elegy.nets.resnet.BottleneckResNetBlock]] Which ResNet block type to use. required lowres bool Optional, whether to use the low resolution version as described in subsection 4.2 of the orignal paper. This version is better suited for datasets like CIFAR10. (Default: False) False weights Optional[str] One of None (random initialization) or a path to a weights file None dtype Optional[Any] Optional dtype of the convolutions and linear operations, either jnp.float32 (default) or jnp.float16 for mixed precision. <class 'jax._src.numpy.lax_numpy.float32'> Source code in elegy/nets/resnet.py def __init__ ( self , stages : tp . List [ int ], block_type : tp . Union [ tp . Type [ ResNetBlock ], tp . Type [ BottleneckResNetBlock ]], lowres : bool = False , weights : tp . Optional [ str ] = None , dtype : tp . Optional [ tp . Any ] = jnp . float32 , * args , ** kwargs , ): \"\"\" Arguments: stages: A list of integers representing the number of blocks in each stage. e.g: [3, 4, 6, 3] for a ResNet50 block_type: Which ResNet block type to use. lowres: Optional, whether to use the low resolution version as described in subsection 4.2 of the orignal paper. This version is better suited for datasets like CIFAR10. (Default: False) weights: One of None (random initialization) or a path to a weights file dtype: Optional dtype of the convolutions and linear operations, either jnp.float32 (default) or jnp.float16 for mixed precision. \"\"\" super () . __init__ ( * args , ** kwargs ) self . stages = stages self . block_type = block_type self . lowres = lowres if weights is not None : if weights . endswith ( \".pkl\" ): collections = pickle . load ( open ( weights , \"rb\" )) elif weights == \"imagenet\" : clsname = self . __class__ . __name__ urldict = PRETRAINED_URLS . get ( clsname , None ) if urldict is None : raise ValueError ( f \"No pretrained weights for { clsname } available\" ) fname = utils . download_file ( urldict [ \"url\" ], sha256 = urldict [ \"sha256\" ]) collections = pickle . load ( open ( fname , \"rb\" )) else : raise ValueError ( \"Unknown weights value: \" , weights ) if isinstance ( collections , tuple ): parameters , collections = collections elif \"parameters\" in collections : parameters = collections . pop ( \"parameters\" ) else : raise ValueError ( \"Unknown parameters structure, expected either tuple (parameters, collections) or a collections dict with a 'parameters' field.\" ) x = np . empty ([ 0 , 224 , 224 , 3 ], dtype = self . dtype ) # quick but dirty module initialization jax . eval_shape ( self . init ( rng = types . RNGSeq ( 42 )), x ) self . set_default_parameters ( parameters , collections )","title":"ResNet"},{"location":"api/nets/ResNet/#elegynetsresnet","text":"","title":"elegy.nets.ResNet"},{"location":"api/nets/ResNet/#elegy.nets.resnet.ResNet","text":"A generic ResNet V1 architecture that can be customized for non-standard configurations Original Paper: Deep Residual Learning for Image Recognition","title":"elegy.nets.resnet.ResNet"},{"location":"api/nets/ResNet/#elegy.nets.resnet.ResNet.__init__","text":"Parameters: Name Type Description Default stages List[int] A list of integers representing the number of blocks in each stage. e.g: [3, 4, 6, 3] for a ResNet50 required block_type Union[Type[elegy.nets.resnet.ResNetBlock], Type[elegy.nets.resnet.BottleneckResNetBlock]] Which ResNet block type to use. required lowres bool Optional, whether to use the low resolution version as described in subsection 4.2 of the orignal paper. This version is better suited for datasets like CIFAR10. (Default: False) False weights Optional[str] One of None (random initialization) or a path to a weights file None dtype Optional[Any] Optional dtype of the convolutions and linear operations, either jnp.float32 (default) or jnp.float16 for mixed precision. <class 'jax._src.numpy.lax_numpy.float32'> Source code in elegy/nets/resnet.py def __init__ ( self , stages : tp . List [ int ], block_type : tp . Union [ tp . Type [ ResNetBlock ], tp . Type [ BottleneckResNetBlock ]], lowres : bool = False , weights : tp . Optional [ str ] = None , dtype : tp . Optional [ tp . Any ] = jnp . float32 , * args , ** kwargs , ): \"\"\" Arguments: stages: A list of integers representing the number of blocks in each stage. e.g: [3, 4, 6, 3] for a ResNet50 block_type: Which ResNet block type to use. lowres: Optional, whether to use the low resolution version as described in subsection 4.2 of the orignal paper. This version is better suited for datasets like CIFAR10. (Default: False) weights: One of None (random initialization) or a path to a weights file dtype: Optional dtype of the convolutions and linear operations, either jnp.float32 (default) or jnp.float16 for mixed precision. \"\"\" super () . __init__ ( * args , ** kwargs ) self . stages = stages self . block_type = block_type self . lowres = lowres if weights is not None : if weights . endswith ( \".pkl\" ): collections = pickle . load ( open ( weights , \"rb\" )) elif weights == \"imagenet\" : clsname = self . __class__ . __name__ urldict = PRETRAINED_URLS . get ( clsname , None ) if urldict is None : raise ValueError ( f \"No pretrained weights for { clsname } available\" ) fname = utils . download_file ( urldict [ \"url\" ], sha256 = urldict [ \"sha256\" ]) collections = pickle . load ( open ( fname , \"rb\" )) else : raise ValueError ( \"Unknown weights value: \" , weights ) if isinstance ( collections , tuple ): parameters , collections = collections elif \"parameters\" in collections : parameters = collections . pop ( \"parameters\" ) else : raise ValueError ( \"Unknown parameters structure, expected either tuple (parameters, collections) or a collections dict with a 'parameters' field.\" ) x = np . empty ([ 0 , 224 , 224 , 3 ], dtype = self . dtype ) # quick but dirty module initialization jax . eval_shape ( self . init ( rng = types . RNGSeq ( 42 )), x ) self . set_default_parameters ( parameters , collections )","title":"__init__()"},{"location":"api/nets/ResNet101/","text":"elegy.nets.ResNet101 __init__ ( self , lowres = False , weights = None , dtype =< class ' jax . _src . numpy . lax_numpy . float32 '>, * args , ** kwargs ) special Instantiates the ResNet101 architecture from Deep Residual Learning for Image Recognition Parameters: Name Type Description Default lowres bool Optional, whether to use the low resolution version as described in subsection 4.2 of the orignal paper. This version is better suited for datasets like CIFAR10. (Default: False) False weights Optional[str] One of None (random initialization), 'imagenet' (automatic download of weights pretrained on ImageNet) or a path to a weights file None dtype Optional[Any] Optional dtype of the convolutions and linear operations, either jnp.float32 (default) or jnp.float16 for mixed precision. <class 'jax._src.numpy.lax_numpy.float32'> Source code in elegy/nets/resnet.py def __init__ ( self , lowres : bool = False , weights : tp . Optional [ str ] = None , dtype : tp . Optional [ tp . Any ] = jnp . float32 , * args , ** kwargs , ): super () . __init__ ( stages = [ 3 , 4 , 23 , 3 ], block_type = BottleneckResNetBlock , lowres = lowres , weights = weights , dtype = dtype , * args , ** kwargs , )","title":"ResNet101"},{"location":"api/nets/ResNet101/#elegynetsresnet101","text":"","title":"elegy.nets.ResNet101"},{"location":"api/nets/ResNet101/#elegy.nets.resnet.ResNet101","text":"","title":"elegy.nets.resnet.ResNet101"},{"location":"api/nets/ResNet101/#elegy.nets.resnet.ResNet101.__init__","text":"Instantiates the ResNet101 architecture from Deep Residual Learning for Image Recognition Parameters: Name Type Description Default lowres bool Optional, whether to use the low resolution version as described in subsection 4.2 of the orignal paper. This version is better suited for datasets like CIFAR10. (Default: False) False weights Optional[str] One of None (random initialization), 'imagenet' (automatic download of weights pretrained on ImageNet) or a path to a weights file None dtype Optional[Any] Optional dtype of the convolutions and linear operations, either jnp.float32 (default) or jnp.float16 for mixed precision. <class 'jax._src.numpy.lax_numpy.float32'> Source code in elegy/nets/resnet.py def __init__ ( self , lowres : bool = False , weights : tp . Optional [ str ] = None , dtype : tp . Optional [ tp . Any ] = jnp . float32 , * args , ** kwargs , ): super () . __init__ ( stages = [ 3 , 4 , 23 , 3 ], block_type = BottleneckResNetBlock , lowres = lowres , weights = weights , dtype = dtype , * args , ** kwargs , )","title":"__init__()"},{"location":"api/nets/ResNet152/","text":"elegy.nets.ResNet152 __init__ ( self , lowres = False , weights = None , dtype =< class ' jax . _src . numpy . lax_numpy . float32 '>, * args , ** kwargs ) special Instantiates the ResNet152 architecture from Deep Residual Learning for Image Recognition Parameters: Name Type Description Default lowres bool Optional, whether to use the low resolution version as described in subsection 4.2 of the orignal paper. This version is better suited for datasets like CIFAR10. (Default: False) False weights Optional[str] One of None (random initialization), 'imagenet' (automatic download of weights pretrained on ImageNet) or a path to a weights file None dtype Optional[Any] Optional dtype of the convolutions and linear operations, either jnp.float32 (default) or jnp.float16 for mixed precision. <class 'jax._src.numpy.lax_numpy.float32'> Source code in elegy/nets/resnet.py def __init__ ( self , lowres : bool = False , weights : tp . Optional [ str ] = None , dtype : tp . Optional [ tp . Any ] = jnp . float32 , * args , ** kwargs , ): super () . __init__ ( stages = [ 3 , 8 , 36 , 3 ], block_type = BottleneckResNetBlock , lowres = lowres , weights = weights , dtype = dtype , * args , ** kwargs , )","title":"ResNet152"},{"location":"api/nets/ResNet152/#elegynetsresnet152","text":"","title":"elegy.nets.ResNet152"},{"location":"api/nets/ResNet152/#elegy.nets.resnet.ResNet152","text":"","title":"elegy.nets.resnet.ResNet152"},{"location":"api/nets/ResNet152/#elegy.nets.resnet.ResNet152.__init__","text":"Instantiates the ResNet152 architecture from Deep Residual Learning for Image Recognition Parameters: Name Type Description Default lowres bool Optional, whether to use the low resolution version as described in subsection 4.2 of the orignal paper. This version is better suited for datasets like CIFAR10. (Default: False) False weights Optional[str] One of None (random initialization), 'imagenet' (automatic download of weights pretrained on ImageNet) or a path to a weights file None dtype Optional[Any] Optional dtype of the convolutions and linear operations, either jnp.float32 (default) or jnp.float16 for mixed precision. <class 'jax._src.numpy.lax_numpy.float32'> Source code in elegy/nets/resnet.py def __init__ ( self , lowres : bool = False , weights : tp . Optional [ str ] = None , dtype : tp . Optional [ tp . Any ] = jnp . float32 , * args , ** kwargs , ): super () . __init__ ( stages = [ 3 , 8 , 36 , 3 ], block_type = BottleneckResNetBlock , lowres = lowres , weights = weights , dtype = dtype , * args , ** kwargs , )","title":"__init__()"},{"location":"api/nets/ResNet18/","text":"elegy.nets.ResNet18 __init__ ( self , lowres = False , weights = None , dtype =< class ' jax . _src . numpy . lax_numpy . float32 '>, * args , ** kwargs ) special Instantiates the ResNet18 architecture from Deep Residual Learning for Image Recognition Parameters: Name Type Description Default lowres bool Optional, whether to use the low resolution version as described in subsection 4.2 of the orignal paper. This version is better suited for datasets like CIFAR10. (Default: False) False weights Optional[str] One of None (random initialization), 'imagenet' (automatic download of weights pretrained on ImageNet) or a path to a weights file None dtype Optional[Any] Optional dtype of the convolutions and linear operations, either jnp.float32 (default) or jnp.float16 for mixed precision. <class 'jax._src.numpy.lax_numpy.float32'> Source code in elegy/nets/resnet.py def __init__ ( self , lowres : bool = False , weights : tp . Optional [ str ] = None , dtype : tp . Optional [ tp . Any ] = jnp . float32 , * args , ** kwargs , ): super () . __init__ ( stages = [ 2 , 2 , 2 , 2 ], block_type = ResNetBlock , lowres = lowres , weights = weights , dtype = dtype , * args , ** kwargs , )","title":"ResNet18"},{"location":"api/nets/ResNet18/#elegynetsresnet18","text":"","title":"elegy.nets.ResNet18"},{"location":"api/nets/ResNet18/#elegy.nets.resnet.ResNet18","text":"","title":"elegy.nets.resnet.ResNet18"},{"location":"api/nets/ResNet18/#elegy.nets.resnet.ResNet18.__init__","text":"Instantiates the ResNet18 architecture from Deep Residual Learning for Image Recognition Parameters: Name Type Description Default lowres bool Optional, whether to use the low resolution version as described in subsection 4.2 of the orignal paper. This version is better suited for datasets like CIFAR10. (Default: False) False weights Optional[str] One of None (random initialization), 'imagenet' (automatic download of weights pretrained on ImageNet) or a path to a weights file None dtype Optional[Any] Optional dtype of the convolutions and linear operations, either jnp.float32 (default) or jnp.float16 for mixed precision. <class 'jax._src.numpy.lax_numpy.float32'> Source code in elegy/nets/resnet.py def __init__ ( self , lowres : bool = False , weights : tp . Optional [ str ] = None , dtype : tp . Optional [ tp . Any ] = jnp . float32 , * args , ** kwargs , ): super () . __init__ ( stages = [ 2 , 2 , 2 , 2 ], block_type = ResNetBlock , lowres = lowres , weights = weights , dtype = dtype , * args , ** kwargs , )","title":"__init__()"},{"location":"api/nets/ResNet200/","text":"elegy.nets.ResNet200 __init__ ( self , lowres = False , weights = None , dtype =< class ' jax . _src . numpy . lax_numpy . float32 '>, * args , ** kwargs ) special Instantiates the ResNet200 architecture from Deep Residual Learning for Image Recognition Parameters: Name Type Description Default lowres bool Optional, whether to use the low resolution version as described in subsection 4.2 of the orignal paper. This version is better suited for datasets like CIFAR10. (Default: False) False weights Optional[str] One of None (random initialization), 'imagenet' (automatic download of weights pretrained on ImageNet) or a path to a weights file None dtype Optional[Any] Optional dtype of the convolutions and linear operations, either jnp.float32 (default) or jnp.float16 for mixed precision. <class 'jax._src.numpy.lax_numpy.float32'> Source code in elegy/nets/resnet.py def __init__ ( self , lowres : bool = False , weights : tp . Optional [ str ] = None , dtype : tp . Optional [ tp . Any ] = jnp . float32 , * args , ** kwargs , ): super () . __init__ ( stages = [ 3 , 24 , 36 , 3 ], block_type = BottleneckResNetBlock , lowres = lowres , weights = weights , dtype = dtype , * args , ** kwargs , )","title":"ResNet200"},{"location":"api/nets/ResNet200/#elegynetsresnet200","text":"","title":"elegy.nets.ResNet200"},{"location":"api/nets/ResNet200/#elegy.nets.resnet.ResNet200","text":"","title":"elegy.nets.resnet.ResNet200"},{"location":"api/nets/ResNet200/#elegy.nets.resnet.ResNet200.__init__","text":"Instantiates the ResNet200 architecture from Deep Residual Learning for Image Recognition Parameters: Name Type Description Default lowres bool Optional, whether to use the low resolution version as described in subsection 4.2 of the orignal paper. This version is better suited for datasets like CIFAR10. (Default: False) False weights Optional[str] One of None (random initialization), 'imagenet' (automatic download of weights pretrained on ImageNet) or a path to a weights file None dtype Optional[Any] Optional dtype of the convolutions and linear operations, either jnp.float32 (default) or jnp.float16 for mixed precision. <class 'jax._src.numpy.lax_numpy.float32'> Source code in elegy/nets/resnet.py def __init__ ( self , lowres : bool = False , weights : tp . Optional [ str ] = None , dtype : tp . Optional [ tp . Any ] = jnp . float32 , * args , ** kwargs , ): super () . __init__ ( stages = [ 3 , 24 , 36 , 3 ], block_type = BottleneckResNetBlock , lowres = lowres , weights = weights , dtype = dtype , * args , ** kwargs , )","title":"__init__()"},{"location":"api/nets/ResNet34/","text":"elegy.nets.ResNet34 __init__ ( self , lowres = False , weights = None , dtype =< class ' jax . _src . numpy . lax_numpy . float32 '>, * args , ** kwargs ) special Instantiates the ResNet34 architecture from Deep Residual Learning for Image Recognition Parameters: Name Type Description Default lowres bool Optional, whether to use the low resolution version as described in subsection 4.2 of the orignal paper. This version is better suited for datasets like CIFAR10. (Default: False) False weights Optional[str] One of None (random initialization), 'imagenet' (automatic download of weights pretrained on ImageNet) or a path to a weights file None dtype Optional[Any] Optional dtype of the convolutions and linear operations, either jnp.float32 (default) or jnp.float16 for mixed precision. <class 'jax._src.numpy.lax_numpy.float32'> Source code in elegy/nets/resnet.py def __init__ ( self , lowres : bool = False , weights : tp . Optional [ str ] = None , dtype : tp . Optional [ tp . Any ] = jnp . float32 , * args , ** kwargs , ): super () . __init__ ( stages = [ 3 , 4 , 6 , 3 ], block_type = ResNetBlock , lowres = lowres , weights = weights , dtype = dtype , * args , ** kwargs , )","title":"ResNet34"},{"location":"api/nets/ResNet34/#elegynetsresnet34","text":"","title":"elegy.nets.ResNet34"},{"location":"api/nets/ResNet34/#elegy.nets.resnet.ResNet34","text":"","title":"elegy.nets.resnet.ResNet34"},{"location":"api/nets/ResNet34/#elegy.nets.resnet.ResNet34.__init__","text":"Instantiates the ResNet34 architecture from Deep Residual Learning for Image Recognition Parameters: Name Type Description Default lowres bool Optional, whether to use the low resolution version as described in subsection 4.2 of the orignal paper. This version is better suited for datasets like CIFAR10. (Default: False) False weights Optional[str] One of None (random initialization), 'imagenet' (automatic download of weights pretrained on ImageNet) or a path to a weights file None dtype Optional[Any] Optional dtype of the convolutions and linear operations, either jnp.float32 (default) or jnp.float16 for mixed precision. <class 'jax._src.numpy.lax_numpy.float32'> Source code in elegy/nets/resnet.py def __init__ ( self , lowres : bool = False , weights : tp . Optional [ str ] = None , dtype : tp . Optional [ tp . Any ] = jnp . float32 , * args , ** kwargs , ): super () . __init__ ( stages = [ 3 , 4 , 6 , 3 ], block_type = ResNetBlock , lowres = lowres , weights = weights , dtype = dtype , * args , ** kwargs , )","title":"__init__()"},{"location":"api/nets/ResNet50/","text":"elegy.nets.ResNet50 __init__ ( self , lowres = False , weights = None , dtype =< class ' jax . _src . numpy . lax_numpy . float32 '>, * args , ** kwargs ) special Instantiates the ResNet50 architecture from Deep Residual Learning for Image Recognition Parameters: Name Type Description Default lowres bool Optional, whether to use the low resolution version as described in subsection 4.2 of the orignal paper. This version is better suited for datasets like CIFAR10. (Default: False) False weights Optional[str] One of None (random initialization), 'imagenet' (automatic download of weights pretrained on ImageNet) or a path to a weights file None dtype Optional[Any] Optional dtype of the convolutions and linear operations, either jnp.float32 (default) or jnp.float16 for mixed precision. <class 'jax._src.numpy.lax_numpy.float32'> Source code in elegy/nets/resnet.py def __init__ ( self , lowres : bool = False , weights : tp . Optional [ str ] = None , dtype : tp . Optional [ tp . Any ] = jnp . float32 , * args , ** kwargs , ): super () . __init__ ( stages = [ 3 , 4 , 6 , 3 ], block_type = BottleneckResNetBlock , lowres = lowres , weights = weights , dtype = dtype , * args , ** kwargs , )","title":"ResNet50"},{"location":"api/nets/ResNet50/#elegynetsresnet50","text":"","title":"elegy.nets.ResNet50"},{"location":"api/nets/ResNet50/#elegy.nets.resnet.ResNet50","text":"","title":"elegy.nets.resnet.ResNet50"},{"location":"api/nets/ResNet50/#elegy.nets.resnet.ResNet50.__init__","text":"Instantiates the ResNet50 architecture from Deep Residual Learning for Image Recognition Parameters: Name Type Description Default lowres bool Optional, whether to use the low resolution version as described in subsection 4.2 of the orignal paper. This version is better suited for datasets like CIFAR10. (Default: False) False weights Optional[str] One of None (random initialization), 'imagenet' (automatic download of weights pretrained on ImageNet) or a path to a weights file None dtype Optional[Any] Optional dtype of the convolutions and linear operations, either jnp.float32 (default) or jnp.float16 for mixed precision. <class 'jax._src.numpy.lax_numpy.float32'> Source code in elegy/nets/resnet.py def __init__ ( self , lowres : bool = False , weights : tp . Optional [ str ] = None , dtype : tp . Optional [ tp . Any ] = jnp . float32 , * args , ** kwargs , ): super () . __init__ ( stages = [ 3 , 4 , 6 , 3 ], block_type = BottleneckResNetBlock , lowres = lowres , weights = weights , dtype = dtype , * args , ** kwargs , )","title":"__init__()"},{"location":"api/nn/AvgPool/","text":"elegy.nn.AvgPool Average pool. Equivalent to partial application of :func: avg_pool . __init__ ( self , window_shape , strides , padding , channel_axis =- 1 , name = None ) special Average pool. Parameters: Name Type Description Default window_shape Union[int, Sequence[int]] Shape of window to pool over. Same rank as value or int . required strides Union[int, Sequence[int]] Strides for the window. Same rank as value or int . required padding str Padding algorithm. Either VALID or SAME . required channel_axis Optional[int] Axis of the spatial channels for which pooling is skipped. -1 Source code in elegy/nn/pool.py def __init__ ( self , window_shape : Union [ int , Sequence [ int ]], strides : Union [ int , Sequence [ int ]], padding : str , channel_axis : Optional [ int ] = - 1 , name : Optional [ str ] = None , ): \"\"\"Average pool. Args: window_shape: Shape of window to pool over. Same rank as value or ``int``. strides: Strides for the window. Same rank as value or ``int``. padding: Padding algorithm. Either ``VALID`` or ``SAME``. channel_axis: Axis of the spatial channels for which pooling is skipped. \"\"\" super () . __init__ ( name = name ) self . window_shape = window_shape self . strides = strides self . padding = padding self . channel_axis = channel_axis add_parameter ( self , name , initializer , collection = None , trainable = True , regularizer = None , constraint = None ) inherited Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required initializer Callable[[], Any] A callable that takes not arguments returns the initial value. required collection Optional[str] Optional name of the parameter collection, if not defined it will be se to \"parameters\" if trainable=True else it will be set to \"states\" . None trainable bool Specify whether this parameter should be added to the default trainable \"parameters\" collection or to the default non-trainable \"states\" collection. If collection is passed this parameter will ignored. True regularizer Optional[Callable[[Any], jax._src.numpy.lax_numpy.ndarray]] Regularizer instance (callable). None constraint Optional[Callable[[Any], Any]] Constraint instance (callable). None Returns: Type Description Any The value of the parameter. Source code in elegy/nn/pool.py def add_parameter ( self , name : str , initializer : tp . Callable [[], tp . Any ], collection : tp . Optional [ str ] = None , trainable : bool = True , regularizer : tp . Optional [ tp . Callable [[ tp . Any ], jnp . ndarray ]] = None , constraint : tp . Optional [ tp . Callable [[ tp . Any ], tp . Any ]] = None , ) -> tp . Any : \"\"\" Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. initializer: A callable that takes not arguments returns the initial value. collection: Optional name of the parameter collection, if not defined it will be se to `\"parameters\"` if `trainable=True` else it will be set to `\"states\"`. trainable: Specify whether this parameter should be added to the default trainable `\"parameters\"` collection or to the default non-trainable `\"states\"` collection. If collection is passed this parameter will ignored. regularizer: Regularizer instance (callable). constraint: Constraint instance (callable). Returns: The value of the parameter. \"\"\" if collection is None : collection = \"parameters\" if trainable else \"states\" if self . _scope_params is None : self . _scope_params = {} if name not in self . _scope_params : if not self . is_initializing (): raise ValueError ( f \"Cannot add parameter { name } outside `init`\" ) if name in self . _default_params : parameter = self . _default_params [ name ] assert collection == parameter . collection initial_value = parameter . value else : initial_value = initializer () initial_value = jax . tree_map ( jnp . asarray , initial_value ) parameter = types . Parameter ( collection , initial_value ) self . _register_parameter ( name , parameter ) self . _scope_params [ name ] = parameter parameter = self . _scope_params [ name ] if parameter . collection != collection : raise ValueError ( f \"types.Parameter { name } previously found in collection { parameter . collection } \" f \"but currently being added for collection { collection } \" ) value = parameter . value if constraint is not None : value = constraint ( value ) if regularizer is not None : hooks . add_loss ( name = utils . get_name ( regularizer ), value = regularizer ( value ), ) return value init ( self , * , rng = None , set_defaults = False ) inherited Initializes the module, Source code in elegy/nn/pool.py def init ( self , * , rng : tp . Optional [ types . RNGSeq ] = None , set_defaults : bool = False , ) -> tp . Callable [ ... , tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ] ]: \"\"\" Initializes the module, \"\"\" def init_callable ( * args , ** kwargs ) -> tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ]: with module_context ( module = self , collections = None , initializing = True , training = True , rng = rng ): y = self ( * args , ** kwargs ) parameters , collections = self . get_parameters_internal () self . _mark_initialized_recursive () if set_defaults : self . set_default_parameters ( parameters , collections ) return y , parameters , collections init_callable . _signature_f = self . call return init_callable","title":"AvgPool"},{"location":"api/nn/AvgPool/#elegynnavgpool","text":"","title":"elegy.nn.AvgPool"},{"location":"api/nn/AvgPool/#elegy.nn.pool.AvgPool","text":"Average pool. Equivalent to partial application of :func: avg_pool .","title":"elegy.nn.pool.AvgPool"},{"location":"api/nn/AvgPool/#elegy.nn.pool.AvgPool.__init__","text":"Average pool. Parameters: Name Type Description Default window_shape Union[int, Sequence[int]] Shape of window to pool over. Same rank as value or int . required strides Union[int, Sequence[int]] Strides for the window. Same rank as value or int . required padding str Padding algorithm. Either VALID or SAME . required channel_axis Optional[int] Axis of the spatial channels for which pooling is skipped. -1 Source code in elegy/nn/pool.py def __init__ ( self , window_shape : Union [ int , Sequence [ int ]], strides : Union [ int , Sequence [ int ]], padding : str , channel_axis : Optional [ int ] = - 1 , name : Optional [ str ] = None , ): \"\"\"Average pool. Args: window_shape: Shape of window to pool over. Same rank as value or ``int``. strides: Strides for the window. Same rank as value or ``int``. padding: Padding algorithm. Either ``VALID`` or ``SAME``. channel_axis: Axis of the spatial channels for which pooling is skipped. \"\"\" super () . __init__ ( name = name ) self . window_shape = window_shape self . strides = strides self . padding = padding self . channel_axis = channel_axis","title":"__init__()"},{"location":"api/nn/AvgPool/#elegy.nn.pool.AvgPool.add_parameter","text":"Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required initializer Callable[[], Any] A callable that takes not arguments returns the initial value. required collection Optional[str] Optional name of the parameter collection, if not defined it will be se to \"parameters\" if trainable=True else it will be set to \"states\" . None trainable bool Specify whether this parameter should be added to the default trainable \"parameters\" collection or to the default non-trainable \"states\" collection. If collection is passed this parameter will ignored. True regularizer Optional[Callable[[Any], jax._src.numpy.lax_numpy.ndarray]] Regularizer instance (callable). None constraint Optional[Callable[[Any], Any]] Constraint instance (callable). None Returns: Type Description Any The value of the parameter. Source code in elegy/nn/pool.py def add_parameter ( self , name : str , initializer : tp . Callable [[], tp . Any ], collection : tp . Optional [ str ] = None , trainable : bool = True , regularizer : tp . Optional [ tp . Callable [[ tp . Any ], jnp . ndarray ]] = None , constraint : tp . Optional [ tp . Callable [[ tp . Any ], tp . Any ]] = None , ) -> tp . Any : \"\"\" Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. initializer: A callable that takes not arguments returns the initial value. collection: Optional name of the parameter collection, if not defined it will be se to `\"parameters\"` if `trainable=True` else it will be set to `\"states\"`. trainable: Specify whether this parameter should be added to the default trainable `\"parameters\"` collection or to the default non-trainable `\"states\"` collection. If collection is passed this parameter will ignored. regularizer: Regularizer instance (callable). constraint: Constraint instance (callable). Returns: The value of the parameter. \"\"\" if collection is None : collection = \"parameters\" if trainable else \"states\" if self . _scope_params is None : self . _scope_params = {} if name not in self . _scope_params : if not self . is_initializing (): raise ValueError ( f \"Cannot add parameter { name } outside `init`\" ) if name in self . _default_params : parameter = self . _default_params [ name ] assert collection == parameter . collection initial_value = parameter . value else : initial_value = initializer () initial_value = jax . tree_map ( jnp . asarray , initial_value ) parameter = types . Parameter ( collection , initial_value ) self . _register_parameter ( name , parameter ) self . _scope_params [ name ] = parameter parameter = self . _scope_params [ name ] if parameter . collection != collection : raise ValueError ( f \"types.Parameter { name } previously found in collection { parameter . collection } \" f \"but currently being added for collection { collection } \" ) value = parameter . value if constraint is not None : value = constraint ( value ) if regularizer is not None : hooks . add_loss ( name = utils . get_name ( regularizer ), value = regularizer ( value ), ) return value","title":"add_parameter()"},{"location":"api/nn/AvgPool/#elegy.nn.pool.AvgPool.init","text":"Initializes the module, Source code in elegy/nn/pool.py def init ( self , * , rng : tp . Optional [ types . RNGSeq ] = None , set_defaults : bool = False , ) -> tp . Callable [ ... , tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ] ]: \"\"\" Initializes the module, \"\"\" def init_callable ( * args , ** kwargs ) -> tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ]: with module_context ( module = self , collections = None , initializing = True , training = True , rng = rng ): y = self ( * args , ** kwargs ) parameters , collections = self . get_parameters_internal () self . _mark_initialized_recursive () if set_defaults : self . set_default_parameters ( parameters , collections ) return y , parameters , collections init_callable . _signature_f = self . call return init_callable","title":"init()"},{"location":"api/nn/BatchNormalization/","text":"elegy.nn.BatchNormalization Normalizes inputs to maintain a mean of ~0 and stddev of ~1. See: https://arxiv.org/abs/1502.03167. There are many different variations for how users want to manage scale and offset if they require them at all. These are: No scale/offset in which case create_* should be set to False and scale / offset aren't passed when the module is called. Trainable scale/offset in which case create_* should be set to True and again scale / offset aren't passed when the module is called. In this case this module creates and owns the scale / offset variables. Externally generated scale / offset , such as for conditional normalization, in which case create_* should be set to False and then the values fed in at call time. NOTE: jax.vmap(hk.transform(BatchNorm)) will update summary statistics and normalize values on a per-batch basis; we currently do not support normalizing across a batch axis introduced by vmap. __init__ ( self , create_scale = True , create_offset = True , decay_rate = 0.99 , eps = 1e-05 , scale_init = None , offset_init = None , axis = None , cross_replica_axis = None , data_format = 'channels_last' , ** kwargs ) special Constructs a BatchNorm module. Parameters: Name Type Description Default create_scale bool Whether to include a trainable scaling factor. True create_offset bool Whether to include a trainable offset. True decay_rate float Decay rate for EMA. 0.99 eps float Small epsilon to avoid division by zero variance. Defaults 1e-5 , as in the paper and Sonnet. 1e-05 scale_init Optional[elegy.types.Initializer] Optional initializer for gain (aka scale). Can only be set if create_scale=True . By default, 1 . None offset_init Optional[elegy.types.Initializer] Optional initializer for bias (aka offset). Can only be set if create_offset=True . By default, 0 . None axis Optional[Sequence[int]] Which axes to reduce over. The default ( None ) signifies that all but the channel axis should be normalized. Otherwise this is a list of axis indices which will have normalization statistics calculated. None cross_replica_axis Optional[str] If not None , it should be a string representing the axis name over which this module is being run within a jax.pmap . Supplying this argument means that batch statistics are calculated across all replicas on that axis. None data_format str The data format of the input. Can be either channels_first , channels_last , N...C or NC... . By default it is channels_last . 'channels_last' kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/batch_normalization.py def __init__ ( self , create_scale : bool = True , create_offset : bool = True , decay_rate : float = 0.99 , eps : float = 1e-5 , scale_init : Optional [ types . Initializer ] = None , offset_init : Optional [ types . Initializer ] = None , axis : Optional [ Sequence [ int ]] = None , cross_replica_axis : Optional [ str ] = None , data_format : str = \"channels_last\" , ** kwargs ): \"\"\"Constructs a BatchNorm module. Args: create_scale: Whether to include a trainable scaling factor. create_offset: Whether to include a trainable offset. decay_rate: Decay rate for EMA. eps: Small epsilon to avoid division by zero variance. Defaults ``1e-5``, as in the paper and Sonnet. scale_init: Optional initializer for gain (aka scale). Can only be set if ``create_scale=True``. By default, ``1``. offset_init: Optional initializer for bias (aka offset). Can only be set if ``create_offset=True``. By default, ``0``. axis: Which axes to reduce over. The default (``None``) signifies that all but the channel axis should be normalized. Otherwise this is a list of axis indices which will have normalization statistics calculated. cross_replica_axis: If not ``None``, it should be a string representing the axis name over which this module is being run within a ``jax.pmap``. Supplying this argument means that batch statistics are calculated across all replicas on that axis. data_format: The data format of the input. Can be either ``channels_first``, ``channels_last``, ``N...C`` or ``NC...``. By default it is ``channels_last``. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( ** kwargs ) if not create_scale and scale_init is not None : raise ValueError ( \"Cannot set `scale_init` if `create_scale=False`\" ) if not create_offset and offset_init is not None : raise ValueError ( \"Cannot set `offset_init` if `create_offset=False`\" ) self . create_scale = create_scale self . create_offset = create_offset self . eps = eps self . scale_init = scale_init or jnp . ones self . offset_init = offset_init or jnp . zeros self . axis = axis self . cross_replica_axis = cross_replica_axis self . channel_index = haiku_utils . get_channel_index ( data_format ) self . mean_ema = ExponentialMovingAverage ( decay_rate , name = \"mean_ema\" ) self . var_ema = ExponentialMovingAverage ( decay_rate , name = \"var_ema\" ) add_parameter ( self , name , initializer , collection = None , trainable = True , regularizer = None , constraint = None ) inherited Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required initializer Callable[[], Any] A callable that takes not arguments returns the initial value. required collection Optional[str] Optional name of the parameter collection, if not defined it will be se to \"parameters\" if trainable=True else it will be set to \"states\" . None trainable bool Specify whether this parameter should be added to the default trainable \"parameters\" collection or to the default non-trainable \"states\" collection. If collection is passed this parameter will ignored. True regularizer Optional[Callable[[Any], jax._src.numpy.lax_numpy.ndarray]] Regularizer instance (callable). None constraint Optional[Callable[[Any], Any]] Constraint instance (callable). None Returns: Type Description Any The value of the parameter. Source code in elegy/nn/batch_normalization.py def add_parameter ( self , name : str , initializer : tp . Callable [[], tp . Any ], collection : tp . Optional [ str ] = None , trainable : bool = True , regularizer : tp . Optional [ tp . Callable [[ tp . Any ], jnp . ndarray ]] = None , constraint : tp . Optional [ tp . Callable [[ tp . Any ], tp . Any ]] = None , ) -> tp . Any : \"\"\" Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. initializer: A callable that takes not arguments returns the initial value. collection: Optional name of the parameter collection, if not defined it will be se to `\"parameters\"` if `trainable=True` else it will be set to `\"states\"`. trainable: Specify whether this parameter should be added to the default trainable `\"parameters\"` collection or to the default non-trainable `\"states\"` collection. If collection is passed this parameter will ignored. regularizer: Regularizer instance (callable). constraint: Constraint instance (callable). Returns: The value of the parameter. \"\"\" if collection is None : collection = \"parameters\" if trainable else \"states\" if self . _scope_params is None : self . _scope_params = {} if name not in self . _scope_params : if not self . is_initializing (): raise ValueError ( f \"Cannot add parameter { name } outside `init`\" ) if name in self . _default_params : parameter = self . _default_params [ name ] assert collection == parameter . collection initial_value = parameter . value else : initial_value = initializer () initial_value = jax . tree_map ( jnp . asarray , initial_value ) parameter = types . Parameter ( collection , initial_value ) self . _register_parameter ( name , parameter ) self . _scope_params [ name ] = parameter parameter = self . _scope_params [ name ] if parameter . collection != collection : raise ValueError ( f \"types.Parameter { name } previously found in collection { parameter . collection } \" f \"but currently being added for collection { collection } \" ) value = parameter . value if constraint is not None : value = constraint ( value ) if regularizer is not None : hooks . add_loss ( name = utils . get_name ( regularizer ), value = regularizer ( value ), ) return value call ( self , inputs , training = None , test_local_stats = False , scale = None , offset = None ) Computes the normalized version of the input. Parameters: Name Type Description Default inputs ndarray An array, where the data format is [..., C] . required training Optional[bool] Whether training is currently happening. None test_local_stats bool Whether local stats are used when training=False. False scale Optional[jax._src.numpy.lax_numpy.ndarray] An array up to n-D. The shape of this tensor must be broadcastable to the shape of inputs . This is the scale applied to the normalized inputs. This cannot be passed in if the module was constructed with create_scale=True . None offset Optional[jax._src.numpy.lax_numpy.ndarray] An array up to n-D. The shape of this tensor must be broadcastable to the shape of inputs . This is the offset applied to the normalized inputs. This cannot be passed in if the module was constructed with create_offset=True . None Returns: Type Description ndarray The array, normalized across all but the last dimension. Source code in elegy/nn/batch_normalization.py def call ( self , inputs : jnp . ndarray , training : tp . Optional [ bool ] = None , test_local_stats : bool = False , scale : Optional [ jnp . ndarray ] = None , offset : Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\"Computes the normalized version of the input. Args: inputs: An array, where the data format is ``[..., C]``. training: Whether training is currently happening. test_local_stats: Whether local stats are used when training=False. scale: An array up to n-D. The shape of this tensor must be broadcastable to the shape of ``inputs``. This is the scale applied to the normalized inputs. This cannot be passed in if the module was constructed with ``create_scale=True``. offset: An array up to n-D. The shape of this tensor must be broadcastable to the shape of ``inputs``. This is the offset applied to the normalized inputs. This cannot be passed in if the module was constructed with ``create_offset=True``. Returns: The array, normalized across all but the last dimension. \"\"\" inputs = jnp . asarray ( inputs , jnp . float32 ) if training is None : training = self . is_training () if self . create_scale and scale is not None : raise ValueError ( \"Cannot pass `scale` at call time if `create_scale=True`.\" ) if self . create_offset and offset is not None : raise ValueError ( \"Cannot pass `offset` at call time if `create_offset=True`.\" ) channel_index = self . channel_index if channel_index < 0 : channel_index += inputs . ndim if self . axis is not None : axis = self . axis else : axis = [ i for i in range ( inputs . ndim ) if i != channel_index ] if training or test_local_stats or not hasattr ( self . mean_ema , \"average\" ): cross_replica_axis = self . cross_replica_axis if self . cross_replica_axis : mean = jnp . mean ( inputs , axis , keepdims = True ) mean = jax . lax . pmean ( mean , cross_replica_axis ) mean_of_squares = jnp . mean ( inputs ** 2 , axis , keepdims = True ) mean_of_squares = jax . lax . pmean ( mean_of_squares , cross_replica_axis ) var = mean_of_squares - mean ** 2 else : mean = jnp . mean ( inputs , axis , keepdims = True ) # This uses E[(X - E[X])^2]. # TODO(tycai): Consider the faster, but possibly less stable # E[X^2] - E[X]^2 method. var = jnp . var ( inputs , axis , keepdims = True ) else : mean = self . mean_ema . average var = self . var_ema . average if training or not hasattr ( self . mean_ema , \"average\" ): self . mean_ema ( mean ) self . var_ema ( var ) w_shape = [ 1 if i in axis else inputs . shape [ i ] for i in range ( inputs . ndim )] w_dtype = jnp . float32 if self . create_scale : scale = self . add_parameter ( \"scale\" , lambda : self . scale_init ( w_shape , w_dtype ) ) elif scale is None : scale = np . ones ([], dtype = w_dtype ) if self . create_offset : offset = self . add_parameter ( \"offset\" , lambda : self . offset_init ( w_shape , w_dtype ) ) elif offset is None : offset = np . zeros ([], dtype = w_dtype ) inv = scale * jax . lax . rsqrt ( var + self . eps ) output = ( inputs - mean ) * inv + offset return jnp . asarray ( output , self . dtype ) init ( self , * , rng = None , set_defaults = False ) inherited Initializes the module, Source code in elegy/nn/batch_normalization.py def init ( self , * , rng : tp . Optional [ types . RNGSeq ] = None , set_defaults : bool = False , ) -> tp . Callable [ ... , tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ] ]: \"\"\" Initializes the module, \"\"\" def init_callable ( * args , ** kwargs ) -> tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ]: with module_context ( module = self , collections = None , initializing = True , training = True , rng = rng ): y = self ( * args , ** kwargs ) parameters , collections = self . get_parameters_internal () self . _mark_initialized_recursive () if set_defaults : self . set_default_parameters ( parameters , collections ) return y , parameters , collections init_callable . _signature_f = self . call return init_callable","title":"BatchNormalization"},{"location":"api/nn/BatchNormalization/#elegynnbatchnormalization","text":"","title":"elegy.nn.BatchNormalization"},{"location":"api/nn/BatchNormalization/#elegy.nn.batch_normalization.BatchNormalization","text":"Normalizes inputs to maintain a mean of ~0 and stddev of ~1. See: https://arxiv.org/abs/1502.03167. There are many different variations for how users want to manage scale and offset if they require them at all. These are: No scale/offset in which case create_* should be set to False and scale / offset aren't passed when the module is called. Trainable scale/offset in which case create_* should be set to True and again scale / offset aren't passed when the module is called. In this case this module creates and owns the scale / offset variables. Externally generated scale / offset , such as for conditional normalization, in which case create_* should be set to False and then the values fed in at call time. NOTE: jax.vmap(hk.transform(BatchNorm)) will update summary statistics and normalize values on a per-batch basis; we currently do not support normalizing across a batch axis introduced by vmap.","title":"elegy.nn.batch_normalization.BatchNormalization"},{"location":"api/nn/BatchNormalization/#elegy.nn.batch_normalization.BatchNormalization.__init__","text":"Constructs a BatchNorm module. Parameters: Name Type Description Default create_scale bool Whether to include a trainable scaling factor. True create_offset bool Whether to include a trainable offset. True decay_rate float Decay rate for EMA. 0.99 eps float Small epsilon to avoid division by zero variance. Defaults 1e-5 , as in the paper and Sonnet. 1e-05 scale_init Optional[elegy.types.Initializer] Optional initializer for gain (aka scale). Can only be set if create_scale=True . By default, 1 . None offset_init Optional[elegy.types.Initializer] Optional initializer for bias (aka offset). Can only be set if create_offset=True . By default, 0 . None axis Optional[Sequence[int]] Which axes to reduce over. The default ( None ) signifies that all but the channel axis should be normalized. Otherwise this is a list of axis indices which will have normalization statistics calculated. None cross_replica_axis Optional[str] If not None , it should be a string representing the axis name over which this module is being run within a jax.pmap . Supplying this argument means that batch statistics are calculated across all replicas on that axis. None data_format str The data format of the input. Can be either channels_first , channels_last , N...C or NC... . By default it is channels_last . 'channels_last' kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/batch_normalization.py def __init__ ( self , create_scale : bool = True , create_offset : bool = True , decay_rate : float = 0.99 , eps : float = 1e-5 , scale_init : Optional [ types . Initializer ] = None , offset_init : Optional [ types . Initializer ] = None , axis : Optional [ Sequence [ int ]] = None , cross_replica_axis : Optional [ str ] = None , data_format : str = \"channels_last\" , ** kwargs ): \"\"\"Constructs a BatchNorm module. Args: create_scale: Whether to include a trainable scaling factor. create_offset: Whether to include a trainable offset. decay_rate: Decay rate for EMA. eps: Small epsilon to avoid division by zero variance. Defaults ``1e-5``, as in the paper and Sonnet. scale_init: Optional initializer for gain (aka scale). Can only be set if ``create_scale=True``. By default, ``1``. offset_init: Optional initializer for bias (aka offset). Can only be set if ``create_offset=True``. By default, ``0``. axis: Which axes to reduce over. The default (``None``) signifies that all but the channel axis should be normalized. Otherwise this is a list of axis indices which will have normalization statistics calculated. cross_replica_axis: If not ``None``, it should be a string representing the axis name over which this module is being run within a ``jax.pmap``. Supplying this argument means that batch statistics are calculated across all replicas on that axis. data_format: The data format of the input. Can be either ``channels_first``, ``channels_last``, ``N...C`` or ``NC...``. By default it is ``channels_last``. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( ** kwargs ) if not create_scale and scale_init is not None : raise ValueError ( \"Cannot set `scale_init` if `create_scale=False`\" ) if not create_offset and offset_init is not None : raise ValueError ( \"Cannot set `offset_init` if `create_offset=False`\" ) self . create_scale = create_scale self . create_offset = create_offset self . eps = eps self . scale_init = scale_init or jnp . ones self . offset_init = offset_init or jnp . zeros self . axis = axis self . cross_replica_axis = cross_replica_axis self . channel_index = haiku_utils . get_channel_index ( data_format ) self . mean_ema = ExponentialMovingAverage ( decay_rate , name = \"mean_ema\" ) self . var_ema = ExponentialMovingAverage ( decay_rate , name = \"var_ema\" )","title":"__init__()"},{"location":"api/nn/BatchNormalization/#elegy.nn.batch_normalization.BatchNormalization.add_parameter","text":"Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required initializer Callable[[], Any] A callable that takes not arguments returns the initial value. required collection Optional[str] Optional name of the parameter collection, if not defined it will be se to \"parameters\" if trainable=True else it will be set to \"states\" . None trainable bool Specify whether this parameter should be added to the default trainable \"parameters\" collection or to the default non-trainable \"states\" collection. If collection is passed this parameter will ignored. True regularizer Optional[Callable[[Any], jax._src.numpy.lax_numpy.ndarray]] Regularizer instance (callable). None constraint Optional[Callable[[Any], Any]] Constraint instance (callable). None Returns: Type Description Any The value of the parameter. Source code in elegy/nn/batch_normalization.py def add_parameter ( self , name : str , initializer : tp . Callable [[], tp . Any ], collection : tp . Optional [ str ] = None , trainable : bool = True , regularizer : tp . Optional [ tp . Callable [[ tp . Any ], jnp . ndarray ]] = None , constraint : tp . Optional [ tp . Callable [[ tp . Any ], tp . Any ]] = None , ) -> tp . Any : \"\"\" Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. initializer: A callable that takes not arguments returns the initial value. collection: Optional name of the parameter collection, if not defined it will be se to `\"parameters\"` if `trainable=True` else it will be set to `\"states\"`. trainable: Specify whether this parameter should be added to the default trainable `\"parameters\"` collection or to the default non-trainable `\"states\"` collection. If collection is passed this parameter will ignored. regularizer: Regularizer instance (callable). constraint: Constraint instance (callable). Returns: The value of the parameter. \"\"\" if collection is None : collection = \"parameters\" if trainable else \"states\" if self . _scope_params is None : self . _scope_params = {} if name not in self . _scope_params : if not self . is_initializing (): raise ValueError ( f \"Cannot add parameter { name } outside `init`\" ) if name in self . _default_params : parameter = self . _default_params [ name ] assert collection == parameter . collection initial_value = parameter . value else : initial_value = initializer () initial_value = jax . tree_map ( jnp . asarray , initial_value ) parameter = types . Parameter ( collection , initial_value ) self . _register_parameter ( name , parameter ) self . _scope_params [ name ] = parameter parameter = self . _scope_params [ name ] if parameter . collection != collection : raise ValueError ( f \"types.Parameter { name } previously found in collection { parameter . collection } \" f \"but currently being added for collection { collection } \" ) value = parameter . value if constraint is not None : value = constraint ( value ) if regularizer is not None : hooks . add_loss ( name = utils . get_name ( regularizer ), value = regularizer ( value ), ) return value","title":"add_parameter()"},{"location":"api/nn/BatchNormalization/#elegy.nn.batch_normalization.BatchNormalization.call","text":"Computes the normalized version of the input. Parameters: Name Type Description Default inputs ndarray An array, where the data format is [..., C] . required training Optional[bool] Whether training is currently happening. None test_local_stats bool Whether local stats are used when training=False. False scale Optional[jax._src.numpy.lax_numpy.ndarray] An array up to n-D. The shape of this tensor must be broadcastable to the shape of inputs . This is the scale applied to the normalized inputs. This cannot be passed in if the module was constructed with create_scale=True . None offset Optional[jax._src.numpy.lax_numpy.ndarray] An array up to n-D. The shape of this tensor must be broadcastable to the shape of inputs . This is the offset applied to the normalized inputs. This cannot be passed in if the module was constructed with create_offset=True . None Returns: Type Description ndarray The array, normalized across all but the last dimension. Source code in elegy/nn/batch_normalization.py def call ( self , inputs : jnp . ndarray , training : tp . Optional [ bool ] = None , test_local_stats : bool = False , scale : Optional [ jnp . ndarray ] = None , offset : Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\"Computes the normalized version of the input. Args: inputs: An array, where the data format is ``[..., C]``. training: Whether training is currently happening. test_local_stats: Whether local stats are used when training=False. scale: An array up to n-D. The shape of this tensor must be broadcastable to the shape of ``inputs``. This is the scale applied to the normalized inputs. This cannot be passed in if the module was constructed with ``create_scale=True``. offset: An array up to n-D. The shape of this tensor must be broadcastable to the shape of ``inputs``. This is the offset applied to the normalized inputs. This cannot be passed in if the module was constructed with ``create_offset=True``. Returns: The array, normalized across all but the last dimension. \"\"\" inputs = jnp . asarray ( inputs , jnp . float32 ) if training is None : training = self . is_training () if self . create_scale and scale is not None : raise ValueError ( \"Cannot pass `scale` at call time if `create_scale=True`.\" ) if self . create_offset and offset is not None : raise ValueError ( \"Cannot pass `offset` at call time if `create_offset=True`.\" ) channel_index = self . channel_index if channel_index < 0 : channel_index += inputs . ndim if self . axis is not None : axis = self . axis else : axis = [ i for i in range ( inputs . ndim ) if i != channel_index ] if training or test_local_stats or not hasattr ( self . mean_ema , \"average\" ): cross_replica_axis = self . cross_replica_axis if self . cross_replica_axis : mean = jnp . mean ( inputs , axis , keepdims = True ) mean = jax . lax . pmean ( mean , cross_replica_axis ) mean_of_squares = jnp . mean ( inputs ** 2 , axis , keepdims = True ) mean_of_squares = jax . lax . pmean ( mean_of_squares , cross_replica_axis ) var = mean_of_squares - mean ** 2 else : mean = jnp . mean ( inputs , axis , keepdims = True ) # This uses E[(X - E[X])^2]. # TODO(tycai): Consider the faster, but possibly less stable # E[X^2] - E[X]^2 method. var = jnp . var ( inputs , axis , keepdims = True ) else : mean = self . mean_ema . average var = self . var_ema . average if training or not hasattr ( self . mean_ema , \"average\" ): self . mean_ema ( mean ) self . var_ema ( var ) w_shape = [ 1 if i in axis else inputs . shape [ i ] for i in range ( inputs . ndim )] w_dtype = jnp . float32 if self . create_scale : scale = self . add_parameter ( \"scale\" , lambda : self . scale_init ( w_shape , w_dtype ) ) elif scale is None : scale = np . ones ([], dtype = w_dtype ) if self . create_offset : offset = self . add_parameter ( \"offset\" , lambda : self . offset_init ( w_shape , w_dtype ) ) elif offset is None : offset = np . zeros ([], dtype = w_dtype ) inv = scale * jax . lax . rsqrt ( var + self . eps ) output = ( inputs - mean ) * inv + offset return jnp . asarray ( output , self . dtype )","title":"call()"},{"location":"api/nn/BatchNormalization/#elegy.nn.batch_normalization.BatchNormalization.init","text":"Initializes the module, Source code in elegy/nn/batch_normalization.py def init ( self , * , rng : tp . Optional [ types . RNGSeq ] = None , set_defaults : bool = False , ) -> tp . Callable [ ... , tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ] ]: \"\"\" Initializes the module, \"\"\" def init_callable ( * args , ** kwargs ) -> tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ]: with module_context ( module = self , collections = None , initializing = True , training = True , rng = rng ): y = self ( * args , ** kwargs ) parameters , collections = self . get_parameters_internal () self . _mark_initialized_recursive () if set_defaults : self . set_default_parameters ( parameters , collections ) return y , parameters , collections init_callable . _signature_f = self . call return init_callable","title":"init()"},{"location":"api/nn/Conv1D/","text":"elegy.nn.Conv1D One dimensional convolution. __init__ ( self , output_channels , kernel_shape , stride = 1 , rate = 1 , padding = 'SAME' , with_bias = True , w_init = None , b_init = None , data_format = 'NWC' , mask = None , groups = 1 , ** kwargs ) special Initializes the module. Parameters: Name Type Description Default output_channels int Number of output channels. required kernel_shape Union[int, Sequence[int]] The shape of the kernel. Either an integer or a sequence of length 1. required stride Union[int, Sequence[int]] tp.Optional stride for the kernel. Either an integer or a sequence of length 1. Defaults to 1. 1 rate Union[int, Sequence[int]] tp.Optional kernel dilation rate. Either an integer or a sequence of length 1. 1 corresponds to standard ND convolution, rate > 1 corresponds to dilated convolution. Defaults to 1. 1 padding Union[str, Sequence[Tuple[int, int]], Callable[[int], Tuple[int, int]], Sequence[Callable[[int], Tuple[int, int]]]] tp.Optional padding algorithm. Either VALID or SAME or a callable or sequence of callables of length 1. Any callables must take a single integer argument equal to the effective kernel size and return a list of two integers representing the padding before and after. See haiku.pad.* for more details and example functions. Defaults to SAME . See: https://www.tensorflow.org/xla/operation_semantics#conv_convolution. 'SAME' with_bias bool Whether to add a bias. By default, true. True w_init Optional[elegy.types.Initializer] tp.Optional weight initialization. By default, truncated normal. None b_init Optional[elegy.types.Initializer] tp.Optional bias initialization. By default, zeros. None data_format str The data format of the input. Either NWC or NCW . By default, NWC . 'NWC' mask Optional[numpy.ndarray] tp.Optional mask of the weights. None groups int A positive integer specifying the number of groups in which the input is split along the channel axis. Each group is convolved separately with filters / groups filters. The output is the concatenation of all the groups results along the channel axis. Input channels and filters must both be divisible by groups. 1 kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/conv.py def __init__ ( self , output_channels : int , kernel_shape : tp . Union [ int , tp . Sequence [ int ]], stride : tp . Union [ int , tp . Sequence [ int ]] = 1 , rate : tp . Union [ int , tp . Sequence [ int ]] = 1 , padding : tp . Union [ str , tp . Sequence [ tp . Tuple [ int , int ]], types . PadFnOrFns ] = \"SAME\" , with_bias : bool = True , w_init : tp . Optional [ types . Initializer ] = None , b_init : tp . Optional [ types . Initializer ] = None , data_format : str = \"NWC\" , mask : tp . Optional [ np . ndarray ] = None , groups : int = 1 , ** kwargs , ): \"\"\" Initializes the module. Args: output_channels: Number of output channels. kernel_shape: The shape of the kernel. Either an integer or a sequence of length 1. stride: tp.Optional stride for the kernel. Either an integer or a sequence of length 1. Defaults to 1. rate: tp.Optional kernel dilation rate. Either an integer or a sequence of length 1. 1 corresponds to standard ND convolution, ``rate > 1`` corresponds to dilated convolution. Defaults to 1. padding: tp.Optional padding algorithm. Either ``VALID`` or ``SAME`` or a callable or sequence of callables of length 1. Any callables must take a single integer argument equal to the effective kernel size and return a list of two integers representing the padding before and after. See haiku.pad.* for more details and example functions. Defaults to ``SAME``. See: https://www.tensorflow.org/xla/operation_semantics#conv_convolution. with_bias: Whether to add a bias. By default, true. w_init: tp.Optional weight initialization. By default, truncated normal. b_init: tp.Optional bias initialization. By default, zeros. data_format: The data format of the input. Either ``NWC`` or ``NCW``. By default, ``NWC``. mask: tp.Optional mask of the weights. groups: A positive integer specifying the number of groups in which the input is split along the channel axis. Each group is convolved separately with filters / groups filters. The output is the concatenation of all the groups results along the channel axis. Input channels and filters must both be divisible by groups. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( num_spatial_dims = 1 , output_channels = output_channels , kernel_shape = kernel_shape , stride = stride , rate = rate , padding = padding , with_bias = with_bias , w_init = w_init , b_init = b_init , data_format = data_format , mask = mask , groups = groups , ** kwargs , ) add_parameter ( self , name , initializer , collection = None , trainable = True , regularizer = None , constraint = None ) inherited Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required initializer Callable[[], Any] A callable that takes not arguments returns the initial value. required collection Optional[str] Optional name of the parameter collection, if not defined it will be se to \"parameters\" if trainable=True else it will be set to \"states\" . None trainable bool Specify whether this parameter should be added to the default trainable \"parameters\" collection or to the default non-trainable \"states\" collection. If collection is passed this parameter will ignored. True regularizer Optional[Callable[[Any], jax._src.numpy.lax_numpy.ndarray]] Regularizer instance (callable). None constraint Optional[Callable[[Any], Any]] Constraint instance (callable). None Returns: Type Description Any The value of the parameter. Source code in elegy/nn/conv.py def add_parameter ( self , name : str , initializer : tp . Callable [[], tp . Any ], collection : tp . Optional [ str ] = None , trainable : bool = True , regularizer : tp . Optional [ tp . Callable [[ tp . Any ], jnp . ndarray ]] = None , constraint : tp . Optional [ tp . Callable [[ tp . Any ], tp . Any ]] = None , ) -> tp . Any : \"\"\" Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. initializer: A callable that takes not arguments returns the initial value. collection: Optional name of the parameter collection, if not defined it will be se to `\"parameters\"` if `trainable=True` else it will be set to `\"states\"`. trainable: Specify whether this parameter should be added to the default trainable `\"parameters\"` collection or to the default non-trainable `\"states\"` collection. If collection is passed this parameter will ignored. regularizer: Regularizer instance (callable). constraint: Constraint instance (callable). Returns: The value of the parameter. \"\"\" if collection is None : collection = \"parameters\" if trainable else \"states\" if self . _scope_params is None : self . _scope_params = {} if name not in self . _scope_params : if not self . is_initializing (): raise ValueError ( f \"Cannot add parameter { name } outside `init`\" ) if name in self . _default_params : parameter = self . _default_params [ name ] assert collection == parameter . collection initial_value = parameter . value else : initial_value = initializer () initial_value = jax . tree_map ( jnp . asarray , initial_value ) parameter = types . Parameter ( collection , initial_value ) self . _register_parameter ( name , parameter ) self . _scope_params [ name ] = parameter parameter = self . _scope_params [ name ] if parameter . collection != collection : raise ValueError ( f \"types.Parameter { name } previously found in collection { parameter . collection } \" f \"but currently being added for collection { collection } \" ) value = parameter . value if constraint is not None : value = constraint ( value ) if regularizer is not None : hooks . add_loss ( name = utils . get_name ( regularizer ), value = regularizer ( value ), ) return value call ( self , inputs ) inherited Connects ConvND layer. Parameters: Name Type Description Default inputs ndarray A rank-N+2 array with shape [N, spatial_dims, C] . required Returns: Type Description ndarray A rank-N+2 array with shape [N, spatial_dims, output_channels] . Source code in elegy/nn/conv.py def call ( self , inputs : np . ndarray ) -> np . ndarray : \"\"\" Connects ``ConvND`` layer. Args: inputs: A rank-N+2 array with shape ``[N, spatial_dims, C]``. Returns: A rank-N+2 array with shape ``[N, spatial_dims, output_channels]``. \"\"\" required_rank = self . num_spatial_dims + 2 if inputs . ndim != required_rank : raise ValueError ( f \"Input to ConvND needs to have rank { required_rank } , \" f \"but input has shape { inputs . shape } .\" ) w_shape = self . kernel_shape + ( inputs . shape [ self . channel_index ] // self . groups , self . output_channels , ) if self . mask is not None and self . mask . shape != w_shape : raise ValueError ( \"Mask needs to have the same shape as weights. \" f \"Shapes are: { self . mask . shape } , { w_shape } \" ) w_init = self . w_init if w_init is None : fan_in_shape = np . prod ( w_shape [: - 1 ]) stddev = 1.0 / np . sqrt ( fan_in_shape ) w_init = initializers . TruncatedNormal ( stddev = stddev ) w = self . add_parameter ( \"w\" , lambda : w_init ( w_shape , jnp . float32 )) if self . mask is not None : w *= self . mask inputs = jnp . asarray ( inputs , dtype = self . dtype ) w = jnp . asarray ( w , dtype = self . dtype ) out = lax . conv_general_dilated ( inputs , w , window_strides = self . stride , padding = self . padding , lhs_dilation = self . lhs_dilation , rhs_dilation = self . kernel_dilation , dimension_numbers = self . dimension_numbers , feature_group_count = self . groups , ) if self . with_bias : if self . channel_index == - 1 : bias_shape = ( self . output_channels ,) else : bias_shape = ( self . output_channels ,) + ( 1 ,) * self . num_spatial_dims b = self . add_parameter ( \"b\" , lambda : self . b_init ( bias_shape , jnp . float32 )) b = jnp . broadcast_to ( b , out . shape ) b = jnp . asarray ( b , self . dtype ) out = out + b return out init ( self , * , rng = None , set_defaults = False ) inherited Initializes the module, Source code in elegy/nn/conv.py def init ( self , * , rng : tp . Optional [ types . RNGSeq ] = None , set_defaults : bool = False , ) -> tp . Callable [ ... , tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ] ]: \"\"\" Initializes the module, \"\"\" def init_callable ( * args , ** kwargs ) -> tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ]: with module_context ( module = self , collections = None , initializing = True , training = True , rng = rng ): y = self ( * args , ** kwargs ) parameters , collections = self . get_parameters_internal () self . _mark_initialized_recursive () if set_defaults : self . set_default_parameters ( parameters , collections ) return y , parameters , collections init_callable . _signature_f = self . call return init_callable","title":"Conv1D"},{"location":"api/nn/Conv1D/#elegynnconv1d","text":"","title":"elegy.nn.Conv1D"},{"location":"api/nn/Conv1D/#elegy.nn.conv.Conv1D","text":"One dimensional convolution.","title":"elegy.nn.conv.Conv1D"},{"location":"api/nn/Conv1D/#elegy.nn.conv.Conv1D.__init__","text":"Initializes the module. Parameters: Name Type Description Default output_channels int Number of output channels. required kernel_shape Union[int, Sequence[int]] The shape of the kernel. Either an integer or a sequence of length 1. required stride Union[int, Sequence[int]] tp.Optional stride for the kernel. Either an integer or a sequence of length 1. Defaults to 1. 1 rate Union[int, Sequence[int]] tp.Optional kernel dilation rate. Either an integer or a sequence of length 1. 1 corresponds to standard ND convolution, rate > 1 corresponds to dilated convolution. Defaults to 1. 1 padding Union[str, Sequence[Tuple[int, int]], Callable[[int], Tuple[int, int]], Sequence[Callable[[int], Tuple[int, int]]]] tp.Optional padding algorithm. Either VALID or SAME or a callable or sequence of callables of length 1. Any callables must take a single integer argument equal to the effective kernel size and return a list of two integers representing the padding before and after. See haiku.pad.* for more details and example functions. Defaults to SAME . See: https://www.tensorflow.org/xla/operation_semantics#conv_convolution. 'SAME' with_bias bool Whether to add a bias. By default, true. True w_init Optional[elegy.types.Initializer] tp.Optional weight initialization. By default, truncated normal. None b_init Optional[elegy.types.Initializer] tp.Optional bias initialization. By default, zeros. None data_format str The data format of the input. Either NWC or NCW . By default, NWC . 'NWC' mask Optional[numpy.ndarray] tp.Optional mask of the weights. None groups int A positive integer specifying the number of groups in which the input is split along the channel axis. Each group is convolved separately with filters / groups filters. The output is the concatenation of all the groups results along the channel axis. Input channels and filters must both be divisible by groups. 1 kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/conv.py def __init__ ( self , output_channels : int , kernel_shape : tp . Union [ int , tp . Sequence [ int ]], stride : tp . Union [ int , tp . Sequence [ int ]] = 1 , rate : tp . Union [ int , tp . Sequence [ int ]] = 1 , padding : tp . Union [ str , tp . Sequence [ tp . Tuple [ int , int ]], types . PadFnOrFns ] = \"SAME\" , with_bias : bool = True , w_init : tp . Optional [ types . Initializer ] = None , b_init : tp . Optional [ types . Initializer ] = None , data_format : str = \"NWC\" , mask : tp . Optional [ np . ndarray ] = None , groups : int = 1 , ** kwargs , ): \"\"\" Initializes the module. Args: output_channels: Number of output channels. kernel_shape: The shape of the kernel. Either an integer or a sequence of length 1. stride: tp.Optional stride for the kernel. Either an integer or a sequence of length 1. Defaults to 1. rate: tp.Optional kernel dilation rate. Either an integer or a sequence of length 1. 1 corresponds to standard ND convolution, ``rate > 1`` corresponds to dilated convolution. Defaults to 1. padding: tp.Optional padding algorithm. Either ``VALID`` or ``SAME`` or a callable or sequence of callables of length 1. Any callables must take a single integer argument equal to the effective kernel size and return a list of two integers representing the padding before and after. See haiku.pad.* for more details and example functions. Defaults to ``SAME``. See: https://www.tensorflow.org/xla/operation_semantics#conv_convolution. with_bias: Whether to add a bias. By default, true. w_init: tp.Optional weight initialization. By default, truncated normal. b_init: tp.Optional bias initialization. By default, zeros. data_format: The data format of the input. Either ``NWC`` or ``NCW``. By default, ``NWC``. mask: tp.Optional mask of the weights. groups: A positive integer specifying the number of groups in which the input is split along the channel axis. Each group is convolved separately with filters / groups filters. The output is the concatenation of all the groups results along the channel axis. Input channels and filters must both be divisible by groups. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( num_spatial_dims = 1 , output_channels = output_channels , kernel_shape = kernel_shape , stride = stride , rate = rate , padding = padding , with_bias = with_bias , w_init = w_init , b_init = b_init , data_format = data_format , mask = mask , groups = groups , ** kwargs , )","title":"__init__()"},{"location":"api/nn/Conv1D/#elegy.nn.conv.Conv1D.add_parameter","text":"Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required initializer Callable[[], Any] A callable that takes not arguments returns the initial value. required collection Optional[str] Optional name of the parameter collection, if not defined it will be se to \"parameters\" if trainable=True else it will be set to \"states\" . None trainable bool Specify whether this parameter should be added to the default trainable \"parameters\" collection or to the default non-trainable \"states\" collection. If collection is passed this parameter will ignored. True regularizer Optional[Callable[[Any], jax._src.numpy.lax_numpy.ndarray]] Regularizer instance (callable). None constraint Optional[Callable[[Any], Any]] Constraint instance (callable). None Returns: Type Description Any The value of the parameter. Source code in elegy/nn/conv.py def add_parameter ( self , name : str , initializer : tp . Callable [[], tp . Any ], collection : tp . Optional [ str ] = None , trainable : bool = True , regularizer : tp . Optional [ tp . Callable [[ tp . Any ], jnp . ndarray ]] = None , constraint : tp . Optional [ tp . Callable [[ tp . Any ], tp . Any ]] = None , ) -> tp . Any : \"\"\" Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. initializer: A callable that takes not arguments returns the initial value. collection: Optional name of the parameter collection, if not defined it will be se to `\"parameters\"` if `trainable=True` else it will be set to `\"states\"`. trainable: Specify whether this parameter should be added to the default trainable `\"parameters\"` collection or to the default non-trainable `\"states\"` collection. If collection is passed this parameter will ignored. regularizer: Regularizer instance (callable). constraint: Constraint instance (callable). Returns: The value of the parameter. \"\"\" if collection is None : collection = \"parameters\" if trainable else \"states\" if self . _scope_params is None : self . _scope_params = {} if name not in self . _scope_params : if not self . is_initializing (): raise ValueError ( f \"Cannot add parameter { name } outside `init`\" ) if name in self . _default_params : parameter = self . _default_params [ name ] assert collection == parameter . collection initial_value = parameter . value else : initial_value = initializer () initial_value = jax . tree_map ( jnp . asarray , initial_value ) parameter = types . Parameter ( collection , initial_value ) self . _register_parameter ( name , parameter ) self . _scope_params [ name ] = parameter parameter = self . _scope_params [ name ] if parameter . collection != collection : raise ValueError ( f \"types.Parameter { name } previously found in collection { parameter . collection } \" f \"but currently being added for collection { collection } \" ) value = parameter . value if constraint is not None : value = constraint ( value ) if regularizer is not None : hooks . add_loss ( name = utils . get_name ( regularizer ), value = regularizer ( value ), ) return value","title":"add_parameter()"},{"location":"api/nn/Conv1D/#elegy.nn.conv.Conv1D.call","text":"Connects ConvND layer. Parameters: Name Type Description Default inputs ndarray A rank-N+2 array with shape [N, spatial_dims, C] . required Returns: Type Description ndarray A rank-N+2 array with shape [N, spatial_dims, output_channels] . Source code in elegy/nn/conv.py def call ( self , inputs : np . ndarray ) -> np . ndarray : \"\"\" Connects ``ConvND`` layer. Args: inputs: A rank-N+2 array with shape ``[N, spatial_dims, C]``. Returns: A rank-N+2 array with shape ``[N, spatial_dims, output_channels]``. \"\"\" required_rank = self . num_spatial_dims + 2 if inputs . ndim != required_rank : raise ValueError ( f \"Input to ConvND needs to have rank { required_rank } , \" f \"but input has shape { inputs . shape } .\" ) w_shape = self . kernel_shape + ( inputs . shape [ self . channel_index ] // self . groups , self . output_channels , ) if self . mask is not None and self . mask . shape != w_shape : raise ValueError ( \"Mask needs to have the same shape as weights. \" f \"Shapes are: { self . mask . shape } , { w_shape } \" ) w_init = self . w_init if w_init is None : fan_in_shape = np . prod ( w_shape [: - 1 ]) stddev = 1.0 / np . sqrt ( fan_in_shape ) w_init = initializers . TruncatedNormal ( stddev = stddev ) w = self . add_parameter ( \"w\" , lambda : w_init ( w_shape , jnp . float32 )) if self . mask is not None : w *= self . mask inputs = jnp . asarray ( inputs , dtype = self . dtype ) w = jnp . asarray ( w , dtype = self . dtype ) out = lax . conv_general_dilated ( inputs , w , window_strides = self . stride , padding = self . padding , lhs_dilation = self . lhs_dilation , rhs_dilation = self . kernel_dilation , dimension_numbers = self . dimension_numbers , feature_group_count = self . groups , ) if self . with_bias : if self . channel_index == - 1 : bias_shape = ( self . output_channels ,) else : bias_shape = ( self . output_channels ,) + ( 1 ,) * self . num_spatial_dims b = self . add_parameter ( \"b\" , lambda : self . b_init ( bias_shape , jnp . float32 )) b = jnp . broadcast_to ( b , out . shape ) b = jnp . asarray ( b , self . dtype ) out = out + b return out","title":"call()"},{"location":"api/nn/Conv1D/#elegy.nn.conv.Conv1D.init","text":"Initializes the module, Source code in elegy/nn/conv.py def init ( self , * , rng : tp . Optional [ types . RNGSeq ] = None , set_defaults : bool = False , ) -> tp . Callable [ ... , tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ] ]: \"\"\" Initializes the module, \"\"\" def init_callable ( * args , ** kwargs ) -> tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ]: with module_context ( module = self , collections = None , initializing = True , training = True , rng = rng ): y = self ( * args , ** kwargs ) parameters , collections = self . get_parameters_internal () self . _mark_initialized_recursive () if set_defaults : self . set_default_parameters ( parameters , collections ) return y , parameters , collections init_callable . _signature_f = self . call return init_callable","title":"init()"},{"location":"api/nn/Conv2D/","text":"elegy.nn.Conv2D Two dimensional convolution. __init__ ( self , output_channels , kernel_shape , stride = 1 , rate = 1 , padding = 'SAME' , with_bias = True , w_init = None , b_init = None , data_format = 'NHWC' , mask = None , groups = 1 , ** kwargs ) special Initializes the module. Parameters: Name Type Description Default output_channels int Number of output channels. required kernel_shape Union[int, Sequence[int]] The shape of the kernel. Either an integer or a sequence of length 2. required stride Union[int, Sequence[int]] tp.Optional stride for the kernel. Either an integer or a sequence of length 2. Defaults to 1. 1 rate Union[int, Sequence[int]] tp.Optional kernel dilation rate. Either an integer or a sequence of length 2. 1 corresponds to standard ND convolution, rate > 1 corresponds to dilated convolution. Defaults to 1. 1 padding Union[str, Sequence[Tuple[int, int]], Callable[[int], Tuple[int, int]], Sequence[Callable[[int], Tuple[int, int]]]] tp.Optional padding algorithm. Either VALID or SAME or a callable or sequence of callables of length 2. Any callables must take a single integer argument equal to the effective kernel size and return a list of two integers representing the padding before and after. See haiku.pad.* for more details and example functions. Defaults to SAME . See: https://www.tensorflow.org/xla/operation_semantics#conv_convolution. 'SAME' with_bias bool Whether to add a bias. By default, true. True w_init Optional[elegy.types.Initializer] tp.Optional weight initialization. By default, truncated normal. None b_init Optional[elegy.types.Initializer] tp.Optional bias initialization. By default, zeros. None data_format str The data format of the input. Either NHWC or NCHW . By default, NHWC . 'NHWC' mask Optional[numpy.ndarray] tp.Optional mask of the weights. None groups int A positive integer specifying the number of groups in which the input is split along the channel axis. Each group is convolved separately with filters / groups filters. The output is the concatenation of all the groups results along the channel axis. Input channels and filters must both be divisible by groups. 1 kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/conv.py def __init__ ( self , output_channels : int , kernel_shape : tp . Union [ int , tp . Sequence [ int ]], stride : tp . Union [ int , tp . Sequence [ int ]] = 1 , rate : tp . Union [ int , tp . Sequence [ int ]] = 1 , padding : tp . Union [ str , tp . Sequence [ tp . Tuple [ int , int ]], types . PadFnOrFns ] = \"SAME\" , with_bias : bool = True , w_init : tp . Optional [ types . Initializer ] = None , b_init : tp . Optional [ types . Initializer ] = None , data_format : str = \"NHWC\" , mask : tp . Optional [ np . ndarray ] = None , groups : int = 1 , ** kwargs , ): \"\"\" Initializes the module. Args: output_channels: Number of output channels. kernel_shape: The shape of the kernel. Either an integer or a sequence of length 2. stride: tp.Optional stride for the kernel. Either an integer or a sequence of length 2. Defaults to 1. rate: tp.Optional kernel dilation rate. Either an integer or a sequence of length 2. 1 corresponds to standard ND convolution, ``rate > 1`` corresponds to dilated convolution. Defaults to 1. padding: tp.Optional padding algorithm. Either ``VALID`` or ``SAME`` or a callable or sequence of callables of length 2. Any callables must take a single integer argument equal to the effective kernel size and return a list of two integers representing the padding before and after. See haiku.pad.* for more details and example functions. Defaults to ``SAME``. See: https://www.tensorflow.org/xla/operation_semantics#conv_convolution. with_bias: Whether to add a bias. By default, true. w_init: tp.Optional weight initialization. By default, truncated normal. b_init: tp.Optional bias initialization. By default, zeros. data_format: The data format of the input. Either ``NHWC`` or ``NCHW``. By default, ``NHWC``. mask: tp.Optional mask of the weights. groups: A positive integer specifying the number of groups in which the input is split along the channel axis. Each group is convolved separately with filters / groups filters. The output is the concatenation of all the groups results along the channel axis. Input channels and filters must both be divisible by groups. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( num_spatial_dims = 2 , output_channels = output_channels , kernel_shape = kernel_shape , stride = stride , rate = rate , padding = padding , with_bias = with_bias , w_init = w_init , b_init = b_init , data_format = data_format , mask = mask , groups = groups , ** kwargs , ) add_parameter ( self , name , initializer , collection = None , trainable = True , regularizer = None , constraint = None ) inherited Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required initializer Callable[[], Any] A callable that takes not arguments returns the initial value. required collection Optional[str] Optional name of the parameter collection, if not defined it will be se to \"parameters\" if trainable=True else it will be set to \"states\" . None trainable bool Specify whether this parameter should be added to the default trainable \"parameters\" collection or to the default non-trainable \"states\" collection. If collection is passed this parameter will ignored. True regularizer Optional[Callable[[Any], jax._src.numpy.lax_numpy.ndarray]] Regularizer instance (callable). None constraint Optional[Callable[[Any], Any]] Constraint instance (callable). None Returns: Type Description Any The value of the parameter. Source code in elegy/nn/conv.py def add_parameter ( self , name : str , initializer : tp . Callable [[], tp . Any ], collection : tp . Optional [ str ] = None , trainable : bool = True , regularizer : tp . Optional [ tp . Callable [[ tp . Any ], jnp . ndarray ]] = None , constraint : tp . Optional [ tp . Callable [[ tp . Any ], tp . Any ]] = None , ) -> tp . Any : \"\"\" Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. initializer: A callable that takes not arguments returns the initial value. collection: Optional name of the parameter collection, if not defined it will be se to `\"parameters\"` if `trainable=True` else it will be set to `\"states\"`. trainable: Specify whether this parameter should be added to the default trainable `\"parameters\"` collection or to the default non-trainable `\"states\"` collection. If collection is passed this parameter will ignored. regularizer: Regularizer instance (callable). constraint: Constraint instance (callable). Returns: The value of the parameter. \"\"\" if collection is None : collection = \"parameters\" if trainable else \"states\" if self . _scope_params is None : self . _scope_params = {} if name not in self . _scope_params : if not self . is_initializing (): raise ValueError ( f \"Cannot add parameter { name } outside `init`\" ) if name in self . _default_params : parameter = self . _default_params [ name ] assert collection == parameter . collection initial_value = parameter . value else : initial_value = initializer () initial_value = jax . tree_map ( jnp . asarray , initial_value ) parameter = types . Parameter ( collection , initial_value ) self . _register_parameter ( name , parameter ) self . _scope_params [ name ] = parameter parameter = self . _scope_params [ name ] if parameter . collection != collection : raise ValueError ( f \"types.Parameter { name } previously found in collection { parameter . collection } \" f \"but currently being added for collection { collection } \" ) value = parameter . value if constraint is not None : value = constraint ( value ) if regularizer is not None : hooks . add_loss ( name = utils . get_name ( regularizer ), value = regularizer ( value ), ) return value call ( self , inputs ) inherited Connects ConvND layer. Parameters: Name Type Description Default inputs ndarray A rank-N+2 array with shape [N, spatial_dims, C] . required Returns: Type Description ndarray A rank-N+2 array with shape [N, spatial_dims, output_channels] . Source code in elegy/nn/conv.py def call ( self , inputs : np . ndarray ) -> np . ndarray : \"\"\" Connects ``ConvND`` layer. Args: inputs: A rank-N+2 array with shape ``[N, spatial_dims, C]``. Returns: A rank-N+2 array with shape ``[N, spatial_dims, output_channels]``. \"\"\" required_rank = self . num_spatial_dims + 2 if inputs . ndim != required_rank : raise ValueError ( f \"Input to ConvND needs to have rank { required_rank } , \" f \"but input has shape { inputs . shape } .\" ) w_shape = self . kernel_shape + ( inputs . shape [ self . channel_index ] // self . groups , self . output_channels , ) if self . mask is not None and self . mask . shape != w_shape : raise ValueError ( \"Mask needs to have the same shape as weights. \" f \"Shapes are: { self . mask . shape } , { w_shape } \" ) w_init = self . w_init if w_init is None : fan_in_shape = np . prod ( w_shape [: - 1 ]) stddev = 1.0 / np . sqrt ( fan_in_shape ) w_init = initializers . TruncatedNormal ( stddev = stddev ) w = self . add_parameter ( \"w\" , lambda : w_init ( w_shape , jnp . float32 )) if self . mask is not None : w *= self . mask inputs = jnp . asarray ( inputs , dtype = self . dtype ) w = jnp . asarray ( w , dtype = self . dtype ) out = lax . conv_general_dilated ( inputs , w , window_strides = self . stride , padding = self . padding , lhs_dilation = self . lhs_dilation , rhs_dilation = self . kernel_dilation , dimension_numbers = self . dimension_numbers , feature_group_count = self . groups , ) if self . with_bias : if self . channel_index == - 1 : bias_shape = ( self . output_channels ,) else : bias_shape = ( self . output_channels ,) + ( 1 ,) * self . num_spatial_dims b = self . add_parameter ( \"b\" , lambda : self . b_init ( bias_shape , jnp . float32 )) b = jnp . broadcast_to ( b , out . shape ) b = jnp . asarray ( b , self . dtype ) out = out + b return out init ( self , * , rng = None , set_defaults = False ) inherited Initializes the module, Source code in elegy/nn/conv.py def init ( self , * , rng : tp . Optional [ types . RNGSeq ] = None , set_defaults : bool = False , ) -> tp . Callable [ ... , tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ] ]: \"\"\" Initializes the module, \"\"\" def init_callable ( * args , ** kwargs ) -> tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ]: with module_context ( module = self , collections = None , initializing = True , training = True , rng = rng ): y = self ( * args , ** kwargs ) parameters , collections = self . get_parameters_internal () self . _mark_initialized_recursive () if set_defaults : self . set_default_parameters ( parameters , collections ) return y , parameters , collections init_callable . _signature_f = self . call return init_callable","title":"Conv2D"},{"location":"api/nn/Conv2D/#elegynnconv2d","text":"","title":"elegy.nn.Conv2D"},{"location":"api/nn/Conv2D/#elegy.nn.conv.Conv2D","text":"Two dimensional convolution.","title":"elegy.nn.conv.Conv2D"},{"location":"api/nn/Conv2D/#elegy.nn.conv.Conv2D.__init__","text":"Initializes the module. Parameters: Name Type Description Default output_channels int Number of output channels. required kernel_shape Union[int, Sequence[int]] The shape of the kernel. Either an integer or a sequence of length 2. required stride Union[int, Sequence[int]] tp.Optional stride for the kernel. Either an integer or a sequence of length 2. Defaults to 1. 1 rate Union[int, Sequence[int]] tp.Optional kernel dilation rate. Either an integer or a sequence of length 2. 1 corresponds to standard ND convolution, rate > 1 corresponds to dilated convolution. Defaults to 1. 1 padding Union[str, Sequence[Tuple[int, int]], Callable[[int], Tuple[int, int]], Sequence[Callable[[int], Tuple[int, int]]]] tp.Optional padding algorithm. Either VALID or SAME or a callable or sequence of callables of length 2. Any callables must take a single integer argument equal to the effective kernel size and return a list of two integers representing the padding before and after. See haiku.pad.* for more details and example functions. Defaults to SAME . See: https://www.tensorflow.org/xla/operation_semantics#conv_convolution. 'SAME' with_bias bool Whether to add a bias. By default, true. True w_init Optional[elegy.types.Initializer] tp.Optional weight initialization. By default, truncated normal. None b_init Optional[elegy.types.Initializer] tp.Optional bias initialization. By default, zeros. None data_format str The data format of the input. Either NHWC or NCHW . By default, NHWC . 'NHWC' mask Optional[numpy.ndarray] tp.Optional mask of the weights. None groups int A positive integer specifying the number of groups in which the input is split along the channel axis. Each group is convolved separately with filters / groups filters. The output is the concatenation of all the groups results along the channel axis. Input channels and filters must both be divisible by groups. 1 kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/conv.py def __init__ ( self , output_channels : int , kernel_shape : tp . Union [ int , tp . Sequence [ int ]], stride : tp . Union [ int , tp . Sequence [ int ]] = 1 , rate : tp . Union [ int , tp . Sequence [ int ]] = 1 , padding : tp . Union [ str , tp . Sequence [ tp . Tuple [ int , int ]], types . PadFnOrFns ] = \"SAME\" , with_bias : bool = True , w_init : tp . Optional [ types . Initializer ] = None , b_init : tp . Optional [ types . Initializer ] = None , data_format : str = \"NHWC\" , mask : tp . Optional [ np . ndarray ] = None , groups : int = 1 , ** kwargs , ): \"\"\" Initializes the module. Args: output_channels: Number of output channels. kernel_shape: The shape of the kernel. Either an integer or a sequence of length 2. stride: tp.Optional stride for the kernel. Either an integer or a sequence of length 2. Defaults to 1. rate: tp.Optional kernel dilation rate. Either an integer or a sequence of length 2. 1 corresponds to standard ND convolution, ``rate > 1`` corresponds to dilated convolution. Defaults to 1. padding: tp.Optional padding algorithm. Either ``VALID`` or ``SAME`` or a callable or sequence of callables of length 2. Any callables must take a single integer argument equal to the effective kernel size and return a list of two integers representing the padding before and after. See haiku.pad.* for more details and example functions. Defaults to ``SAME``. See: https://www.tensorflow.org/xla/operation_semantics#conv_convolution. with_bias: Whether to add a bias. By default, true. w_init: tp.Optional weight initialization. By default, truncated normal. b_init: tp.Optional bias initialization. By default, zeros. data_format: The data format of the input. Either ``NHWC`` or ``NCHW``. By default, ``NHWC``. mask: tp.Optional mask of the weights. groups: A positive integer specifying the number of groups in which the input is split along the channel axis. Each group is convolved separately with filters / groups filters. The output is the concatenation of all the groups results along the channel axis. Input channels and filters must both be divisible by groups. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( num_spatial_dims = 2 , output_channels = output_channels , kernel_shape = kernel_shape , stride = stride , rate = rate , padding = padding , with_bias = with_bias , w_init = w_init , b_init = b_init , data_format = data_format , mask = mask , groups = groups , ** kwargs , )","title":"__init__()"},{"location":"api/nn/Conv2D/#elegy.nn.conv.Conv2D.add_parameter","text":"Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required initializer Callable[[], Any] A callable that takes not arguments returns the initial value. required collection Optional[str] Optional name of the parameter collection, if not defined it will be se to \"parameters\" if trainable=True else it will be set to \"states\" . None trainable bool Specify whether this parameter should be added to the default trainable \"parameters\" collection or to the default non-trainable \"states\" collection. If collection is passed this parameter will ignored. True regularizer Optional[Callable[[Any], jax._src.numpy.lax_numpy.ndarray]] Regularizer instance (callable). None constraint Optional[Callable[[Any], Any]] Constraint instance (callable). None Returns: Type Description Any The value of the parameter. Source code in elegy/nn/conv.py def add_parameter ( self , name : str , initializer : tp . Callable [[], tp . Any ], collection : tp . Optional [ str ] = None , trainable : bool = True , regularizer : tp . Optional [ tp . Callable [[ tp . Any ], jnp . ndarray ]] = None , constraint : tp . Optional [ tp . Callable [[ tp . Any ], tp . Any ]] = None , ) -> tp . Any : \"\"\" Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. initializer: A callable that takes not arguments returns the initial value. collection: Optional name of the parameter collection, if not defined it will be se to `\"parameters\"` if `trainable=True` else it will be set to `\"states\"`. trainable: Specify whether this parameter should be added to the default trainable `\"parameters\"` collection or to the default non-trainable `\"states\"` collection. If collection is passed this parameter will ignored. regularizer: Regularizer instance (callable). constraint: Constraint instance (callable). Returns: The value of the parameter. \"\"\" if collection is None : collection = \"parameters\" if trainable else \"states\" if self . _scope_params is None : self . _scope_params = {} if name not in self . _scope_params : if not self . is_initializing (): raise ValueError ( f \"Cannot add parameter { name } outside `init`\" ) if name in self . _default_params : parameter = self . _default_params [ name ] assert collection == parameter . collection initial_value = parameter . value else : initial_value = initializer () initial_value = jax . tree_map ( jnp . asarray , initial_value ) parameter = types . Parameter ( collection , initial_value ) self . _register_parameter ( name , parameter ) self . _scope_params [ name ] = parameter parameter = self . _scope_params [ name ] if parameter . collection != collection : raise ValueError ( f \"types.Parameter { name } previously found in collection { parameter . collection } \" f \"but currently being added for collection { collection } \" ) value = parameter . value if constraint is not None : value = constraint ( value ) if regularizer is not None : hooks . add_loss ( name = utils . get_name ( regularizer ), value = regularizer ( value ), ) return value","title":"add_parameter()"},{"location":"api/nn/Conv2D/#elegy.nn.conv.Conv2D.call","text":"Connects ConvND layer. Parameters: Name Type Description Default inputs ndarray A rank-N+2 array with shape [N, spatial_dims, C] . required Returns: Type Description ndarray A rank-N+2 array with shape [N, spatial_dims, output_channels] . Source code in elegy/nn/conv.py def call ( self , inputs : np . ndarray ) -> np . ndarray : \"\"\" Connects ``ConvND`` layer. Args: inputs: A rank-N+2 array with shape ``[N, spatial_dims, C]``. Returns: A rank-N+2 array with shape ``[N, spatial_dims, output_channels]``. \"\"\" required_rank = self . num_spatial_dims + 2 if inputs . ndim != required_rank : raise ValueError ( f \"Input to ConvND needs to have rank { required_rank } , \" f \"but input has shape { inputs . shape } .\" ) w_shape = self . kernel_shape + ( inputs . shape [ self . channel_index ] // self . groups , self . output_channels , ) if self . mask is not None and self . mask . shape != w_shape : raise ValueError ( \"Mask needs to have the same shape as weights. \" f \"Shapes are: { self . mask . shape } , { w_shape } \" ) w_init = self . w_init if w_init is None : fan_in_shape = np . prod ( w_shape [: - 1 ]) stddev = 1.0 / np . sqrt ( fan_in_shape ) w_init = initializers . TruncatedNormal ( stddev = stddev ) w = self . add_parameter ( \"w\" , lambda : w_init ( w_shape , jnp . float32 )) if self . mask is not None : w *= self . mask inputs = jnp . asarray ( inputs , dtype = self . dtype ) w = jnp . asarray ( w , dtype = self . dtype ) out = lax . conv_general_dilated ( inputs , w , window_strides = self . stride , padding = self . padding , lhs_dilation = self . lhs_dilation , rhs_dilation = self . kernel_dilation , dimension_numbers = self . dimension_numbers , feature_group_count = self . groups , ) if self . with_bias : if self . channel_index == - 1 : bias_shape = ( self . output_channels ,) else : bias_shape = ( self . output_channels ,) + ( 1 ,) * self . num_spatial_dims b = self . add_parameter ( \"b\" , lambda : self . b_init ( bias_shape , jnp . float32 )) b = jnp . broadcast_to ( b , out . shape ) b = jnp . asarray ( b , self . dtype ) out = out + b return out","title":"call()"},{"location":"api/nn/Conv2D/#elegy.nn.conv.Conv2D.init","text":"Initializes the module, Source code in elegy/nn/conv.py def init ( self , * , rng : tp . Optional [ types . RNGSeq ] = None , set_defaults : bool = False , ) -> tp . Callable [ ... , tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ] ]: \"\"\" Initializes the module, \"\"\" def init_callable ( * args , ** kwargs ) -> tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ]: with module_context ( module = self , collections = None , initializing = True , training = True , rng = rng ): y = self ( * args , ** kwargs ) parameters , collections = self . get_parameters_internal () self . _mark_initialized_recursive () if set_defaults : self . set_default_parameters ( parameters , collections ) return y , parameters , collections init_callable . _signature_f = self . call return init_callable","title":"init()"},{"location":"api/nn/Conv3D/","text":"elegy.nn.Conv3D Three dimensional convolution. __init__ ( self , output_channels , kernel_shape , stride = 1 , rate = 1 , padding = 'SAME' , with_bias = True , w_init = None , b_init = None , data_format = 'NDHWC' , mask = None , groups = 1 , ** kwargs ) special Initializes the module. Parameters: Name Type Description Default output_channels int Number of output channels. required kernel_shape Union[int, Sequence[int]] The shape of the kernel. Either an integer or a sequence of length 3. required stride Union[int, Sequence[int]] tp.Optional stride for the kernel. Either an integer or a sequence of length 3. Defaults to 1. 1 rate Union[int, Sequence[int]] tp.Optional kernel dilation rate. Either an integer or a sequence of length 3. 1 corresponds to standard ND convolution, rate > 1 corresponds to dilated convolution. Defaults to 1. 1 padding Union[str, Sequence[Tuple[int, int]], Callable[[int], Tuple[int, int]], Sequence[Callable[[int], Tuple[int, int]]]] tp.Optional padding algorithm. Either VALID or SAME or a callable or sequence of callables of length 3. Any callables must take a single integer argument equal to the effective kernel size and return a list of two integers representing the padding before and after. See haiku.pad.* for more details and example functions. Defaults to SAME . See: https://www.tensorflow.org/xla/operation_semantics#conv_convolution. 'SAME' with_bias bool Whether to add a bias. By default, true. True w_init Optional[elegy.types.Initializer] tp.Optional weight initialization. By default, truncated normal. None b_init Optional[elegy.types.Initializer] tp.Optional bias initialization. By default, zeros. None data_format str The data format of the input. Either NDHWC or NCDHW . By default, NDHWC . 'NDHWC' mask Optional[numpy.ndarray] tp.Optional mask of the weights. None groups int A positive integer specifying the number of groups in which the input is split along the channel axis. Each group is convolved separately with filters / groups filters. The output is the concatenation of all the groups results along the channel axis. Input channels and filters must both be divisible by groups. 1 kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/conv.py def __init__ ( self , output_channels : int , kernel_shape : tp . Union [ int , tp . Sequence [ int ]], stride : tp . Union [ int , tp . Sequence [ int ]] = 1 , rate : tp . Union [ int , tp . Sequence [ int ]] = 1 , padding : tp . Union [ str , tp . Sequence [ tp . Tuple [ int , int ]], types . PadFnOrFns ] = \"SAME\" , with_bias : bool = True , w_init : tp . Optional [ types . Initializer ] = None , b_init : tp . Optional [ types . Initializer ] = None , data_format : str = \"NDHWC\" , mask : tp . Optional [ np . ndarray ] = None , groups : int = 1 , ** kwargs , ): \"\"\" Initializes the module. Args: output_channels: Number of output channels. kernel_shape: The shape of the kernel. Either an integer or a sequence of length 3. stride: tp.Optional stride for the kernel. Either an integer or a sequence of length 3. Defaults to 1. rate: tp.Optional kernel dilation rate. Either an integer or a sequence of length 3. 1 corresponds to standard ND convolution, `rate > 1` corresponds to dilated convolution. Defaults to 1. padding: tp.Optional padding algorithm. Either ``VALID`` or ``SAME`` or a callable or sequence of callables of length 3. Any callables must take a single integer argument equal to the effective kernel size and return a list of two integers representing the padding before and after. See haiku.pad.* for more details and example functions. Defaults to ``SAME``. See: https://www.tensorflow.org/xla/operation_semantics#conv_convolution. with_bias: Whether to add a bias. By default, true. w_init: tp.Optional weight initialization. By default, truncated normal. b_init: tp.Optional bias initialization. By default, zeros. data_format: The data format of the input. Either ``NDHWC`` or ``NCDHW``. By default, ``NDHWC``. mask: tp.Optional mask of the weights. groups: A positive integer specifying the number of groups in which the input is split along the channel axis. Each group is convolved separately with filters / groups filters. The output is the concatenation of all the groups results along the channel axis. Input channels and filters must both be divisible by groups. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( num_spatial_dims = 3 , output_channels = output_channels , kernel_shape = kernel_shape , stride = stride , rate = rate , padding = padding , with_bias = with_bias , w_init = w_init , b_init = b_init , data_format = data_format , mask = mask , groups = groups , ** kwargs , ) add_parameter ( self , name , initializer , collection = None , trainable = True , regularizer = None , constraint = None ) inherited Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required initializer Callable[[], Any] A callable that takes not arguments returns the initial value. required collection Optional[str] Optional name of the parameter collection, if not defined it will be se to \"parameters\" if trainable=True else it will be set to \"states\" . None trainable bool Specify whether this parameter should be added to the default trainable \"parameters\" collection or to the default non-trainable \"states\" collection. If collection is passed this parameter will ignored. True regularizer Optional[Callable[[Any], jax._src.numpy.lax_numpy.ndarray]] Regularizer instance (callable). None constraint Optional[Callable[[Any], Any]] Constraint instance (callable). None Returns: Type Description Any The value of the parameter. Source code in elegy/nn/conv.py def add_parameter ( self , name : str , initializer : tp . Callable [[], tp . Any ], collection : tp . Optional [ str ] = None , trainable : bool = True , regularizer : tp . Optional [ tp . Callable [[ tp . Any ], jnp . ndarray ]] = None , constraint : tp . Optional [ tp . Callable [[ tp . Any ], tp . Any ]] = None , ) -> tp . Any : \"\"\" Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. initializer: A callable that takes not arguments returns the initial value. collection: Optional name of the parameter collection, if not defined it will be se to `\"parameters\"` if `trainable=True` else it will be set to `\"states\"`. trainable: Specify whether this parameter should be added to the default trainable `\"parameters\"` collection or to the default non-trainable `\"states\"` collection. If collection is passed this parameter will ignored. regularizer: Regularizer instance (callable). constraint: Constraint instance (callable). Returns: The value of the parameter. \"\"\" if collection is None : collection = \"parameters\" if trainable else \"states\" if self . _scope_params is None : self . _scope_params = {} if name not in self . _scope_params : if not self . is_initializing (): raise ValueError ( f \"Cannot add parameter { name } outside `init`\" ) if name in self . _default_params : parameter = self . _default_params [ name ] assert collection == parameter . collection initial_value = parameter . value else : initial_value = initializer () initial_value = jax . tree_map ( jnp . asarray , initial_value ) parameter = types . Parameter ( collection , initial_value ) self . _register_parameter ( name , parameter ) self . _scope_params [ name ] = parameter parameter = self . _scope_params [ name ] if parameter . collection != collection : raise ValueError ( f \"types.Parameter { name } previously found in collection { parameter . collection } \" f \"but currently being added for collection { collection } \" ) value = parameter . value if constraint is not None : value = constraint ( value ) if regularizer is not None : hooks . add_loss ( name = utils . get_name ( regularizer ), value = regularizer ( value ), ) return value call ( self , inputs ) inherited Connects ConvND layer. Parameters: Name Type Description Default inputs ndarray A rank-N+2 array with shape [N, spatial_dims, C] . required Returns: Type Description ndarray A rank-N+2 array with shape [N, spatial_dims, output_channels] . Source code in elegy/nn/conv.py def call ( self , inputs : np . ndarray ) -> np . ndarray : \"\"\" Connects ``ConvND`` layer. Args: inputs: A rank-N+2 array with shape ``[N, spatial_dims, C]``. Returns: A rank-N+2 array with shape ``[N, spatial_dims, output_channels]``. \"\"\" required_rank = self . num_spatial_dims + 2 if inputs . ndim != required_rank : raise ValueError ( f \"Input to ConvND needs to have rank { required_rank } , \" f \"but input has shape { inputs . shape } .\" ) w_shape = self . kernel_shape + ( inputs . shape [ self . channel_index ] // self . groups , self . output_channels , ) if self . mask is not None and self . mask . shape != w_shape : raise ValueError ( \"Mask needs to have the same shape as weights. \" f \"Shapes are: { self . mask . shape } , { w_shape } \" ) w_init = self . w_init if w_init is None : fan_in_shape = np . prod ( w_shape [: - 1 ]) stddev = 1.0 / np . sqrt ( fan_in_shape ) w_init = initializers . TruncatedNormal ( stddev = stddev ) w = self . add_parameter ( \"w\" , lambda : w_init ( w_shape , jnp . float32 )) if self . mask is not None : w *= self . mask inputs = jnp . asarray ( inputs , dtype = self . dtype ) w = jnp . asarray ( w , dtype = self . dtype ) out = lax . conv_general_dilated ( inputs , w , window_strides = self . stride , padding = self . padding , lhs_dilation = self . lhs_dilation , rhs_dilation = self . kernel_dilation , dimension_numbers = self . dimension_numbers , feature_group_count = self . groups , ) if self . with_bias : if self . channel_index == - 1 : bias_shape = ( self . output_channels ,) else : bias_shape = ( self . output_channels ,) + ( 1 ,) * self . num_spatial_dims b = self . add_parameter ( \"b\" , lambda : self . b_init ( bias_shape , jnp . float32 )) b = jnp . broadcast_to ( b , out . shape ) b = jnp . asarray ( b , self . dtype ) out = out + b return out init ( self , * , rng = None , set_defaults = False ) inherited Initializes the module, Source code in elegy/nn/conv.py def init ( self , * , rng : tp . Optional [ types . RNGSeq ] = None , set_defaults : bool = False , ) -> tp . Callable [ ... , tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ] ]: \"\"\" Initializes the module, \"\"\" def init_callable ( * args , ** kwargs ) -> tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ]: with module_context ( module = self , collections = None , initializing = True , training = True , rng = rng ): y = self ( * args , ** kwargs ) parameters , collections = self . get_parameters_internal () self . _mark_initialized_recursive () if set_defaults : self . set_default_parameters ( parameters , collections ) return y , parameters , collections init_callable . _signature_f = self . call return init_callable","title":"Conv3D"},{"location":"api/nn/Conv3D/#elegynnconv3d","text":"","title":"elegy.nn.Conv3D"},{"location":"api/nn/Conv3D/#elegy.nn.conv.Conv3D","text":"Three dimensional convolution.","title":"elegy.nn.conv.Conv3D"},{"location":"api/nn/Conv3D/#elegy.nn.conv.Conv3D.__init__","text":"Initializes the module. Parameters: Name Type Description Default output_channels int Number of output channels. required kernel_shape Union[int, Sequence[int]] The shape of the kernel. Either an integer or a sequence of length 3. required stride Union[int, Sequence[int]] tp.Optional stride for the kernel. Either an integer or a sequence of length 3. Defaults to 1. 1 rate Union[int, Sequence[int]] tp.Optional kernel dilation rate. Either an integer or a sequence of length 3. 1 corresponds to standard ND convolution, rate > 1 corresponds to dilated convolution. Defaults to 1. 1 padding Union[str, Sequence[Tuple[int, int]], Callable[[int], Tuple[int, int]], Sequence[Callable[[int], Tuple[int, int]]]] tp.Optional padding algorithm. Either VALID or SAME or a callable or sequence of callables of length 3. Any callables must take a single integer argument equal to the effective kernel size and return a list of two integers representing the padding before and after. See haiku.pad.* for more details and example functions. Defaults to SAME . See: https://www.tensorflow.org/xla/operation_semantics#conv_convolution. 'SAME' with_bias bool Whether to add a bias. By default, true. True w_init Optional[elegy.types.Initializer] tp.Optional weight initialization. By default, truncated normal. None b_init Optional[elegy.types.Initializer] tp.Optional bias initialization. By default, zeros. None data_format str The data format of the input. Either NDHWC or NCDHW . By default, NDHWC . 'NDHWC' mask Optional[numpy.ndarray] tp.Optional mask of the weights. None groups int A positive integer specifying the number of groups in which the input is split along the channel axis. Each group is convolved separately with filters / groups filters. The output is the concatenation of all the groups results along the channel axis. Input channels and filters must both be divisible by groups. 1 kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/conv.py def __init__ ( self , output_channels : int , kernel_shape : tp . Union [ int , tp . Sequence [ int ]], stride : tp . Union [ int , tp . Sequence [ int ]] = 1 , rate : tp . Union [ int , tp . Sequence [ int ]] = 1 , padding : tp . Union [ str , tp . Sequence [ tp . Tuple [ int , int ]], types . PadFnOrFns ] = \"SAME\" , with_bias : bool = True , w_init : tp . Optional [ types . Initializer ] = None , b_init : tp . Optional [ types . Initializer ] = None , data_format : str = \"NDHWC\" , mask : tp . Optional [ np . ndarray ] = None , groups : int = 1 , ** kwargs , ): \"\"\" Initializes the module. Args: output_channels: Number of output channels. kernel_shape: The shape of the kernel. Either an integer or a sequence of length 3. stride: tp.Optional stride for the kernel. Either an integer or a sequence of length 3. Defaults to 1. rate: tp.Optional kernel dilation rate. Either an integer or a sequence of length 3. 1 corresponds to standard ND convolution, `rate > 1` corresponds to dilated convolution. Defaults to 1. padding: tp.Optional padding algorithm. Either ``VALID`` or ``SAME`` or a callable or sequence of callables of length 3. Any callables must take a single integer argument equal to the effective kernel size and return a list of two integers representing the padding before and after. See haiku.pad.* for more details and example functions. Defaults to ``SAME``. See: https://www.tensorflow.org/xla/operation_semantics#conv_convolution. with_bias: Whether to add a bias. By default, true. w_init: tp.Optional weight initialization. By default, truncated normal. b_init: tp.Optional bias initialization. By default, zeros. data_format: The data format of the input. Either ``NDHWC`` or ``NCDHW``. By default, ``NDHWC``. mask: tp.Optional mask of the weights. groups: A positive integer specifying the number of groups in which the input is split along the channel axis. Each group is convolved separately with filters / groups filters. The output is the concatenation of all the groups results along the channel axis. Input channels and filters must both be divisible by groups. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( num_spatial_dims = 3 , output_channels = output_channels , kernel_shape = kernel_shape , stride = stride , rate = rate , padding = padding , with_bias = with_bias , w_init = w_init , b_init = b_init , data_format = data_format , mask = mask , groups = groups , ** kwargs , )","title":"__init__()"},{"location":"api/nn/Conv3D/#elegy.nn.conv.Conv3D.add_parameter","text":"Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required initializer Callable[[], Any] A callable that takes not arguments returns the initial value. required collection Optional[str] Optional name of the parameter collection, if not defined it will be se to \"parameters\" if trainable=True else it will be set to \"states\" . None trainable bool Specify whether this parameter should be added to the default trainable \"parameters\" collection or to the default non-trainable \"states\" collection. If collection is passed this parameter will ignored. True regularizer Optional[Callable[[Any], jax._src.numpy.lax_numpy.ndarray]] Regularizer instance (callable). None constraint Optional[Callable[[Any], Any]] Constraint instance (callable). None Returns: Type Description Any The value of the parameter. Source code in elegy/nn/conv.py def add_parameter ( self , name : str , initializer : tp . Callable [[], tp . Any ], collection : tp . Optional [ str ] = None , trainable : bool = True , regularizer : tp . Optional [ tp . Callable [[ tp . Any ], jnp . ndarray ]] = None , constraint : tp . Optional [ tp . Callable [[ tp . Any ], tp . Any ]] = None , ) -> tp . Any : \"\"\" Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. initializer: A callable that takes not arguments returns the initial value. collection: Optional name of the parameter collection, if not defined it will be se to `\"parameters\"` if `trainable=True` else it will be set to `\"states\"`. trainable: Specify whether this parameter should be added to the default trainable `\"parameters\"` collection or to the default non-trainable `\"states\"` collection. If collection is passed this parameter will ignored. regularizer: Regularizer instance (callable). constraint: Constraint instance (callable). Returns: The value of the parameter. \"\"\" if collection is None : collection = \"parameters\" if trainable else \"states\" if self . _scope_params is None : self . _scope_params = {} if name not in self . _scope_params : if not self . is_initializing (): raise ValueError ( f \"Cannot add parameter { name } outside `init`\" ) if name in self . _default_params : parameter = self . _default_params [ name ] assert collection == parameter . collection initial_value = parameter . value else : initial_value = initializer () initial_value = jax . tree_map ( jnp . asarray , initial_value ) parameter = types . Parameter ( collection , initial_value ) self . _register_parameter ( name , parameter ) self . _scope_params [ name ] = parameter parameter = self . _scope_params [ name ] if parameter . collection != collection : raise ValueError ( f \"types.Parameter { name } previously found in collection { parameter . collection } \" f \"but currently being added for collection { collection } \" ) value = parameter . value if constraint is not None : value = constraint ( value ) if regularizer is not None : hooks . add_loss ( name = utils . get_name ( regularizer ), value = regularizer ( value ), ) return value","title":"add_parameter()"},{"location":"api/nn/Conv3D/#elegy.nn.conv.Conv3D.call","text":"Connects ConvND layer. Parameters: Name Type Description Default inputs ndarray A rank-N+2 array with shape [N, spatial_dims, C] . required Returns: Type Description ndarray A rank-N+2 array with shape [N, spatial_dims, output_channels] . Source code in elegy/nn/conv.py def call ( self , inputs : np . ndarray ) -> np . ndarray : \"\"\" Connects ``ConvND`` layer. Args: inputs: A rank-N+2 array with shape ``[N, spatial_dims, C]``. Returns: A rank-N+2 array with shape ``[N, spatial_dims, output_channels]``. \"\"\" required_rank = self . num_spatial_dims + 2 if inputs . ndim != required_rank : raise ValueError ( f \"Input to ConvND needs to have rank { required_rank } , \" f \"but input has shape { inputs . shape } .\" ) w_shape = self . kernel_shape + ( inputs . shape [ self . channel_index ] // self . groups , self . output_channels , ) if self . mask is not None and self . mask . shape != w_shape : raise ValueError ( \"Mask needs to have the same shape as weights. \" f \"Shapes are: { self . mask . shape } , { w_shape } \" ) w_init = self . w_init if w_init is None : fan_in_shape = np . prod ( w_shape [: - 1 ]) stddev = 1.0 / np . sqrt ( fan_in_shape ) w_init = initializers . TruncatedNormal ( stddev = stddev ) w = self . add_parameter ( \"w\" , lambda : w_init ( w_shape , jnp . float32 )) if self . mask is not None : w *= self . mask inputs = jnp . asarray ( inputs , dtype = self . dtype ) w = jnp . asarray ( w , dtype = self . dtype ) out = lax . conv_general_dilated ( inputs , w , window_strides = self . stride , padding = self . padding , lhs_dilation = self . lhs_dilation , rhs_dilation = self . kernel_dilation , dimension_numbers = self . dimension_numbers , feature_group_count = self . groups , ) if self . with_bias : if self . channel_index == - 1 : bias_shape = ( self . output_channels ,) else : bias_shape = ( self . output_channels ,) + ( 1 ,) * self . num_spatial_dims b = self . add_parameter ( \"b\" , lambda : self . b_init ( bias_shape , jnp . float32 )) b = jnp . broadcast_to ( b , out . shape ) b = jnp . asarray ( b , self . dtype ) out = out + b return out","title":"call()"},{"location":"api/nn/Conv3D/#elegy.nn.conv.Conv3D.init","text":"Initializes the module, Source code in elegy/nn/conv.py def init ( self , * , rng : tp . Optional [ types . RNGSeq ] = None , set_defaults : bool = False , ) -> tp . Callable [ ... , tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ] ]: \"\"\" Initializes the module, \"\"\" def init_callable ( * args , ** kwargs ) -> tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ]: with module_context ( module = self , collections = None , initializing = True , training = True , rng = rng ): y = self ( * args , ** kwargs ) parameters , collections = self . get_parameters_internal () self . _mark_initialized_recursive () if set_defaults : self . set_default_parameters ( parameters , collections ) return y , parameters , collections init_callable . _signature_f = self . call return init_callable","title":"init()"},{"location":"api/nn/ConvND/","text":"elegy.nn.ConvND General N-dimensional convolutional. __init__ ( self , num_spatial_dims , output_channels , kernel_shape , stride = 1 , rate = 1 , padding = 'SAME' , with_bias = True , w_init = None , b_init = None , data_format = 'channels_last' , mask = None , groups = 1 , ** kwargs ) special Initializes the module. Parameters: Name Type Description Default num_spatial_dims int The number of spatial dimensions of the input. required output_channels int Number of output channels. required kernel_shape Union[int, Sequence[int]] The shape of the kernel. Either an integer or a sequence of length num_spatial_dims . required stride Union[int, Sequence[int]] tp.Optional stride for the kernel. Either an integer or a sequence of length num_spatial_dims . Defaults to 1. 1 rate Union[int, Sequence[int]] tp.Optional kernel dilation rate. Either an integer or a sequence of length num_spatial_dims . 1 corresponds to standard ND convolution, rate > 1 corresponds to dilated convolution. Defaults to 1. 1 padding Union[str, Sequence[Tuple[int, int]], Callable[[int], Tuple[int, int]], Sequence[Callable[[int], Tuple[int, int]]]] tp.Optional padding algorithm. Either VALID or SAME or a sequence of n (low, high) integer pairs that give the padding to apply before and after each spatial dimension. or a callable or sequence of callables of size num_spatial_dims . Any callables must take a single integer argument equal to the effective kernel size and return a sequence of two integers representing the padding before and after. See haiku.pad.* for more details and example functions. Defaults to SAME . See: https://www.tensorflow.org/xla/operation_semantics#conv_convolution. 'SAME' with_bias bool Whether to add a bias. By default, true. True w_init Optional[elegy.types.Initializer] tp.Optional weight initialization. By default, truncated normal. None b_init Optional[elegy.types.Initializer] tp.Optional bias initialization. By default, zeros. None data_format str The data format of the input. Can be either channels_first , channels_last , N...C or NC... . By default, channels_last . 'channels_last' mask Optional[numpy.ndarray] tp.Optional mask of the weights. None groups int A positive integer specifying the number of groups in which the input is split along the channel axis. Each group is convolved separately with filters / groups filters. The output is the concatenation of all the groups results along the channel axis. Input channels and filters must both be divisible by groups. 1 kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/conv.py def __init__ ( self , num_spatial_dims : int , output_channels : int , kernel_shape : tp . Union [ int , tp . Sequence [ int ]], stride : tp . Union [ int , tp . Sequence [ int ]] = 1 , rate : tp . Union [ int , tp . Sequence [ int ]] = 1 , padding : tp . Union [ str , tp . Sequence [ tp . Tuple [ int , int ]], types . PadFnOrFns ] = \"SAME\" , with_bias : bool = True , w_init : tp . Optional [ types . Initializer ] = None , b_init : tp . Optional [ types . Initializer ] = None , data_format : str = \"channels_last\" , mask : tp . Optional [ np . ndarray ] = None , groups : int = 1 , ** kwargs , ): \"\"\" Initializes the module. Args: num_spatial_dims: The number of spatial dimensions of the input. output_channels: Number of output channels. kernel_shape: The shape of the kernel. Either an integer or a sequence of length ``num_spatial_dims``. stride: tp.Optional stride for the kernel. Either an integer or a sequence of length ``num_spatial_dims``. Defaults to 1. rate: tp.Optional kernel dilation rate. Either an integer or a sequence of length ``num_spatial_dims``. 1 corresponds to standard ND convolution, ``rate > 1`` corresponds to dilated convolution. Defaults to 1. padding: tp.Optional padding algorithm. Either ``VALID`` or ``SAME`` or a sequence of n ``(low, high)`` integer pairs that give the padding to apply before and after each spatial dimension. or a callable or sequence of callables of size ``num_spatial_dims``. Any callables must take a single integer argument equal to the effective kernel size and return a sequence of two integers representing the padding before and after. See ``haiku.pad.*`` for more details and example functions. Defaults to ``SAME``. See: https://www.tensorflow.org/xla/operation_semantics#conv_convolution. with_bias: Whether to add a bias. By default, true. w_init: tp.Optional weight initialization. By default, truncated normal. b_init: tp.Optional bias initialization. By default, zeros. data_format: The data format of the input. Can be either ``channels_first``, ``channels_last``, ``N...C`` or ``NC...``. By default, ``channels_last``. mask: tp.Optional mask of the weights. groups: A positive integer specifying the number of groups in which the input is split along the channel axis. Each group is convolved separately with filters / groups filters. The output is the concatenation of all the groups results along the channel axis. Input channels and filters must both be divisible by groups. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( ** kwargs ) if num_spatial_dims <= 0 : raise ValueError ( \"We only support convolution operations for `num_spatial_dims` \" f \"greater than 0, received num_spatial_dims= { num_spatial_dims } .\" ) self . num_spatial_dims = num_spatial_dims self . output_channels = output_channels self . kernel_shape = hk_utils . replicate ( kernel_shape , num_spatial_dims , \"kernel_shape\" ) self . with_bias = with_bias self . stride = hk_utils . replicate ( stride , num_spatial_dims , \"strides\" ) self . w_init = w_init self . b_init = b_init or jnp . zeros self . mask = mask self . lhs_dilation = hk_utils . replicate ( 1 , num_spatial_dims , \"lhs_dilation\" ) self . kernel_dilation = hk_utils . replicate ( rate , num_spatial_dims , \"kernel_dilation\" ) self . data_format = data_format self . channel_index = hk_utils . get_channel_index ( data_format ) self . dimension_numbers = to_dimension_numbers ( num_spatial_dims , channels_last = ( self . channel_index == - 1 ), transpose = False ) self . groups = groups if isinstance ( padding , str ): self . padding = padding . upper () else : self . padding = hk . pad . create ( padding = padding , kernel = self . kernel_shape , rate = self . kernel_dilation , n = self . num_spatial_dims , ) add_parameter ( self , name , initializer , collection = None , trainable = True , regularizer = None , constraint = None ) inherited Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required initializer Callable[[], Any] A callable that takes not arguments returns the initial value. required collection Optional[str] Optional name of the parameter collection, if not defined it will be se to \"parameters\" if trainable=True else it will be set to \"states\" . None trainable bool Specify whether this parameter should be added to the default trainable \"parameters\" collection or to the default non-trainable \"states\" collection. If collection is passed this parameter will ignored. True regularizer Optional[Callable[[Any], jax._src.numpy.lax_numpy.ndarray]] Regularizer instance (callable). None constraint Optional[Callable[[Any], Any]] Constraint instance (callable). None Returns: Type Description Any The value of the parameter. Source code in elegy/nn/conv.py def add_parameter ( self , name : str , initializer : tp . Callable [[], tp . Any ], collection : tp . Optional [ str ] = None , trainable : bool = True , regularizer : tp . Optional [ tp . Callable [[ tp . Any ], jnp . ndarray ]] = None , constraint : tp . Optional [ tp . Callable [[ tp . Any ], tp . Any ]] = None , ) -> tp . Any : \"\"\" Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. initializer: A callable that takes not arguments returns the initial value. collection: Optional name of the parameter collection, if not defined it will be se to `\"parameters\"` if `trainable=True` else it will be set to `\"states\"`. trainable: Specify whether this parameter should be added to the default trainable `\"parameters\"` collection or to the default non-trainable `\"states\"` collection. If collection is passed this parameter will ignored. regularizer: Regularizer instance (callable). constraint: Constraint instance (callable). Returns: The value of the parameter. \"\"\" if collection is None : collection = \"parameters\" if trainable else \"states\" if self . _scope_params is None : self . _scope_params = {} if name not in self . _scope_params : if not self . is_initializing (): raise ValueError ( f \"Cannot add parameter { name } outside `init`\" ) if name in self . _default_params : parameter = self . _default_params [ name ] assert collection == parameter . collection initial_value = parameter . value else : initial_value = initializer () initial_value = jax . tree_map ( jnp . asarray , initial_value ) parameter = types . Parameter ( collection , initial_value ) self . _register_parameter ( name , parameter ) self . _scope_params [ name ] = parameter parameter = self . _scope_params [ name ] if parameter . collection != collection : raise ValueError ( f \"types.Parameter { name } previously found in collection { parameter . collection } \" f \"but currently being added for collection { collection } \" ) value = parameter . value if constraint is not None : value = constraint ( value ) if regularizer is not None : hooks . add_loss ( name = utils . get_name ( regularizer ), value = regularizer ( value ), ) return value call ( self , inputs ) Connects ConvND layer. Parameters: Name Type Description Default inputs ndarray A rank-N+2 array with shape [N, spatial_dims, C] . required Returns: Type Description ndarray A rank-N+2 array with shape [N, spatial_dims, output_channels] . Source code in elegy/nn/conv.py def call ( self , inputs : np . ndarray ) -> np . ndarray : \"\"\" Connects ``ConvND`` layer. Args: inputs: A rank-N+2 array with shape ``[N, spatial_dims, C]``. Returns: A rank-N+2 array with shape ``[N, spatial_dims, output_channels]``. \"\"\" required_rank = self . num_spatial_dims + 2 if inputs . ndim != required_rank : raise ValueError ( f \"Input to ConvND needs to have rank { required_rank } , \" f \"but input has shape { inputs . shape } .\" ) w_shape = self . kernel_shape + ( inputs . shape [ self . channel_index ] // self . groups , self . output_channels , ) if self . mask is not None and self . mask . shape != w_shape : raise ValueError ( \"Mask needs to have the same shape as weights. \" f \"Shapes are: { self . mask . shape } , { w_shape } \" ) w_init = self . w_init if w_init is None : fan_in_shape = np . prod ( w_shape [: - 1 ]) stddev = 1.0 / np . sqrt ( fan_in_shape ) w_init = initializers . TruncatedNormal ( stddev = stddev ) w = self . add_parameter ( \"w\" , lambda : w_init ( w_shape , jnp . float32 )) if self . mask is not None : w *= self . mask inputs = jnp . asarray ( inputs , dtype = self . dtype ) w = jnp . asarray ( w , dtype = self . dtype ) out = lax . conv_general_dilated ( inputs , w , window_strides = self . stride , padding = self . padding , lhs_dilation = self . lhs_dilation , rhs_dilation = self . kernel_dilation , dimension_numbers = self . dimension_numbers , feature_group_count = self . groups , ) if self . with_bias : if self . channel_index == - 1 : bias_shape = ( self . output_channels ,) else : bias_shape = ( self . output_channels ,) + ( 1 ,) * self . num_spatial_dims b = self . add_parameter ( \"b\" , lambda : self . b_init ( bias_shape , jnp . float32 )) b = jnp . broadcast_to ( b , out . shape ) b = jnp . asarray ( b , self . dtype ) out = out + b return out init ( self , * , rng = None , set_defaults = False ) inherited Initializes the module, Source code in elegy/nn/conv.py def init ( self , * , rng : tp . Optional [ types . RNGSeq ] = None , set_defaults : bool = False , ) -> tp . Callable [ ... , tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ] ]: \"\"\" Initializes the module, \"\"\" def init_callable ( * args , ** kwargs ) -> tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ]: with module_context ( module = self , collections = None , initializing = True , training = True , rng = rng ): y = self ( * args , ** kwargs ) parameters , collections = self . get_parameters_internal () self . _mark_initialized_recursive () if set_defaults : self . set_default_parameters ( parameters , collections ) return y , parameters , collections init_callable . _signature_f = self . call return init_callable","title":"ConvND"},{"location":"api/nn/ConvND/#elegynnconvnd","text":"","title":"elegy.nn.ConvND"},{"location":"api/nn/ConvND/#elegy.nn.conv.ConvND","text":"General N-dimensional convolutional.","title":"elegy.nn.conv.ConvND"},{"location":"api/nn/ConvND/#elegy.nn.conv.ConvND.__init__","text":"Initializes the module. Parameters: Name Type Description Default num_spatial_dims int The number of spatial dimensions of the input. required output_channels int Number of output channels. required kernel_shape Union[int, Sequence[int]] The shape of the kernel. Either an integer or a sequence of length num_spatial_dims . required stride Union[int, Sequence[int]] tp.Optional stride for the kernel. Either an integer or a sequence of length num_spatial_dims . Defaults to 1. 1 rate Union[int, Sequence[int]] tp.Optional kernel dilation rate. Either an integer or a sequence of length num_spatial_dims . 1 corresponds to standard ND convolution, rate > 1 corresponds to dilated convolution. Defaults to 1. 1 padding Union[str, Sequence[Tuple[int, int]], Callable[[int], Tuple[int, int]], Sequence[Callable[[int], Tuple[int, int]]]] tp.Optional padding algorithm. Either VALID or SAME or a sequence of n (low, high) integer pairs that give the padding to apply before and after each spatial dimension. or a callable or sequence of callables of size num_spatial_dims . Any callables must take a single integer argument equal to the effective kernel size and return a sequence of two integers representing the padding before and after. See haiku.pad.* for more details and example functions. Defaults to SAME . See: https://www.tensorflow.org/xla/operation_semantics#conv_convolution. 'SAME' with_bias bool Whether to add a bias. By default, true. True w_init Optional[elegy.types.Initializer] tp.Optional weight initialization. By default, truncated normal. None b_init Optional[elegy.types.Initializer] tp.Optional bias initialization. By default, zeros. None data_format str The data format of the input. Can be either channels_first , channels_last , N...C or NC... . By default, channels_last . 'channels_last' mask Optional[numpy.ndarray] tp.Optional mask of the weights. None groups int A positive integer specifying the number of groups in which the input is split along the channel axis. Each group is convolved separately with filters / groups filters. The output is the concatenation of all the groups results along the channel axis. Input channels and filters must both be divisible by groups. 1 kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/conv.py def __init__ ( self , num_spatial_dims : int , output_channels : int , kernel_shape : tp . Union [ int , tp . Sequence [ int ]], stride : tp . Union [ int , tp . Sequence [ int ]] = 1 , rate : tp . Union [ int , tp . Sequence [ int ]] = 1 , padding : tp . Union [ str , tp . Sequence [ tp . Tuple [ int , int ]], types . PadFnOrFns ] = \"SAME\" , with_bias : bool = True , w_init : tp . Optional [ types . Initializer ] = None , b_init : tp . Optional [ types . Initializer ] = None , data_format : str = \"channels_last\" , mask : tp . Optional [ np . ndarray ] = None , groups : int = 1 , ** kwargs , ): \"\"\" Initializes the module. Args: num_spatial_dims: The number of spatial dimensions of the input. output_channels: Number of output channels. kernel_shape: The shape of the kernel. Either an integer or a sequence of length ``num_spatial_dims``. stride: tp.Optional stride for the kernel. Either an integer or a sequence of length ``num_spatial_dims``. Defaults to 1. rate: tp.Optional kernel dilation rate. Either an integer or a sequence of length ``num_spatial_dims``. 1 corresponds to standard ND convolution, ``rate > 1`` corresponds to dilated convolution. Defaults to 1. padding: tp.Optional padding algorithm. Either ``VALID`` or ``SAME`` or a sequence of n ``(low, high)`` integer pairs that give the padding to apply before and after each spatial dimension. or a callable or sequence of callables of size ``num_spatial_dims``. Any callables must take a single integer argument equal to the effective kernel size and return a sequence of two integers representing the padding before and after. See ``haiku.pad.*`` for more details and example functions. Defaults to ``SAME``. See: https://www.tensorflow.org/xla/operation_semantics#conv_convolution. with_bias: Whether to add a bias. By default, true. w_init: tp.Optional weight initialization. By default, truncated normal. b_init: tp.Optional bias initialization. By default, zeros. data_format: The data format of the input. Can be either ``channels_first``, ``channels_last``, ``N...C`` or ``NC...``. By default, ``channels_last``. mask: tp.Optional mask of the weights. groups: A positive integer specifying the number of groups in which the input is split along the channel axis. Each group is convolved separately with filters / groups filters. The output is the concatenation of all the groups results along the channel axis. Input channels and filters must both be divisible by groups. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( ** kwargs ) if num_spatial_dims <= 0 : raise ValueError ( \"We only support convolution operations for `num_spatial_dims` \" f \"greater than 0, received num_spatial_dims= { num_spatial_dims } .\" ) self . num_spatial_dims = num_spatial_dims self . output_channels = output_channels self . kernel_shape = hk_utils . replicate ( kernel_shape , num_spatial_dims , \"kernel_shape\" ) self . with_bias = with_bias self . stride = hk_utils . replicate ( stride , num_spatial_dims , \"strides\" ) self . w_init = w_init self . b_init = b_init or jnp . zeros self . mask = mask self . lhs_dilation = hk_utils . replicate ( 1 , num_spatial_dims , \"lhs_dilation\" ) self . kernel_dilation = hk_utils . replicate ( rate , num_spatial_dims , \"kernel_dilation\" ) self . data_format = data_format self . channel_index = hk_utils . get_channel_index ( data_format ) self . dimension_numbers = to_dimension_numbers ( num_spatial_dims , channels_last = ( self . channel_index == - 1 ), transpose = False ) self . groups = groups if isinstance ( padding , str ): self . padding = padding . upper () else : self . padding = hk . pad . create ( padding = padding , kernel = self . kernel_shape , rate = self . kernel_dilation , n = self . num_spatial_dims , )","title":"__init__()"},{"location":"api/nn/ConvND/#elegy.nn.conv.ConvND.add_parameter","text":"Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required initializer Callable[[], Any] A callable that takes not arguments returns the initial value. required collection Optional[str] Optional name of the parameter collection, if not defined it will be se to \"parameters\" if trainable=True else it will be set to \"states\" . None trainable bool Specify whether this parameter should be added to the default trainable \"parameters\" collection or to the default non-trainable \"states\" collection. If collection is passed this parameter will ignored. True regularizer Optional[Callable[[Any], jax._src.numpy.lax_numpy.ndarray]] Regularizer instance (callable). None constraint Optional[Callable[[Any], Any]] Constraint instance (callable). None Returns: Type Description Any The value of the parameter. Source code in elegy/nn/conv.py def add_parameter ( self , name : str , initializer : tp . Callable [[], tp . Any ], collection : tp . Optional [ str ] = None , trainable : bool = True , regularizer : tp . Optional [ tp . Callable [[ tp . Any ], jnp . ndarray ]] = None , constraint : tp . Optional [ tp . Callable [[ tp . Any ], tp . Any ]] = None , ) -> tp . Any : \"\"\" Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. initializer: A callable that takes not arguments returns the initial value. collection: Optional name of the parameter collection, if not defined it will be se to `\"parameters\"` if `trainable=True` else it will be set to `\"states\"`. trainable: Specify whether this parameter should be added to the default trainable `\"parameters\"` collection or to the default non-trainable `\"states\"` collection. If collection is passed this parameter will ignored. regularizer: Regularizer instance (callable). constraint: Constraint instance (callable). Returns: The value of the parameter. \"\"\" if collection is None : collection = \"parameters\" if trainable else \"states\" if self . _scope_params is None : self . _scope_params = {} if name not in self . _scope_params : if not self . is_initializing (): raise ValueError ( f \"Cannot add parameter { name } outside `init`\" ) if name in self . _default_params : parameter = self . _default_params [ name ] assert collection == parameter . collection initial_value = parameter . value else : initial_value = initializer () initial_value = jax . tree_map ( jnp . asarray , initial_value ) parameter = types . Parameter ( collection , initial_value ) self . _register_parameter ( name , parameter ) self . _scope_params [ name ] = parameter parameter = self . _scope_params [ name ] if parameter . collection != collection : raise ValueError ( f \"types.Parameter { name } previously found in collection { parameter . collection } \" f \"but currently being added for collection { collection } \" ) value = parameter . value if constraint is not None : value = constraint ( value ) if regularizer is not None : hooks . add_loss ( name = utils . get_name ( regularizer ), value = regularizer ( value ), ) return value","title":"add_parameter()"},{"location":"api/nn/ConvND/#elegy.nn.conv.ConvND.call","text":"Connects ConvND layer. Parameters: Name Type Description Default inputs ndarray A rank-N+2 array with shape [N, spatial_dims, C] . required Returns: Type Description ndarray A rank-N+2 array with shape [N, spatial_dims, output_channels] . Source code in elegy/nn/conv.py def call ( self , inputs : np . ndarray ) -> np . ndarray : \"\"\" Connects ``ConvND`` layer. Args: inputs: A rank-N+2 array with shape ``[N, spatial_dims, C]``. Returns: A rank-N+2 array with shape ``[N, spatial_dims, output_channels]``. \"\"\" required_rank = self . num_spatial_dims + 2 if inputs . ndim != required_rank : raise ValueError ( f \"Input to ConvND needs to have rank { required_rank } , \" f \"but input has shape { inputs . shape } .\" ) w_shape = self . kernel_shape + ( inputs . shape [ self . channel_index ] // self . groups , self . output_channels , ) if self . mask is not None and self . mask . shape != w_shape : raise ValueError ( \"Mask needs to have the same shape as weights. \" f \"Shapes are: { self . mask . shape } , { w_shape } \" ) w_init = self . w_init if w_init is None : fan_in_shape = np . prod ( w_shape [: - 1 ]) stddev = 1.0 / np . sqrt ( fan_in_shape ) w_init = initializers . TruncatedNormal ( stddev = stddev ) w = self . add_parameter ( \"w\" , lambda : w_init ( w_shape , jnp . float32 )) if self . mask is not None : w *= self . mask inputs = jnp . asarray ( inputs , dtype = self . dtype ) w = jnp . asarray ( w , dtype = self . dtype ) out = lax . conv_general_dilated ( inputs , w , window_strides = self . stride , padding = self . padding , lhs_dilation = self . lhs_dilation , rhs_dilation = self . kernel_dilation , dimension_numbers = self . dimension_numbers , feature_group_count = self . groups , ) if self . with_bias : if self . channel_index == - 1 : bias_shape = ( self . output_channels ,) else : bias_shape = ( self . output_channels ,) + ( 1 ,) * self . num_spatial_dims b = self . add_parameter ( \"b\" , lambda : self . b_init ( bias_shape , jnp . float32 )) b = jnp . broadcast_to ( b , out . shape ) b = jnp . asarray ( b , self . dtype ) out = out + b return out","title":"call()"},{"location":"api/nn/ConvND/#elegy.nn.conv.ConvND.init","text":"Initializes the module, Source code in elegy/nn/conv.py def init ( self , * , rng : tp . Optional [ types . RNGSeq ] = None , set_defaults : bool = False , ) -> tp . Callable [ ... , tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ] ]: \"\"\" Initializes the module, \"\"\" def init_callable ( * args , ** kwargs ) -> tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ]: with module_context ( module = self , collections = None , initializing = True , training = True , rng = rng ): y = self ( * args , ** kwargs ) parameters , collections = self . get_parameters_internal () self . _mark_initialized_recursive () if set_defaults : self . set_default_parameters ( parameters , collections ) return y , parameters , collections init_callable . _signature_f = self . call return init_callable","title":"init()"},{"location":"api/nn/Dropout/","text":"elegy.nn.Dropout Applies Dropout to the input. The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting. Inputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over all inputs is unchanged. Note that the Dropout layer only applies when training is set to True such that no values are dropped during inference. When using model.fit , training will be appropriately set to True automatically, and in other contexts, you can set the kwarg explicitly to True when calling the layer. Example dropout = elegy . nn . Dropout( 0.2 ) data = np . arange( 10 ) . reshape( 5 , 2 ) . astype(np . float32) print (data) # [[0. 1.] # [2. 3.] # [4. 5.] # [6. 7.] # [8. 9.]] outputs = dropout(data, training = True ) print (outputs) # [[ 0. 1.25] # [ 2.5 3.75] # [ 5. 6.25] # [ 7.5 0. ] # [10. 0. ]] add_parameter ( self , name , initializer , collection = None , trainable = True , regularizer = None , constraint = None ) inherited Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required initializer Callable[[], Any] A callable that takes not arguments returns the initial value. required collection Optional[str] Optional name of the parameter collection, if not defined it will be se to \"parameters\" if trainable=True else it will be set to \"states\" . None trainable bool Specify whether this parameter should be added to the default trainable \"parameters\" collection or to the default non-trainable \"states\" collection. If collection is passed this parameter will ignored. True regularizer Optional[Callable[[Any], jax._src.numpy.lax_numpy.ndarray]] Regularizer instance (callable). None constraint Optional[Callable[[Any], Any]] Constraint instance (callable). None Returns: Type Description Any The value of the parameter. Source code in elegy/nn/dropout.py def add_parameter ( self , name : str , initializer : tp . Callable [[], tp . Any ], collection : tp . Optional [ str ] = None , trainable : bool = True , regularizer : tp . Optional [ tp . Callable [[ tp . Any ], jnp . ndarray ]] = None , constraint : tp . Optional [ tp . Callable [[ tp . Any ], tp . Any ]] = None , ) -> tp . Any : \"\"\" Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. initializer: A callable that takes not arguments returns the initial value. collection: Optional name of the parameter collection, if not defined it will be se to `\"parameters\"` if `trainable=True` else it will be set to `\"states\"`. trainable: Specify whether this parameter should be added to the default trainable `\"parameters\"` collection or to the default non-trainable `\"states\"` collection. If collection is passed this parameter will ignored. regularizer: Regularizer instance (callable). constraint: Constraint instance (callable). Returns: The value of the parameter. \"\"\" if collection is None : collection = \"parameters\" if trainable else \"states\" if self . _scope_params is None : self . _scope_params = {} if name not in self . _scope_params : if not self . is_initializing (): raise ValueError ( f \"Cannot add parameter { name } outside `init`\" ) if name in self . _default_params : parameter = self . _default_params [ name ] assert collection == parameter . collection initial_value = parameter . value else : initial_value = initializer () initial_value = jax . tree_map ( jnp . asarray , initial_value ) parameter = types . Parameter ( collection , initial_value ) self . _register_parameter ( name , parameter ) self . _scope_params [ name ] = parameter parameter = self . _scope_params [ name ] if parameter . collection != collection : raise ValueError ( f \"types.Parameter { name } previously found in collection { parameter . collection } \" f \"but currently being added for collection { collection } \" ) value = parameter . value if constraint is not None : value = constraint ( value ) if regularizer is not None : hooks . add_loss ( name = utils . get_name ( regularizer ), value = regularizer ( value ), ) return value call ( self , x , training = None , rng = None ) Parameters: Name Type Description Default x ndarray The value to be dropped out. required training Optional[bool] Whether training is currently happening. None rng Optional[jax._src.numpy.lax_numpy.ndarray] Optional RNGKey. None Returns: Type Description ndarray x but dropped out and scaled by 1 / (1 - rate) . Source code in elegy/nn/dropout.py def call ( self , x : jnp . ndarray , training : tp . Optional [ bool ] = None , rng : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Arguments: x: The value to be dropped out. training: Whether training is currently happening. rng: Optional RNGKey. Returns: x but dropped out and scaled by `1 / (1 - rate)`. \"\"\" if training is None : training = self . is_training () return hk . dropout ( rng = rng if rng is not None else self . next_key (), rate = self . rate if training else 0.0 , x = x , ) init ( self , * , rng = None , set_defaults = False ) inherited Initializes the module, Source code in elegy/nn/dropout.py def init ( self , * , rng : tp . Optional [ types . RNGSeq ] = None , set_defaults : bool = False , ) -> tp . Callable [ ... , tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ] ]: \"\"\" Initializes the module, \"\"\" def init_callable ( * args , ** kwargs ) -> tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ]: with module_context ( module = self , collections = None , initializing = True , training = True , rng = rng ): y = self ( * args , ** kwargs ) parameters , collections = self . get_parameters_internal () self . _mark_initialized_recursive () if set_defaults : self . set_default_parameters ( parameters , collections ) return y , parameters , collections init_callable . _signature_f = self . call return init_callable","title":"Dropout"},{"location":"api/nn/Dropout/#elegynndropout","text":"","title":"elegy.nn.Dropout"},{"location":"api/nn/Dropout/#elegy.nn.dropout.Dropout","text":"Applies Dropout to the input. The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting. Inputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over all inputs is unchanged. Note that the Dropout layer only applies when training is set to True such that no values are dropped during inference. When using model.fit , training will be appropriately set to True automatically, and in other contexts, you can set the kwarg explicitly to True when calling the layer.","title":"elegy.nn.dropout.Dropout"},{"location":"api/nn/Dropout/#elegy.nn.dropout.Dropout--example","text":"dropout = elegy . nn . Dropout( 0.2 ) data = np . arange( 10 ) . reshape( 5 , 2 ) . astype(np . float32) print (data) # [[0. 1.] # [2. 3.] # [4. 5.] # [6. 7.] # [8. 9.]] outputs = dropout(data, training = True ) print (outputs) # [[ 0. 1.25] # [ 2.5 3.75] # [ 5. 6.25] # [ 7.5 0. ] # [10. 0. ]]","title":"Example"},{"location":"api/nn/Dropout/#elegy.nn.dropout.Dropout.add_parameter","text":"Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required initializer Callable[[], Any] A callable that takes not arguments returns the initial value. required collection Optional[str] Optional name of the parameter collection, if not defined it will be se to \"parameters\" if trainable=True else it will be set to \"states\" . None trainable bool Specify whether this parameter should be added to the default trainable \"parameters\" collection or to the default non-trainable \"states\" collection. If collection is passed this parameter will ignored. True regularizer Optional[Callable[[Any], jax._src.numpy.lax_numpy.ndarray]] Regularizer instance (callable). None constraint Optional[Callable[[Any], Any]] Constraint instance (callable). None Returns: Type Description Any The value of the parameter. Source code in elegy/nn/dropout.py def add_parameter ( self , name : str , initializer : tp . Callable [[], tp . Any ], collection : tp . Optional [ str ] = None , trainable : bool = True , regularizer : tp . Optional [ tp . Callable [[ tp . Any ], jnp . ndarray ]] = None , constraint : tp . Optional [ tp . Callable [[ tp . Any ], tp . Any ]] = None , ) -> tp . Any : \"\"\" Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. initializer: A callable that takes not arguments returns the initial value. collection: Optional name of the parameter collection, if not defined it will be se to `\"parameters\"` if `trainable=True` else it will be set to `\"states\"`. trainable: Specify whether this parameter should be added to the default trainable `\"parameters\"` collection or to the default non-trainable `\"states\"` collection. If collection is passed this parameter will ignored. regularizer: Regularizer instance (callable). constraint: Constraint instance (callable). Returns: The value of the parameter. \"\"\" if collection is None : collection = \"parameters\" if trainable else \"states\" if self . _scope_params is None : self . _scope_params = {} if name not in self . _scope_params : if not self . is_initializing (): raise ValueError ( f \"Cannot add parameter { name } outside `init`\" ) if name in self . _default_params : parameter = self . _default_params [ name ] assert collection == parameter . collection initial_value = parameter . value else : initial_value = initializer () initial_value = jax . tree_map ( jnp . asarray , initial_value ) parameter = types . Parameter ( collection , initial_value ) self . _register_parameter ( name , parameter ) self . _scope_params [ name ] = parameter parameter = self . _scope_params [ name ] if parameter . collection != collection : raise ValueError ( f \"types.Parameter { name } previously found in collection { parameter . collection } \" f \"but currently being added for collection { collection } \" ) value = parameter . value if constraint is not None : value = constraint ( value ) if regularizer is not None : hooks . add_loss ( name = utils . get_name ( regularizer ), value = regularizer ( value ), ) return value","title":"add_parameter()"},{"location":"api/nn/Dropout/#elegy.nn.dropout.Dropout.call","text":"Parameters: Name Type Description Default x ndarray The value to be dropped out. required training Optional[bool] Whether training is currently happening. None rng Optional[jax._src.numpy.lax_numpy.ndarray] Optional RNGKey. None Returns: Type Description ndarray x but dropped out and scaled by 1 / (1 - rate) . Source code in elegy/nn/dropout.py def call ( self , x : jnp . ndarray , training : tp . Optional [ bool ] = None , rng : tp . Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\" Arguments: x: The value to be dropped out. training: Whether training is currently happening. rng: Optional RNGKey. Returns: x but dropped out and scaled by `1 / (1 - rate)`. \"\"\" if training is None : training = self . is_training () return hk . dropout ( rng = rng if rng is not None else self . next_key (), rate = self . rate if training else 0.0 , x = x , )","title":"call()"},{"location":"api/nn/Dropout/#elegy.nn.dropout.Dropout.init","text":"Initializes the module, Source code in elegy/nn/dropout.py def init ( self , * , rng : tp . Optional [ types . RNGSeq ] = None , set_defaults : bool = False , ) -> tp . Callable [ ... , tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ] ]: \"\"\" Initializes the module, \"\"\" def init_callable ( * args , ** kwargs ) -> tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ]: with module_context ( module = self , collections = None , initializing = True , training = True , rng = rng ): y = self ( * args , ** kwargs ) parameters , collections = self . get_parameters_internal () self . _mark_initialized_recursive () if set_defaults : self . set_default_parameters ( parameters , collections ) return y , parameters , collections init_callable . _signature_f = self . call return init_callable","title":"init()"},{"location":"api/nn/EMAParamsTree/","text":"elegy.nn.EMAParamsTree Maintains an exponential moving average for all parameters in a tree. While ExponentialMovingAverage is meant to be applied to single parameters within a function, this class is meant to be applied to the entire tree of parameters for a function. Given a set of parameters for some network: >>> import elegy >>> x = jnp . ones([ 1 , 1 ]) >>> linear = elegy . nn . Linear( 10 ) >>> y, params, collections = linear . init(rng = elegy . RNGSeq( 42 ))(x) You might use the EMAParamsTree like follows: >>> ema = elegy . nn . EMAParamsTree( 0.2 ) >>> , , ema_state = ema . init()( dict (params = params, collections = collections)) >>> ema_values, _, ema_state = ema . apply( None , ema_state)( dict (params = params, collections = collections)) Here, we are transforming a Haiku function and constructing its parameters via an init_fn as normal, but are creating a second transformed function which expects a tree of parameters as input. This function is then called with the current parameters as input, which then returns an identical tree with every parameter replaced with its exponentially decayed average. This ema_params object can then be passed into the network_fn as usual, and will cause it to run with EMA weights. __init__ ( self , decay , zero_debias = True , warmup_length = 0 , ignore_regex = '' , ** kwargs ) special Initializes an EMAParamsTree module. Parameters: Name Type Description Default decay The chosen decay. Must in [0, 1). Values close to 1 result in slow decay; values close to 0 result in fast decay. required zero_debias Whether to run with zero-debiasing. True warmup_length A positive integer, EMA has no effect until the internal counter has reached warmup_length at which point the initial value for the decaying average is initialized to the input value after warmup_length iterations. 0 ignore_regex A string. Any parameter in the tree whose name matches this regex will not have any moving average applied to it. The empty string means this module will EMA all parameters. '' kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/moving_averages.py def __init__ ( self , decay , zero_debias = True , warmup_length = 0 , ignore_regex = \"\" , ** kwargs ): \"\"\"Initializes an EMAParamsTree module. Args: decay: The chosen decay. Must in [0, 1). Values close to 1 result in slow decay; values close to 0 result in fast decay. zero_debias: Whether to run with zero-debiasing. warmup_length: A positive integer, EMA has no effect until the internal counter has reached `warmup_length` at which point the initial value for the decaying average is initialized to the input value after `warmup_length` iterations. ignore_regex: A string. Any parameter in the tree whose name matches this regex will not have any moving average applied to it. The empty string means this module will EMA all parameters. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( ** kwargs ) self . _decay = decay self . _zero_debias = zero_debias self . _warmup_length = warmup_length self . _ignore_regex = ignore_regex add_parameter ( self , name , initializer , collection = None , trainable = True , regularizer = None , constraint = None ) inherited Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required initializer Callable[[], Any] A callable that takes not arguments returns the initial value. required collection Optional[str] Optional name of the parameter collection, if not defined it will be se to \"parameters\" if trainable=True else it will be set to \"states\" . None trainable bool Specify whether this parameter should be added to the default trainable \"parameters\" collection or to the default non-trainable \"states\" collection. If collection is passed this parameter will ignored. True regularizer Optional[Callable[[Any], jax._src.numpy.lax_numpy.ndarray]] Regularizer instance (callable). None constraint Optional[Callable[[Any], Any]] Constraint instance (callable). None Returns: Type Description Any The value of the parameter. Source code in elegy/nn/moving_averages.py def add_parameter ( self , name : str , initializer : tp . Callable [[], tp . Any ], collection : tp . Optional [ str ] = None , trainable : bool = True , regularizer : tp . Optional [ tp . Callable [[ tp . Any ], jnp . ndarray ]] = None , constraint : tp . Optional [ tp . Callable [[ tp . Any ], tp . Any ]] = None , ) -> tp . Any : \"\"\" Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. initializer: A callable that takes not arguments returns the initial value. collection: Optional name of the parameter collection, if not defined it will be se to `\"parameters\"` if `trainable=True` else it will be set to `\"states\"`. trainable: Specify whether this parameter should be added to the default trainable `\"parameters\"` collection or to the default non-trainable `\"states\"` collection. If collection is passed this parameter will ignored. regularizer: Regularizer instance (callable). constraint: Constraint instance (callable). Returns: The value of the parameter. \"\"\" if collection is None : collection = \"parameters\" if trainable else \"states\" if self . _scope_params is None : self . _scope_params = {} if name not in self . _scope_params : if not self . is_initializing (): raise ValueError ( f \"Cannot add parameter { name } outside `init`\" ) if name in self . _default_params : parameter = self . _default_params [ name ] assert collection == parameter . collection initial_value = parameter . value else : initial_value = initializer () initial_value = jax . tree_map ( jnp . asarray , initial_value ) parameter = types . Parameter ( collection , initial_value ) self . _register_parameter ( name , parameter ) self . _scope_params [ name ] = parameter parameter = self . _scope_params [ name ] if parameter . collection != collection : raise ValueError ( f \"types.Parameter { name } previously found in collection { parameter . collection } \" f \"but currently being added for collection { collection } \" ) value = parameter . value if constraint is not None : value = constraint ( value ) if regularizer is not None : hooks . add_loss ( name = utils . get_name ( regularizer ), value = regularizer ( value ), ) return value init ( self , * , rng = None , set_defaults = False ) inherited Initializes the module, Source code in elegy/nn/moving_averages.py def init ( self , * , rng : tp . Optional [ types . RNGSeq ] = None , set_defaults : bool = False , ) -> tp . Callable [ ... , tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ] ]: \"\"\" Initializes the module, \"\"\" def init_callable ( * args , ** kwargs ) -> tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ]: with module_context ( module = self , collections = None , initializing = True , training = True , rng = rng ): y = self ( * args , ** kwargs ) parameters , collections = self . get_parameters_internal () self . _mark_initialized_recursive () if set_defaults : self . set_default_parameters ( parameters , collections ) return y , parameters , collections init_callable . _signature_f = self . call return init_callable","title":"EMAParamsTree"},{"location":"api/nn/EMAParamsTree/#elegynnemaparamstree","text":"","title":"elegy.nn.EMAParamsTree"},{"location":"api/nn/EMAParamsTree/#elegy.nn.moving_averages.EMAParamsTree","text":"Maintains an exponential moving average for all parameters in a tree. While ExponentialMovingAverage is meant to be applied to single parameters within a function, this class is meant to be applied to the entire tree of parameters for a function. Given a set of parameters for some network: >>> import elegy >>> x = jnp . ones([ 1 , 1 ]) >>> linear = elegy . nn . Linear( 10 ) >>> y, params, collections = linear . init(rng = elegy . RNGSeq( 42 ))(x) You might use the EMAParamsTree like follows: >>> ema = elegy . nn . EMAParamsTree( 0.2 ) >>> , , ema_state = ema . init()( dict (params = params, collections = collections)) >>> ema_values, _, ema_state = ema . apply( None , ema_state)( dict (params = params, collections = collections)) Here, we are transforming a Haiku function and constructing its parameters via an init_fn as normal, but are creating a second transformed function which expects a tree of parameters as input. This function is then called with the current parameters as input, which then returns an identical tree with every parameter replaced with its exponentially decayed average. This ema_params object can then be passed into the network_fn as usual, and will cause it to run with EMA weights.","title":"elegy.nn.moving_averages.EMAParamsTree"},{"location":"api/nn/EMAParamsTree/#elegy.nn.moving_averages.EMAParamsTree.__init__","text":"Initializes an EMAParamsTree module. Parameters: Name Type Description Default decay The chosen decay. Must in [0, 1). Values close to 1 result in slow decay; values close to 0 result in fast decay. required zero_debias Whether to run with zero-debiasing. True warmup_length A positive integer, EMA has no effect until the internal counter has reached warmup_length at which point the initial value for the decaying average is initialized to the input value after warmup_length iterations. 0 ignore_regex A string. Any parameter in the tree whose name matches this regex will not have any moving average applied to it. The empty string means this module will EMA all parameters. '' kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/moving_averages.py def __init__ ( self , decay , zero_debias = True , warmup_length = 0 , ignore_regex = \"\" , ** kwargs ): \"\"\"Initializes an EMAParamsTree module. Args: decay: The chosen decay. Must in [0, 1). Values close to 1 result in slow decay; values close to 0 result in fast decay. zero_debias: Whether to run with zero-debiasing. warmup_length: A positive integer, EMA has no effect until the internal counter has reached `warmup_length` at which point the initial value for the decaying average is initialized to the input value after `warmup_length` iterations. ignore_regex: A string. Any parameter in the tree whose name matches this regex will not have any moving average applied to it. The empty string means this module will EMA all parameters. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( ** kwargs ) self . _decay = decay self . _zero_debias = zero_debias self . _warmup_length = warmup_length self . _ignore_regex = ignore_regex","title":"__init__()"},{"location":"api/nn/EMAParamsTree/#elegy.nn.moving_averages.EMAParamsTree.add_parameter","text":"Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required initializer Callable[[], Any] A callable that takes not arguments returns the initial value. required collection Optional[str] Optional name of the parameter collection, if not defined it will be se to \"parameters\" if trainable=True else it will be set to \"states\" . None trainable bool Specify whether this parameter should be added to the default trainable \"parameters\" collection or to the default non-trainable \"states\" collection. If collection is passed this parameter will ignored. True regularizer Optional[Callable[[Any], jax._src.numpy.lax_numpy.ndarray]] Regularizer instance (callable). None constraint Optional[Callable[[Any], Any]] Constraint instance (callable). None Returns: Type Description Any The value of the parameter. Source code in elegy/nn/moving_averages.py def add_parameter ( self , name : str , initializer : tp . Callable [[], tp . Any ], collection : tp . Optional [ str ] = None , trainable : bool = True , regularizer : tp . Optional [ tp . Callable [[ tp . Any ], jnp . ndarray ]] = None , constraint : tp . Optional [ tp . Callable [[ tp . Any ], tp . Any ]] = None , ) -> tp . Any : \"\"\" Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. initializer: A callable that takes not arguments returns the initial value. collection: Optional name of the parameter collection, if not defined it will be se to `\"parameters\"` if `trainable=True` else it will be set to `\"states\"`. trainable: Specify whether this parameter should be added to the default trainable `\"parameters\"` collection or to the default non-trainable `\"states\"` collection. If collection is passed this parameter will ignored. regularizer: Regularizer instance (callable). constraint: Constraint instance (callable). Returns: The value of the parameter. \"\"\" if collection is None : collection = \"parameters\" if trainable else \"states\" if self . _scope_params is None : self . _scope_params = {} if name not in self . _scope_params : if not self . is_initializing (): raise ValueError ( f \"Cannot add parameter { name } outside `init`\" ) if name in self . _default_params : parameter = self . _default_params [ name ] assert collection == parameter . collection initial_value = parameter . value else : initial_value = initializer () initial_value = jax . tree_map ( jnp . asarray , initial_value ) parameter = types . Parameter ( collection , initial_value ) self . _register_parameter ( name , parameter ) self . _scope_params [ name ] = parameter parameter = self . _scope_params [ name ] if parameter . collection != collection : raise ValueError ( f \"types.Parameter { name } previously found in collection { parameter . collection } \" f \"but currently being added for collection { collection } \" ) value = parameter . value if constraint is not None : value = constraint ( value ) if regularizer is not None : hooks . add_loss ( name = utils . get_name ( regularizer ), value = regularizer ( value ), ) return value","title":"add_parameter()"},{"location":"api/nn/EMAParamsTree/#elegy.nn.moving_averages.EMAParamsTree.init","text":"Initializes the module, Source code in elegy/nn/moving_averages.py def init ( self , * , rng : tp . Optional [ types . RNGSeq ] = None , set_defaults : bool = False , ) -> tp . Callable [ ... , tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ] ]: \"\"\" Initializes the module, \"\"\" def init_callable ( * args , ** kwargs ) -> tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ]: with module_context ( module = self , collections = None , initializing = True , training = True , rng = rng ): y = self ( * args , ** kwargs ) parameters , collections = self . get_parameters_internal () self . _mark_initialized_recursive () if set_defaults : self . set_default_parameters ( parameters , collections ) return y , parameters , collections init_callable . _signature_f = self . call return init_callable","title":"init()"},{"location":"api/nn/EmbedLookupStyle/","text":"elegy.nn.EmbedLookupStyle How to return the embedding matrices given IDs. __class__ inherited Metaclass for Enum __members__ property readonly special Returns a mapping of member name->value. This mapping lists all enum members, including aliases. Note that this is a read-only view of the internal mapping. __bool__ ( self ) special classes/types should always be True. Source code in elegy/nn/embedding.py def __bool__ ( self ): \"\"\" classes/types should always be True. \"\"\" return True __call__ ( cls , value , names = None , * , module = None , qualname = None , type = None , start = 1 ) special Either returns an existing member, or creates a new enum class. This method is used both when an enum class is given a value to match to an enumeration member (i.e. Color(3)) and for the functional API (i.e. Color = Enum('Color', names='RED GREEN BLUE')). When used for the functional API: value will be the name of the new class. names should be either a string of white-space/comma delimited names (values will start at start ), or an iterator/mapping of name, value pairs. module should be set to the module this class is being created in; if it is not set, an attempt to find that module will be made, but if it fails the class will not be picklable. qualname should be set to the actual location this class can be found at in its module; by default it is set to the global scope. If this is not correct, unpickling will fail in some circumstances. type , if set, will be mixed in as the first base class. Source code in elegy/nn/embedding.py def __call__ ( cls , value , names = None , * , module = None , qualname = None , type = None , start = 1 ): \"\"\"Either returns an existing member, or creates a new enum class. This method is used both when an enum class is given a value to match to an enumeration member (i.e. Color(3)) and for the functional API (i.e. Color = Enum('Color', names='RED GREEN BLUE')). When used for the functional API: `value` will be the name of the new class. `names` should be either a string of white-space/comma delimited names (values will start at `start`), or an iterator/mapping of name, value pairs. `module` should be set to the module this class is being created in; if it is not set, an attempt to find that module will be made, but if it fails the class will not be picklable. `qualname` should be set to the actual location this class can be found at in its module; by default it is set to the global scope. If this is not correct, unpickling will fail in some circumstances. `type`, if set, will be mixed in as the first base class. \"\"\" if names is None : # simple value lookup return cls . __new__ ( cls , value ) # otherwise, functional API: we're creating a new Enum type return cls . _create_ ( value , names , module = module , qualname = qualname , type = type , start = start ) __getattr__ ( cls , name ) special Return the enum member matching name We use getattr instead of descriptors or inserting into the enum class' dict in order to support name and value being both properties for enum members (which live in the class' dict ) and enum members themselves. Source code in elegy/nn/embedding.py def __getattr__ ( cls , name ): \"\"\"Return the enum member matching `name` We use __getattr__ instead of descriptors or inserting into the enum class' __dict__ in order to support `name` and `value` being both properties for enum members (which live in the class' __dict__) and enum members themselves. \"\"\" if _is_dunder ( name ): raise AttributeError ( name ) try : return cls . _member_map_ [ name ] except KeyError : raise AttributeError ( name ) from None __new__ ( metacls , cls , bases , classdict ) special staticmethod Create and return a new object. See help(type) for accurate signature. Source code in elegy/nn/embedding.py def __new__ ( metacls , cls , bases , classdict ): # an Enum class is final once enumeration items have been defined; it # cannot be mixed with other types (int, float, etc.) if it has an # inherited __new__ unless a new __new__ is defined (or the resulting # class will fail). # # remove any keys listed in _ignore_ classdict . setdefault ( '_ignore_' , []) . append ( '_ignore_' ) ignore = classdict [ '_ignore_' ] for key in ignore : classdict . pop ( key , None ) member_type , first_enum = metacls . _get_mixins_ ( bases ) __new__ , save_new , use_args = metacls . _find_new_ ( classdict , member_type , first_enum ) # save enum items into separate mapping so they don't get baked into # the new class enum_members = { k : classdict [ k ] for k in classdict . _member_names } for name in classdict . _member_names : del classdict [ name ] # adjust the sunders _order_ = classdict . pop ( '_order_' , None ) # check for illegal enum names (any others?) invalid_names = set ( enum_members ) & { 'mro' , '' } if invalid_names : raise ValueError ( 'Invalid enum member name: {0} ' . format ( ',' . join ( invalid_names ))) # create a default docstring if one has not been provided if '__doc__' not in classdict : classdict [ '__doc__' ] = 'An enumeration.' # create our new Enum type enum_class = super () . __new__ ( metacls , cls , bases , classdict ) enum_class . _member_names_ = [] # names in definition order enum_class . _member_map_ = {} # name->value map enum_class . _member_type_ = member_type # save DynamicClassAttribute attributes from super classes so we know # if we can take the shortcut of storing members in the class dict dynamic_attributes = { k for c in enum_class . mro () for k , v in c . __dict__ . items () if isinstance ( v , DynamicClassAttribute )} # Reverse value->name map for hashable values. enum_class . _value2member_map_ = {} # If a custom type is mixed into the Enum, and it does not know how # to pickle itself, pickle.dumps will succeed but pickle.loads will # fail. Rather than have the error show up later and possibly far # from the source, sabotage the pickle protocol for this class so # that pickle.dumps also fails. # # However, if the new class implements its own __reduce_ex__, do not # sabotage -- it's on them to make sure it works correctly. We use # __reduce_ex__ instead of any of the others as it is preferred by # pickle over __reduce__, and it handles all pickle protocols. if '__reduce_ex__' not in classdict : if member_type is not object : methods = ( '__getnewargs_ex__' , '__getnewargs__' , '__reduce_ex__' , '__reduce__' ) if not any ( m in member_type . __dict__ for m in methods ): _make_class_unpicklable ( enum_class ) # instantiate them, checking for duplicates as we go # we instantiate first instead of checking for duplicates first in case # a custom __new__ is doing something funky with the values -- such as # auto-numbering ;) for member_name in classdict . _member_names : value = enum_members [ member_name ] if not isinstance ( value , tuple ): args = ( value , ) else : args = value if member_type is tuple : # special case for tuple enums args = ( args , ) # wrap it one more time if not use_args : enum_member = __new__ ( enum_class ) if not hasattr ( enum_member , '_value_' ): enum_member . _value_ = value else : enum_member = __new__ ( enum_class , * args ) if not hasattr ( enum_member , '_value_' ): if member_type is object : enum_member . _value_ = value else : enum_member . _value_ = member_type ( * args ) value = enum_member . _value_ enum_member . _name_ = member_name enum_member . __objclass__ = enum_class enum_member . __init__ ( * args ) # If another member with the same value was already defined, the # new member becomes an alias to the existing one. for name , canonical_member in enum_class . _member_map_ . items (): if canonical_member . _value_ == enum_member . _value_ : enum_member = canonical_member break else : # Aliases don't appear in member names (only in __members__). enum_class . _member_names_ . append ( member_name ) # performance boost for any member that would not shadow # a DynamicClassAttribute if member_name not in dynamic_attributes : setattr ( enum_class , member_name , enum_member ) # now add to _member_map_ enum_class . _member_map_ [ member_name ] = enum_member try : # This may fail if value is not hashable. We can't add the value # to the map, and by-value lookups for this value will be # linear. enum_class . _value2member_map_ [ value ] = enum_member except TypeError : pass # double check that repr and friends are not the mixin's or various # things break (such as pickle) for name in ( '__repr__' , '__str__' , '__format__' , '__reduce_ex__' ): class_method = getattr ( enum_class , name ) obj_method = getattr ( member_type , name , None ) enum_method = getattr ( first_enum , name , None ) if obj_method is not None and obj_method is class_method : setattr ( enum_class , name , enum_method ) # replace any other __new__ with our own (as long as Enum is not None, # anyway) -- again, this is to support pickle if Enum is not None : # if the user defined their own __new__, save it before it gets # clobbered in case they subclass later if save_new : enum_class . __new_member__ = __new__ enum_class . __new__ = Enum . __new__ # py3 support for definition order (helps keep py2/py3 code in sync) if _order_ is not None : if isinstance ( _order_ , str ): _order_ = _order_ . replace ( ',' , ' ' ) . split () if _order_ != enum_class . _member_names_ : raise TypeError ( 'member order does not match _order_' ) return enum_class __prepare__ ( cls , bases ) classmethod special prepare () -> dict used to create the namespace for the class statement Source code in elegy/nn/embedding.py @classmethod def __prepare__ ( metacls , cls , bases ): # create the namespace dict enum_dict = _EnumDict () # inherit previous flags and _generate_next_value_ function member_type , first_enum = metacls . _get_mixins_ ( bases ) if first_enum is not None : enum_dict [ '_generate_next_value_' ] = getattr ( first_enum , '_generate_next_value_' , None ) return enum_dict __setattr__ ( cls , name , value ) special Block attempts to reassign Enum members. A simple assignment to the class namespace only changes one of the several possible ways to get an Enum member from the Enum class, resulting in an inconsistent Enumeration. Source code in elegy/nn/embedding.py def __setattr__ ( cls , name , value ): \"\"\"Block attempts to reassign Enum members. A simple assignment to the class namespace only changes one of the several possible ways to get an Enum member from the Enum class, resulting in an inconsistent Enumeration. \"\"\" member_map = cls . __dict__ . get ( '_member_map_' , {}) if name in member_map : raise AttributeError ( 'Cannot reassign members.' ) super () . __setattr__ ( name , value )","title":"EmbedLookupStyle"},{"location":"api/nn/EmbedLookupStyle/#elegynnembedlookupstyle","text":"","title":"elegy.nn.EmbedLookupStyle"},{"location":"api/nn/EmbedLookupStyle/#elegy.nn.embedding.EmbedLookupStyle","text":"How to return the embedding matrices given IDs.","title":"elegy.nn.embedding.EmbedLookupStyle"},{"location":"api/nn/EmbedLookupStyle/#elegy.nn.embedding.EmbedLookupStyle.__class__","text":"Metaclass for Enum","title":"__class__"},{"location":"api/nn/EmbedLookupStyle/#elegy.nn.embedding.EmbedLookupStyle.__class__.__members__","text":"Returns a mapping of member name->value. This mapping lists all enum members, including aliases. Note that this is a read-only view of the internal mapping.","title":"__members__"},{"location":"api/nn/EmbedLookupStyle/#elegy.nn.embedding.EmbedLookupStyle.__class__.__bool__","text":"classes/types should always be True. Source code in elegy/nn/embedding.py def __bool__ ( self ): \"\"\" classes/types should always be True. \"\"\" return True","title":"__bool__()"},{"location":"api/nn/EmbedLookupStyle/#elegy.nn.embedding.EmbedLookupStyle.__class__.__call__","text":"Either returns an existing member, or creates a new enum class. This method is used both when an enum class is given a value to match to an enumeration member (i.e. Color(3)) and for the functional API (i.e. Color = Enum('Color', names='RED GREEN BLUE')). When used for the functional API: value will be the name of the new class. names should be either a string of white-space/comma delimited names (values will start at start ), or an iterator/mapping of name, value pairs. module should be set to the module this class is being created in; if it is not set, an attempt to find that module will be made, but if it fails the class will not be picklable. qualname should be set to the actual location this class can be found at in its module; by default it is set to the global scope. If this is not correct, unpickling will fail in some circumstances. type , if set, will be mixed in as the first base class. Source code in elegy/nn/embedding.py def __call__ ( cls , value , names = None , * , module = None , qualname = None , type = None , start = 1 ): \"\"\"Either returns an existing member, or creates a new enum class. This method is used both when an enum class is given a value to match to an enumeration member (i.e. Color(3)) and for the functional API (i.e. Color = Enum('Color', names='RED GREEN BLUE')). When used for the functional API: `value` will be the name of the new class. `names` should be either a string of white-space/comma delimited names (values will start at `start`), or an iterator/mapping of name, value pairs. `module` should be set to the module this class is being created in; if it is not set, an attempt to find that module will be made, but if it fails the class will not be picklable. `qualname` should be set to the actual location this class can be found at in its module; by default it is set to the global scope. If this is not correct, unpickling will fail in some circumstances. `type`, if set, will be mixed in as the first base class. \"\"\" if names is None : # simple value lookup return cls . __new__ ( cls , value ) # otherwise, functional API: we're creating a new Enum type return cls . _create_ ( value , names , module = module , qualname = qualname , type = type , start = start )","title":"__call__()"},{"location":"api/nn/EmbedLookupStyle/#elegy.nn.embedding.EmbedLookupStyle.__class__.__getattr__","text":"Return the enum member matching name We use getattr instead of descriptors or inserting into the enum class' dict in order to support name and value being both properties for enum members (which live in the class' dict ) and enum members themselves. Source code in elegy/nn/embedding.py def __getattr__ ( cls , name ): \"\"\"Return the enum member matching `name` We use __getattr__ instead of descriptors or inserting into the enum class' __dict__ in order to support `name` and `value` being both properties for enum members (which live in the class' __dict__) and enum members themselves. \"\"\" if _is_dunder ( name ): raise AttributeError ( name ) try : return cls . _member_map_ [ name ] except KeyError : raise AttributeError ( name ) from None","title":"__getattr__()"},{"location":"api/nn/EmbedLookupStyle/#elegy.nn.embedding.EmbedLookupStyle.__class__.__new__","text":"Create and return a new object. See help(type) for accurate signature. Source code in elegy/nn/embedding.py def __new__ ( metacls , cls , bases , classdict ): # an Enum class is final once enumeration items have been defined; it # cannot be mixed with other types (int, float, etc.) if it has an # inherited __new__ unless a new __new__ is defined (or the resulting # class will fail). # # remove any keys listed in _ignore_ classdict . setdefault ( '_ignore_' , []) . append ( '_ignore_' ) ignore = classdict [ '_ignore_' ] for key in ignore : classdict . pop ( key , None ) member_type , first_enum = metacls . _get_mixins_ ( bases ) __new__ , save_new , use_args = metacls . _find_new_ ( classdict , member_type , first_enum ) # save enum items into separate mapping so they don't get baked into # the new class enum_members = { k : classdict [ k ] for k in classdict . _member_names } for name in classdict . _member_names : del classdict [ name ] # adjust the sunders _order_ = classdict . pop ( '_order_' , None ) # check for illegal enum names (any others?) invalid_names = set ( enum_members ) & { 'mro' , '' } if invalid_names : raise ValueError ( 'Invalid enum member name: {0} ' . format ( ',' . join ( invalid_names ))) # create a default docstring if one has not been provided if '__doc__' not in classdict : classdict [ '__doc__' ] = 'An enumeration.' # create our new Enum type enum_class = super () . __new__ ( metacls , cls , bases , classdict ) enum_class . _member_names_ = [] # names in definition order enum_class . _member_map_ = {} # name->value map enum_class . _member_type_ = member_type # save DynamicClassAttribute attributes from super classes so we know # if we can take the shortcut of storing members in the class dict dynamic_attributes = { k for c in enum_class . mro () for k , v in c . __dict__ . items () if isinstance ( v , DynamicClassAttribute )} # Reverse value->name map for hashable values. enum_class . _value2member_map_ = {} # If a custom type is mixed into the Enum, and it does not know how # to pickle itself, pickle.dumps will succeed but pickle.loads will # fail. Rather than have the error show up later and possibly far # from the source, sabotage the pickle protocol for this class so # that pickle.dumps also fails. # # However, if the new class implements its own __reduce_ex__, do not # sabotage -- it's on them to make sure it works correctly. We use # __reduce_ex__ instead of any of the others as it is preferred by # pickle over __reduce__, and it handles all pickle protocols. if '__reduce_ex__' not in classdict : if member_type is not object : methods = ( '__getnewargs_ex__' , '__getnewargs__' , '__reduce_ex__' , '__reduce__' ) if not any ( m in member_type . __dict__ for m in methods ): _make_class_unpicklable ( enum_class ) # instantiate them, checking for duplicates as we go # we instantiate first instead of checking for duplicates first in case # a custom __new__ is doing something funky with the values -- such as # auto-numbering ;) for member_name in classdict . _member_names : value = enum_members [ member_name ] if not isinstance ( value , tuple ): args = ( value , ) else : args = value if member_type is tuple : # special case for tuple enums args = ( args , ) # wrap it one more time if not use_args : enum_member = __new__ ( enum_class ) if not hasattr ( enum_member , '_value_' ): enum_member . _value_ = value else : enum_member = __new__ ( enum_class , * args ) if not hasattr ( enum_member , '_value_' ): if member_type is object : enum_member . _value_ = value else : enum_member . _value_ = member_type ( * args ) value = enum_member . _value_ enum_member . _name_ = member_name enum_member . __objclass__ = enum_class enum_member . __init__ ( * args ) # If another member with the same value was already defined, the # new member becomes an alias to the existing one. for name , canonical_member in enum_class . _member_map_ . items (): if canonical_member . _value_ == enum_member . _value_ : enum_member = canonical_member break else : # Aliases don't appear in member names (only in __members__). enum_class . _member_names_ . append ( member_name ) # performance boost for any member that would not shadow # a DynamicClassAttribute if member_name not in dynamic_attributes : setattr ( enum_class , member_name , enum_member ) # now add to _member_map_ enum_class . _member_map_ [ member_name ] = enum_member try : # This may fail if value is not hashable. We can't add the value # to the map, and by-value lookups for this value will be # linear. enum_class . _value2member_map_ [ value ] = enum_member except TypeError : pass # double check that repr and friends are not the mixin's or various # things break (such as pickle) for name in ( '__repr__' , '__str__' , '__format__' , '__reduce_ex__' ): class_method = getattr ( enum_class , name ) obj_method = getattr ( member_type , name , None ) enum_method = getattr ( first_enum , name , None ) if obj_method is not None and obj_method is class_method : setattr ( enum_class , name , enum_method ) # replace any other __new__ with our own (as long as Enum is not None, # anyway) -- again, this is to support pickle if Enum is not None : # if the user defined their own __new__, save it before it gets # clobbered in case they subclass later if save_new : enum_class . __new_member__ = __new__ enum_class . __new__ = Enum . __new__ # py3 support for definition order (helps keep py2/py3 code in sync) if _order_ is not None : if isinstance ( _order_ , str ): _order_ = _order_ . replace ( ',' , ' ' ) . split () if _order_ != enum_class . _member_names_ : raise TypeError ( 'member order does not match _order_' ) return enum_class","title":"__new__()"},{"location":"api/nn/EmbedLookupStyle/#elegy.nn.embedding.EmbedLookupStyle.__class__.__prepare__","text":"prepare () -> dict used to create the namespace for the class statement Source code in elegy/nn/embedding.py @classmethod def __prepare__ ( metacls , cls , bases ): # create the namespace dict enum_dict = _EnumDict () # inherit previous flags and _generate_next_value_ function member_type , first_enum = metacls . _get_mixins_ ( bases ) if first_enum is not None : enum_dict [ '_generate_next_value_' ] = getattr ( first_enum , '_generate_next_value_' , None ) return enum_dict","title":"__prepare__()"},{"location":"api/nn/EmbedLookupStyle/#elegy.nn.embedding.EmbedLookupStyle.__class__.__setattr__","text":"Block attempts to reassign Enum members. A simple assignment to the class namespace only changes one of the several possible ways to get an Enum member from the Enum class, resulting in an inconsistent Enumeration. Source code in elegy/nn/embedding.py def __setattr__ ( cls , name , value ): \"\"\"Block attempts to reassign Enum members. A simple assignment to the class namespace only changes one of the several possible ways to get an Enum member from the Enum class, resulting in an inconsistent Enumeration. \"\"\" member_map = cls . __dict__ . get ( '_member_map_' , {}) if name in member_map : raise AttributeError ( 'Cannot reassign members.' ) super () . __setattr__ ( name , value )","title":"__setattr__()"},{"location":"api/nn/Embedding/","text":"elegy.nn.Embedding Module for embedding tokens in a low-dimensional space. __init__ ( self , vocab_size = None , embed_dim = None , embedding_matrix = None , w_init = None , lookup_style = 'ARRAY_INDEX' , name = None ) special Constructs an Embed module. Parameters: Name Type Description Default vocab_size Optional[int] The number of unique tokens to embed. If not provided, an existing vocabulary matrix from which vocab_size can be inferred must be provided as existing_vocab . None embed_dim Optional[int] Number of dimensions to assign to each embedding. If an existing vocabulary matrix initializes the module, this should not be provided as it will be inferred. None embedding_matrix Optional[jax._src.numpy.lax_numpy.ndarray] A matrix-like object equivalent in size to [vocab_size, embed_dim] . If given, it is used as the initial value for the embedding matrix and neither vocab_size or embed_dim need be given. If they are given, their values are checked to be consistent with the dimensions of embedding_matrix . None w_init Optional[elegy.types.Initializer] An initializer for the embeddings matrix. As a default, embeddings are initialized via a truncated normal distribution. None lookup_style Union[str, elegy.nn.embedding.EmbedLookupStyle] One of the enum values of :class: EmbedLookupStyle determining how to access the value of the embeddings given an ID. Regardless the input should be a dense array of integer values representing ids. This setting changes how internally this module maps those ides to embeddings. The result is the same, but the speed and memory tradeoffs are different. It default to using numpy-style array indexing. This value is only the default for the module, and at any given invocation can be overridden in :meth: __call__ . 'ARRAY_INDEX' name Optional[str] Optional name for this module. None Exceptions: Type Description ValueError If none of embed_dim , embedding_matrix and vocab_size are supplied, or if embedding_matrix is supplied and embed_dim or vocab_size is not consistent with the supplied matrix. Source code in elegy/nn/embedding.py def __init__ ( self , vocab_size : Optional [ int ] = None , embed_dim : Optional [ int ] = None , embedding_matrix : Optional [ jnp . ndarray ] = None , w_init : Optional [ types . Initializer ] = None , lookup_style : Union [ str , EmbedLookupStyle ] = \"ARRAY_INDEX\" , name : Optional [ str ] = None , ): \"\"\" Constructs an Embed module. Args: vocab_size: The number of unique tokens to embed. If not provided, an existing vocabulary matrix from which ``vocab_size`` can be inferred must be provided as ``existing_vocab``. embed_dim: Number of dimensions to assign to each embedding. If an existing vocabulary matrix initializes the module, this should not be provided as it will be inferred. embedding_matrix: A matrix-like object equivalent in size to ``[vocab_size, embed_dim]``. If given, it is used as the initial value for the embedding matrix and neither ``vocab_size`` or ``embed_dim`` need be given. If they are given, their values are checked to be consistent with the dimensions of ``embedding_matrix``. w_init: An initializer for the embeddings matrix. As a default, embeddings are initialized via a truncated normal distribution. lookup_style: One of the enum values of :class:`EmbedLookupStyle` determining how to access the value of the embeddings given an ID. Regardless the input should be a dense array of integer values representing ids. This setting changes how internally this module maps those ides to embeddings. The result is the same, but the speed and memory tradeoffs are different. It default to using numpy-style array indexing. This value is only the default for the module, and at any given invocation can be overridden in :meth:`__call__`. name: Optional name for this module. Raises: ValueError: If none of ``embed_dim``, ``embedding_matrix`` and ``vocab_size`` are supplied, or if ``embedding_matrix`` is supplied and ``embed_dim`` or ``vocab_size`` is not consistent with the supplied matrix. \"\"\" super () . __init__ ( name = name ) if embedding_matrix is None and not ( vocab_size and embed_dim ): raise ValueError ( \"Embedding must be supplied either with an initial `embedding_matrix` \" \"or with `embed_dim` and `vocab_size`.\" ) if embedding_matrix is not None : embedding_matrix = jnp . asarray ( embedding_matrix ) if vocab_size and embedding_matrix . shape [ 0 ] != vocab_size : raise ValueError ( \"An `embedding_matrix` was supplied but the `vocab_size` of \" f \" { vocab_size } was not consistent with its shape \" f \" { embedding_matrix . shape } .\" ) if embed_dim and embedding_matrix . shape [ 1 ] != embed_dim : raise ValueError ( \"An `embedding_matrix` was supplied but the `embed_dim` of \" f \" { embed_dim } was not consistent with its shape \" f \" { embedding_matrix . shape } .\" ) self . embeddings = self . add_parameter ( \"embeddings\" , embedding_matrix . shape , initializer = lambda _ , __ : embedding_matrix , ) else : assert embed_dim is not None assert vocab_size is not None w_init = w_init or initializers . TruncatedNormal () self . embeddings = self . add_parameter ( \"embeddings\" , [ vocab_size , embed_dim ], initializer = w_init ) self . vocab_size = vocab_size or embedding_matrix . shape [ 0 ] self . embed_dim = embed_dim or embedding_matrix . shape [ 1 ] self . lookup_style = lookup_style add_parameter ( self , name , initializer , collection = None , trainable = True , regularizer = None , constraint = None ) inherited Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required initializer Callable[[], Any] A callable that takes not arguments returns the initial value. required collection Optional[str] Optional name of the parameter collection, if not defined it will be se to \"parameters\" if trainable=True else it will be set to \"states\" . None trainable bool Specify whether this parameter should be added to the default trainable \"parameters\" collection or to the default non-trainable \"states\" collection. If collection is passed this parameter will ignored. True regularizer Optional[Callable[[Any], jax._src.numpy.lax_numpy.ndarray]] Regularizer instance (callable). None constraint Optional[Callable[[Any], Any]] Constraint instance (callable). None Returns: Type Description Any The value of the parameter. Source code in elegy/nn/embedding.py def add_parameter ( self , name : str , initializer : tp . Callable [[], tp . Any ], collection : tp . Optional [ str ] = None , trainable : bool = True , regularizer : tp . Optional [ tp . Callable [[ tp . Any ], jnp . ndarray ]] = None , constraint : tp . Optional [ tp . Callable [[ tp . Any ], tp . Any ]] = None , ) -> tp . Any : \"\"\" Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. initializer: A callable that takes not arguments returns the initial value. collection: Optional name of the parameter collection, if not defined it will be se to `\"parameters\"` if `trainable=True` else it will be set to `\"states\"`. trainable: Specify whether this parameter should be added to the default trainable `\"parameters\"` collection or to the default non-trainable `\"states\"` collection. If collection is passed this parameter will ignored. regularizer: Regularizer instance (callable). constraint: Constraint instance (callable). Returns: The value of the parameter. \"\"\" if collection is None : collection = \"parameters\" if trainable else \"states\" if self . _scope_params is None : self . _scope_params = {} if name not in self . _scope_params : if not self . is_initializing (): raise ValueError ( f \"Cannot add parameter { name } outside `init`\" ) if name in self . _default_params : parameter = self . _default_params [ name ] assert collection == parameter . collection initial_value = parameter . value else : initial_value = initializer () initial_value = jax . tree_map ( jnp . asarray , initial_value ) parameter = types . Parameter ( collection , initial_value ) self . _register_parameter ( name , parameter ) self . _scope_params [ name ] = parameter parameter = self . _scope_params [ name ] if parameter . collection != collection : raise ValueError ( f \"types.Parameter { name } previously found in collection { parameter . collection } \" f \"but currently being added for collection { collection } \" ) value = parameter . value if constraint is not None : value = constraint ( value ) if regularizer is not None : hooks . add_loss ( name = utils . get_name ( regularizer ), value = regularizer ( value ), ) return value call ( self , ids , lookup_style = None ) Lookup embeddings. Looks up an embedding vector for each value in ids . All ids must be within [0, vocab_size) to prevent NaN \\ s from propagating. Parameters: Name Type Description Default ids ndarray integer array. required lookup_style Union[str, elegy.nn.embedding.EmbedLookupStyle] Overrides the lookup_style given in the constructor. None Returns: Type Description ndarray Tensor of ids.shape + [embedding_dim] . Exceptions: Type Description AttributeError If lookup_style is not valid. ValueError If ids is not an integer array. Source code in elegy/nn/embedding.py def call ( self , ids : jnp . ndarray , lookup_style : Optional [ Union [ str , EmbedLookupStyle ]] = None , ) -> jnp . ndarray : r \"\"\" Lookup embeddings. Looks up an embedding vector for each value in ``ids``. All ids must be within ``[0, vocab_size)`` to prevent ``NaN``\\ s from propagating. Args: ids: integer array. lookup_style: Overrides the ``lookup_style`` given in the constructor. Returns: Tensor of ``ids.shape + [embedding_dim]``. Raises: AttributeError: If ``lookup_style`` is not valid. ValueError: If ``ids`` is not an integer array. \"\"\" # TODO(tomhennigan) Consider removing asarray here. ids = jnp . asarray ( ids ) if not jnp . issubdtype ( ids . dtype , jnp . integer ): raise ValueError ( \"Embedding's __call__ method must take an array of \" \"integer dtype but was called with an array of \" f \" { ids . dtype } \" ) lookup_style = lookup_style or self . lookup_style if isinstance ( lookup_style , str ): lookup_style = getattr ( EmbedLookupStyle , lookup_style . upper ()) if lookup_style == EmbedLookupStyle . ARRAY_INDEX : return self . embeddings [( ids ,)] elif lookup_style == EmbedLookupStyle . ONE_HOT : one_hot_ids = jax . nn . one_hot ( ids , self . vocab_size )[ ... , None ] return ( self . embeddings * one_hot_ids ) . sum ( axis =- 2 ) else : raise NotImplementedError ( f \" { lookup_style } is not supported by Embedding.\" ) init ( self , * , rng = None , set_defaults = False ) inherited Initializes the module, Source code in elegy/nn/embedding.py def init ( self , * , rng : tp . Optional [ types . RNGSeq ] = None , set_defaults : bool = False , ) -> tp . Callable [ ... , tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ] ]: \"\"\" Initializes the module, \"\"\" def init_callable ( * args , ** kwargs ) -> tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ]: with module_context ( module = self , collections = None , initializing = True , training = True , rng = rng ): y = self ( * args , ** kwargs ) parameters , collections = self . get_parameters_internal () self . _mark_initialized_recursive () if set_defaults : self . set_default_parameters ( parameters , collections ) return y , parameters , collections init_callable . _signature_f = self . call return init_callable","title":"Embedding"},{"location":"api/nn/Embedding/#elegynnembedding","text":"","title":"elegy.nn.Embedding"},{"location":"api/nn/Embedding/#elegy.nn.embedding.Embedding","text":"Module for embedding tokens in a low-dimensional space.","title":"elegy.nn.embedding.Embedding"},{"location":"api/nn/Embedding/#elegy.nn.embedding.Embedding.__init__","text":"Constructs an Embed module. Parameters: Name Type Description Default vocab_size Optional[int] The number of unique tokens to embed. If not provided, an existing vocabulary matrix from which vocab_size can be inferred must be provided as existing_vocab . None embed_dim Optional[int] Number of dimensions to assign to each embedding. If an existing vocabulary matrix initializes the module, this should not be provided as it will be inferred. None embedding_matrix Optional[jax._src.numpy.lax_numpy.ndarray] A matrix-like object equivalent in size to [vocab_size, embed_dim] . If given, it is used as the initial value for the embedding matrix and neither vocab_size or embed_dim need be given. If they are given, their values are checked to be consistent with the dimensions of embedding_matrix . None w_init Optional[elegy.types.Initializer] An initializer for the embeddings matrix. As a default, embeddings are initialized via a truncated normal distribution. None lookup_style Union[str, elegy.nn.embedding.EmbedLookupStyle] One of the enum values of :class: EmbedLookupStyle determining how to access the value of the embeddings given an ID. Regardless the input should be a dense array of integer values representing ids. This setting changes how internally this module maps those ides to embeddings. The result is the same, but the speed and memory tradeoffs are different. It default to using numpy-style array indexing. This value is only the default for the module, and at any given invocation can be overridden in :meth: __call__ . 'ARRAY_INDEX' name Optional[str] Optional name for this module. None Exceptions: Type Description ValueError If none of embed_dim , embedding_matrix and vocab_size are supplied, or if embedding_matrix is supplied and embed_dim or vocab_size is not consistent with the supplied matrix. Source code in elegy/nn/embedding.py def __init__ ( self , vocab_size : Optional [ int ] = None , embed_dim : Optional [ int ] = None , embedding_matrix : Optional [ jnp . ndarray ] = None , w_init : Optional [ types . Initializer ] = None , lookup_style : Union [ str , EmbedLookupStyle ] = \"ARRAY_INDEX\" , name : Optional [ str ] = None , ): \"\"\" Constructs an Embed module. Args: vocab_size: The number of unique tokens to embed. If not provided, an existing vocabulary matrix from which ``vocab_size`` can be inferred must be provided as ``existing_vocab``. embed_dim: Number of dimensions to assign to each embedding. If an existing vocabulary matrix initializes the module, this should not be provided as it will be inferred. embedding_matrix: A matrix-like object equivalent in size to ``[vocab_size, embed_dim]``. If given, it is used as the initial value for the embedding matrix and neither ``vocab_size`` or ``embed_dim`` need be given. If they are given, their values are checked to be consistent with the dimensions of ``embedding_matrix``. w_init: An initializer for the embeddings matrix. As a default, embeddings are initialized via a truncated normal distribution. lookup_style: One of the enum values of :class:`EmbedLookupStyle` determining how to access the value of the embeddings given an ID. Regardless the input should be a dense array of integer values representing ids. This setting changes how internally this module maps those ides to embeddings. The result is the same, but the speed and memory tradeoffs are different. It default to using numpy-style array indexing. This value is only the default for the module, and at any given invocation can be overridden in :meth:`__call__`. name: Optional name for this module. Raises: ValueError: If none of ``embed_dim``, ``embedding_matrix`` and ``vocab_size`` are supplied, or if ``embedding_matrix`` is supplied and ``embed_dim`` or ``vocab_size`` is not consistent with the supplied matrix. \"\"\" super () . __init__ ( name = name ) if embedding_matrix is None and not ( vocab_size and embed_dim ): raise ValueError ( \"Embedding must be supplied either with an initial `embedding_matrix` \" \"or with `embed_dim` and `vocab_size`.\" ) if embedding_matrix is not None : embedding_matrix = jnp . asarray ( embedding_matrix ) if vocab_size and embedding_matrix . shape [ 0 ] != vocab_size : raise ValueError ( \"An `embedding_matrix` was supplied but the `vocab_size` of \" f \" { vocab_size } was not consistent with its shape \" f \" { embedding_matrix . shape } .\" ) if embed_dim and embedding_matrix . shape [ 1 ] != embed_dim : raise ValueError ( \"An `embedding_matrix` was supplied but the `embed_dim` of \" f \" { embed_dim } was not consistent with its shape \" f \" { embedding_matrix . shape } .\" ) self . embeddings = self . add_parameter ( \"embeddings\" , embedding_matrix . shape , initializer = lambda _ , __ : embedding_matrix , ) else : assert embed_dim is not None assert vocab_size is not None w_init = w_init or initializers . TruncatedNormal () self . embeddings = self . add_parameter ( \"embeddings\" , [ vocab_size , embed_dim ], initializer = w_init ) self . vocab_size = vocab_size or embedding_matrix . shape [ 0 ] self . embed_dim = embed_dim or embedding_matrix . shape [ 1 ] self . lookup_style = lookup_style","title":"__init__()"},{"location":"api/nn/Embedding/#elegy.nn.embedding.Embedding.add_parameter","text":"Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required initializer Callable[[], Any] A callable that takes not arguments returns the initial value. required collection Optional[str] Optional name of the parameter collection, if not defined it will be se to \"parameters\" if trainable=True else it will be set to \"states\" . None trainable bool Specify whether this parameter should be added to the default trainable \"parameters\" collection or to the default non-trainable \"states\" collection. If collection is passed this parameter will ignored. True regularizer Optional[Callable[[Any], jax._src.numpy.lax_numpy.ndarray]] Regularizer instance (callable). None constraint Optional[Callable[[Any], Any]] Constraint instance (callable). None Returns: Type Description Any The value of the parameter. Source code in elegy/nn/embedding.py def add_parameter ( self , name : str , initializer : tp . Callable [[], tp . Any ], collection : tp . Optional [ str ] = None , trainable : bool = True , regularizer : tp . Optional [ tp . Callable [[ tp . Any ], jnp . ndarray ]] = None , constraint : tp . Optional [ tp . Callable [[ tp . Any ], tp . Any ]] = None , ) -> tp . Any : \"\"\" Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. initializer: A callable that takes not arguments returns the initial value. collection: Optional name of the parameter collection, if not defined it will be se to `\"parameters\"` if `trainable=True` else it will be set to `\"states\"`. trainable: Specify whether this parameter should be added to the default trainable `\"parameters\"` collection or to the default non-trainable `\"states\"` collection. If collection is passed this parameter will ignored. regularizer: Regularizer instance (callable). constraint: Constraint instance (callable). Returns: The value of the parameter. \"\"\" if collection is None : collection = \"parameters\" if trainable else \"states\" if self . _scope_params is None : self . _scope_params = {} if name not in self . _scope_params : if not self . is_initializing (): raise ValueError ( f \"Cannot add parameter { name } outside `init`\" ) if name in self . _default_params : parameter = self . _default_params [ name ] assert collection == parameter . collection initial_value = parameter . value else : initial_value = initializer () initial_value = jax . tree_map ( jnp . asarray , initial_value ) parameter = types . Parameter ( collection , initial_value ) self . _register_parameter ( name , parameter ) self . _scope_params [ name ] = parameter parameter = self . _scope_params [ name ] if parameter . collection != collection : raise ValueError ( f \"types.Parameter { name } previously found in collection { parameter . collection } \" f \"but currently being added for collection { collection } \" ) value = parameter . value if constraint is not None : value = constraint ( value ) if regularizer is not None : hooks . add_loss ( name = utils . get_name ( regularizer ), value = regularizer ( value ), ) return value","title":"add_parameter()"},{"location":"api/nn/Embedding/#elegy.nn.embedding.Embedding.call","text":"Lookup embeddings. Looks up an embedding vector for each value in ids . All ids must be within [0, vocab_size) to prevent NaN \\ s from propagating. Parameters: Name Type Description Default ids ndarray integer array. required lookup_style Union[str, elegy.nn.embedding.EmbedLookupStyle] Overrides the lookup_style given in the constructor. None Returns: Type Description ndarray Tensor of ids.shape + [embedding_dim] . Exceptions: Type Description AttributeError If lookup_style is not valid. ValueError If ids is not an integer array. Source code in elegy/nn/embedding.py def call ( self , ids : jnp . ndarray , lookup_style : Optional [ Union [ str , EmbedLookupStyle ]] = None , ) -> jnp . ndarray : r \"\"\" Lookup embeddings. Looks up an embedding vector for each value in ``ids``. All ids must be within ``[0, vocab_size)`` to prevent ``NaN``\\ s from propagating. Args: ids: integer array. lookup_style: Overrides the ``lookup_style`` given in the constructor. Returns: Tensor of ``ids.shape + [embedding_dim]``. Raises: AttributeError: If ``lookup_style`` is not valid. ValueError: If ``ids`` is not an integer array. \"\"\" # TODO(tomhennigan) Consider removing asarray here. ids = jnp . asarray ( ids ) if not jnp . issubdtype ( ids . dtype , jnp . integer ): raise ValueError ( \"Embedding's __call__ method must take an array of \" \"integer dtype but was called with an array of \" f \" { ids . dtype } \" ) lookup_style = lookup_style or self . lookup_style if isinstance ( lookup_style , str ): lookup_style = getattr ( EmbedLookupStyle , lookup_style . upper ()) if lookup_style == EmbedLookupStyle . ARRAY_INDEX : return self . embeddings [( ids ,)] elif lookup_style == EmbedLookupStyle . ONE_HOT : one_hot_ids = jax . nn . one_hot ( ids , self . vocab_size )[ ... , None ] return ( self . embeddings * one_hot_ids ) . sum ( axis =- 2 ) else : raise NotImplementedError ( f \" { lookup_style } is not supported by Embedding.\" )","title":"call()"},{"location":"api/nn/Embedding/#elegy.nn.embedding.Embedding.init","text":"Initializes the module, Source code in elegy/nn/embedding.py def init ( self , * , rng : tp . Optional [ types . RNGSeq ] = None , set_defaults : bool = False , ) -> tp . Callable [ ... , tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ] ]: \"\"\" Initializes the module, \"\"\" def init_callable ( * args , ** kwargs ) -> tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ]: with module_context ( module = self , collections = None , initializing = True , training = True , rng = rng ): y = self ( * args , ** kwargs ) parameters , collections = self . get_parameters_internal () self . _mark_initialized_recursive () if set_defaults : self . set_default_parameters ( parameters , collections ) return y , parameters , collections init_callable . _signature_f = self . call return init_callable","title":"init()"},{"location":"api/nn/Flatten/","text":"elegy.nn.Flatten Flattens the input, preserving the batch dimension(s). By default, Flatten combines all dimensions except the first. Additional leading dimensions can be preserved by setting preserve_dims. x = jnp . ones([ 3 , 2 , 4 ]) flat = elegy . nn . Flatten() assert flat(x) . shape == ( 3 , 8 ) When the input to flatten has fewer than preserve_dims dimensions it is returned unchanged: x = jnp . ones([ 3 ]) assert flat(x) . shape == ( 3 ,) add_parameter ( self , name , initializer , collection = None , trainable = True , regularizer = None , constraint = None ) inherited Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required initializer Callable[[], Any] A callable that takes not arguments returns the initial value. required collection Optional[str] Optional name of the parameter collection, if not defined it will be se to \"parameters\" if trainable=True else it will be set to \"states\" . None trainable bool Specify whether this parameter should be added to the default trainable \"parameters\" collection or to the default non-trainable \"states\" collection. If collection is passed this parameter will ignored. True regularizer Optional[Callable[[Any], jax._src.numpy.lax_numpy.ndarray]] Regularizer instance (callable). None constraint Optional[Callable[[Any], Any]] Constraint instance (callable). None Returns: Type Description Any The value of the parameter. Source code in elegy/nn/flatten.py def add_parameter ( self , name : str , initializer : tp . Callable [[], tp . Any ], collection : tp . Optional [ str ] = None , trainable : bool = True , regularizer : tp . Optional [ tp . Callable [[ tp . Any ], jnp . ndarray ]] = None , constraint : tp . Optional [ tp . Callable [[ tp . Any ], tp . Any ]] = None , ) -> tp . Any : \"\"\" Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. initializer: A callable that takes not arguments returns the initial value. collection: Optional name of the parameter collection, if not defined it will be se to `\"parameters\"` if `trainable=True` else it will be set to `\"states\"`. trainable: Specify whether this parameter should be added to the default trainable `\"parameters\"` collection or to the default non-trainable `\"states\"` collection. If collection is passed this parameter will ignored. regularizer: Regularizer instance (callable). constraint: Constraint instance (callable). Returns: The value of the parameter. \"\"\" if collection is None : collection = \"parameters\" if trainable else \"states\" if self . _scope_params is None : self . _scope_params = {} if name not in self . _scope_params : if not self . is_initializing (): raise ValueError ( f \"Cannot add parameter { name } outside `init`\" ) if name in self . _default_params : parameter = self . _default_params [ name ] assert collection == parameter . collection initial_value = parameter . value else : initial_value = initializer () initial_value = jax . tree_map ( jnp . asarray , initial_value ) parameter = types . Parameter ( collection , initial_value ) self . _register_parameter ( name , parameter ) self . _scope_params [ name ] = parameter parameter = self . _scope_params [ name ] if parameter . collection != collection : raise ValueError ( f \"types.Parameter { name } previously found in collection { parameter . collection } \" f \"but currently being added for collection { collection } \" ) value = parameter . value if constraint is not None : value = constraint ( value ) if regularizer is not None : hooks . add_loss ( name = utils . get_name ( regularizer ), value = regularizer ( value ), ) return value call ( self , inputs ) inherited Parameters: Name Type Description Default inputs ndarray the array to be reshaped. required Returns: Type Description ndarray A reshaped array. Source code in elegy/nn/flatten.py def call ( self , inputs : np . ndarray ) -> np . ndarray : \"\"\" Arguments: inputs: the array to be reshaped. Returns: A reshaped array. \"\"\" if inputs . ndim <= self . preserve_dims : return inputs if - 1 in self . output_shape : reshaped_shape = _infer_shape ( self . output_shape , inputs . shape [ self . preserve_dims :] ) else : reshaped_shape = self . output_shape shape = inputs . shape [: self . preserve_dims ] + reshaped_shape return jnp . reshape ( inputs , shape ) init ( self , * , rng = None , set_defaults = False ) inherited Initializes the module, Source code in elegy/nn/flatten.py def init ( self , * , rng : tp . Optional [ types . RNGSeq ] = None , set_defaults : bool = False , ) -> tp . Callable [ ... , tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ] ]: \"\"\" Initializes the module, \"\"\" def init_callable ( * args , ** kwargs ) -> tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ]: with module_context ( module = self , collections = None , initializing = True , training = True , rng = rng ): y = self ( * args , ** kwargs ) parameters , collections = self . get_parameters_internal () self . _mark_initialized_recursive () if set_defaults : self . set_default_parameters ( parameters , collections ) return y , parameters , collections init_callable . _signature_f = self . call return init_callable","title":"Flatten"},{"location":"api/nn/Flatten/#elegynnflatten","text":"","title":"elegy.nn.Flatten"},{"location":"api/nn/Flatten/#elegy.nn.flatten.Flatten","text":"Flattens the input, preserving the batch dimension(s). By default, Flatten combines all dimensions except the first. Additional leading dimensions can be preserved by setting preserve_dims. x = jnp . ones([ 3 , 2 , 4 ]) flat = elegy . nn . Flatten() assert flat(x) . shape == ( 3 , 8 ) When the input to flatten has fewer than preserve_dims dimensions it is returned unchanged: x = jnp . ones([ 3 ]) assert flat(x) . shape == ( 3 ,)","title":"elegy.nn.flatten.Flatten"},{"location":"api/nn/Flatten/#elegy.nn.flatten.Flatten.add_parameter","text":"Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required initializer Callable[[], Any] A callable that takes not arguments returns the initial value. required collection Optional[str] Optional name of the parameter collection, if not defined it will be se to \"parameters\" if trainable=True else it will be set to \"states\" . None trainable bool Specify whether this parameter should be added to the default trainable \"parameters\" collection or to the default non-trainable \"states\" collection. If collection is passed this parameter will ignored. True regularizer Optional[Callable[[Any], jax._src.numpy.lax_numpy.ndarray]] Regularizer instance (callable). None constraint Optional[Callable[[Any], Any]] Constraint instance (callable). None Returns: Type Description Any The value of the parameter. Source code in elegy/nn/flatten.py def add_parameter ( self , name : str , initializer : tp . Callable [[], tp . Any ], collection : tp . Optional [ str ] = None , trainable : bool = True , regularizer : tp . Optional [ tp . Callable [[ tp . Any ], jnp . ndarray ]] = None , constraint : tp . Optional [ tp . Callable [[ tp . Any ], tp . Any ]] = None , ) -> tp . Any : \"\"\" Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. initializer: A callable that takes not arguments returns the initial value. collection: Optional name of the parameter collection, if not defined it will be se to `\"parameters\"` if `trainable=True` else it will be set to `\"states\"`. trainable: Specify whether this parameter should be added to the default trainable `\"parameters\"` collection or to the default non-trainable `\"states\"` collection. If collection is passed this parameter will ignored. regularizer: Regularizer instance (callable). constraint: Constraint instance (callable). Returns: The value of the parameter. \"\"\" if collection is None : collection = \"parameters\" if trainable else \"states\" if self . _scope_params is None : self . _scope_params = {} if name not in self . _scope_params : if not self . is_initializing (): raise ValueError ( f \"Cannot add parameter { name } outside `init`\" ) if name in self . _default_params : parameter = self . _default_params [ name ] assert collection == parameter . collection initial_value = parameter . value else : initial_value = initializer () initial_value = jax . tree_map ( jnp . asarray , initial_value ) parameter = types . Parameter ( collection , initial_value ) self . _register_parameter ( name , parameter ) self . _scope_params [ name ] = parameter parameter = self . _scope_params [ name ] if parameter . collection != collection : raise ValueError ( f \"types.Parameter { name } previously found in collection { parameter . collection } \" f \"but currently being added for collection { collection } \" ) value = parameter . value if constraint is not None : value = constraint ( value ) if regularizer is not None : hooks . add_loss ( name = utils . get_name ( regularizer ), value = regularizer ( value ), ) return value","title":"add_parameter()"},{"location":"api/nn/Flatten/#elegy.nn.flatten.Flatten.call","text":"Parameters: Name Type Description Default inputs ndarray the array to be reshaped. required Returns: Type Description ndarray A reshaped array. Source code in elegy/nn/flatten.py def call ( self , inputs : np . ndarray ) -> np . ndarray : \"\"\" Arguments: inputs: the array to be reshaped. Returns: A reshaped array. \"\"\" if inputs . ndim <= self . preserve_dims : return inputs if - 1 in self . output_shape : reshaped_shape = _infer_shape ( self . output_shape , inputs . shape [ self . preserve_dims :] ) else : reshaped_shape = self . output_shape shape = inputs . shape [: self . preserve_dims ] + reshaped_shape return jnp . reshape ( inputs , shape )","title":"call()"},{"location":"api/nn/Flatten/#elegy.nn.flatten.Flatten.init","text":"Initializes the module, Source code in elegy/nn/flatten.py def init ( self , * , rng : tp . Optional [ types . RNGSeq ] = None , set_defaults : bool = False , ) -> tp . Callable [ ... , tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ] ]: \"\"\" Initializes the module, \"\"\" def init_callable ( * args , ** kwargs ) -> tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ]: with module_context ( module = self , collections = None , initializing = True , training = True , rng = rng ): y = self ( * args , ** kwargs ) parameters , collections = self . get_parameters_internal () self . _mark_initialized_recursive () if set_defaults : self . set_default_parameters ( parameters , collections ) return y , parameters , collections init_callable . _signature_f = self . call return init_callable","title":"init()"},{"location":"api/nn/InstanceNormalization/","text":"elegy.nn.InstanceNormalization Normalizes inputs along the spatial dimensions. See LayerNorm for more details. __init__ ( self , create_scale = True , create_offset = True , eps = 1e-05 , scale_init = None , offset_init = None , data_format = 'channels_last' , ** kwargs ) special Constructs an InstanceNormalization module. This method creates a module which normalizes over the spatial dimensions. Parameters: Name Type Description Default create_scale bool bool representing whether to create a trainable scale per channel applied after the normalization. True create_offset bool bool representing whether to create a trainable offset per channel applied after normalization and scaling. True eps float Small epsilon to avoid division by zero variance. Defaults to 1e-5 . 1e-05 scale_init Optional[elegy.types.Initializer] Optional initializer for the scale variable. Can only be set if create_scale=True . By default scale is initialized to 1 . None offset_init Optional[elegy.types.Initializer] Optional initializer for the offset variable. Can only be set if create_offset=True . By default offset is initialized to 0 . None data_format str The data format of the input. Can be either channels_first , channels_last , N...C or NC... . By default it is channels_last . 'channels_last' kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/layer_normalization.py def __init__ ( self , create_scale : bool = True , create_offset : bool = True , eps : float = 1e-5 , scale_init : Optional [ types . Initializer ] = None , offset_init : Optional [ types . Initializer ] = None , data_format : str = \"channels_last\" , ** kwargs ): \"\"\"Constructs an `InstanceNormalization` module. This method creates a module which normalizes over the spatial dimensions. Args: create_scale: ``bool`` representing whether to create a trainable scale per channel applied after the normalization. create_offset: ``bool`` representing whether to create a trainable offset per channel applied after normalization and scaling. eps: Small epsilon to avoid division by zero variance. Defaults to ``1e-5``. scale_init: Optional initializer for the scale variable. Can only be set if ``create_scale=True``. By default scale is initialized to ``1``. offset_init: Optional initializer for the offset variable. Can only be set if ``create_offset=True``. By default offset is initialized to ``0``. data_format: The data format of the input. Can be either ``channels_first``, ``channels_last``, ``N...C`` or ``NC...``. By default it is ``channels_last``. kwargs: Additional keyword arguments passed to Module. \"\"\" if hk_utils . get_channel_index ( data_format ) == 1 : axis = slice ( 2 , None ) else : # channel_index = -1 axis = slice ( 1 , - 1 ) super () . __init__ ( axis = axis , create_scale = create_scale , create_offset = create_offset , eps = eps , scale_init = scale_init , offset_init = offset_init , ** kwargs ) add_parameter ( self , name , initializer , collection = None , trainable = True , regularizer = None , constraint = None ) inherited Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required initializer Callable[[], Any] A callable that takes not arguments returns the initial value. required collection Optional[str] Optional name of the parameter collection, if not defined it will be se to \"parameters\" if trainable=True else it will be set to \"states\" . None trainable bool Specify whether this parameter should be added to the default trainable \"parameters\" collection or to the default non-trainable \"states\" collection. If collection is passed this parameter will ignored. True regularizer Optional[Callable[[Any], jax._src.numpy.lax_numpy.ndarray]] Regularizer instance (callable). None constraint Optional[Callable[[Any], Any]] Constraint instance (callable). None Returns: Type Description Any The value of the parameter. Source code in elegy/nn/layer_normalization.py def add_parameter ( self , name : str , initializer : tp . Callable [[], tp . Any ], collection : tp . Optional [ str ] = None , trainable : bool = True , regularizer : tp . Optional [ tp . Callable [[ tp . Any ], jnp . ndarray ]] = None , constraint : tp . Optional [ tp . Callable [[ tp . Any ], tp . Any ]] = None , ) -> tp . Any : \"\"\" Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. initializer: A callable that takes not arguments returns the initial value. collection: Optional name of the parameter collection, if not defined it will be se to `\"parameters\"` if `trainable=True` else it will be set to `\"states\"`. trainable: Specify whether this parameter should be added to the default trainable `\"parameters\"` collection or to the default non-trainable `\"states\"` collection. If collection is passed this parameter will ignored. regularizer: Regularizer instance (callable). constraint: Constraint instance (callable). Returns: The value of the parameter. \"\"\" if collection is None : collection = \"parameters\" if trainable else \"states\" if self . _scope_params is None : self . _scope_params = {} if name not in self . _scope_params : if not self . is_initializing (): raise ValueError ( f \"Cannot add parameter { name } outside `init`\" ) if name in self . _default_params : parameter = self . _default_params [ name ] assert collection == parameter . collection initial_value = parameter . value else : initial_value = initializer () initial_value = jax . tree_map ( jnp . asarray , initial_value ) parameter = types . Parameter ( collection , initial_value ) self . _register_parameter ( name , parameter ) self . _scope_params [ name ] = parameter parameter = self . _scope_params [ name ] if parameter . collection != collection : raise ValueError ( f \"types.Parameter { name } previously found in collection { parameter . collection } \" f \"but currently being added for collection { collection } \" ) value = parameter . value if constraint is not None : value = constraint ( value ) if regularizer is not None : hooks . add_loss ( name = utils . get_name ( regularizer ), value = regularizer ( value ), ) return value call ( self , inputs , scale = None , offset = None ) inherited Connects the layer norm. Parameters: Name Type Description Default inputs ndarray An array, where the data format is [N, ..., C] . required scale Optional[jax._src.numpy.lax_numpy.ndarray] An array up to n-D. The shape of this tensor must be broadcastable to the shape of inputs . This is the scale applied to the normalized inputs. This cannot be passed in if the module was constructed with create_scale=True . None offset Optional[jax._src.numpy.lax_numpy.ndarray] An array up to n-D. The shape of this tensor must be broadcastable to the shape of inputs . This is the offset applied to the normalized inputs. This cannot be passed in if the module was constructed with create_offset=True . None Returns: Type Description ndarray The array, normalized. Source code in elegy/nn/layer_normalization.py def call ( self , inputs : jnp . ndarray , scale : Optional [ jnp . ndarray ] = None , offset : Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\"Connects the layer norm. Args: inputs: An array, where the data format is ``[N, ..., C]``. scale: An array up to n-D. The shape of this tensor must be broadcastable to the shape of ``inputs``. This is the scale applied to the normalized inputs. This cannot be passed in if the module was constructed with ``create_scale=True``. offset: An array up to n-D. The shape of this tensor must be broadcastable to the shape of ``inputs``. This is the offset applied to the normalized inputs. This cannot be passed in if the module was constructed with ``create_offset=True``. Returns: The array, normalized. \"\"\" if self . create_scale and scale is not None : raise ValueError ( \"Cannot pass `scale` at call time if `create_scale=True`.\" ) if self . create_offset and offset is not None : raise ValueError ( \"Cannot pass `offset` at call time if `create_offset=True`.\" ) axis = self . axis if isinstance ( axis , slice ): axis = tuple ( range ( inputs . ndim )[ axis ]) mean = jnp . mean ( inputs , axis = axis , keepdims = True ) variance = jnp . var ( inputs , axis = axis , keepdims = True ) param_shape = inputs . shape [ - 1 :] if self . create_scale : scale = self . add_parameter ( \"scale\" , lambda : self . scale_init ( param_shape , jnp . float32 , ), ) elif scale is None : scale = np . array ( 1.0 , dtype = inputs . dtype ) if self . create_offset : offset = self . add_parameter ( \"offset\" , lambda : self . offset_init ( param_shape , jnp . float32 , ), ) elif offset is None : offset = np . array ( 0.0 , dtype = inputs . dtype ) scale = jnp . broadcast_to ( scale , inputs . shape ) offset = jnp . broadcast_to ( offset , inputs . shape ) mean = jnp . broadcast_to ( mean , inputs . shape ) inv = scale * jax . lax . rsqrt ( variance + self . eps ) return inv * ( inputs - mean ) + offset init ( self , * , rng = None , set_defaults = False ) inherited Initializes the module, Source code in elegy/nn/layer_normalization.py def init ( self , * , rng : tp . Optional [ types . RNGSeq ] = None , set_defaults : bool = False , ) -> tp . Callable [ ... , tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ] ]: \"\"\" Initializes the module, \"\"\" def init_callable ( * args , ** kwargs ) -> tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ]: with module_context ( module = self , collections = None , initializing = True , training = True , rng = rng ): y = self ( * args , ** kwargs ) parameters , collections = self . get_parameters_internal () self . _mark_initialized_recursive () if set_defaults : self . set_default_parameters ( parameters , collections ) return y , parameters , collections init_callable . _signature_f = self . call return init_callable","title":"InstanceNormalization"},{"location":"api/nn/InstanceNormalization/#elegynninstancenormalization","text":"","title":"elegy.nn.InstanceNormalization"},{"location":"api/nn/InstanceNormalization/#elegy.nn.layer_normalization.InstanceNormalization","text":"Normalizes inputs along the spatial dimensions. See LayerNorm for more details.","title":"elegy.nn.layer_normalization.InstanceNormalization"},{"location":"api/nn/InstanceNormalization/#elegy.nn.layer_normalization.InstanceNormalization.__init__","text":"Constructs an InstanceNormalization module. This method creates a module which normalizes over the spatial dimensions. Parameters: Name Type Description Default create_scale bool bool representing whether to create a trainable scale per channel applied after the normalization. True create_offset bool bool representing whether to create a trainable offset per channel applied after normalization and scaling. True eps float Small epsilon to avoid division by zero variance. Defaults to 1e-5 . 1e-05 scale_init Optional[elegy.types.Initializer] Optional initializer for the scale variable. Can only be set if create_scale=True . By default scale is initialized to 1 . None offset_init Optional[elegy.types.Initializer] Optional initializer for the offset variable. Can only be set if create_offset=True . By default offset is initialized to 0 . None data_format str The data format of the input. Can be either channels_first , channels_last , N...C or NC... . By default it is channels_last . 'channels_last' kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/layer_normalization.py def __init__ ( self , create_scale : bool = True , create_offset : bool = True , eps : float = 1e-5 , scale_init : Optional [ types . Initializer ] = None , offset_init : Optional [ types . Initializer ] = None , data_format : str = \"channels_last\" , ** kwargs ): \"\"\"Constructs an `InstanceNormalization` module. This method creates a module which normalizes over the spatial dimensions. Args: create_scale: ``bool`` representing whether to create a trainable scale per channel applied after the normalization. create_offset: ``bool`` representing whether to create a trainable offset per channel applied after normalization and scaling. eps: Small epsilon to avoid division by zero variance. Defaults to ``1e-5``. scale_init: Optional initializer for the scale variable. Can only be set if ``create_scale=True``. By default scale is initialized to ``1``. offset_init: Optional initializer for the offset variable. Can only be set if ``create_offset=True``. By default offset is initialized to ``0``. data_format: The data format of the input. Can be either ``channels_first``, ``channels_last``, ``N...C`` or ``NC...``. By default it is ``channels_last``. kwargs: Additional keyword arguments passed to Module. \"\"\" if hk_utils . get_channel_index ( data_format ) == 1 : axis = slice ( 2 , None ) else : # channel_index = -1 axis = slice ( 1 , - 1 ) super () . __init__ ( axis = axis , create_scale = create_scale , create_offset = create_offset , eps = eps , scale_init = scale_init , offset_init = offset_init , ** kwargs )","title":"__init__()"},{"location":"api/nn/InstanceNormalization/#elegy.nn.layer_normalization.InstanceNormalization.add_parameter","text":"Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required initializer Callable[[], Any] A callable that takes not arguments returns the initial value. required collection Optional[str] Optional name of the parameter collection, if not defined it will be se to \"parameters\" if trainable=True else it will be set to \"states\" . None trainable bool Specify whether this parameter should be added to the default trainable \"parameters\" collection or to the default non-trainable \"states\" collection. If collection is passed this parameter will ignored. True regularizer Optional[Callable[[Any], jax._src.numpy.lax_numpy.ndarray]] Regularizer instance (callable). None constraint Optional[Callable[[Any], Any]] Constraint instance (callable). None Returns: Type Description Any The value of the parameter. Source code in elegy/nn/layer_normalization.py def add_parameter ( self , name : str , initializer : tp . Callable [[], tp . Any ], collection : tp . Optional [ str ] = None , trainable : bool = True , regularizer : tp . Optional [ tp . Callable [[ tp . Any ], jnp . ndarray ]] = None , constraint : tp . Optional [ tp . Callable [[ tp . Any ], tp . Any ]] = None , ) -> tp . Any : \"\"\" Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. initializer: A callable that takes not arguments returns the initial value. collection: Optional name of the parameter collection, if not defined it will be se to `\"parameters\"` if `trainable=True` else it will be set to `\"states\"`. trainable: Specify whether this parameter should be added to the default trainable `\"parameters\"` collection or to the default non-trainable `\"states\"` collection. If collection is passed this parameter will ignored. regularizer: Regularizer instance (callable). constraint: Constraint instance (callable). Returns: The value of the parameter. \"\"\" if collection is None : collection = \"parameters\" if trainable else \"states\" if self . _scope_params is None : self . _scope_params = {} if name not in self . _scope_params : if not self . is_initializing (): raise ValueError ( f \"Cannot add parameter { name } outside `init`\" ) if name in self . _default_params : parameter = self . _default_params [ name ] assert collection == parameter . collection initial_value = parameter . value else : initial_value = initializer () initial_value = jax . tree_map ( jnp . asarray , initial_value ) parameter = types . Parameter ( collection , initial_value ) self . _register_parameter ( name , parameter ) self . _scope_params [ name ] = parameter parameter = self . _scope_params [ name ] if parameter . collection != collection : raise ValueError ( f \"types.Parameter { name } previously found in collection { parameter . collection } \" f \"but currently being added for collection { collection } \" ) value = parameter . value if constraint is not None : value = constraint ( value ) if regularizer is not None : hooks . add_loss ( name = utils . get_name ( regularizer ), value = regularizer ( value ), ) return value","title":"add_parameter()"},{"location":"api/nn/InstanceNormalization/#elegy.nn.layer_normalization.InstanceNormalization.call","text":"Connects the layer norm. Parameters: Name Type Description Default inputs ndarray An array, where the data format is [N, ..., C] . required scale Optional[jax._src.numpy.lax_numpy.ndarray] An array up to n-D. The shape of this tensor must be broadcastable to the shape of inputs . This is the scale applied to the normalized inputs. This cannot be passed in if the module was constructed with create_scale=True . None offset Optional[jax._src.numpy.lax_numpy.ndarray] An array up to n-D. The shape of this tensor must be broadcastable to the shape of inputs . This is the offset applied to the normalized inputs. This cannot be passed in if the module was constructed with create_offset=True . None Returns: Type Description ndarray The array, normalized. Source code in elegy/nn/layer_normalization.py def call ( self , inputs : jnp . ndarray , scale : Optional [ jnp . ndarray ] = None , offset : Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\"Connects the layer norm. Args: inputs: An array, where the data format is ``[N, ..., C]``. scale: An array up to n-D. The shape of this tensor must be broadcastable to the shape of ``inputs``. This is the scale applied to the normalized inputs. This cannot be passed in if the module was constructed with ``create_scale=True``. offset: An array up to n-D. The shape of this tensor must be broadcastable to the shape of ``inputs``. This is the offset applied to the normalized inputs. This cannot be passed in if the module was constructed with ``create_offset=True``. Returns: The array, normalized. \"\"\" if self . create_scale and scale is not None : raise ValueError ( \"Cannot pass `scale` at call time if `create_scale=True`.\" ) if self . create_offset and offset is not None : raise ValueError ( \"Cannot pass `offset` at call time if `create_offset=True`.\" ) axis = self . axis if isinstance ( axis , slice ): axis = tuple ( range ( inputs . ndim )[ axis ]) mean = jnp . mean ( inputs , axis = axis , keepdims = True ) variance = jnp . var ( inputs , axis = axis , keepdims = True ) param_shape = inputs . shape [ - 1 :] if self . create_scale : scale = self . add_parameter ( \"scale\" , lambda : self . scale_init ( param_shape , jnp . float32 , ), ) elif scale is None : scale = np . array ( 1.0 , dtype = inputs . dtype ) if self . create_offset : offset = self . add_parameter ( \"offset\" , lambda : self . offset_init ( param_shape , jnp . float32 , ), ) elif offset is None : offset = np . array ( 0.0 , dtype = inputs . dtype ) scale = jnp . broadcast_to ( scale , inputs . shape ) offset = jnp . broadcast_to ( offset , inputs . shape ) mean = jnp . broadcast_to ( mean , inputs . shape ) inv = scale * jax . lax . rsqrt ( variance + self . eps ) return inv * ( inputs - mean ) + offset","title":"call()"},{"location":"api/nn/InstanceNormalization/#elegy.nn.layer_normalization.InstanceNormalization.init","text":"Initializes the module, Source code in elegy/nn/layer_normalization.py def init ( self , * , rng : tp . Optional [ types . RNGSeq ] = None , set_defaults : bool = False , ) -> tp . Callable [ ... , tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ] ]: \"\"\" Initializes the module, \"\"\" def init_callable ( * args , ** kwargs ) -> tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ]: with module_context ( module = self , collections = None , initializing = True , training = True , rng = rng ): y = self ( * args , ** kwargs ) parameters , collections = self . get_parameters_internal () self . _mark_initialized_recursive () if set_defaults : self . set_default_parameters ( parameters , collections ) return y , parameters , collections init_callable . _signature_f = self . call return init_callable","title":"init()"},{"location":"api/nn/LayerNormalization/","text":"elegy.nn.LayerNormalization LayerNorm module. See: https://arxiv.org/abs/1607.06450. __init__ ( self , axis =- 1 , create_scale = True , create_offset = True , eps = 1e-05 , scale_init = None , offset_init = None , ** kwargs ) special Constructs a LayerNorm module. Parameters: Name Type Description Default axis Union[int, Sequence[int], slice] Integer, list of integers, or slice indicating which axes to normalize over. -1 create_scale bool Bool, defines whether to create a trainable scale per channel applied after the normalization. True create_offset bool Bool, defines whether to create a trainable offset per channel applied after normalization and scaling. True eps float Small epsilon to avoid division by zero variance. Defaults 1e-5 , as in the paper and Sonnet. 1e-05 scale_init Optional[elegy.types.Initializer] Optional initializer for gain (aka scale). By default, one. None offset_init Optional[elegy.types.Initializer] Optional initializer for bias (aka offset). By default, zero. None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/layer_normalization.py def __init__ ( self , axis : Union [ int , Sequence [ int ], slice ] = - 1 , create_scale : bool = True , create_offset : bool = True , eps : float = 1e-5 , scale_init : Optional [ types . Initializer ] = None , offset_init : Optional [ types . Initializer ] = None , ** kwargs ): \"\"\"Constructs a LayerNorm module. Args: axis: Integer, list of integers, or slice indicating which axes to normalize over. create_scale: Bool, defines whether to create a trainable scale per channel applied after the normalization. create_offset: Bool, defines whether to create a trainable offset per channel applied after normalization and scaling. eps: Small epsilon to avoid division by zero variance. Defaults ``1e-5``, as in the paper and Sonnet. scale_init: Optional initializer for gain (aka scale). By default, one. offset_init: Optional initializer for bias (aka offset). By default, zero. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( ** kwargs ) if not create_scale and scale_init is not None : raise ValueError ( \"Cannot set `scale_init` if `create_scale=False`.\" ) if not create_offset and offset_init is not None : raise ValueError ( \"Cannot set `offset_init` if `create_offset=False`.\" ) if isinstance ( axis , slice ): self . axis = axis elif isinstance ( axis , int ): self . axis = ( axis ,) elif isinstance ( axis , collections . Iterable ) and all ( isinstance ( ax , int ) for ax in axis ): self . axis = tuple ( axis ) else : raise ValueError ( \"`axis` should be an int, slice or iterable of ints.\" ) self . eps = eps self . create_scale = create_scale self . create_offset = create_offset self . scale_init = scale_init or jnp . ones self . offset_init = offset_init or jnp . zeros add_parameter ( self , name , initializer , collection = None , trainable = True , regularizer = None , constraint = None ) inherited Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required initializer Callable[[], Any] A callable that takes not arguments returns the initial value. required collection Optional[str] Optional name of the parameter collection, if not defined it will be se to \"parameters\" if trainable=True else it will be set to \"states\" . None trainable bool Specify whether this parameter should be added to the default trainable \"parameters\" collection or to the default non-trainable \"states\" collection. If collection is passed this parameter will ignored. True regularizer Optional[Callable[[Any], jax._src.numpy.lax_numpy.ndarray]] Regularizer instance (callable). None constraint Optional[Callable[[Any], Any]] Constraint instance (callable). None Returns: Type Description Any The value of the parameter. Source code in elegy/nn/layer_normalization.py def add_parameter ( self , name : str , initializer : tp . Callable [[], tp . Any ], collection : tp . Optional [ str ] = None , trainable : bool = True , regularizer : tp . Optional [ tp . Callable [[ tp . Any ], jnp . ndarray ]] = None , constraint : tp . Optional [ tp . Callable [[ tp . Any ], tp . Any ]] = None , ) -> tp . Any : \"\"\" Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. initializer: A callable that takes not arguments returns the initial value. collection: Optional name of the parameter collection, if not defined it will be se to `\"parameters\"` if `trainable=True` else it will be set to `\"states\"`. trainable: Specify whether this parameter should be added to the default trainable `\"parameters\"` collection or to the default non-trainable `\"states\"` collection. If collection is passed this parameter will ignored. regularizer: Regularizer instance (callable). constraint: Constraint instance (callable). Returns: The value of the parameter. \"\"\" if collection is None : collection = \"parameters\" if trainable else \"states\" if self . _scope_params is None : self . _scope_params = {} if name not in self . _scope_params : if not self . is_initializing (): raise ValueError ( f \"Cannot add parameter { name } outside `init`\" ) if name in self . _default_params : parameter = self . _default_params [ name ] assert collection == parameter . collection initial_value = parameter . value else : initial_value = initializer () initial_value = jax . tree_map ( jnp . asarray , initial_value ) parameter = types . Parameter ( collection , initial_value ) self . _register_parameter ( name , parameter ) self . _scope_params [ name ] = parameter parameter = self . _scope_params [ name ] if parameter . collection != collection : raise ValueError ( f \"types.Parameter { name } previously found in collection { parameter . collection } \" f \"but currently being added for collection { collection } \" ) value = parameter . value if constraint is not None : value = constraint ( value ) if regularizer is not None : hooks . add_loss ( name = utils . get_name ( regularizer ), value = regularizer ( value ), ) return value call ( self , inputs , scale = None , offset = None ) Connects the layer norm. Parameters: Name Type Description Default inputs ndarray An array, where the data format is [N, ..., C] . required scale Optional[jax._src.numpy.lax_numpy.ndarray] An array up to n-D. The shape of this tensor must be broadcastable to the shape of inputs . This is the scale applied to the normalized inputs. This cannot be passed in if the module was constructed with create_scale=True . None offset Optional[jax._src.numpy.lax_numpy.ndarray] An array up to n-D. The shape of this tensor must be broadcastable to the shape of inputs . This is the offset applied to the normalized inputs. This cannot be passed in if the module was constructed with create_offset=True . None Returns: Type Description ndarray The array, normalized. Source code in elegy/nn/layer_normalization.py def call ( self , inputs : jnp . ndarray , scale : Optional [ jnp . ndarray ] = None , offset : Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\"Connects the layer norm. Args: inputs: An array, where the data format is ``[N, ..., C]``. scale: An array up to n-D. The shape of this tensor must be broadcastable to the shape of ``inputs``. This is the scale applied to the normalized inputs. This cannot be passed in if the module was constructed with ``create_scale=True``. offset: An array up to n-D. The shape of this tensor must be broadcastable to the shape of ``inputs``. This is the offset applied to the normalized inputs. This cannot be passed in if the module was constructed with ``create_offset=True``. Returns: The array, normalized. \"\"\" if self . create_scale and scale is not None : raise ValueError ( \"Cannot pass `scale` at call time if `create_scale=True`.\" ) if self . create_offset and offset is not None : raise ValueError ( \"Cannot pass `offset` at call time if `create_offset=True`.\" ) axis = self . axis if isinstance ( axis , slice ): axis = tuple ( range ( inputs . ndim )[ axis ]) mean = jnp . mean ( inputs , axis = axis , keepdims = True ) variance = jnp . var ( inputs , axis = axis , keepdims = True ) param_shape = inputs . shape [ - 1 :] if self . create_scale : scale = self . add_parameter ( \"scale\" , lambda : self . scale_init ( param_shape , jnp . float32 , ), ) elif scale is None : scale = np . array ( 1.0 , dtype = inputs . dtype ) if self . create_offset : offset = self . add_parameter ( \"offset\" , lambda : self . offset_init ( param_shape , jnp . float32 , ), ) elif offset is None : offset = np . array ( 0.0 , dtype = inputs . dtype ) scale = jnp . broadcast_to ( scale , inputs . shape ) offset = jnp . broadcast_to ( offset , inputs . shape ) mean = jnp . broadcast_to ( mean , inputs . shape ) inv = scale * jax . lax . rsqrt ( variance + self . eps ) return inv * ( inputs - mean ) + offset init ( self , * , rng = None , set_defaults = False ) inherited Initializes the module, Source code in elegy/nn/layer_normalization.py def init ( self , * , rng : tp . Optional [ types . RNGSeq ] = None , set_defaults : bool = False , ) -> tp . Callable [ ... , tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ] ]: \"\"\" Initializes the module, \"\"\" def init_callable ( * args , ** kwargs ) -> tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ]: with module_context ( module = self , collections = None , initializing = True , training = True , rng = rng ): y = self ( * args , ** kwargs ) parameters , collections = self . get_parameters_internal () self . _mark_initialized_recursive () if set_defaults : self . set_default_parameters ( parameters , collections ) return y , parameters , collections init_callable . _signature_f = self . call return init_callable","title":"LayerNormalization"},{"location":"api/nn/LayerNormalization/#elegynnlayernormalization","text":"","title":"elegy.nn.LayerNormalization"},{"location":"api/nn/LayerNormalization/#elegy.nn.layer_normalization.LayerNormalization","text":"LayerNorm module. See: https://arxiv.org/abs/1607.06450.","title":"elegy.nn.layer_normalization.LayerNormalization"},{"location":"api/nn/LayerNormalization/#elegy.nn.layer_normalization.LayerNormalization.__init__","text":"Constructs a LayerNorm module. Parameters: Name Type Description Default axis Union[int, Sequence[int], slice] Integer, list of integers, or slice indicating which axes to normalize over. -1 create_scale bool Bool, defines whether to create a trainable scale per channel applied after the normalization. True create_offset bool Bool, defines whether to create a trainable offset per channel applied after normalization and scaling. True eps float Small epsilon to avoid division by zero variance. Defaults 1e-5 , as in the paper and Sonnet. 1e-05 scale_init Optional[elegy.types.Initializer] Optional initializer for gain (aka scale). By default, one. None offset_init Optional[elegy.types.Initializer] Optional initializer for bias (aka offset). By default, zero. None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/layer_normalization.py def __init__ ( self , axis : Union [ int , Sequence [ int ], slice ] = - 1 , create_scale : bool = True , create_offset : bool = True , eps : float = 1e-5 , scale_init : Optional [ types . Initializer ] = None , offset_init : Optional [ types . Initializer ] = None , ** kwargs ): \"\"\"Constructs a LayerNorm module. Args: axis: Integer, list of integers, or slice indicating which axes to normalize over. create_scale: Bool, defines whether to create a trainable scale per channel applied after the normalization. create_offset: Bool, defines whether to create a trainable offset per channel applied after normalization and scaling. eps: Small epsilon to avoid division by zero variance. Defaults ``1e-5``, as in the paper and Sonnet. scale_init: Optional initializer for gain (aka scale). By default, one. offset_init: Optional initializer for bias (aka offset). By default, zero. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( ** kwargs ) if not create_scale and scale_init is not None : raise ValueError ( \"Cannot set `scale_init` if `create_scale=False`.\" ) if not create_offset and offset_init is not None : raise ValueError ( \"Cannot set `offset_init` if `create_offset=False`.\" ) if isinstance ( axis , slice ): self . axis = axis elif isinstance ( axis , int ): self . axis = ( axis ,) elif isinstance ( axis , collections . Iterable ) and all ( isinstance ( ax , int ) for ax in axis ): self . axis = tuple ( axis ) else : raise ValueError ( \"`axis` should be an int, slice or iterable of ints.\" ) self . eps = eps self . create_scale = create_scale self . create_offset = create_offset self . scale_init = scale_init or jnp . ones self . offset_init = offset_init or jnp . zeros","title":"__init__()"},{"location":"api/nn/LayerNormalization/#elegy.nn.layer_normalization.LayerNormalization.add_parameter","text":"Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required initializer Callable[[], Any] A callable that takes not arguments returns the initial value. required collection Optional[str] Optional name of the parameter collection, if not defined it will be se to \"parameters\" if trainable=True else it will be set to \"states\" . None trainable bool Specify whether this parameter should be added to the default trainable \"parameters\" collection or to the default non-trainable \"states\" collection. If collection is passed this parameter will ignored. True regularizer Optional[Callable[[Any], jax._src.numpy.lax_numpy.ndarray]] Regularizer instance (callable). None constraint Optional[Callable[[Any], Any]] Constraint instance (callable). None Returns: Type Description Any The value of the parameter. Source code in elegy/nn/layer_normalization.py def add_parameter ( self , name : str , initializer : tp . Callable [[], tp . Any ], collection : tp . Optional [ str ] = None , trainable : bool = True , regularizer : tp . Optional [ tp . Callable [[ tp . Any ], jnp . ndarray ]] = None , constraint : tp . Optional [ tp . Callable [[ tp . Any ], tp . Any ]] = None , ) -> tp . Any : \"\"\" Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. initializer: A callable that takes not arguments returns the initial value. collection: Optional name of the parameter collection, if not defined it will be se to `\"parameters\"` if `trainable=True` else it will be set to `\"states\"`. trainable: Specify whether this parameter should be added to the default trainable `\"parameters\"` collection or to the default non-trainable `\"states\"` collection. If collection is passed this parameter will ignored. regularizer: Regularizer instance (callable). constraint: Constraint instance (callable). Returns: The value of the parameter. \"\"\" if collection is None : collection = \"parameters\" if trainable else \"states\" if self . _scope_params is None : self . _scope_params = {} if name not in self . _scope_params : if not self . is_initializing (): raise ValueError ( f \"Cannot add parameter { name } outside `init`\" ) if name in self . _default_params : parameter = self . _default_params [ name ] assert collection == parameter . collection initial_value = parameter . value else : initial_value = initializer () initial_value = jax . tree_map ( jnp . asarray , initial_value ) parameter = types . Parameter ( collection , initial_value ) self . _register_parameter ( name , parameter ) self . _scope_params [ name ] = parameter parameter = self . _scope_params [ name ] if parameter . collection != collection : raise ValueError ( f \"types.Parameter { name } previously found in collection { parameter . collection } \" f \"but currently being added for collection { collection } \" ) value = parameter . value if constraint is not None : value = constraint ( value ) if regularizer is not None : hooks . add_loss ( name = utils . get_name ( regularizer ), value = regularizer ( value ), ) return value","title":"add_parameter()"},{"location":"api/nn/LayerNormalization/#elegy.nn.layer_normalization.LayerNormalization.call","text":"Connects the layer norm. Parameters: Name Type Description Default inputs ndarray An array, where the data format is [N, ..., C] . required scale Optional[jax._src.numpy.lax_numpy.ndarray] An array up to n-D. The shape of this tensor must be broadcastable to the shape of inputs . This is the scale applied to the normalized inputs. This cannot be passed in if the module was constructed with create_scale=True . None offset Optional[jax._src.numpy.lax_numpy.ndarray] An array up to n-D. The shape of this tensor must be broadcastable to the shape of inputs . This is the offset applied to the normalized inputs. This cannot be passed in if the module was constructed with create_offset=True . None Returns: Type Description ndarray The array, normalized. Source code in elegy/nn/layer_normalization.py def call ( self , inputs : jnp . ndarray , scale : Optional [ jnp . ndarray ] = None , offset : Optional [ jnp . ndarray ] = None , ) -> jnp . ndarray : \"\"\"Connects the layer norm. Args: inputs: An array, where the data format is ``[N, ..., C]``. scale: An array up to n-D. The shape of this tensor must be broadcastable to the shape of ``inputs``. This is the scale applied to the normalized inputs. This cannot be passed in if the module was constructed with ``create_scale=True``. offset: An array up to n-D. The shape of this tensor must be broadcastable to the shape of ``inputs``. This is the offset applied to the normalized inputs. This cannot be passed in if the module was constructed with ``create_offset=True``. Returns: The array, normalized. \"\"\" if self . create_scale and scale is not None : raise ValueError ( \"Cannot pass `scale` at call time if `create_scale=True`.\" ) if self . create_offset and offset is not None : raise ValueError ( \"Cannot pass `offset` at call time if `create_offset=True`.\" ) axis = self . axis if isinstance ( axis , slice ): axis = tuple ( range ( inputs . ndim )[ axis ]) mean = jnp . mean ( inputs , axis = axis , keepdims = True ) variance = jnp . var ( inputs , axis = axis , keepdims = True ) param_shape = inputs . shape [ - 1 :] if self . create_scale : scale = self . add_parameter ( \"scale\" , lambda : self . scale_init ( param_shape , jnp . float32 , ), ) elif scale is None : scale = np . array ( 1.0 , dtype = inputs . dtype ) if self . create_offset : offset = self . add_parameter ( \"offset\" , lambda : self . offset_init ( param_shape , jnp . float32 , ), ) elif offset is None : offset = np . array ( 0.0 , dtype = inputs . dtype ) scale = jnp . broadcast_to ( scale , inputs . shape ) offset = jnp . broadcast_to ( offset , inputs . shape ) mean = jnp . broadcast_to ( mean , inputs . shape ) inv = scale * jax . lax . rsqrt ( variance + self . eps ) return inv * ( inputs - mean ) + offset","title":"call()"},{"location":"api/nn/LayerNormalization/#elegy.nn.layer_normalization.LayerNormalization.init","text":"Initializes the module, Source code in elegy/nn/layer_normalization.py def init ( self , * , rng : tp . Optional [ types . RNGSeq ] = None , set_defaults : bool = False , ) -> tp . Callable [ ... , tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ] ]: \"\"\" Initializes the module, \"\"\" def init_callable ( * args , ** kwargs ) -> tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ]: with module_context ( module = self , collections = None , initializing = True , training = True , rng = rng ): y = self ( * args , ** kwargs ) parameters , collections = self . get_parameters_internal () self . _mark_initialized_recursive () if set_defaults : self . set_default_parameters ( parameters , collections ) return y , parameters , collections init_callable . _signature_f = self . call return init_callable","title":"init()"},{"location":"api/nn/Linear/","text":"elegy.nn.Linear Linear module. __init__ ( self , output_size , with_bias = True , w_init = None , b_init = None , ** kwargs ) special Constructs the Linear module. Parameters: Name Type Description Default output_size int Output dimensionality. required with_bias bool Whether to add a bias to the output. True w_init Optional[elegy.types.Initializer] Optional initializer for weights. By default, uses random values from truncated normal, with stddev 1 / sqrt(fan_in) . See https://arxiv.org/abs/1502.03167v3. None b_init Optional[elegy.types.Initializer] Optional initializer for bias. By default, zero. None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/linear.py def __init__ ( self , output_size : int , with_bias : bool = True , w_init : tp . Optional [ types . Initializer ] = None , b_init : tp . Optional [ types . Initializer ] = None , ** kwargs ): \"\"\" Constructs the Linear module. Arguments: output_size: Output dimensionality. with_bias: Whether to add a bias to the output. w_init: Optional initializer for weights. By default, uses random values from truncated normal, with stddev `1 / sqrt(fan_in)`. See https://arxiv.org/abs/1502.03167v3. b_init: Optional initializer for bias. By default, zero. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( ** kwargs ) self . input_size = None self . output_size = output_size self . with_bias = with_bias self . w_init = w_init self . b_init = b_init or jnp . zeros add_parameter ( self , name , initializer , collection = None , trainable = True , regularizer = None , constraint = None ) inherited Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required initializer Callable[[], Any] A callable that takes not arguments returns the initial value. required collection Optional[str] Optional name of the parameter collection, if not defined it will be se to \"parameters\" if trainable=True else it will be set to \"states\" . None trainable bool Specify whether this parameter should be added to the default trainable \"parameters\" collection or to the default non-trainable \"states\" collection. If collection is passed this parameter will ignored. True regularizer Optional[Callable[[Any], jax._src.numpy.lax_numpy.ndarray]] Regularizer instance (callable). None constraint Optional[Callable[[Any], Any]] Constraint instance (callable). None Returns: Type Description Any The value of the parameter. Source code in elegy/nn/linear.py def add_parameter ( self , name : str , initializer : tp . Callable [[], tp . Any ], collection : tp . Optional [ str ] = None , trainable : bool = True , regularizer : tp . Optional [ tp . Callable [[ tp . Any ], jnp . ndarray ]] = None , constraint : tp . Optional [ tp . Callable [[ tp . Any ], tp . Any ]] = None , ) -> tp . Any : \"\"\" Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. initializer: A callable that takes not arguments returns the initial value. collection: Optional name of the parameter collection, if not defined it will be se to `\"parameters\"` if `trainable=True` else it will be set to `\"states\"`. trainable: Specify whether this parameter should be added to the default trainable `\"parameters\"` collection or to the default non-trainable `\"states\"` collection. If collection is passed this parameter will ignored. regularizer: Regularizer instance (callable). constraint: Constraint instance (callable). Returns: The value of the parameter. \"\"\" if collection is None : collection = \"parameters\" if trainable else \"states\" if self . _scope_params is None : self . _scope_params = {} if name not in self . _scope_params : if not self . is_initializing (): raise ValueError ( f \"Cannot add parameter { name } outside `init`\" ) if name in self . _default_params : parameter = self . _default_params [ name ] assert collection == parameter . collection initial_value = parameter . value else : initial_value = initializer () initial_value = jax . tree_map ( jnp . asarray , initial_value ) parameter = types . Parameter ( collection , initial_value ) self . _register_parameter ( name , parameter ) self . _scope_params [ name ] = parameter parameter = self . _scope_params [ name ] if parameter . collection != collection : raise ValueError ( f \"types.Parameter { name } previously found in collection { parameter . collection } \" f \"but currently being added for collection { collection } \" ) value = parameter . value if constraint is not None : value = constraint ( value ) if regularizer is not None : hooks . add_loss ( name = utils . get_name ( regularizer ), value = regularizer ( value ), ) return value init ( self , * , rng = None , set_defaults = False ) inherited Initializes the module, Source code in elegy/nn/linear.py def init ( self , * , rng : tp . Optional [ types . RNGSeq ] = None , set_defaults : bool = False , ) -> tp . Callable [ ... , tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ] ]: \"\"\" Initializes the module, \"\"\" def init_callable ( * args , ** kwargs ) -> tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ]: with module_context ( module = self , collections = None , initializing = True , training = True , rng = rng ): y = self ( * args , ** kwargs ) parameters , collections = self . get_parameters_internal () self . _mark_initialized_recursive () if set_defaults : self . set_default_parameters ( parameters , collections ) return y , parameters , collections init_callable . _signature_f = self . call return init_callable","title":"Linear"},{"location":"api/nn/Linear/#elegynnlinear","text":"","title":"elegy.nn.Linear"},{"location":"api/nn/Linear/#elegy.nn.linear.Linear","text":"Linear module.","title":"elegy.nn.linear.Linear"},{"location":"api/nn/Linear/#elegy.nn.linear.Linear.__init__","text":"Constructs the Linear module. Parameters: Name Type Description Default output_size int Output dimensionality. required with_bias bool Whether to add a bias to the output. True w_init Optional[elegy.types.Initializer] Optional initializer for weights. By default, uses random values from truncated normal, with stddev 1 / sqrt(fan_in) . See https://arxiv.org/abs/1502.03167v3. None b_init Optional[elegy.types.Initializer] Optional initializer for bias. By default, zero. None kwargs Additional keyword arguments passed to Module. {} Source code in elegy/nn/linear.py def __init__ ( self , output_size : int , with_bias : bool = True , w_init : tp . Optional [ types . Initializer ] = None , b_init : tp . Optional [ types . Initializer ] = None , ** kwargs ): \"\"\" Constructs the Linear module. Arguments: output_size: Output dimensionality. with_bias: Whether to add a bias to the output. w_init: Optional initializer for weights. By default, uses random values from truncated normal, with stddev `1 / sqrt(fan_in)`. See https://arxiv.org/abs/1502.03167v3. b_init: Optional initializer for bias. By default, zero. kwargs: Additional keyword arguments passed to Module. \"\"\" super () . __init__ ( ** kwargs ) self . input_size = None self . output_size = output_size self . with_bias = with_bias self . w_init = w_init self . b_init = b_init or jnp . zeros","title":"__init__()"},{"location":"api/nn/Linear/#elegy.nn.linear.Linear.add_parameter","text":"Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required initializer Callable[[], Any] A callable that takes not arguments returns the initial value. required collection Optional[str] Optional name of the parameter collection, if not defined it will be se to \"parameters\" if trainable=True else it will be set to \"states\" . None trainable bool Specify whether this parameter should be added to the default trainable \"parameters\" collection or to the default non-trainable \"states\" collection. If collection is passed this parameter will ignored. True regularizer Optional[Callable[[Any], jax._src.numpy.lax_numpy.ndarray]] Regularizer instance (callable). None constraint Optional[Callable[[Any], Any]] Constraint instance (callable). None Returns: Type Description Any The value of the parameter. Source code in elegy/nn/linear.py def add_parameter ( self , name : str , initializer : tp . Callable [[], tp . Any ], collection : tp . Optional [ str ] = None , trainable : bool = True , regularizer : tp . Optional [ tp . Callable [[ tp . Any ], jnp . ndarray ]] = None , constraint : tp . Optional [ tp . Callable [[ tp . Any ], tp . Any ]] = None , ) -> tp . Any : \"\"\" Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. initializer: A callable that takes not arguments returns the initial value. collection: Optional name of the parameter collection, if not defined it will be se to `\"parameters\"` if `trainable=True` else it will be set to `\"states\"`. trainable: Specify whether this parameter should be added to the default trainable `\"parameters\"` collection or to the default non-trainable `\"states\"` collection. If collection is passed this parameter will ignored. regularizer: Regularizer instance (callable). constraint: Constraint instance (callable). Returns: The value of the parameter. \"\"\" if collection is None : collection = \"parameters\" if trainable else \"states\" if self . _scope_params is None : self . _scope_params = {} if name not in self . _scope_params : if not self . is_initializing (): raise ValueError ( f \"Cannot add parameter { name } outside `init`\" ) if name in self . _default_params : parameter = self . _default_params [ name ] assert collection == parameter . collection initial_value = parameter . value else : initial_value = initializer () initial_value = jax . tree_map ( jnp . asarray , initial_value ) parameter = types . Parameter ( collection , initial_value ) self . _register_parameter ( name , parameter ) self . _scope_params [ name ] = parameter parameter = self . _scope_params [ name ] if parameter . collection != collection : raise ValueError ( f \"types.Parameter { name } previously found in collection { parameter . collection } \" f \"but currently being added for collection { collection } \" ) value = parameter . value if constraint is not None : value = constraint ( value ) if regularizer is not None : hooks . add_loss ( name = utils . get_name ( regularizer ), value = regularizer ( value ), ) return value","title":"add_parameter()"},{"location":"api/nn/Linear/#elegy.nn.linear.Linear.init","text":"Initializes the module, Source code in elegy/nn/linear.py def init ( self , * , rng : tp . Optional [ types . RNGSeq ] = None , set_defaults : bool = False , ) -> tp . Callable [ ... , tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ] ]: \"\"\" Initializes the module, \"\"\" def init_callable ( * args , ** kwargs ) -> tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ]: with module_context ( module = self , collections = None , initializing = True , training = True , rng = rng ): y = self ( * args , ** kwargs ) parameters , collections = self . get_parameters_internal () self . _mark_initialized_recursive () if set_defaults : self . set_default_parameters ( parameters , collections ) return y , parameters , collections init_callable . _signature_f = self . call return init_callable","title":"init()"},{"location":"api/nn/MaxPool/","text":"elegy.nn.MaxPool Max pool. Equivalent to partial application of :func: max_pool . __init__ ( self , window_shape , strides , padding , channel_axis =- 1 , name = None ) special Max pool. Parameters: Name Type Description Default window_shape Union[int, Sequence[int]] Shape of window to pool over. Same rank as value or int . required strides Union[int, Sequence[int]] Strides for the window. Same rank as value or int . required padding str Padding algorithm. Either VALID or SAME . required channel_axis Optional[int] Axis of the spatial channels for which pooling is skipped. -1 Source code in elegy/nn/pool.py def __init__ ( self , window_shape : Union [ int , Sequence [ int ]], strides : Union [ int , Sequence [ int ]], padding : str , channel_axis : Optional [ int ] = - 1 , name : Optional [ str ] = None , ): \"\"\"Max pool. Args: window_shape: Shape of window to pool over. Same rank as value or ``int``. strides: Strides for the window. Same rank as value or ``int``. padding: Padding algorithm. Either ``VALID`` or ``SAME``. channel_axis: Axis of the spatial channels for which pooling is skipped. \"\"\" super () . __init__ ( name = name ) self . window_shape = window_shape self . strides = strides self . padding = padding self . channel_axis = channel_axis add_parameter ( self , name , initializer , collection = None , trainable = True , regularizer = None , constraint = None ) inherited Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required initializer Callable[[], Any] A callable that takes not arguments returns the initial value. required collection Optional[str] Optional name of the parameter collection, if not defined it will be se to \"parameters\" if trainable=True else it will be set to \"states\" . None trainable bool Specify whether this parameter should be added to the default trainable \"parameters\" collection or to the default non-trainable \"states\" collection. If collection is passed this parameter will ignored. True regularizer Optional[Callable[[Any], jax._src.numpy.lax_numpy.ndarray]] Regularizer instance (callable). None constraint Optional[Callable[[Any], Any]] Constraint instance (callable). None Returns: Type Description Any The value of the parameter. Source code in elegy/nn/pool.py def add_parameter ( self , name : str , initializer : tp . Callable [[], tp . Any ], collection : tp . Optional [ str ] = None , trainable : bool = True , regularizer : tp . Optional [ tp . Callable [[ tp . Any ], jnp . ndarray ]] = None , constraint : tp . Optional [ tp . Callable [[ tp . Any ], tp . Any ]] = None , ) -> tp . Any : \"\"\" Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. initializer: A callable that takes not arguments returns the initial value. collection: Optional name of the parameter collection, if not defined it will be se to `\"parameters\"` if `trainable=True` else it will be set to `\"states\"`. trainable: Specify whether this parameter should be added to the default trainable `\"parameters\"` collection or to the default non-trainable `\"states\"` collection. If collection is passed this parameter will ignored. regularizer: Regularizer instance (callable). constraint: Constraint instance (callable). Returns: The value of the parameter. \"\"\" if collection is None : collection = \"parameters\" if trainable else \"states\" if self . _scope_params is None : self . _scope_params = {} if name not in self . _scope_params : if not self . is_initializing (): raise ValueError ( f \"Cannot add parameter { name } outside `init`\" ) if name in self . _default_params : parameter = self . _default_params [ name ] assert collection == parameter . collection initial_value = parameter . value else : initial_value = initializer () initial_value = jax . tree_map ( jnp . asarray , initial_value ) parameter = types . Parameter ( collection , initial_value ) self . _register_parameter ( name , parameter ) self . _scope_params [ name ] = parameter parameter = self . _scope_params [ name ] if parameter . collection != collection : raise ValueError ( f \"types.Parameter { name } previously found in collection { parameter . collection } \" f \"but currently being added for collection { collection } \" ) value = parameter . value if constraint is not None : value = constraint ( value ) if regularizer is not None : hooks . add_loss ( name = utils . get_name ( regularizer ), value = regularizer ( value ), ) return value init ( self , * , rng = None , set_defaults = False ) inherited Initializes the module, Source code in elegy/nn/pool.py def init ( self , * , rng : tp . Optional [ types . RNGSeq ] = None , set_defaults : bool = False , ) -> tp . Callable [ ... , tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ] ]: \"\"\" Initializes the module, \"\"\" def init_callable ( * args , ** kwargs ) -> tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ]: with module_context ( module = self , collections = None , initializing = True , training = True , rng = rng ): y = self ( * args , ** kwargs ) parameters , collections = self . get_parameters_internal () self . _mark_initialized_recursive () if set_defaults : self . set_default_parameters ( parameters , collections ) return y , parameters , collections init_callable . _signature_f = self . call return init_callable","title":"MaxPool"},{"location":"api/nn/MaxPool/#elegynnmaxpool","text":"","title":"elegy.nn.MaxPool"},{"location":"api/nn/MaxPool/#elegy.nn.pool.MaxPool","text":"Max pool. Equivalent to partial application of :func: max_pool .","title":"elegy.nn.pool.MaxPool"},{"location":"api/nn/MaxPool/#elegy.nn.pool.MaxPool.__init__","text":"Max pool. Parameters: Name Type Description Default window_shape Union[int, Sequence[int]] Shape of window to pool over. Same rank as value or int . required strides Union[int, Sequence[int]] Strides for the window. Same rank as value or int . required padding str Padding algorithm. Either VALID or SAME . required channel_axis Optional[int] Axis of the spatial channels for which pooling is skipped. -1 Source code in elegy/nn/pool.py def __init__ ( self , window_shape : Union [ int , Sequence [ int ]], strides : Union [ int , Sequence [ int ]], padding : str , channel_axis : Optional [ int ] = - 1 , name : Optional [ str ] = None , ): \"\"\"Max pool. Args: window_shape: Shape of window to pool over. Same rank as value or ``int``. strides: Strides for the window. Same rank as value or ``int``. padding: Padding algorithm. Either ``VALID`` or ``SAME``. channel_axis: Axis of the spatial channels for which pooling is skipped. \"\"\" super () . __init__ ( name = name ) self . window_shape = window_shape self . strides = strides self . padding = padding self . channel_axis = channel_axis","title":"__init__()"},{"location":"api/nn/MaxPool/#elegy.nn.pool.MaxPool.add_parameter","text":"Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required initializer Callable[[], Any] A callable that takes not arguments returns the initial value. required collection Optional[str] Optional name of the parameter collection, if not defined it will be se to \"parameters\" if trainable=True else it will be set to \"states\" . None trainable bool Specify whether this parameter should be added to the default trainable \"parameters\" collection or to the default non-trainable \"states\" collection. If collection is passed this parameter will ignored. True regularizer Optional[Callable[[Any], jax._src.numpy.lax_numpy.ndarray]] Regularizer instance (callable). None constraint Optional[Callable[[Any], Any]] Constraint instance (callable). None Returns: Type Description Any The value of the parameter. Source code in elegy/nn/pool.py def add_parameter ( self , name : str , initializer : tp . Callable [[], tp . Any ], collection : tp . Optional [ str ] = None , trainable : bool = True , regularizer : tp . Optional [ tp . Callable [[ tp . Any ], jnp . ndarray ]] = None , constraint : tp . Optional [ tp . Callable [[ tp . Any ], tp . Any ]] = None , ) -> tp . Any : \"\"\" Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. initializer: A callable that takes not arguments returns the initial value. collection: Optional name of the parameter collection, if not defined it will be se to `\"parameters\"` if `trainable=True` else it will be set to `\"states\"`. trainable: Specify whether this parameter should be added to the default trainable `\"parameters\"` collection or to the default non-trainable `\"states\"` collection. If collection is passed this parameter will ignored. regularizer: Regularizer instance (callable). constraint: Constraint instance (callable). Returns: The value of the parameter. \"\"\" if collection is None : collection = \"parameters\" if trainable else \"states\" if self . _scope_params is None : self . _scope_params = {} if name not in self . _scope_params : if not self . is_initializing (): raise ValueError ( f \"Cannot add parameter { name } outside `init`\" ) if name in self . _default_params : parameter = self . _default_params [ name ] assert collection == parameter . collection initial_value = parameter . value else : initial_value = initializer () initial_value = jax . tree_map ( jnp . asarray , initial_value ) parameter = types . Parameter ( collection , initial_value ) self . _register_parameter ( name , parameter ) self . _scope_params [ name ] = parameter parameter = self . _scope_params [ name ] if parameter . collection != collection : raise ValueError ( f \"types.Parameter { name } previously found in collection { parameter . collection } \" f \"but currently being added for collection { collection } \" ) value = parameter . value if constraint is not None : value = constraint ( value ) if regularizer is not None : hooks . add_loss ( name = utils . get_name ( regularizer ), value = regularizer ( value ), ) return value","title":"add_parameter()"},{"location":"api/nn/MaxPool/#elegy.nn.pool.MaxPool.init","text":"Initializes the module, Source code in elegy/nn/pool.py def init ( self , * , rng : tp . Optional [ types . RNGSeq ] = None , set_defaults : bool = False , ) -> tp . Callable [ ... , tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ] ]: \"\"\" Initializes the module, \"\"\" def init_callable ( * args , ** kwargs ) -> tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ]: with module_context ( module = self , collections = None , initializing = True , training = True , rng = rng ): y = self ( * args , ** kwargs ) parameters , collections = self . get_parameters_internal () self . _mark_initialized_recursive () if set_defaults : self . set_default_parameters ( parameters , collections ) return y , parameters , collections init_callable . _signature_f = self . call return init_callable","title":"init()"},{"location":"api/nn/MultiHeadAttention/","text":"elegy.nn.MultiHeadAttention MultiHead Attention layer. Defines the MultiHead Attention operation as described in Attention Is All You Need which takes in the tensors query , key , and value , and returns the dot-product attention between them: mha = MultiHeadAttention(head_size =128 , num_heads =12 ) query = tf . random . uniform(( 32 , 20 , 200 )) # (batch_size, query_elements, query_depth) key = tf . random . uniform(( 32 , 15 , 300 )) # (batch_size, key_elements, key_depth) value = tf . random . uniform(( 32 , 15 , 400 )) # (batch_size, key_elements, value_depth) attention = mha([query, key, value]) # (batch_size, query_elements, value_depth) If value is not given then internally value = key will be used: mha = MultiHeadAttention(head_size =128 , num_heads =12 ) query = tf . random . uniform(( 32 , 20 , 200 )) # (batch_size, query_elements, query_depth) key = tf . random . uniform(( 32 , 15 , 300 )) # (batch_size, key_elements, key_depth) attention = mha([query, key]) # (batch_size, query_elements, key_depth) Parameters: Name Type Description Default head_size int, dimensionality of the query , key and value tensors required num_heads int, number of attention heads. required output_size int, dimensionality of the output space, if None then the required dropout float, rate parameter for the dropout layer that is required use_projection_bias bool, whether to use a bias term after the linear required return_attn_coef bool, if True , return the attention coefficients as required kernel_initializer initializer, initializer for the kernel weights. required kernel_regularizer regularizer, regularizer for the kernel weights. required kernel_constraint constraint, constraint for the kernel weights. required bias_initializer initializer, initializer for the bias weights. required bias_regularizer regularizer, regularizer for the bias weights. required bias_constraint constraint, constraint for the bias weights. required add_parameter ( self , name , initializer , collection = None , trainable = True , regularizer = None , constraint = None ) inherited Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required initializer Callable[[], Any] A callable that takes not arguments returns the initial value. required collection Optional[str] Optional name of the parameter collection, if not defined it will be se to \"parameters\" if trainable=True else it will be set to \"states\" . None trainable bool Specify whether this parameter should be added to the default trainable \"parameters\" collection or to the default non-trainable \"states\" collection. If collection is passed this parameter will ignored. True regularizer Optional[Callable[[Any], jax._src.numpy.lax_numpy.ndarray]] Regularizer instance (callable). None constraint Optional[Callable[[Any], Any]] Constraint instance (callable). None Returns: Type Description Any The value of the parameter. Source code in elegy/nn/multi_head_attention.py def add_parameter ( self , name : str , initializer : tp . Callable [[], tp . Any ], collection : tp . Optional [ str ] = None , trainable : bool = True , regularizer : tp . Optional [ tp . Callable [[ tp . Any ], jnp . ndarray ]] = None , constraint : tp . Optional [ tp . Callable [[ tp . Any ], tp . Any ]] = None , ) -> tp . Any : \"\"\" Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. initializer: A callable that takes not arguments returns the initial value. collection: Optional name of the parameter collection, if not defined it will be se to `\"parameters\"` if `trainable=True` else it will be set to `\"states\"`. trainable: Specify whether this parameter should be added to the default trainable `\"parameters\"` collection or to the default non-trainable `\"states\"` collection. If collection is passed this parameter will ignored. regularizer: Regularizer instance (callable). constraint: Constraint instance (callable). Returns: The value of the parameter. \"\"\" if collection is None : collection = \"parameters\" if trainable else \"states\" if self . _scope_params is None : self . _scope_params = {} if name not in self . _scope_params : if not self . is_initializing (): raise ValueError ( f \"Cannot add parameter { name } outside `init`\" ) if name in self . _default_params : parameter = self . _default_params [ name ] assert collection == parameter . collection initial_value = parameter . value else : initial_value = initializer () initial_value = jax . tree_map ( jnp . asarray , initial_value ) parameter = types . Parameter ( collection , initial_value ) self . _register_parameter ( name , parameter ) self . _scope_params [ name ] = parameter parameter = self . _scope_params [ name ] if parameter . collection != collection : raise ValueError ( f \"types.Parameter { name } previously found in collection { parameter . collection } \" f \"but currently being added for collection { collection } \" ) value = parameter . value if constraint is not None : value = constraint ( value ) if regularizer is not None : hooks . add_loss ( name = utils . get_name ( regularizer ), value = regularizer ( value ), ) return value call ( self , query , key = None , value = None , mask = None , training = None ) Parameters: Name Type Description Default inputs List of [query, key, value] where * query : np.ndarray of shape (..., query_elements, query_depth) * key : np.ndarray of shape '(..., key_elements, key_depth) * value : np.ndarray of shape (..., key_elements, value_depth) , optional, if not given key will be used. required mask a binary np.ndarray of shape [batch_size?, num_heads?, query_elements, key_elements] which specifies which query elements can attendo to which key elements, 1 indicates attention and 0 indicates no attention. None Output shape: * (..., query_elements, output_size) if output_size is given, else * (..., query_elements, value_depth) if value is given, else * (..., query_elements, key_depth) Source code in elegy/nn/multi_head_attention.py def call ( self , query : jnp . ndarray , key : tp . Optional [ jnp . ndarray ] = None , value : tp . Optional [ jnp . ndarray ] = None , mask = None , training = None , ): \"\"\" Arguments: inputs: List of `[query, key, value]` where * `query`: np.ndarray of shape `(..., query_elements, query_depth)` * `key`: `np.ndarray of shape '(..., key_elements, key_depth)` * `value`: np.ndarray of shape `(..., key_elements, value_depth)`, optional, if not given `key` will be used. mask: a binary np.ndarray of shape `[batch_size?, num_heads?, query_elements, key_elements]` which specifies which query elements can attendo to which key elements, `1` indicates attention and `0` indicates no attention. Output shape: * `(..., query_elements, output_size)` if `output_size` is given, else * `(..., query_elements, value_depth)` if `value` is given, else * `(..., query_elements, key_depth)` \"\"\" # einsum nomenclature # ------------------------ # N = query elements # M = key/value elements # H = heads # I = input features # O = output features if key is None : key = query if value is None : value = key output_size = ( self . output_size if self . output_size is not None else value . shape [ - 1 ] ) # verify shapes if key . shape [ - 2 ] != value . shape [ - 2 ]: raise ValueError ( \"the number of elements in 'key' must be equal to the same as the number of elements in 'value'\" ) if mask is not None : if len ( mask . shape ) < 2 : raise ValueError ( \"'mask' must have atleast 2 dimensions\" ) if query . shape [ - 2 ] != mask . shape [ - 2 ]: raise ValueError ( \"mask's second to last dimension must be equal to the number of elements in 'query'\" ) if key . shape [ - 2 ] != mask . shape [ - 1 ]: raise ValueError ( \"mask's last dimension must be equal to the number of elements in 'key'\" ) # get weights query_kernel = self . add_parameter ( \"query_kernel\" , lambda : self . kernel_initializer ( [ self . num_heads , query . shape [ - 1 ], self . head_size ], jnp . float32 ), ) key_kernel = self . add_parameter ( \"key_kernel\" , lambda : self . kernel_initializer ( [ self . num_heads , key . shape [ - 1 ], self . head_size ], jnp . float32 ), ) value_kernel = self . add_parameter ( \"value_kernel\" , lambda : self . kernel_initializer ( [ self . num_heads , value . shape [ - 1 ], self . head_size ], jnp . float32 ), ) projection_kernel = self . add_parameter ( \"projection_kernel\" , lambda : self . kernel_initializer ( [ self . num_heads , self . head_size , output_size ], jnp . float32 ), ) # Linear transformations query = jnp . einsum ( \"...NI , HIO -> ...NHO\" , query , query_kernel ) key = jnp . einsum ( \"...MI , HIO -> ...MHO\" , key , key_kernel ) value = jnp . einsum ( \"...MI , HIO -> ...MHO\" , value , value_kernel ) # Scale dot-product, doing the division to either query or key # instead of their product saves some computation query /= jnp . sqrt ( self . head_size ) # Calculate dot product attention logits = jnp . einsum ( \"...NHO,...MHO->...HNM\" , query , key ) # apply mask if mask is not None : mask = mask . astype ( jnp . float32 ) # possibly expand on the head dimension so broadcasting works if len ( mask . shape ) != len ( logits . shape ): mask = jnp . expand_dims ( mask , - 3 ) logits += - 10e9 * ( 1.0 - mask ) attn_coef = jax . nn . softmax ( logits ) # attention dropout attn_coef_dropout = Dropout ( self . droput_rate )( attn_coef , training = training ) # attention * value multihead_output = jnp . einsum ( \"...HNM,...MHI->...NHI\" , attn_coef_dropout , value ) # Run the outputs through another linear projection layer. Recombining heads # is automatically done. output = jnp . einsum ( \"...NHI,HIO->...NO\" , multihead_output , projection_kernel ) if self . use_projection_bias : output += self . add_parameter ( \"projection_bias\" , lambda : self . bias_initializer ([ output_size ], jnp . float32 ), ) if self . return_attn_coef : return output , attn_coef else : return output init ( self , * , rng = None , set_defaults = False ) inherited Initializes the module, Source code in elegy/nn/multi_head_attention.py def init ( self , * , rng : tp . Optional [ types . RNGSeq ] = None , set_defaults : bool = False , ) -> tp . Callable [ ... , tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ] ]: \"\"\" Initializes the module, \"\"\" def init_callable ( * args , ** kwargs ) -> tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ]: with module_context ( module = self , collections = None , initializing = True , training = True , rng = rng ): y = self ( * args , ** kwargs ) parameters , collections = self . get_parameters_internal () self . _mark_initialized_recursive () if set_defaults : self . set_default_parameters ( parameters , collections ) return y , parameters , collections init_callable . _signature_f = self . call return init_callable","title":"MultiHeadAttention"},{"location":"api/nn/MultiHeadAttention/#elegynnmultiheadattention","text":"","title":"elegy.nn.MultiHeadAttention"},{"location":"api/nn/MultiHeadAttention/#elegy.nn.multi_head_attention.MultiHeadAttention","text":"MultiHead Attention layer. Defines the MultiHead Attention operation as described in Attention Is All You Need which takes in the tensors query , key , and value , and returns the dot-product attention between them: mha = MultiHeadAttention(head_size =128 , num_heads =12 ) query = tf . random . uniform(( 32 , 20 , 200 )) # (batch_size, query_elements, query_depth) key = tf . random . uniform(( 32 , 15 , 300 )) # (batch_size, key_elements, key_depth) value = tf . random . uniform(( 32 , 15 , 400 )) # (batch_size, key_elements, value_depth) attention = mha([query, key, value]) # (batch_size, query_elements, value_depth) If value is not given then internally value = key will be used: mha = MultiHeadAttention(head_size =128 , num_heads =12 ) query = tf . random . uniform(( 32 , 20 , 200 )) # (batch_size, query_elements, query_depth) key = tf . random . uniform(( 32 , 15 , 300 )) # (batch_size, key_elements, key_depth) attention = mha([query, key]) # (batch_size, query_elements, key_depth) Parameters: Name Type Description Default head_size int, dimensionality of the query , key and value tensors required num_heads int, number of attention heads. required output_size int, dimensionality of the output space, if None then the required dropout float, rate parameter for the dropout layer that is required use_projection_bias bool, whether to use a bias term after the linear required return_attn_coef bool, if True , return the attention coefficients as required kernel_initializer initializer, initializer for the kernel weights. required kernel_regularizer regularizer, regularizer for the kernel weights. required kernel_constraint constraint, constraint for the kernel weights. required bias_initializer initializer, initializer for the bias weights. required bias_regularizer regularizer, regularizer for the bias weights. required bias_constraint constraint, constraint for the bias weights. required","title":"elegy.nn.multi_head_attention.MultiHeadAttention"},{"location":"api/nn/MultiHeadAttention/#elegy.nn.multi_head_attention.MultiHeadAttention.add_parameter","text":"Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required initializer Callable[[], Any] A callable that takes not arguments returns the initial value. required collection Optional[str] Optional name of the parameter collection, if not defined it will be se to \"parameters\" if trainable=True else it will be set to \"states\" . None trainable bool Specify whether this parameter should be added to the default trainable \"parameters\" collection or to the default non-trainable \"states\" collection. If collection is passed this parameter will ignored. True regularizer Optional[Callable[[Any], jax._src.numpy.lax_numpy.ndarray]] Regularizer instance (callable). None constraint Optional[Callable[[Any], Any]] Constraint instance (callable). None Returns: Type Description Any The value of the parameter. Source code in elegy/nn/multi_head_attention.py def add_parameter ( self , name : str , initializer : tp . Callable [[], tp . Any ], collection : tp . Optional [ str ] = None , trainable : bool = True , regularizer : tp . Optional [ tp . Callable [[ tp . Any ], jnp . ndarray ]] = None , constraint : tp . Optional [ tp . Callable [[ tp . Any ], tp . Any ]] = None , ) -> tp . Any : \"\"\" Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. initializer: A callable that takes not arguments returns the initial value. collection: Optional name of the parameter collection, if not defined it will be se to `\"parameters\"` if `trainable=True` else it will be set to `\"states\"`. trainable: Specify whether this parameter should be added to the default trainable `\"parameters\"` collection or to the default non-trainable `\"states\"` collection. If collection is passed this parameter will ignored. regularizer: Regularizer instance (callable). constraint: Constraint instance (callable). Returns: The value of the parameter. \"\"\" if collection is None : collection = \"parameters\" if trainable else \"states\" if self . _scope_params is None : self . _scope_params = {} if name not in self . _scope_params : if not self . is_initializing (): raise ValueError ( f \"Cannot add parameter { name } outside `init`\" ) if name in self . _default_params : parameter = self . _default_params [ name ] assert collection == parameter . collection initial_value = parameter . value else : initial_value = initializer () initial_value = jax . tree_map ( jnp . asarray , initial_value ) parameter = types . Parameter ( collection , initial_value ) self . _register_parameter ( name , parameter ) self . _scope_params [ name ] = parameter parameter = self . _scope_params [ name ] if parameter . collection != collection : raise ValueError ( f \"types.Parameter { name } previously found in collection { parameter . collection } \" f \"but currently being added for collection { collection } \" ) value = parameter . value if constraint is not None : value = constraint ( value ) if regularizer is not None : hooks . add_loss ( name = utils . get_name ( regularizer ), value = regularizer ( value ), ) return value","title":"add_parameter()"},{"location":"api/nn/MultiHeadAttention/#elegy.nn.multi_head_attention.MultiHeadAttention.call","text":"Parameters: Name Type Description Default inputs List of [query, key, value] where * query : np.ndarray of shape (..., query_elements, query_depth) * key : np.ndarray of shape '(..., key_elements, key_depth) * value : np.ndarray of shape (..., key_elements, value_depth) , optional, if not given key will be used. required mask a binary np.ndarray of shape [batch_size?, num_heads?, query_elements, key_elements] which specifies which query elements can attendo to which key elements, 1 indicates attention and 0 indicates no attention. None Output shape: * (..., query_elements, output_size) if output_size is given, else * (..., query_elements, value_depth) if value is given, else * (..., query_elements, key_depth) Source code in elegy/nn/multi_head_attention.py def call ( self , query : jnp . ndarray , key : tp . Optional [ jnp . ndarray ] = None , value : tp . Optional [ jnp . ndarray ] = None , mask = None , training = None , ): \"\"\" Arguments: inputs: List of `[query, key, value]` where * `query`: np.ndarray of shape `(..., query_elements, query_depth)` * `key`: `np.ndarray of shape '(..., key_elements, key_depth)` * `value`: np.ndarray of shape `(..., key_elements, value_depth)`, optional, if not given `key` will be used. mask: a binary np.ndarray of shape `[batch_size?, num_heads?, query_elements, key_elements]` which specifies which query elements can attendo to which key elements, `1` indicates attention and `0` indicates no attention. Output shape: * `(..., query_elements, output_size)` if `output_size` is given, else * `(..., query_elements, value_depth)` if `value` is given, else * `(..., query_elements, key_depth)` \"\"\" # einsum nomenclature # ------------------------ # N = query elements # M = key/value elements # H = heads # I = input features # O = output features if key is None : key = query if value is None : value = key output_size = ( self . output_size if self . output_size is not None else value . shape [ - 1 ] ) # verify shapes if key . shape [ - 2 ] != value . shape [ - 2 ]: raise ValueError ( \"the number of elements in 'key' must be equal to the same as the number of elements in 'value'\" ) if mask is not None : if len ( mask . shape ) < 2 : raise ValueError ( \"'mask' must have atleast 2 dimensions\" ) if query . shape [ - 2 ] != mask . shape [ - 2 ]: raise ValueError ( \"mask's second to last dimension must be equal to the number of elements in 'query'\" ) if key . shape [ - 2 ] != mask . shape [ - 1 ]: raise ValueError ( \"mask's last dimension must be equal to the number of elements in 'key'\" ) # get weights query_kernel = self . add_parameter ( \"query_kernel\" , lambda : self . kernel_initializer ( [ self . num_heads , query . shape [ - 1 ], self . head_size ], jnp . float32 ), ) key_kernel = self . add_parameter ( \"key_kernel\" , lambda : self . kernel_initializer ( [ self . num_heads , key . shape [ - 1 ], self . head_size ], jnp . float32 ), ) value_kernel = self . add_parameter ( \"value_kernel\" , lambda : self . kernel_initializer ( [ self . num_heads , value . shape [ - 1 ], self . head_size ], jnp . float32 ), ) projection_kernel = self . add_parameter ( \"projection_kernel\" , lambda : self . kernel_initializer ( [ self . num_heads , self . head_size , output_size ], jnp . float32 ), ) # Linear transformations query = jnp . einsum ( \"...NI , HIO -> ...NHO\" , query , query_kernel ) key = jnp . einsum ( \"...MI , HIO -> ...MHO\" , key , key_kernel ) value = jnp . einsum ( \"...MI , HIO -> ...MHO\" , value , value_kernel ) # Scale dot-product, doing the division to either query or key # instead of their product saves some computation query /= jnp . sqrt ( self . head_size ) # Calculate dot product attention logits = jnp . einsum ( \"...NHO,...MHO->...HNM\" , query , key ) # apply mask if mask is not None : mask = mask . astype ( jnp . float32 ) # possibly expand on the head dimension so broadcasting works if len ( mask . shape ) != len ( logits . shape ): mask = jnp . expand_dims ( mask , - 3 ) logits += - 10e9 * ( 1.0 - mask ) attn_coef = jax . nn . softmax ( logits ) # attention dropout attn_coef_dropout = Dropout ( self . droput_rate )( attn_coef , training = training ) # attention * value multihead_output = jnp . einsum ( \"...HNM,...MHI->...NHI\" , attn_coef_dropout , value ) # Run the outputs through another linear projection layer. Recombining heads # is automatically done. output = jnp . einsum ( \"...NHI,HIO->...NO\" , multihead_output , projection_kernel ) if self . use_projection_bias : output += self . add_parameter ( \"projection_bias\" , lambda : self . bias_initializer ([ output_size ], jnp . float32 ), ) if self . return_attn_coef : return output , attn_coef else : return output","title":"call()"},{"location":"api/nn/MultiHeadAttention/#elegy.nn.multi_head_attention.MultiHeadAttention.init","text":"Initializes the module, Source code in elegy/nn/multi_head_attention.py def init ( self , * , rng : tp . Optional [ types . RNGSeq ] = None , set_defaults : bool = False , ) -> tp . Callable [ ... , tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ] ]: \"\"\" Initializes the module, \"\"\" def init_callable ( * args , ** kwargs ) -> tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ]: with module_context ( module = self , collections = None , initializing = True , training = True , rng = rng ): y = self ( * args , ** kwargs ) parameters , collections = self . get_parameters_internal () self . _mark_initialized_recursive () if set_defaults : self . set_default_parameters ( parameters , collections ) return y , parameters , collections init_callable . _signature_f = self . call return init_callable","title":"init()"},{"location":"api/nn/Reshape/","text":"elegy.nn.Reshape Reshapes input Tensor, preserving the batch dimension. For example, given an input tensor with shape [B, H, W, C, D] : B, H, W, C, D = range ( 1 , 6 ) x = jnp . ones([B, H, W, C, D]) The default behavior when output_shape is (-1, D) is to flatten all dimensions between B and D : mod = elegy . nn . Reshape(output_shape = ( -1 , D)) assert mod(x) . shape == (B, H * W * C, D) You can change the number of preserved leading dimensions via preserve_dims : mod = elegy . nn . Reshape(output_shape = ( -1 , D), preserve_dims =2 ) assert mod(x) . shape == (B, H, W * C, D) mod = elegy . nn . Reshape(output_shape = ( -1 , D), preserve_dims =3 ) assert mod(x) . shape == (B, H, W, C, D) mod = elegy . nn . Reshape(output_shape = ( -1 , D), preserve_dims =4 ) assert mod(x) . shape == (B, H, W, C, 1 , D) __init__ ( self , output_shape , preserve_dims = 1 , ** kwargs ) special Constructs a Reshape module. Parameters: Name Type Description Default output_shape Sequence[int] Shape to reshape the input tensor to while preserving its first preserve_dims dimensions. When the special value -1 appears in output_shape the corresponding size is automatically inferred. Note that -1 can only appear once in output_shape . To flatten all non-batch dimensions use Flatten . required preserve_dims int Number of leading dimensions that will not be reshaped. 1 kwargs Additional keyword arguments passed to Module. {} Exceptions: Type Description ValueError If preserve_dims is not positive. Source code in elegy/nn/flatten.py def __init__ ( self , output_shape : types . Shape , preserve_dims : int = 1 , ** kwargs ): \"\"\" Constructs a `Reshape` module. Args: output_shape: Shape to reshape the input tensor to while preserving its first `preserve_dims` dimensions. When the special value -1 appears in `output_shape` the corresponding size is automatically inferred. Note that -1 can only appear once in `output_shape`. To flatten all non-batch dimensions use `Flatten`. preserve_dims: Number of leading dimensions that will not be reshaped. kwargs: Additional keyword arguments passed to Module. Raises: ValueError: If `preserve_dims` is not positive. \"\"\" super () . __init__ ( ** kwargs ) if preserve_dims <= 0 : raise ValueError ( \"Argument preserve_dims should be >= 1.\" ) if output_shape . count ( - 1 ) > 1 : raise ValueError ( \"-1 can only occur once in `output_shape`.\" ) self . output_shape = tuple ( output_shape ) self . preserve_dims = preserve_dims add_parameter ( self , name , initializer , collection = None , trainable = True , regularizer = None , constraint = None ) inherited Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required initializer Callable[[], Any] A callable that takes not arguments returns the initial value. required collection Optional[str] Optional name of the parameter collection, if not defined it will be se to \"parameters\" if trainable=True else it will be set to \"states\" . None trainable bool Specify whether this parameter should be added to the default trainable \"parameters\" collection or to the default non-trainable \"states\" collection. If collection is passed this parameter will ignored. True regularizer Optional[Callable[[Any], jax._src.numpy.lax_numpy.ndarray]] Regularizer instance (callable). None constraint Optional[Callable[[Any], Any]] Constraint instance (callable). None Returns: Type Description Any The value of the parameter. Source code in elegy/nn/flatten.py def add_parameter ( self , name : str , initializer : tp . Callable [[], tp . Any ], collection : tp . Optional [ str ] = None , trainable : bool = True , regularizer : tp . Optional [ tp . Callable [[ tp . Any ], jnp . ndarray ]] = None , constraint : tp . Optional [ tp . Callable [[ tp . Any ], tp . Any ]] = None , ) -> tp . Any : \"\"\" Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. initializer: A callable that takes not arguments returns the initial value. collection: Optional name of the parameter collection, if not defined it will be se to `\"parameters\"` if `trainable=True` else it will be set to `\"states\"`. trainable: Specify whether this parameter should be added to the default trainable `\"parameters\"` collection or to the default non-trainable `\"states\"` collection. If collection is passed this parameter will ignored. regularizer: Regularizer instance (callable). constraint: Constraint instance (callable). Returns: The value of the parameter. \"\"\" if collection is None : collection = \"parameters\" if trainable else \"states\" if self . _scope_params is None : self . _scope_params = {} if name not in self . _scope_params : if not self . is_initializing (): raise ValueError ( f \"Cannot add parameter { name } outside `init`\" ) if name in self . _default_params : parameter = self . _default_params [ name ] assert collection == parameter . collection initial_value = parameter . value else : initial_value = initializer () initial_value = jax . tree_map ( jnp . asarray , initial_value ) parameter = types . Parameter ( collection , initial_value ) self . _register_parameter ( name , parameter ) self . _scope_params [ name ] = parameter parameter = self . _scope_params [ name ] if parameter . collection != collection : raise ValueError ( f \"types.Parameter { name } previously found in collection { parameter . collection } \" f \"but currently being added for collection { collection } \" ) value = parameter . value if constraint is not None : value = constraint ( value ) if regularizer is not None : hooks . add_loss ( name = utils . get_name ( regularizer ), value = regularizer ( value ), ) return value call ( self , inputs ) Parameters: Name Type Description Default inputs ndarray the array to be reshaped. required Returns: Type Description ndarray A reshaped array. Source code in elegy/nn/flatten.py def call ( self , inputs : np . ndarray ) -> np . ndarray : \"\"\" Arguments: inputs: the array to be reshaped. Returns: A reshaped array. \"\"\" if inputs . ndim <= self . preserve_dims : return inputs if - 1 in self . output_shape : reshaped_shape = _infer_shape ( self . output_shape , inputs . shape [ self . preserve_dims :] ) else : reshaped_shape = self . output_shape shape = inputs . shape [: self . preserve_dims ] + reshaped_shape return jnp . reshape ( inputs , shape ) init ( self , * , rng = None , set_defaults = False ) inherited Initializes the module, Source code in elegy/nn/flatten.py def init ( self , * , rng : tp . Optional [ types . RNGSeq ] = None , set_defaults : bool = False , ) -> tp . Callable [ ... , tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ] ]: \"\"\" Initializes the module, \"\"\" def init_callable ( * args , ** kwargs ) -> tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ]: with module_context ( module = self , collections = None , initializing = True , training = True , rng = rng ): y = self ( * args , ** kwargs ) parameters , collections = self . get_parameters_internal () self . _mark_initialized_recursive () if set_defaults : self . set_default_parameters ( parameters , collections ) return y , parameters , collections init_callable . _signature_f = self . call return init_callable","title":"Reshape"},{"location":"api/nn/Reshape/#elegynnreshape","text":"","title":"elegy.nn.Reshape"},{"location":"api/nn/Reshape/#elegy.nn.flatten.Reshape","text":"Reshapes input Tensor, preserving the batch dimension. For example, given an input tensor with shape [B, H, W, C, D] : B, H, W, C, D = range ( 1 , 6 ) x = jnp . ones([B, H, W, C, D]) The default behavior when output_shape is (-1, D) is to flatten all dimensions between B and D : mod = elegy . nn . Reshape(output_shape = ( -1 , D)) assert mod(x) . shape == (B, H * W * C, D) You can change the number of preserved leading dimensions via preserve_dims : mod = elegy . nn . Reshape(output_shape = ( -1 , D), preserve_dims =2 ) assert mod(x) . shape == (B, H, W * C, D) mod = elegy . nn . Reshape(output_shape = ( -1 , D), preserve_dims =3 ) assert mod(x) . shape == (B, H, W, C, D) mod = elegy . nn . Reshape(output_shape = ( -1 , D), preserve_dims =4 ) assert mod(x) . shape == (B, H, W, C, 1 , D)","title":"elegy.nn.flatten.Reshape"},{"location":"api/nn/Reshape/#elegy.nn.flatten.Reshape.__init__","text":"Constructs a Reshape module. Parameters: Name Type Description Default output_shape Sequence[int] Shape to reshape the input tensor to while preserving its first preserve_dims dimensions. When the special value -1 appears in output_shape the corresponding size is automatically inferred. Note that -1 can only appear once in output_shape . To flatten all non-batch dimensions use Flatten . required preserve_dims int Number of leading dimensions that will not be reshaped. 1 kwargs Additional keyword arguments passed to Module. {} Exceptions: Type Description ValueError If preserve_dims is not positive. Source code in elegy/nn/flatten.py def __init__ ( self , output_shape : types . Shape , preserve_dims : int = 1 , ** kwargs ): \"\"\" Constructs a `Reshape` module. Args: output_shape: Shape to reshape the input tensor to while preserving its first `preserve_dims` dimensions. When the special value -1 appears in `output_shape` the corresponding size is automatically inferred. Note that -1 can only appear once in `output_shape`. To flatten all non-batch dimensions use `Flatten`. preserve_dims: Number of leading dimensions that will not be reshaped. kwargs: Additional keyword arguments passed to Module. Raises: ValueError: If `preserve_dims` is not positive. \"\"\" super () . __init__ ( ** kwargs ) if preserve_dims <= 0 : raise ValueError ( \"Argument preserve_dims should be >= 1.\" ) if output_shape . count ( - 1 ) > 1 : raise ValueError ( \"-1 can only occur once in `output_shape`.\" ) self . output_shape = tuple ( output_shape ) self . preserve_dims = preserve_dims","title":"__init__()"},{"location":"api/nn/Reshape/#elegy.nn.flatten.Reshape.add_parameter","text":"Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required initializer Callable[[], Any] A callable that takes not arguments returns the initial value. required collection Optional[str] Optional name of the parameter collection, if not defined it will be se to \"parameters\" if trainable=True else it will be set to \"states\" . None trainable bool Specify whether this parameter should be added to the default trainable \"parameters\" collection or to the default non-trainable \"states\" collection. If collection is passed this parameter will ignored. True regularizer Optional[Callable[[Any], jax._src.numpy.lax_numpy.ndarray]] Regularizer instance (callable). None constraint Optional[Callable[[Any], Any]] Constraint instance (callable). None Returns: Type Description Any The value of the parameter. Source code in elegy/nn/flatten.py def add_parameter ( self , name : str , initializer : tp . Callable [[], tp . Any ], collection : tp . Optional [ str ] = None , trainable : bool = True , regularizer : tp . Optional [ tp . Callable [[ tp . Any ], jnp . ndarray ]] = None , constraint : tp . Optional [ tp . Callable [[ tp . Any ], tp . Any ]] = None , ) -> tp . Any : \"\"\" Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. initializer: A callable that takes not arguments returns the initial value. collection: Optional name of the parameter collection, if not defined it will be se to `\"parameters\"` if `trainable=True` else it will be set to `\"states\"`. trainable: Specify whether this parameter should be added to the default trainable `\"parameters\"` collection or to the default non-trainable `\"states\"` collection. If collection is passed this parameter will ignored. regularizer: Regularizer instance (callable). constraint: Constraint instance (callable). Returns: The value of the parameter. \"\"\" if collection is None : collection = \"parameters\" if trainable else \"states\" if self . _scope_params is None : self . _scope_params = {} if name not in self . _scope_params : if not self . is_initializing (): raise ValueError ( f \"Cannot add parameter { name } outside `init`\" ) if name in self . _default_params : parameter = self . _default_params [ name ] assert collection == parameter . collection initial_value = parameter . value else : initial_value = initializer () initial_value = jax . tree_map ( jnp . asarray , initial_value ) parameter = types . Parameter ( collection , initial_value ) self . _register_parameter ( name , parameter ) self . _scope_params [ name ] = parameter parameter = self . _scope_params [ name ] if parameter . collection != collection : raise ValueError ( f \"types.Parameter { name } previously found in collection { parameter . collection } \" f \"but currently being added for collection { collection } \" ) value = parameter . value if constraint is not None : value = constraint ( value ) if regularizer is not None : hooks . add_loss ( name = utils . get_name ( regularizer ), value = regularizer ( value ), ) return value","title":"add_parameter()"},{"location":"api/nn/Reshape/#elegy.nn.flatten.Reshape.call","text":"Parameters: Name Type Description Default inputs ndarray the array to be reshaped. required Returns: Type Description ndarray A reshaped array. Source code in elegy/nn/flatten.py def call ( self , inputs : np . ndarray ) -> np . ndarray : \"\"\" Arguments: inputs: the array to be reshaped. Returns: A reshaped array. \"\"\" if inputs . ndim <= self . preserve_dims : return inputs if - 1 in self . output_shape : reshaped_shape = _infer_shape ( self . output_shape , inputs . shape [ self . preserve_dims :] ) else : reshaped_shape = self . output_shape shape = inputs . shape [: self . preserve_dims ] + reshaped_shape return jnp . reshape ( inputs , shape )","title":"call()"},{"location":"api/nn/Reshape/#elegy.nn.flatten.Reshape.init","text":"Initializes the module, Source code in elegy/nn/flatten.py def init ( self , * , rng : tp . Optional [ types . RNGSeq ] = None , set_defaults : bool = False , ) -> tp . Callable [ ... , tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ] ]: \"\"\" Initializes the module, \"\"\" def init_callable ( * args , ** kwargs ) -> tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ]: with module_context ( module = self , collections = None , initializing = True , training = True , rng = rng ): y = self ( * args , ** kwargs ) parameters , collections = self . get_parameters_internal () self . _mark_initialized_recursive () if set_defaults : self . set_default_parameters ( parameters , collections ) return y , parameters , collections init_callable . _signature_f = self . call return init_callable","title":"init()"},{"location":"api/nn/Sequential/","text":"elegy.nn.Sequential Creates a Module from a zero argument lambda that produces a list of Modules or function to be executed sequentially. The lambda is necessary so that all sub-modules are instantiated inside the context of the Sequential module. mlp = elegy . nn . Sequential( lambda : [ elegy . nn . Linear( 64 ), jax . nn . relu, elegy . nn . Linear( 32 ), jax . nn . relu, elegy . nn . Linear( 10 ), jax . nn . softmax, ] ) y = mlp(x) add_parameter ( self , name , initializer , collection = None , trainable = True , regularizer = None , constraint = None ) inherited Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required initializer Callable[[], Any] A callable that takes not arguments returns the initial value. required collection Optional[str] Optional name of the parameter collection, if not defined it will be se to \"parameters\" if trainable=True else it will be set to \"states\" . None trainable bool Specify whether this parameter should be added to the default trainable \"parameters\" collection or to the default non-trainable \"states\" collection. If collection is passed this parameter will ignored. True regularizer Optional[Callable[[Any], jax._src.numpy.lax_numpy.ndarray]] Regularizer instance (callable). None constraint Optional[Callable[[Any], Any]] Constraint instance (callable). None Returns: Type Description Any The value of the parameter. Source code in elegy/nn/sequential_module.py def add_parameter ( self , name : str , initializer : tp . Callable [[], tp . Any ], collection : tp . Optional [ str ] = None , trainable : bool = True , regularizer : tp . Optional [ tp . Callable [[ tp . Any ], jnp . ndarray ]] = None , constraint : tp . Optional [ tp . Callable [[ tp . Any ], tp . Any ]] = None , ) -> tp . Any : \"\"\" Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. initializer: A callable that takes not arguments returns the initial value. collection: Optional name of the parameter collection, if not defined it will be se to `\"parameters\"` if `trainable=True` else it will be set to `\"states\"`. trainable: Specify whether this parameter should be added to the default trainable `\"parameters\"` collection or to the default non-trainable `\"states\"` collection. If collection is passed this parameter will ignored. regularizer: Regularizer instance (callable). constraint: Constraint instance (callable). Returns: The value of the parameter. \"\"\" if collection is None : collection = \"parameters\" if trainable else \"states\" if self . _scope_params is None : self . _scope_params = {} if name not in self . _scope_params : if not self . is_initializing (): raise ValueError ( f \"Cannot add parameter { name } outside `init`\" ) if name in self . _default_params : parameter = self . _default_params [ name ] assert collection == parameter . collection initial_value = parameter . value else : initial_value = initializer () initial_value = jax . tree_map ( jnp . asarray , initial_value ) parameter = types . Parameter ( collection , initial_value ) self . _register_parameter ( name , parameter ) self . _scope_params [ name ] = parameter parameter = self . _scope_params [ name ] if parameter . collection != collection : raise ValueError ( f \"types.Parameter { name } previously found in collection { parameter . collection } \" f \"but currently being added for collection { collection } \" ) value = parameter . value if constraint is not None : value = constraint ( value ) if regularizer is not None : hooks . add_loss ( name = utils . get_name ( regularizer ), value = regularizer ( value ), ) return value call ( self , * args , ** kwargs ) Connects all layers. *args and **kwargs are passed to the first layer. Source code in elegy/nn/sequential_module.py def call ( self , * args , ** kwargs ): \"\"\"Connects all layers. `*args` and `**kwargs` are passed to the first layer.\"\"\" return sequential ( * self . layers )( * args , ** kwargs ) init ( self , * , rng = None , set_defaults = False ) inherited Initializes the module, Source code in elegy/nn/sequential_module.py def init ( self , * , rng : tp . Optional [ types . RNGSeq ] = None , set_defaults : bool = False , ) -> tp . Callable [ ... , tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ] ]: \"\"\" Initializes the module, \"\"\" def init_callable ( * args , ** kwargs ) -> tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ]: with module_context ( module = self , collections = None , initializing = True , training = True , rng = rng ): y = self ( * args , ** kwargs ) parameters , collections = self . get_parameters_internal () self . _mark_initialized_recursive () if set_defaults : self . set_default_parameters ( parameters , collections ) return y , parameters , collections init_callable . _signature_f = self . call return init_callable","title":"Sequential"},{"location":"api/nn/Sequential/#elegynnsequential","text":"","title":"elegy.nn.Sequential"},{"location":"api/nn/Sequential/#elegy.nn.sequential_module.Sequential","text":"Creates a Module from a zero argument lambda that produces a list of Modules or function to be executed sequentially. The lambda is necessary so that all sub-modules are instantiated inside the context of the Sequential module. mlp = elegy . nn . Sequential( lambda : [ elegy . nn . Linear( 64 ), jax . nn . relu, elegy . nn . Linear( 32 ), jax . nn . relu, elegy . nn . Linear( 10 ), jax . nn . softmax, ] ) y = mlp(x)","title":"elegy.nn.sequential_module.Sequential"},{"location":"api/nn/Sequential/#elegy.nn.sequential_module.Sequential.add_parameter","text":"Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required initializer Callable[[], Any] A callable that takes not arguments returns the initial value. required collection Optional[str] Optional name of the parameter collection, if not defined it will be se to \"parameters\" if trainable=True else it will be set to \"states\" . None trainable bool Specify whether this parameter should be added to the default trainable \"parameters\" collection or to the default non-trainable \"states\" collection. If collection is passed this parameter will ignored. True regularizer Optional[Callable[[Any], jax._src.numpy.lax_numpy.ndarray]] Regularizer instance (callable). None constraint Optional[Callable[[Any], Any]] Constraint instance (callable). None Returns: Type Description Any The value of the parameter. Source code in elegy/nn/sequential_module.py def add_parameter ( self , name : str , initializer : tp . Callable [[], tp . Any ], collection : tp . Optional [ str ] = None , trainable : bool = True , regularizer : tp . Optional [ tp . Callable [[ tp . Any ], jnp . ndarray ]] = None , constraint : tp . Optional [ tp . Callable [[ tp . Any ], tp . Any ]] = None , ) -> tp . Any : \"\"\" Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. initializer: A callable that takes not arguments returns the initial value. collection: Optional name of the parameter collection, if not defined it will be se to `\"parameters\"` if `trainable=True` else it will be set to `\"states\"`. trainable: Specify whether this parameter should be added to the default trainable `\"parameters\"` collection or to the default non-trainable `\"states\"` collection. If collection is passed this parameter will ignored. regularizer: Regularizer instance (callable). constraint: Constraint instance (callable). Returns: The value of the parameter. \"\"\" if collection is None : collection = \"parameters\" if trainable else \"states\" if self . _scope_params is None : self . _scope_params = {} if name not in self . _scope_params : if not self . is_initializing (): raise ValueError ( f \"Cannot add parameter { name } outside `init`\" ) if name in self . _default_params : parameter = self . _default_params [ name ] assert collection == parameter . collection initial_value = parameter . value else : initial_value = initializer () initial_value = jax . tree_map ( jnp . asarray , initial_value ) parameter = types . Parameter ( collection , initial_value ) self . _register_parameter ( name , parameter ) self . _scope_params [ name ] = parameter parameter = self . _scope_params [ name ] if parameter . collection != collection : raise ValueError ( f \"types.Parameter { name } previously found in collection { parameter . collection } \" f \"but currently being added for collection { collection } \" ) value = parameter . value if constraint is not None : value = constraint ( value ) if regularizer is not None : hooks . add_loss ( name = utils . get_name ( regularizer ), value = regularizer ( value ), ) return value","title":"add_parameter()"},{"location":"api/nn/Sequential/#elegy.nn.sequential_module.Sequential.call","text":"Connects all layers. *args and **kwargs are passed to the first layer. Source code in elegy/nn/sequential_module.py def call ( self , * args , ** kwargs ): \"\"\"Connects all layers. `*args` and `**kwargs` are passed to the first layer.\"\"\" return sequential ( * self . layers )( * args , ** kwargs )","title":"call()"},{"location":"api/nn/Sequential/#elegy.nn.sequential_module.Sequential.init","text":"Initializes the module, Source code in elegy/nn/sequential_module.py def init ( self , * , rng : tp . Optional [ types . RNGSeq ] = None , set_defaults : bool = False , ) -> tp . Callable [ ... , tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ] ]: \"\"\" Initializes the module, \"\"\" def init_callable ( * args , ** kwargs ) -> tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ]: with module_context ( module = self , collections = None , initializing = True , training = True , rng = rng ): y = self ( * args , ** kwargs ) parameters , collections = self . get_parameters_internal () self . _mark_initialized_recursive () if set_defaults : self . set_default_parameters ( parameters , collections ) return y , parameters , collections init_callable . _signature_f = self . call return init_callable","title":"init()"},{"location":"api/nn/Transformer/","text":"elegy.nn.Transformer A transformer model. User is able to modify the attributes as needed. The architecture is based on the paper \"Attention Is All You Need\". Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010. Users can build the BERT(https://arxiv.org/abs/1810.04805) model with corresponding parameters. Parameters: Name Type Description Default head_size the number of expected features in the encoder/decoder inputs (default=512). required num_heads the number of heads in the multiheadattention models (default=8). required num_encoder_layers the number of sub-encoder-layers in the encoder (default=6). required num_decoder_layers the number of sub-decoder-layers in the decoder (default=6). required output_size the dimension of the feedforward network model (default=2048). required dropout the dropout value (default=0.1). required activation the activation function of encoder/decoder intermediate layer, relu or gelu (default=relu). required custom_encoder custom encoder (default=None). required custom_decoder custom decoder (default=None). required Examples: >>> import elegy >>> import numpy as np >>> transformer_model = elegy . nn . Transformer ( ... head_size = 64 , ... num_heads = 4 , ... num_encoder_layers = 2 , ... num_decoder_layers = 2 , ... ) >>> src = np . random . uniform ( size = ( 5 , 32 , 64 )) >>> tgt = np . random . uniform ( size = ( 5 , 32 , 64 )) >>> _ , params , collections = transformer_model . init ( rng = elegy . RNGSeq ( 42 ))( src , tgt ) >>> out , params , collections = transformer_model . apply ( params , collections , rng = elegy . RNGSeq ( 420 ))( src , tgt ) add_parameter ( self , name , initializer , collection = None , trainable = True , regularizer = None , constraint = None ) inherited Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required initializer Callable[[], Any] A callable that takes not arguments returns the initial value. required collection Optional[str] Optional name of the parameter collection, if not defined it will be se to \"parameters\" if trainable=True else it will be set to \"states\" . None trainable bool Specify whether this parameter should be added to the default trainable \"parameters\" collection or to the default non-trainable \"states\" collection. If collection is passed this parameter will ignored. True regularizer Optional[Callable[[Any], jax._src.numpy.lax_numpy.ndarray]] Regularizer instance (callable). None constraint Optional[Callable[[Any], Any]] Constraint instance (callable). None Returns: Type Description Any The value of the parameter. Source code in elegy/nn/transformers.py def add_parameter ( self , name : str , initializer : tp . Callable [[], tp . Any ], collection : tp . Optional [ str ] = None , trainable : bool = True , regularizer : tp . Optional [ tp . Callable [[ tp . Any ], jnp . ndarray ]] = None , constraint : tp . Optional [ tp . Callable [[ tp . Any ], tp . Any ]] = None , ) -> tp . Any : \"\"\" Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. initializer: A callable that takes not arguments returns the initial value. collection: Optional name of the parameter collection, if not defined it will be se to `\"parameters\"` if `trainable=True` else it will be set to `\"states\"`. trainable: Specify whether this parameter should be added to the default trainable `\"parameters\"` collection or to the default non-trainable `\"states\"` collection. If collection is passed this parameter will ignored. regularizer: Regularizer instance (callable). constraint: Constraint instance (callable). Returns: The value of the parameter. \"\"\" if collection is None : collection = \"parameters\" if trainable else \"states\" if self . _scope_params is None : self . _scope_params = {} if name not in self . _scope_params : if not self . is_initializing (): raise ValueError ( f \"Cannot add parameter { name } outside `init`\" ) if name in self . _default_params : parameter = self . _default_params [ name ] assert collection == parameter . collection initial_value = parameter . value else : initial_value = initializer () initial_value = jax . tree_map ( jnp . asarray , initial_value ) parameter = types . Parameter ( collection , initial_value ) self . _register_parameter ( name , parameter ) self . _scope_params [ name ] = parameter parameter = self . _scope_params [ name ] if parameter . collection != collection : raise ValueError ( f \"types.Parameter { name } previously found in collection { parameter . collection } \" f \"but currently being added for collection { collection } \" ) value = parameter . value if constraint is not None : value = constraint ( value ) if regularizer is not None : hooks . add_loss ( name = utils . get_name ( regularizer ), value = regularizer ( value ), ) return value call ( self , src , tgt , src_mask = None , tgt_mask = None , memory_mask = None ) Take in and process masked source/target sequences. Parameters: Name Type Description Default src ndarray the sequence to the encoder (required). required tgt ndarray the sequence to the decoder (required). required src_mask Optional[numpy.ndarray] the additive mask for the src sequence (optional). None tgt_mask Optional[numpy.ndarray] the additive mask for the tgt sequence (optional). None memory_mask Optional[numpy.ndarray] the additive mask for the encoder output (optional). None src_key_padding_mask the ByteTensor mask for src keys per batch (optional). required tgt_key_padding_mask the ByteTensor mask for tgt keys per batch (optional). required memory_key_padding_mask the ByteTensor mask for memory keys per batch (optional). required Shape src: :math: (S, N, E) . tgt: :math: (T, N, E) . src_mask: :math: (S, S) . tgt_mask: :math: (T, T) . memory_mask: :math: (T, S) . src_key_padding_mask: :math: (N, S) . tgt_key_padding_mask: :math: (N, T) . memory_key_padding_mask: :math: (N, S) . Note: [src/tgt/memory]_mask ensures that position i is allowed to attend the unmasked positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend while the zero positions will be unchanged. If a BoolTensor is provided, positions with True are not allowed to attend while False values will be unchanged. If a FloatTensor is provided, it will be added to the attention weight. [src/tgt/memory]_key_padding_mask provides specified elements in the key to be ignored by the attention. If a ByteTensor is provided, the non-zero positions will be ignored while the zero positions will be unchanged. If a BoolTensor is provided, the positions with the value of True will be ignored while the position with the value of False will be unchanged. output: :math: (T, N, E) . Note: Due to the multi-head attention architecture in the transformer model, the output sequence length of a transformer is same as the input sequence (i.e. target) length of the decode. where S is the source sequence length, T is the target sequence length, N is the batch size, E is the feature number Source code in elegy/nn/transformers.py def call ( self , src : np . ndarray , tgt : np . ndarray , src_mask : tp . Optional [ np . ndarray ] = None , tgt_mask : tp . Optional [ np . ndarray ] = None , memory_mask : tp . Optional [ np . ndarray ] = None , # src_key_padding_mask: tp.Optional[np.ndarray] = None, # tgt_key_padding_mask: tp.Optional[np.ndarray] = None, # memory_key_padding_mask: tp.Optional[np.ndarray] = None, ) -> np . ndarray : r \"\"\"Take in and process masked source/target sequences. Arguments: src: the sequence to the encoder (required). tgt: the sequence to the decoder (required). src_mask: the additive mask for the src sequence (optional). tgt_mask: the additive mask for the tgt sequence (optional). memory_mask: the additive mask for the encoder output (optional). src_key_padding_mask: the ByteTensor mask for src keys per batch (optional). tgt_key_padding_mask: the ByteTensor mask for tgt keys per batch (optional). memory_key_padding_mask: the ByteTensor mask for memory keys per batch (optional). Shape: - src: :math:`(S, N, E)`. - tgt: :math:`(T, N, E)`. - src_mask: :math:`(S, S)`. - tgt_mask: :math:`(T, T)`. - memory_mask: :math:`(T, S)`. - src_key_padding_mask: :math:`(N, S)`. - tgt_key_padding_mask: :math:`(N, T)`. - memory_key_padding_mask: :math:`(N, S)`. Note: [src/tgt/memory]_mask ensures that position i is allowed to attend the unmasked positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True`` are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor is provided, it will be added to the attention weight. [src/tgt/memory]_key_padding_mask provides specified elements in the key to be ignored by the attention. If a ByteTensor is provided, the non-zero positions will be ignored while the zero positions will be unchanged. If a BoolTensor is provided, the positions with the value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged. - output: :math:`(T, N, E)`. Note: Due to the multi-head attention architecture in the transformer model, the output sequence length of a transformer is same as the input sequence (i.e. target) length of the decode. where S is the source sequence length, T is the target sequence length, N is the batch size, E is the feature number \"\"\" if src . shape [ 0 ] != tgt . shape [ 0 ]: raise RuntimeError ( \"the batch number of src and tgt must be equal\" ) # if src.shape[2] != self.head_size or tgt.shape[2] != self.head_size: # raise RuntimeError( # \"the feature number of src and tgt must be equal to head_size\" # ) if self . custom_encoder is not None : encoder = self . custom_encoder () else : encoder = TransformerEncoder ( lambda : TransformerEncoderLayer ( self . head_size , self . num_heads , self . output_size , self . dropout , self . activation , ), self . num_encoder_layers , lambda : LayerNormalization (), ) if self . custom_decoder is not None : decoder = self . custom_decoder () else : decoder = TransformerDecoder ( lambda : TransformerDecoderLayer ( self . head_size , self . num_heads , self . output_size , self . dropout , self . activation , ), self . num_decoder_layers , lambda : LayerNormalization (), ) memory = encoder ( src , mask = src_mask , # src_key_padding_mask=src_key_padding_mask ) output = decoder ( tgt , memory , tgt_mask = tgt_mask , memory_mask = memory_mask , # tgt_key_padding_mask=tgt_key_padding_mask, # memory_key_padding_mask=memory_key_padding_mask, ) return output init ( self , * , rng = None , set_defaults = False ) inherited Initializes the module, Source code in elegy/nn/transformers.py def init ( self , * , rng : tp . Optional [ types . RNGSeq ] = None , set_defaults : bool = False , ) -> tp . Callable [ ... , tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ] ]: \"\"\" Initializes the module, \"\"\" def init_callable ( * args , ** kwargs ) -> tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ]: with module_context ( module = self , collections = None , initializing = True , training = True , rng = rng ): y = self ( * args , ** kwargs ) parameters , collections = self . get_parameters_internal () self . _mark_initialized_recursive () if set_defaults : self . set_default_parameters ( parameters , collections ) return y , parameters , collections init_callable . _signature_f = self . call return init_callable","title":"Transformer"},{"location":"api/nn/Transformer/#elegynntransformer","text":"","title":"elegy.nn.Transformer"},{"location":"api/nn/Transformer/#elegy.nn.transformers.Transformer","text":"A transformer model. User is able to modify the attributes as needed. The architecture is based on the paper \"Attention Is All You Need\". Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010. Users can build the BERT(https://arxiv.org/abs/1810.04805) model with corresponding parameters. Parameters: Name Type Description Default head_size the number of expected features in the encoder/decoder inputs (default=512). required num_heads the number of heads in the multiheadattention models (default=8). required num_encoder_layers the number of sub-encoder-layers in the encoder (default=6). required num_decoder_layers the number of sub-decoder-layers in the decoder (default=6). required output_size the dimension of the feedforward network model (default=2048). required dropout the dropout value (default=0.1). required activation the activation function of encoder/decoder intermediate layer, relu or gelu (default=relu). required custom_encoder custom encoder (default=None). required custom_decoder custom decoder (default=None). required Examples: >>> import elegy >>> import numpy as np >>> transformer_model = elegy . nn . Transformer ( ... head_size = 64 , ... num_heads = 4 , ... num_encoder_layers = 2 , ... num_decoder_layers = 2 , ... ) >>> src = np . random . uniform ( size = ( 5 , 32 , 64 )) >>> tgt = np . random . uniform ( size = ( 5 , 32 , 64 )) >>> _ , params , collections = transformer_model . init ( rng = elegy . RNGSeq ( 42 ))( src , tgt ) >>> out , params , collections = transformer_model . apply ( params , collections , rng = elegy . RNGSeq ( 420 ))( src , tgt )","title":"elegy.nn.transformers.Transformer"},{"location":"api/nn/Transformer/#elegy.nn.transformers.Transformer.add_parameter","text":"Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required initializer Callable[[], Any] A callable that takes not arguments returns the initial value. required collection Optional[str] Optional name of the parameter collection, if not defined it will be se to \"parameters\" if trainable=True else it will be set to \"states\" . None trainable bool Specify whether this parameter should be added to the default trainable \"parameters\" collection or to the default non-trainable \"states\" collection. If collection is passed this parameter will ignored. True regularizer Optional[Callable[[Any], jax._src.numpy.lax_numpy.ndarray]] Regularizer instance (callable). None constraint Optional[Callable[[Any], Any]] Constraint instance (callable). None Returns: Type Description Any The value of the parameter. Source code in elegy/nn/transformers.py def add_parameter ( self , name : str , initializer : tp . Callable [[], tp . Any ], collection : tp . Optional [ str ] = None , trainable : bool = True , regularizer : tp . Optional [ tp . Callable [[ tp . Any ], jnp . ndarray ]] = None , constraint : tp . Optional [ tp . Callable [[ tp . Any ], tp . Any ]] = None , ) -> tp . Any : \"\"\" Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. initializer: A callable that takes not arguments returns the initial value. collection: Optional name of the parameter collection, if not defined it will be se to `\"parameters\"` if `trainable=True` else it will be set to `\"states\"`. trainable: Specify whether this parameter should be added to the default trainable `\"parameters\"` collection or to the default non-trainable `\"states\"` collection. If collection is passed this parameter will ignored. regularizer: Regularizer instance (callable). constraint: Constraint instance (callable). Returns: The value of the parameter. \"\"\" if collection is None : collection = \"parameters\" if trainable else \"states\" if self . _scope_params is None : self . _scope_params = {} if name not in self . _scope_params : if not self . is_initializing (): raise ValueError ( f \"Cannot add parameter { name } outside `init`\" ) if name in self . _default_params : parameter = self . _default_params [ name ] assert collection == parameter . collection initial_value = parameter . value else : initial_value = initializer () initial_value = jax . tree_map ( jnp . asarray , initial_value ) parameter = types . Parameter ( collection , initial_value ) self . _register_parameter ( name , parameter ) self . _scope_params [ name ] = parameter parameter = self . _scope_params [ name ] if parameter . collection != collection : raise ValueError ( f \"types.Parameter { name } previously found in collection { parameter . collection } \" f \"but currently being added for collection { collection } \" ) value = parameter . value if constraint is not None : value = constraint ( value ) if regularizer is not None : hooks . add_loss ( name = utils . get_name ( regularizer ), value = regularizer ( value ), ) return value","title":"add_parameter()"},{"location":"api/nn/Transformer/#elegy.nn.transformers.Transformer.call","text":"Take in and process masked source/target sequences. Parameters: Name Type Description Default src ndarray the sequence to the encoder (required). required tgt ndarray the sequence to the decoder (required). required src_mask Optional[numpy.ndarray] the additive mask for the src sequence (optional). None tgt_mask Optional[numpy.ndarray] the additive mask for the tgt sequence (optional). None memory_mask Optional[numpy.ndarray] the additive mask for the encoder output (optional). None src_key_padding_mask the ByteTensor mask for src keys per batch (optional). required tgt_key_padding_mask the ByteTensor mask for tgt keys per batch (optional). required memory_key_padding_mask the ByteTensor mask for memory keys per batch (optional). required Shape src: :math: (S, N, E) . tgt: :math: (T, N, E) . src_mask: :math: (S, S) . tgt_mask: :math: (T, T) . memory_mask: :math: (T, S) . src_key_padding_mask: :math: (N, S) . tgt_key_padding_mask: :math: (N, T) . memory_key_padding_mask: :math: (N, S) . Note: [src/tgt/memory]_mask ensures that position i is allowed to attend the unmasked positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend while the zero positions will be unchanged. If a BoolTensor is provided, positions with True are not allowed to attend while False values will be unchanged. If a FloatTensor is provided, it will be added to the attention weight. [src/tgt/memory]_key_padding_mask provides specified elements in the key to be ignored by the attention. If a ByteTensor is provided, the non-zero positions will be ignored while the zero positions will be unchanged. If a BoolTensor is provided, the positions with the value of True will be ignored while the position with the value of False will be unchanged. output: :math: (T, N, E) . Note: Due to the multi-head attention architecture in the transformer model, the output sequence length of a transformer is same as the input sequence (i.e. target) length of the decode. where S is the source sequence length, T is the target sequence length, N is the batch size, E is the feature number Source code in elegy/nn/transformers.py def call ( self , src : np . ndarray , tgt : np . ndarray , src_mask : tp . Optional [ np . ndarray ] = None , tgt_mask : tp . Optional [ np . ndarray ] = None , memory_mask : tp . Optional [ np . ndarray ] = None , # src_key_padding_mask: tp.Optional[np.ndarray] = None, # tgt_key_padding_mask: tp.Optional[np.ndarray] = None, # memory_key_padding_mask: tp.Optional[np.ndarray] = None, ) -> np . ndarray : r \"\"\"Take in and process masked source/target sequences. Arguments: src: the sequence to the encoder (required). tgt: the sequence to the decoder (required). src_mask: the additive mask for the src sequence (optional). tgt_mask: the additive mask for the tgt sequence (optional). memory_mask: the additive mask for the encoder output (optional). src_key_padding_mask: the ByteTensor mask for src keys per batch (optional). tgt_key_padding_mask: the ByteTensor mask for tgt keys per batch (optional). memory_key_padding_mask: the ByteTensor mask for memory keys per batch (optional). Shape: - src: :math:`(S, N, E)`. - tgt: :math:`(T, N, E)`. - src_mask: :math:`(S, S)`. - tgt_mask: :math:`(T, T)`. - memory_mask: :math:`(T, S)`. - src_key_padding_mask: :math:`(N, S)`. - tgt_key_padding_mask: :math:`(N, T)`. - memory_key_padding_mask: :math:`(N, S)`. Note: [src/tgt/memory]_mask ensures that position i is allowed to attend the unmasked positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True`` are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor is provided, it will be added to the attention weight. [src/tgt/memory]_key_padding_mask provides specified elements in the key to be ignored by the attention. If a ByteTensor is provided, the non-zero positions will be ignored while the zero positions will be unchanged. If a BoolTensor is provided, the positions with the value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged. - output: :math:`(T, N, E)`. Note: Due to the multi-head attention architecture in the transformer model, the output sequence length of a transformer is same as the input sequence (i.e. target) length of the decode. where S is the source sequence length, T is the target sequence length, N is the batch size, E is the feature number \"\"\" if src . shape [ 0 ] != tgt . shape [ 0 ]: raise RuntimeError ( \"the batch number of src and tgt must be equal\" ) # if src.shape[2] != self.head_size or tgt.shape[2] != self.head_size: # raise RuntimeError( # \"the feature number of src and tgt must be equal to head_size\" # ) if self . custom_encoder is not None : encoder = self . custom_encoder () else : encoder = TransformerEncoder ( lambda : TransformerEncoderLayer ( self . head_size , self . num_heads , self . output_size , self . dropout , self . activation , ), self . num_encoder_layers , lambda : LayerNormalization (), ) if self . custom_decoder is not None : decoder = self . custom_decoder () else : decoder = TransformerDecoder ( lambda : TransformerDecoderLayer ( self . head_size , self . num_heads , self . output_size , self . dropout , self . activation , ), self . num_decoder_layers , lambda : LayerNormalization (), ) memory = encoder ( src , mask = src_mask , # src_key_padding_mask=src_key_padding_mask ) output = decoder ( tgt , memory , tgt_mask = tgt_mask , memory_mask = memory_mask , # tgt_key_padding_mask=tgt_key_padding_mask, # memory_key_padding_mask=memory_key_padding_mask, ) return output","title":"call()"},{"location":"api/nn/Transformer/#elegy.nn.transformers.Transformer.init","text":"Initializes the module, Source code in elegy/nn/transformers.py def init ( self , * , rng : tp . Optional [ types . RNGSeq ] = None , set_defaults : bool = False , ) -> tp . Callable [ ... , tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ] ]: \"\"\" Initializes the module, \"\"\" def init_callable ( * args , ** kwargs ) -> tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ]: with module_context ( module = self , collections = None , initializing = True , training = True , rng = rng ): y = self ( * args , ** kwargs ) parameters , collections = self . get_parameters_internal () self . _mark_initialized_recursive () if set_defaults : self . set_default_parameters ( parameters , collections ) return y , parameters , collections init_callable . _signature_f = self . call return init_callable","title":"init()"},{"location":"api/nn/TransformerDecoder/","text":"elegy.nn.TransformerDecoder TransformerDecoder is a stack of N decoder layers Parameters: Name Type Description Default decoder_layer an instance of the TransformerDecoderLayer() class (required). required num_layers the number of sub-decoder-layers in the decoder (required). required norm the layer normalization component (optional). required add_parameter ( self , name , initializer , collection = None , trainable = True , regularizer = None , constraint = None ) inherited Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required initializer Callable[[], Any] A callable that takes not arguments returns the initial value. required collection Optional[str] Optional name of the parameter collection, if not defined it will be se to \"parameters\" if trainable=True else it will be set to \"states\" . None trainable bool Specify whether this parameter should be added to the default trainable \"parameters\" collection or to the default non-trainable \"states\" collection. If collection is passed this parameter will ignored. True regularizer Optional[Callable[[Any], jax._src.numpy.lax_numpy.ndarray]] Regularizer instance (callable). None constraint Optional[Callable[[Any], Any]] Constraint instance (callable). None Returns: Type Description Any The value of the parameter. Source code in elegy/nn/transformers.py def add_parameter ( self , name : str , initializer : tp . Callable [[], tp . Any ], collection : tp . Optional [ str ] = None , trainable : bool = True , regularizer : tp . Optional [ tp . Callable [[ tp . Any ], jnp . ndarray ]] = None , constraint : tp . Optional [ tp . Callable [[ tp . Any ], tp . Any ]] = None , ) -> tp . Any : \"\"\" Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. initializer: A callable that takes not arguments returns the initial value. collection: Optional name of the parameter collection, if not defined it will be se to `\"parameters\"` if `trainable=True` else it will be set to `\"states\"`. trainable: Specify whether this parameter should be added to the default trainable `\"parameters\"` collection or to the default non-trainable `\"states\"` collection. If collection is passed this parameter will ignored. regularizer: Regularizer instance (callable). constraint: Constraint instance (callable). Returns: The value of the parameter. \"\"\" if collection is None : collection = \"parameters\" if trainable else \"states\" if self . _scope_params is None : self . _scope_params = {} if name not in self . _scope_params : if not self . is_initializing (): raise ValueError ( f \"Cannot add parameter { name } outside `init`\" ) if name in self . _default_params : parameter = self . _default_params [ name ] assert collection == parameter . collection initial_value = parameter . value else : initial_value = initializer () initial_value = jax . tree_map ( jnp . asarray , initial_value ) parameter = types . Parameter ( collection , initial_value ) self . _register_parameter ( name , parameter ) self . _scope_params [ name ] = parameter parameter = self . _scope_params [ name ] if parameter . collection != collection : raise ValueError ( f \"types.Parameter { name } previously found in collection { parameter . collection } \" f \"but currently being added for collection { collection } \" ) value = parameter . value if constraint is not None : value = constraint ( value ) if regularizer is not None : hooks . add_loss ( name = utils . get_name ( regularizer ), value = regularizer ( value ), ) return value call ( self , tgt , memory , tgt_mask = None , memory_mask = None ) Pass the inputs (and mask) through the decoder layer in turn. Parameters: Name Type Description Default tgt ndarray the sequence to the decoder (required). required memory ndarray the sequence from the last layer of the encoder (required). required tgt_mask Optional[numpy.ndarray] the mask for the tgt sequence (optional). None memory_mask Optional[numpy.ndarray] the mask for the memory sequence (optional). None tgt_key_padding_mask the mask for the tgt keys per batch (optional). required memory_key_padding_mask the mask for the memory keys per batch (optional). required Shape see the docs in Transformer class. Source code in elegy/nn/transformers.py def call ( self , tgt : np . ndarray , memory : np . ndarray , tgt_mask : tp . Optional [ np . ndarray ] = None , memory_mask : tp . Optional [ np . ndarray ] = None , # tgt_key_padding_mask: tp.Optional[np.ndarray] = None, # memory_key_padding_mask: tp.Optional[np.ndarray] = None, ) -> np . ndarray : r \"\"\"Pass the inputs (and mask) through the decoder layer in turn. Arguments: tgt: the sequence to the decoder (required). memory: the sequence from the last layer of the encoder (required). tgt_mask: the mask for the tgt sequence (optional). memory_mask: the mask for the memory sequence (optional). tgt_key_padding_mask: the mask for the tgt keys per batch (optional). memory_key_padding_mask: the mask for the memory keys per batch (optional). Shape: see the docs in Transformer class. \"\"\" output = tgt for _ in range ( self . num_layers ): output = self . decoder_layer ()( output , memory , tgt_mask = tgt_mask , memory_mask = memory_mask , # tgt_key_padding_mask=tgt_key_padding_mask, # memory_key_padding_mask=memory_key_padding_mask, ) if self . norm is not None : output = self . norm ()( output ) return output init ( self , * , rng = None , set_defaults = False ) inherited Initializes the module, Source code in elegy/nn/transformers.py def init ( self , * , rng : tp . Optional [ types . RNGSeq ] = None , set_defaults : bool = False , ) -> tp . Callable [ ... , tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ] ]: \"\"\" Initializes the module, \"\"\" def init_callable ( * args , ** kwargs ) -> tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ]: with module_context ( module = self , collections = None , initializing = True , training = True , rng = rng ): y = self ( * args , ** kwargs ) parameters , collections = self . get_parameters_internal () self . _mark_initialized_recursive () if set_defaults : self . set_default_parameters ( parameters , collections ) return y , parameters , collections init_callable . _signature_f = self . call return init_callable","title":"TransformerDecoder"},{"location":"api/nn/TransformerDecoder/#elegynntransformerdecoder","text":"","title":"elegy.nn.TransformerDecoder"},{"location":"api/nn/TransformerDecoder/#elegy.nn.transformers.TransformerDecoder","text":"TransformerDecoder is a stack of N decoder layers Parameters: Name Type Description Default decoder_layer an instance of the TransformerDecoderLayer() class (required). required num_layers the number of sub-decoder-layers in the decoder (required). required norm the layer normalization component (optional). required","title":"elegy.nn.transformers.TransformerDecoder"},{"location":"api/nn/TransformerDecoder/#elegy.nn.transformers.TransformerDecoder.add_parameter","text":"Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required initializer Callable[[], Any] A callable that takes not arguments returns the initial value. required collection Optional[str] Optional name of the parameter collection, if not defined it will be se to \"parameters\" if trainable=True else it will be set to \"states\" . None trainable bool Specify whether this parameter should be added to the default trainable \"parameters\" collection or to the default non-trainable \"states\" collection. If collection is passed this parameter will ignored. True regularizer Optional[Callable[[Any], jax._src.numpy.lax_numpy.ndarray]] Regularizer instance (callable). None constraint Optional[Callable[[Any], Any]] Constraint instance (callable). None Returns: Type Description Any The value of the parameter. Source code in elegy/nn/transformers.py def add_parameter ( self , name : str , initializer : tp . Callable [[], tp . Any ], collection : tp . Optional [ str ] = None , trainable : bool = True , regularizer : tp . Optional [ tp . Callable [[ tp . Any ], jnp . ndarray ]] = None , constraint : tp . Optional [ tp . Callable [[ tp . Any ], tp . Any ]] = None , ) -> tp . Any : \"\"\" Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. initializer: A callable that takes not arguments returns the initial value. collection: Optional name of the parameter collection, if not defined it will be se to `\"parameters\"` if `trainable=True` else it will be set to `\"states\"`. trainable: Specify whether this parameter should be added to the default trainable `\"parameters\"` collection or to the default non-trainable `\"states\"` collection. If collection is passed this parameter will ignored. regularizer: Regularizer instance (callable). constraint: Constraint instance (callable). Returns: The value of the parameter. \"\"\" if collection is None : collection = \"parameters\" if trainable else \"states\" if self . _scope_params is None : self . _scope_params = {} if name not in self . _scope_params : if not self . is_initializing (): raise ValueError ( f \"Cannot add parameter { name } outside `init`\" ) if name in self . _default_params : parameter = self . _default_params [ name ] assert collection == parameter . collection initial_value = parameter . value else : initial_value = initializer () initial_value = jax . tree_map ( jnp . asarray , initial_value ) parameter = types . Parameter ( collection , initial_value ) self . _register_parameter ( name , parameter ) self . _scope_params [ name ] = parameter parameter = self . _scope_params [ name ] if parameter . collection != collection : raise ValueError ( f \"types.Parameter { name } previously found in collection { parameter . collection } \" f \"but currently being added for collection { collection } \" ) value = parameter . value if constraint is not None : value = constraint ( value ) if regularizer is not None : hooks . add_loss ( name = utils . get_name ( regularizer ), value = regularizer ( value ), ) return value","title":"add_parameter()"},{"location":"api/nn/TransformerDecoder/#elegy.nn.transformers.TransformerDecoder.call","text":"Pass the inputs (and mask) through the decoder layer in turn. Parameters: Name Type Description Default tgt ndarray the sequence to the decoder (required). required memory ndarray the sequence from the last layer of the encoder (required). required tgt_mask Optional[numpy.ndarray] the mask for the tgt sequence (optional). None memory_mask Optional[numpy.ndarray] the mask for the memory sequence (optional). None tgt_key_padding_mask the mask for the tgt keys per batch (optional). required memory_key_padding_mask the mask for the memory keys per batch (optional). required Shape see the docs in Transformer class. Source code in elegy/nn/transformers.py def call ( self , tgt : np . ndarray , memory : np . ndarray , tgt_mask : tp . Optional [ np . ndarray ] = None , memory_mask : tp . Optional [ np . ndarray ] = None , # tgt_key_padding_mask: tp.Optional[np.ndarray] = None, # memory_key_padding_mask: tp.Optional[np.ndarray] = None, ) -> np . ndarray : r \"\"\"Pass the inputs (and mask) through the decoder layer in turn. Arguments: tgt: the sequence to the decoder (required). memory: the sequence from the last layer of the encoder (required). tgt_mask: the mask for the tgt sequence (optional). memory_mask: the mask for the memory sequence (optional). tgt_key_padding_mask: the mask for the tgt keys per batch (optional). memory_key_padding_mask: the mask for the memory keys per batch (optional). Shape: see the docs in Transformer class. \"\"\" output = tgt for _ in range ( self . num_layers ): output = self . decoder_layer ()( output , memory , tgt_mask = tgt_mask , memory_mask = memory_mask , # tgt_key_padding_mask=tgt_key_padding_mask, # memory_key_padding_mask=memory_key_padding_mask, ) if self . norm is not None : output = self . norm ()( output ) return output","title":"call()"},{"location":"api/nn/TransformerDecoder/#elegy.nn.transformers.TransformerDecoder.init","text":"Initializes the module, Source code in elegy/nn/transformers.py def init ( self , * , rng : tp . Optional [ types . RNGSeq ] = None , set_defaults : bool = False , ) -> tp . Callable [ ... , tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ] ]: \"\"\" Initializes the module, \"\"\" def init_callable ( * args , ** kwargs ) -> tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ]: with module_context ( module = self , collections = None , initializing = True , training = True , rng = rng ): y = self ( * args , ** kwargs ) parameters , collections = self . get_parameters_internal () self . _mark_initialized_recursive () if set_defaults : self . set_default_parameters ( parameters , collections ) return y , parameters , collections init_callable . _signature_f = self . call return init_callable","title":"init()"},{"location":"api/nn/TransformerDecoderLayer/","text":"elegy.nn.TransformerDecoderLayer TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network. This standard decoder layer is based on the paper \"Attention Is All You Need\". Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010. Users may modify or implement in a different way during application. Parameters: Name Type Description Default head_size the number of expected features in the input (required). required num_heads the number of heads in the multiheadattention models (required). required output_size the dimension of the feedforward network model (default=2048). required dropout the dropout value (default=0.1). required activation the activation function of intermediate layer, relu or gelu (default=relu). required add_parameter ( self , name , initializer , collection = None , trainable = True , regularizer = None , constraint = None ) inherited Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required initializer Callable[[], Any] A callable that takes not arguments returns the initial value. required collection Optional[str] Optional name of the parameter collection, if not defined it will be se to \"parameters\" if trainable=True else it will be set to \"states\" . None trainable bool Specify whether this parameter should be added to the default trainable \"parameters\" collection or to the default non-trainable \"states\" collection. If collection is passed this parameter will ignored. True regularizer Optional[Callable[[Any], jax._src.numpy.lax_numpy.ndarray]] Regularizer instance (callable). None constraint Optional[Callable[[Any], Any]] Constraint instance (callable). None Returns: Type Description Any The value of the parameter. Source code in elegy/nn/transformers.py def add_parameter ( self , name : str , initializer : tp . Callable [[], tp . Any ], collection : tp . Optional [ str ] = None , trainable : bool = True , regularizer : tp . Optional [ tp . Callable [[ tp . Any ], jnp . ndarray ]] = None , constraint : tp . Optional [ tp . Callable [[ tp . Any ], tp . Any ]] = None , ) -> tp . Any : \"\"\" Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. initializer: A callable that takes not arguments returns the initial value. collection: Optional name of the parameter collection, if not defined it will be se to `\"parameters\"` if `trainable=True` else it will be set to `\"states\"`. trainable: Specify whether this parameter should be added to the default trainable `\"parameters\"` collection or to the default non-trainable `\"states\"` collection. If collection is passed this parameter will ignored. regularizer: Regularizer instance (callable). constraint: Constraint instance (callable). Returns: The value of the parameter. \"\"\" if collection is None : collection = \"parameters\" if trainable else \"states\" if self . _scope_params is None : self . _scope_params = {} if name not in self . _scope_params : if not self . is_initializing (): raise ValueError ( f \"Cannot add parameter { name } outside `init`\" ) if name in self . _default_params : parameter = self . _default_params [ name ] assert collection == parameter . collection initial_value = parameter . value else : initial_value = initializer () initial_value = jax . tree_map ( jnp . asarray , initial_value ) parameter = types . Parameter ( collection , initial_value ) self . _register_parameter ( name , parameter ) self . _scope_params [ name ] = parameter parameter = self . _scope_params [ name ] if parameter . collection != collection : raise ValueError ( f \"types.Parameter { name } previously found in collection { parameter . collection } \" f \"but currently being added for collection { collection } \" ) value = parameter . value if constraint is not None : value = constraint ( value ) if regularizer is not None : hooks . add_loss ( name = utils . get_name ( regularizer ), value = regularizer ( value ), ) return value call ( self , tgt , memory , tgt_mask = None , memory_mask = None ) Pass the inputs (and mask) through the decoder layer. Parameters: Name Type Description Default tgt ndarray the sequence to the decoder layer (required). required memory ndarray the sequence from the last layer of the encoder (required). required tgt_mask Optional[numpy.ndarray] the mask for the tgt sequence (optional). None memory_mask Optional[numpy.ndarray] the mask for the memory sequence (optional). None tgt_key_padding_mask the mask for the tgt keys per batch (optional). required memory_key_padding_mask the mask for the memory keys per batch (optional). required Shape see the docs in Transformer class. Source code in elegy/nn/transformers.py def call ( self , tgt : np . ndarray , memory : np . ndarray , tgt_mask : tp . Optional [ np . ndarray ] = None , memory_mask : tp . Optional [ np . ndarray ] = None , # tgt_key_padding_mask: tp.Optional[np.ndarray] = None, # memory_key_padding_mask: tp.Optional[np.ndarray] = None, ) -> np . ndarray : r \"\"\"Pass the inputs (and mask) through the decoder layer. Arguments: tgt: the sequence to the decoder layer (required). memory: the sequence from the last layer of the encoder (required). tgt_mask: the mask for the tgt sequence (optional). memory_mask: the mask for the memory sequence (optional). tgt_key_padding_mask: the mask for the tgt keys per batch (optional). memory_key_padding_mask: the mask for the memory keys per batch (optional). Shape: see the docs in Transformer class. \"\"\" # Implementation of Feedforward model tgt2 = MultiHeadAttention ( self . head_size , self . num_heads , dropout = self . dropout )( tgt , mask = tgt_mask ) tgt = tgt + Dropout ( self . dropout )( tgt2 ) tgt = LayerNormalization ()( tgt ) tgt2 = MultiHeadAttention ( self . head_size , self . num_heads , dropout = self . dropout )( tgt , memory , mask = memory_mask , ) tgt = tgt + Dropout ( self . dropout )( tgt2 ) tgt = LayerNormalization ()( tgt ) tgt = tgt + sequential ( Linear ( self . output_size ), self . activation , Dropout ( self . dropout ), Linear ( self . output_size ), Dropout ( self . dropout ), )( tgt ) tgt = LayerNormalization ()( tgt ) return tgt init ( self , * , rng = None , set_defaults = False ) inherited Initializes the module, Source code in elegy/nn/transformers.py def init ( self , * , rng : tp . Optional [ types . RNGSeq ] = None , set_defaults : bool = False , ) -> tp . Callable [ ... , tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ] ]: \"\"\" Initializes the module, \"\"\" def init_callable ( * args , ** kwargs ) -> tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ]: with module_context ( module = self , collections = None , initializing = True , training = True , rng = rng ): y = self ( * args , ** kwargs ) parameters , collections = self . get_parameters_internal () self . _mark_initialized_recursive () if set_defaults : self . set_default_parameters ( parameters , collections ) return y , parameters , collections init_callable . _signature_f = self . call return init_callable","title":"TransformerDecoderLayer"},{"location":"api/nn/TransformerDecoderLayer/#elegynntransformerdecoderlayer","text":"","title":"elegy.nn.TransformerDecoderLayer"},{"location":"api/nn/TransformerDecoderLayer/#elegy.nn.transformers.TransformerDecoderLayer","text":"TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network. This standard decoder layer is based on the paper \"Attention Is All You Need\". Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010. Users may modify or implement in a different way during application. Parameters: Name Type Description Default head_size the number of expected features in the input (required). required num_heads the number of heads in the multiheadattention models (required). required output_size the dimension of the feedforward network model (default=2048). required dropout the dropout value (default=0.1). required activation the activation function of intermediate layer, relu or gelu (default=relu). required","title":"elegy.nn.transformers.TransformerDecoderLayer"},{"location":"api/nn/TransformerDecoderLayer/#elegy.nn.transformers.TransformerDecoderLayer.add_parameter","text":"Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required initializer Callable[[], Any] A callable that takes not arguments returns the initial value. required collection Optional[str] Optional name of the parameter collection, if not defined it will be se to \"parameters\" if trainable=True else it will be set to \"states\" . None trainable bool Specify whether this parameter should be added to the default trainable \"parameters\" collection or to the default non-trainable \"states\" collection. If collection is passed this parameter will ignored. True regularizer Optional[Callable[[Any], jax._src.numpy.lax_numpy.ndarray]] Regularizer instance (callable). None constraint Optional[Callable[[Any], Any]] Constraint instance (callable). None Returns: Type Description Any The value of the parameter. Source code in elegy/nn/transformers.py def add_parameter ( self , name : str , initializer : tp . Callable [[], tp . Any ], collection : tp . Optional [ str ] = None , trainable : bool = True , regularizer : tp . Optional [ tp . Callable [[ tp . Any ], jnp . ndarray ]] = None , constraint : tp . Optional [ tp . Callable [[ tp . Any ], tp . Any ]] = None , ) -> tp . Any : \"\"\" Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. initializer: A callable that takes not arguments returns the initial value. collection: Optional name of the parameter collection, if not defined it will be se to `\"parameters\"` if `trainable=True` else it will be set to `\"states\"`. trainable: Specify whether this parameter should be added to the default trainable `\"parameters\"` collection or to the default non-trainable `\"states\"` collection. If collection is passed this parameter will ignored. regularizer: Regularizer instance (callable). constraint: Constraint instance (callable). Returns: The value of the parameter. \"\"\" if collection is None : collection = \"parameters\" if trainable else \"states\" if self . _scope_params is None : self . _scope_params = {} if name not in self . _scope_params : if not self . is_initializing (): raise ValueError ( f \"Cannot add parameter { name } outside `init`\" ) if name in self . _default_params : parameter = self . _default_params [ name ] assert collection == parameter . collection initial_value = parameter . value else : initial_value = initializer () initial_value = jax . tree_map ( jnp . asarray , initial_value ) parameter = types . Parameter ( collection , initial_value ) self . _register_parameter ( name , parameter ) self . _scope_params [ name ] = parameter parameter = self . _scope_params [ name ] if parameter . collection != collection : raise ValueError ( f \"types.Parameter { name } previously found in collection { parameter . collection } \" f \"but currently being added for collection { collection } \" ) value = parameter . value if constraint is not None : value = constraint ( value ) if regularizer is not None : hooks . add_loss ( name = utils . get_name ( regularizer ), value = regularizer ( value ), ) return value","title":"add_parameter()"},{"location":"api/nn/TransformerDecoderLayer/#elegy.nn.transformers.TransformerDecoderLayer.call","text":"Pass the inputs (and mask) through the decoder layer. Parameters: Name Type Description Default tgt ndarray the sequence to the decoder layer (required). required memory ndarray the sequence from the last layer of the encoder (required). required tgt_mask Optional[numpy.ndarray] the mask for the tgt sequence (optional). None memory_mask Optional[numpy.ndarray] the mask for the memory sequence (optional). None tgt_key_padding_mask the mask for the tgt keys per batch (optional). required memory_key_padding_mask the mask for the memory keys per batch (optional). required Shape see the docs in Transformer class. Source code in elegy/nn/transformers.py def call ( self , tgt : np . ndarray , memory : np . ndarray , tgt_mask : tp . Optional [ np . ndarray ] = None , memory_mask : tp . Optional [ np . ndarray ] = None , # tgt_key_padding_mask: tp.Optional[np.ndarray] = None, # memory_key_padding_mask: tp.Optional[np.ndarray] = None, ) -> np . ndarray : r \"\"\"Pass the inputs (and mask) through the decoder layer. Arguments: tgt: the sequence to the decoder layer (required). memory: the sequence from the last layer of the encoder (required). tgt_mask: the mask for the tgt sequence (optional). memory_mask: the mask for the memory sequence (optional). tgt_key_padding_mask: the mask for the tgt keys per batch (optional). memory_key_padding_mask: the mask for the memory keys per batch (optional). Shape: see the docs in Transformer class. \"\"\" # Implementation of Feedforward model tgt2 = MultiHeadAttention ( self . head_size , self . num_heads , dropout = self . dropout )( tgt , mask = tgt_mask ) tgt = tgt + Dropout ( self . dropout )( tgt2 ) tgt = LayerNormalization ()( tgt ) tgt2 = MultiHeadAttention ( self . head_size , self . num_heads , dropout = self . dropout )( tgt , memory , mask = memory_mask , ) tgt = tgt + Dropout ( self . dropout )( tgt2 ) tgt = LayerNormalization ()( tgt ) tgt = tgt + sequential ( Linear ( self . output_size ), self . activation , Dropout ( self . dropout ), Linear ( self . output_size ), Dropout ( self . dropout ), )( tgt ) tgt = LayerNormalization ()( tgt ) return tgt","title":"call()"},{"location":"api/nn/TransformerDecoderLayer/#elegy.nn.transformers.TransformerDecoderLayer.init","text":"Initializes the module, Source code in elegy/nn/transformers.py def init ( self , * , rng : tp . Optional [ types . RNGSeq ] = None , set_defaults : bool = False , ) -> tp . Callable [ ... , tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ] ]: \"\"\" Initializes the module, \"\"\" def init_callable ( * args , ** kwargs ) -> tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ]: with module_context ( module = self , collections = None , initializing = True , training = True , rng = rng ): y = self ( * args , ** kwargs ) parameters , collections = self . get_parameters_internal () self . _mark_initialized_recursive () if set_defaults : self . set_default_parameters ( parameters , collections ) return y , parameters , collections init_callable . _signature_f = self . call return init_callable","title":"init()"},{"location":"api/nn/TransformerEncoder/","text":"elegy.nn.TransformerEncoder TransformerEncoder is a stack of N encoder layers Parameters: Name Type Description Default encoder_layer an instance of the TransformerEncoderLayer() class (required). required num_layers the number of sub-encoder-layers in the encoder (required). required norm the layer normalization component (optional). required Examples: >>> import elegy >>> transformer_encoder = elegy . nn . transformers . TransformerEncoder ( ... lambda : elegy . nn . transformers . TransformerEncoderLayer ( head_size = 512 , num_heads = 8 ), ... num_layers = 6 , ... ) >>> src = np . random . uniform ( size = ( 10 , 32 , 512 )) >>> out , params , collections = transformer_encoder . init ( rng = elegy . RNGSeq ( 42 ))( src ) add_parameter ( self , name , initializer , collection = None , trainable = True , regularizer = None , constraint = None ) inherited Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required initializer Callable[[], Any] A callable that takes not arguments returns the initial value. required collection Optional[str] Optional name of the parameter collection, if not defined it will be se to \"parameters\" if trainable=True else it will be set to \"states\" . None trainable bool Specify whether this parameter should be added to the default trainable \"parameters\" collection or to the default non-trainable \"states\" collection. If collection is passed this parameter will ignored. True regularizer Optional[Callable[[Any], jax._src.numpy.lax_numpy.ndarray]] Regularizer instance (callable). None constraint Optional[Callable[[Any], Any]] Constraint instance (callable). None Returns: Type Description Any The value of the parameter. Source code in elegy/nn/transformers.py def add_parameter ( self , name : str , initializer : tp . Callable [[], tp . Any ], collection : tp . Optional [ str ] = None , trainable : bool = True , regularizer : tp . Optional [ tp . Callable [[ tp . Any ], jnp . ndarray ]] = None , constraint : tp . Optional [ tp . Callable [[ tp . Any ], tp . Any ]] = None , ) -> tp . Any : \"\"\" Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. initializer: A callable that takes not arguments returns the initial value. collection: Optional name of the parameter collection, if not defined it will be se to `\"parameters\"` if `trainable=True` else it will be set to `\"states\"`. trainable: Specify whether this parameter should be added to the default trainable `\"parameters\"` collection or to the default non-trainable `\"states\"` collection. If collection is passed this parameter will ignored. regularizer: Regularizer instance (callable). constraint: Constraint instance (callable). Returns: The value of the parameter. \"\"\" if collection is None : collection = \"parameters\" if trainable else \"states\" if self . _scope_params is None : self . _scope_params = {} if name not in self . _scope_params : if not self . is_initializing (): raise ValueError ( f \"Cannot add parameter { name } outside `init`\" ) if name in self . _default_params : parameter = self . _default_params [ name ] assert collection == parameter . collection initial_value = parameter . value else : initial_value = initializer () initial_value = jax . tree_map ( jnp . asarray , initial_value ) parameter = types . Parameter ( collection , initial_value ) self . _register_parameter ( name , parameter ) self . _scope_params [ name ] = parameter parameter = self . _scope_params [ name ] if parameter . collection != collection : raise ValueError ( f \"types.Parameter { name } previously found in collection { parameter . collection } \" f \"but currently being added for collection { collection } \" ) value = parameter . value if constraint is not None : value = constraint ( value ) if regularizer is not None : hooks . add_loss ( name = utils . get_name ( regularizer ), value = regularizer ( value ), ) return value call ( self , src , mask = None ) Pass the input through the encoder layers in turn. Parameters: Name Type Description Default src ndarray the sequence to the encoder (required). required mask Optional[numpy.ndarray] the mask for the src sequence (optional). None src_key_padding_mask the mask for the src keys per batch (optional). required Shape see the docs in Transformer class. Source code in elegy/nn/transformers.py def call ( self , src : np . ndarray , mask : tp . Optional [ np . ndarray ] = None , # src_key_padding_mask: tp.Optional[np.ndarray] = None, ) -> np . ndarray : r \"\"\"Pass the input through the encoder layers in turn. Arguments: src: the sequence to the encoder (required). mask: the mask for the src sequence (optional). src_key_padding_mask: the mask for the src keys per batch (optional). Shape: see the docs in Transformer class. \"\"\" output = src for _ in range ( self . num_layers ): output = self . encoder_layer ()( output , mask = mask ) if self . norm is not None : output = self . norm ()( output ) return output init ( self , * , rng = None , set_defaults = False ) inherited Initializes the module, Source code in elegy/nn/transformers.py def init ( self , * , rng : tp . Optional [ types . RNGSeq ] = None , set_defaults : bool = False , ) -> tp . Callable [ ... , tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ] ]: \"\"\" Initializes the module, \"\"\" def init_callable ( * args , ** kwargs ) -> tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ]: with module_context ( module = self , collections = None , initializing = True , training = True , rng = rng ): y = self ( * args , ** kwargs ) parameters , collections = self . get_parameters_internal () self . _mark_initialized_recursive () if set_defaults : self . set_default_parameters ( parameters , collections ) return y , parameters , collections init_callable . _signature_f = self . call return init_callable","title":"TransformerEncoder"},{"location":"api/nn/TransformerEncoder/#elegynntransformerencoder","text":"","title":"elegy.nn.TransformerEncoder"},{"location":"api/nn/TransformerEncoder/#elegy.nn.transformers.TransformerEncoder","text":"TransformerEncoder is a stack of N encoder layers Parameters: Name Type Description Default encoder_layer an instance of the TransformerEncoderLayer() class (required). required num_layers the number of sub-encoder-layers in the encoder (required). required norm the layer normalization component (optional). required Examples: >>> import elegy >>> transformer_encoder = elegy . nn . transformers . TransformerEncoder ( ... lambda : elegy . nn . transformers . TransformerEncoderLayer ( head_size = 512 , num_heads = 8 ), ... num_layers = 6 , ... ) >>> src = np . random . uniform ( size = ( 10 , 32 , 512 )) >>> out , params , collections = transformer_encoder . init ( rng = elegy . RNGSeq ( 42 ))( src )","title":"elegy.nn.transformers.TransformerEncoder"},{"location":"api/nn/TransformerEncoder/#elegy.nn.transformers.TransformerEncoder.add_parameter","text":"Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required initializer Callable[[], Any] A callable that takes not arguments returns the initial value. required collection Optional[str] Optional name of the parameter collection, if not defined it will be se to \"parameters\" if trainable=True else it will be set to \"states\" . None trainable bool Specify whether this parameter should be added to the default trainable \"parameters\" collection or to the default non-trainable \"states\" collection. If collection is passed this parameter will ignored. True regularizer Optional[Callable[[Any], jax._src.numpy.lax_numpy.ndarray]] Regularizer instance (callable). None constraint Optional[Callable[[Any], Any]] Constraint instance (callable). None Returns: Type Description Any The value of the parameter. Source code in elegy/nn/transformers.py def add_parameter ( self , name : str , initializer : tp . Callable [[], tp . Any ], collection : tp . Optional [ str ] = None , trainable : bool = True , regularizer : tp . Optional [ tp . Callable [[ tp . Any ], jnp . ndarray ]] = None , constraint : tp . Optional [ tp . Callable [[ tp . Any ], tp . Any ]] = None , ) -> tp . Any : \"\"\" Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. initializer: A callable that takes not arguments returns the initial value. collection: Optional name of the parameter collection, if not defined it will be se to `\"parameters\"` if `trainable=True` else it will be set to `\"states\"`. trainable: Specify whether this parameter should be added to the default trainable `\"parameters\"` collection or to the default non-trainable `\"states\"` collection. If collection is passed this parameter will ignored. regularizer: Regularizer instance (callable). constraint: Constraint instance (callable). Returns: The value of the parameter. \"\"\" if collection is None : collection = \"parameters\" if trainable else \"states\" if self . _scope_params is None : self . _scope_params = {} if name not in self . _scope_params : if not self . is_initializing (): raise ValueError ( f \"Cannot add parameter { name } outside `init`\" ) if name in self . _default_params : parameter = self . _default_params [ name ] assert collection == parameter . collection initial_value = parameter . value else : initial_value = initializer () initial_value = jax . tree_map ( jnp . asarray , initial_value ) parameter = types . Parameter ( collection , initial_value ) self . _register_parameter ( name , parameter ) self . _scope_params [ name ] = parameter parameter = self . _scope_params [ name ] if parameter . collection != collection : raise ValueError ( f \"types.Parameter { name } previously found in collection { parameter . collection } \" f \"but currently being added for collection { collection } \" ) value = parameter . value if constraint is not None : value = constraint ( value ) if regularizer is not None : hooks . add_loss ( name = utils . get_name ( regularizer ), value = regularizer ( value ), ) return value","title":"add_parameter()"},{"location":"api/nn/TransformerEncoder/#elegy.nn.transformers.TransformerEncoder.call","text":"Pass the input through the encoder layers in turn. Parameters: Name Type Description Default src ndarray the sequence to the encoder (required). required mask Optional[numpy.ndarray] the mask for the src sequence (optional). None src_key_padding_mask the mask for the src keys per batch (optional). required Shape see the docs in Transformer class. Source code in elegy/nn/transformers.py def call ( self , src : np . ndarray , mask : tp . Optional [ np . ndarray ] = None , # src_key_padding_mask: tp.Optional[np.ndarray] = None, ) -> np . ndarray : r \"\"\"Pass the input through the encoder layers in turn. Arguments: src: the sequence to the encoder (required). mask: the mask for the src sequence (optional). src_key_padding_mask: the mask for the src keys per batch (optional). Shape: see the docs in Transformer class. \"\"\" output = src for _ in range ( self . num_layers ): output = self . encoder_layer ()( output , mask = mask ) if self . norm is not None : output = self . norm ()( output ) return output","title":"call()"},{"location":"api/nn/TransformerEncoder/#elegy.nn.transformers.TransformerEncoder.init","text":"Initializes the module, Source code in elegy/nn/transformers.py def init ( self , * , rng : tp . Optional [ types . RNGSeq ] = None , set_defaults : bool = False , ) -> tp . Callable [ ... , tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ] ]: \"\"\" Initializes the module, \"\"\" def init_callable ( * args , ** kwargs ) -> tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ]: with module_context ( module = self , collections = None , initializing = True , training = True , rng = rng ): y = self ( * args , ** kwargs ) parameters , collections = self . get_parameters_internal () self . _mark_initialized_recursive () if set_defaults : self . set_default_parameters ( parameters , collections ) return y , parameters , collections init_callable . _signature_f = self . call return init_callable","title":"init()"},{"location":"api/nn/TransformerEncoderLayer/","text":"elegy.nn.TransformerEncoderLayer TransformerEncoderLayer is made up of self-attn and feedforward network. This standard encoder layer is based on the paper \"Attention Is All You Need\". Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010. Users may modify or implement in a different way during application. Parameters: Name Type Description Default head_size the number of expected features in the input (required). required num_heads the number of heads in the multiheadattention models (required). required output_size the dimension of the feedforward network model (default=2048). required dropout the dropout value (default=0.1). required activation the activation function of intermediate layer, relu or gelu (default=relu). required add_parameter ( self , name , initializer , collection = None , trainable = True , regularizer = None , constraint = None ) inherited Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required initializer Callable[[], Any] A callable that takes not arguments returns the initial value. required collection Optional[str] Optional name of the parameter collection, if not defined it will be se to \"parameters\" if trainable=True else it will be set to \"states\" . None trainable bool Specify whether this parameter should be added to the default trainable \"parameters\" collection or to the default non-trainable \"states\" collection. If collection is passed this parameter will ignored. True regularizer Optional[Callable[[Any], jax._src.numpy.lax_numpy.ndarray]] Regularizer instance (callable). None constraint Optional[Callable[[Any], Any]] Constraint instance (callable). None Returns: Type Description Any The value of the parameter. Source code in elegy/nn/transformers.py def add_parameter ( self , name : str , initializer : tp . Callable [[], tp . Any ], collection : tp . Optional [ str ] = None , trainable : bool = True , regularizer : tp . Optional [ tp . Callable [[ tp . Any ], jnp . ndarray ]] = None , constraint : tp . Optional [ tp . Callable [[ tp . Any ], tp . Any ]] = None , ) -> tp . Any : \"\"\" Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. initializer: A callable that takes not arguments returns the initial value. collection: Optional name of the parameter collection, if not defined it will be se to `\"parameters\"` if `trainable=True` else it will be set to `\"states\"`. trainable: Specify whether this parameter should be added to the default trainable `\"parameters\"` collection or to the default non-trainable `\"states\"` collection. If collection is passed this parameter will ignored. regularizer: Regularizer instance (callable). constraint: Constraint instance (callable). Returns: The value of the parameter. \"\"\" if collection is None : collection = \"parameters\" if trainable else \"states\" if self . _scope_params is None : self . _scope_params = {} if name not in self . _scope_params : if not self . is_initializing (): raise ValueError ( f \"Cannot add parameter { name } outside `init`\" ) if name in self . _default_params : parameter = self . _default_params [ name ] assert collection == parameter . collection initial_value = parameter . value else : initial_value = initializer () initial_value = jax . tree_map ( jnp . asarray , initial_value ) parameter = types . Parameter ( collection , initial_value ) self . _register_parameter ( name , parameter ) self . _scope_params [ name ] = parameter parameter = self . _scope_params [ name ] if parameter . collection != collection : raise ValueError ( f \"types.Parameter { name } previously found in collection { parameter . collection } \" f \"but currently being added for collection { collection } \" ) value = parameter . value if constraint is not None : value = constraint ( value ) if regularizer is not None : hooks . add_loss ( name = utils . get_name ( regularizer ), value = regularizer ( value ), ) return value call ( self , src , mask = None ) Pass the input through the encoder layer. Parameters: Name Type Description Default src ndarray the sequence to the encoder layer (required). required mask Optional[numpy.ndarray] the mask for the src sequence (optional). None src_key_padding_mask the mask for the src keys per batch (optional). required Shape see the docs in Transformer class. Source code in elegy/nn/transformers.py def call ( self , src : np . ndarray , mask : tp . Optional [ np . ndarray ] = None , # src_key_padding_mask: tp.Optional[np.ndarray] = None, ) -> np . ndarray : r \"\"\"Pass the input through the encoder layer. Arguments: src: the sequence to the encoder layer (required). mask: the mask for the src sequence (optional). src_key_padding_mask: the mask for the src keys per batch (optional). Shape: see the docs in Transformer class. \"\"\" # Implementation of Feedforward model output_size : int = ( self . output_size if self . output_size is not None else src . shape [ - 1 ] ) src2 = MultiHeadAttention ( self . head_size , self . num_heads , dropout = self . dropout , )( src , mask = mask ) src = src + Dropout ( self . dropout )( src2 ) src = LayerNormalization ()( src ) src2 = sequential ( Linear ( output_size ), self . activation , Dropout ( self . dropout ), Linear ( output_size ), )( src ) src = src + Dropout ( self . dropout )( src2 ) src = LayerNormalization ()( src ) return src init ( self , * , rng = None , set_defaults = False ) inherited Initializes the module, Source code in elegy/nn/transformers.py def init ( self , * , rng : tp . Optional [ types . RNGSeq ] = None , set_defaults : bool = False , ) -> tp . Callable [ ... , tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ] ]: \"\"\" Initializes the module, \"\"\" def init_callable ( * args , ** kwargs ) -> tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ]: with module_context ( module = self , collections = None , initializing = True , training = True , rng = rng ): y = self ( * args , ** kwargs ) parameters , collections = self . get_parameters_internal () self . _mark_initialized_recursive () if set_defaults : self . set_default_parameters ( parameters , collections ) return y , parameters , collections init_callable . _signature_f = self . call return init_callable","title":"TransformerEncoderLayer"},{"location":"api/nn/TransformerEncoderLayer/#elegynntransformerencoderlayer","text":"","title":"elegy.nn.TransformerEncoderLayer"},{"location":"api/nn/TransformerEncoderLayer/#elegy.nn.transformers.TransformerEncoderLayer","text":"TransformerEncoderLayer is made up of self-attn and feedforward network. This standard encoder layer is based on the paper \"Attention Is All You Need\". Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010. Users may modify or implement in a different way during application. Parameters: Name Type Description Default head_size the number of expected features in the input (required). required num_heads the number of heads in the multiheadattention models (required). required output_size the dimension of the feedforward network model (default=2048). required dropout the dropout value (default=0.1). required activation the activation function of intermediate layer, relu or gelu (default=relu). required","title":"elegy.nn.transformers.TransformerEncoderLayer"},{"location":"api/nn/TransformerEncoderLayer/#elegy.nn.transformers.TransformerEncoderLayer.add_parameter","text":"Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Parameters: Name Type Description Default name str The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. required initializer Callable[[], Any] A callable that takes not arguments returns the initial value. required collection Optional[str] Optional name of the parameter collection, if not defined it will be se to \"parameters\" if trainable=True else it will be set to \"states\" . None trainable bool Specify whether this parameter should be added to the default trainable \"parameters\" collection or to the default non-trainable \"states\" collection. If collection is passed this parameter will ignored. True regularizer Optional[Callable[[Any], jax._src.numpy.lax_numpy.ndarray]] Regularizer instance (callable). None constraint Optional[Callable[[Any], Any]] Constraint instance (callable). None Returns: Type Description Any The value of the parameter. Source code in elegy/nn/transformers.py def add_parameter ( self , name : str , initializer : tp . Callable [[], tp . Any ], collection : tp . Optional [ str ] = None , trainable : bool = True , regularizer : tp . Optional [ tp . Callable [[ tp . Any ], jnp . ndarray ]] = None , constraint : tp . Optional [ tp . Callable [[ tp . Any ], tp . Any ]] = None , ) -> tp . Any : \"\"\" Adds a parameter to the current module. The parameter will only be initialized once and will reused afterwards. Arguments: name: The name of the parameter. It must be unique and no other field/property/method of the instance can have that name. initializer: A callable that takes not arguments returns the initial value. collection: Optional name of the parameter collection, if not defined it will be se to `\"parameters\"` if `trainable=True` else it will be set to `\"states\"`. trainable: Specify whether this parameter should be added to the default trainable `\"parameters\"` collection or to the default non-trainable `\"states\"` collection. If collection is passed this parameter will ignored. regularizer: Regularizer instance (callable). constraint: Constraint instance (callable). Returns: The value of the parameter. \"\"\" if collection is None : collection = \"parameters\" if trainable else \"states\" if self . _scope_params is None : self . _scope_params = {} if name not in self . _scope_params : if not self . is_initializing (): raise ValueError ( f \"Cannot add parameter { name } outside `init`\" ) if name in self . _default_params : parameter = self . _default_params [ name ] assert collection == parameter . collection initial_value = parameter . value else : initial_value = initializer () initial_value = jax . tree_map ( jnp . asarray , initial_value ) parameter = types . Parameter ( collection , initial_value ) self . _register_parameter ( name , parameter ) self . _scope_params [ name ] = parameter parameter = self . _scope_params [ name ] if parameter . collection != collection : raise ValueError ( f \"types.Parameter { name } previously found in collection { parameter . collection } \" f \"but currently being added for collection { collection } \" ) value = parameter . value if constraint is not None : value = constraint ( value ) if regularizer is not None : hooks . add_loss ( name = utils . get_name ( regularizer ), value = regularizer ( value ), ) return value","title":"add_parameter()"},{"location":"api/nn/TransformerEncoderLayer/#elegy.nn.transformers.TransformerEncoderLayer.call","text":"Pass the input through the encoder layer. Parameters: Name Type Description Default src ndarray the sequence to the encoder layer (required). required mask Optional[numpy.ndarray] the mask for the src sequence (optional). None src_key_padding_mask the mask for the src keys per batch (optional). required Shape see the docs in Transformer class. Source code in elegy/nn/transformers.py def call ( self , src : np . ndarray , mask : tp . Optional [ np . ndarray ] = None , # src_key_padding_mask: tp.Optional[np.ndarray] = None, ) -> np . ndarray : r \"\"\"Pass the input through the encoder layer. Arguments: src: the sequence to the encoder layer (required). mask: the mask for the src sequence (optional). src_key_padding_mask: the mask for the src keys per batch (optional). Shape: see the docs in Transformer class. \"\"\" # Implementation of Feedforward model output_size : int = ( self . output_size if self . output_size is not None else src . shape [ - 1 ] ) src2 = MultiHeadAttention ( self . head_size , self . num_heads , dropout = self . dropout , )( src , mask = mask ) src = src + Dropout ( self . dropout )( src2 ) src = LayerNormalization ()( src ) src2 = sequential ( Linear ( output_size ), self . activation , Dropout ( self . dropout ), Linear ( output_size ), )( src ) src = src + Dropout ( self . dropout )( src2 ) src = LayerNormalization ()( src ) return src","title":"call()"},{"location":"api/nn/TransformerEncoderLayer/#elegy.nn.transformers.TransformerEncoderLayer.init","text":"Initializes the module, Source code in elegy/nn/transformers.py def init ( self , * , rng : tp . Optional [ types . RNGSeq ] = None , set_defaults : bool = False , ) -> tp . Callable [ ... , tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ] ]: \"\"\" Initializes the module, \"\"\" def init_callable ( * args , ** kwargs ) -> tp . Tuple [ tp . Any , tp . Optional [ types . Parameters ], types . ParameterCollection ]: with module_context ( module = self , collections = None , initializing = True , training = True , rng = rng ): y = self ( * args , ** kwargs ) parameters , collections = self . get_parameters_internal () self . _mark_initialized_recursive () if set_defaults : self . set_default_parameters ( parameters , collections ) return y , parameters , collections init_callable . _signature_f = self . call return init_callable","title":"init()"},{"location":"api/nn/sequential/","text":"elegy.nn.sequential Connects all layers. *args and **kwargs are passed to the first layer. def call ( self , x): mlp = elegy . nn . sequential( elegy . nn . Linear( 64 ), jax . nn . relu, elegy . nn . Linear( 32 ), jax . nn . relu, elegy . nn . Linear( 10 ), jax . nn . softmax, ) y = mlp(x) ... Note sequential is not a Module , that is, it wont create a scope over the layers it runs, in contrast to Sequential layers are eagerly instantiate outside of sequential and just passed to it to automate the execution. Parameters: Name Type Description Default layers Callable[..., Any] Modules or functions passed as *args () Returns: Type Description Callable[..., Any] A callable that waits for the inputs and applies the layers sequentially. Source code in elegy/nn/sequential_module.py def sequential ( * layers : tp . Callable [ ... , tp . Any ]) -> tp . Callable [ ... , tp . Any ]: \"\"\" Connects all layers. `*args` and `**kwargs` are passed to the first layer. ```python def call(self, x): mlp = elegy.nn.sequential( elegy.nn.Linear(64), jax.nn.relu, elegy.nn.Linear(32), jax.nn.relu, elegy.nn.Linear(10), jax.nn.softmax, ) y = mlp(x) ... ``` !!! Note `sequential` is not a `Module`, that is, it wont create a scope over the layers it runs, in contrast to `Sequential` layers are eagerly instantiate outside of `sequential` and just passed to it to automate the execution. Arguments: layers: Modules or functions passed as `*args` Returns: A callable that waits for the inputs and applies the layers sequentially. \"\"\" def call ( inputs , * args , ** kwargs ): out = inputs for i , layer in enumerate ( layers ): if i == 0 : out = layer ( out , * args , ** kwargs ) else : out = layer ( out ) if not isinstance ( layer , module . Module ): if hooks . summaries_active (): name = utils . get_name ( layer ) path = module . get_module_path () path = path if path is not None else () hooks . add_summary ( path + ( name ,), layer , out ) return out return call","title":"sequential"},{"location":"api/nn/sequential/#elegynnsequential","text":"","title":"elegy.nn.sequential"},{"location":"api/nn/sequential/#elegy.nn.sequential_module.sequential","text":"Connects all layers. *args and **kwargs are passed to the first layer. def call ( self , x): mlp = elegy . nn . sequential( elegy . nn . Linear( 64 ), jax . nn . relu, elegy . nn . Linear( 32 ), jax . nn . relu, elegy . nn . Linear( 10 ), jax . nn . softmax, ) y = mlp(x) ... Note sequential is not a Module , that is, it wont create a scope over the layers it runs, in contrast to Sequential layers are eagerly instantiate outside of sequential and just passed to it to automate the execution. Parameters: Name Type Description Default layers Callable[..., Any] Modules or functions passed as *args () Returns: Type Description Callable[..., Any] A callable that waits for the inputs and applies the layers sequentially. Source code in elegy/nn/sequential_module.py def sequential ( * layers : tp . Callable [ ... , tp . Any ]) -> tp . Callable [ ... , tp . Any ]: \"\"\" Connects all layers. `*args` and `**kwargs` are passed to the first layer. ```python def call(self, x): mlp = elegy.nn.sequential( elegy.nn.Linear(64), jax.nn.relu, elegy.nn.Linear(32), jax.nn.relu, elegy.nn.Linear(10), jax.nn.softmax, ) y = mlp(x) ... ``` !!! Note `sequential` is not a `Module`, that is, it wont create a scope over the layers it runs, in contrast to `Sequential` layers are eagerly instantiate outside of `sequential` and just passed to it to automate the execution. Arguments: layers: Modules or functions passed as `*args` Returns: A callable that waits for the inputs and applies the layers sequentially. \"\"\" def call ( inputs , * args , ** kwargs ): out = inputs for i , layer in enumerate ( layers ): if i == 0 : out = layer ( out , * args , ** kwargs ) else : out = layer ( out ) if not isinstance ( layer , module . Module ): if hooks . summaries_active (): name = utils . get_name ( layer ) path = module . get_module_path () path = path if path is not None else () hooks . add_summary ( path + ( name ,), layer , out ) return out return call","title":"elegy.nn.sequential_module.sequential"},{"location":"api/regularizers/GlobalL1/","text":"elegy.regularizers.GlobalL1 Create a regularizer that applies an L1 regularization penalty. The L1 regularization penalty is computed as: \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| Usage: model = elegy . Model( module_fn, loss = [ elegy . losses . SparseCategoricalCrossentropy(), elegy . regularizers . GlobalL1(l =1e-5 ) ], metrics = lambda : elegy . metrics . SparseCategoricalAccuracy(), ) Parameters: Name Type Description Default l float L1 regularization factor. 0.01 kwargs Additional keyword arguments passed to Module. {} Returns: Type Description GlobalL1L2 An L1 Regularizer with the given regularization factor. Source code in elegy/regularizers/global_l1.py def GlobalL1 ( l : float = 0.01 , reduction : tp . Optional [ Reduction ] = None , name : str = \"l1_regularization\" , ** kwargs ) -> GlobalL1L2 : r \"\"\" Create a regularizer that applies an L1 regularization penalty. The L1 regularization penalty is computed as: $$\\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i|$$ Usage: ```python model = elegy.Model( module_fn, loss=[ elegy.losses.SparseCategoricalCrossentropy(), elegy.regularizers.GlobalL1(l=1e-5) ], metrics=lambda: elegy.metrics.SparseCategoricalAccuracy(), ) ``` Arguments: l: L1 regularization factor. kwargs: Additional keyword arguments passed to Module. Returns: An L1 Regularizer with the given regularization factor. \"\"\" return GlobalL1L2 ( l1 = l , reduction = reduction , name = name , ** kwargs )","title":"GlobalL1"},{"location":"api/regularizers/GlobalL1/#elegyregularizersgloball1","text":"","title":"elegy.regularizers.GlobalL1"},{"location":"api/regularizers/GlobalL1/#elegy.regularizers.global_l1.GlobalL1","text":"Create a regularizer that applies an L1 regularization penalty. The L1 regularization penalty is computed as: \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| Usage: model = elegy . Model( module_fn, loss = [ elegy . losses . SparseCategoricalCrossentropy(), elegy . regularizers . GlobalL1(l =1e-5 ) ], metrics = lambda : elegy . metrics . SparseCategoricalAccuracy(), ) Parameters: Name Type Description Default l float L1 regularization factor. 0.01 kwargs Additional keyword arguments passed to Module. {} Returns: Type Description GlobalL1L2 An L1 Regularizer with the given regularization factor. Source code in elegy/regularizers/global_l1.py def GlobalL1 ( l : float = 0.01 , reduction : tp . Optional [ Reduction ] = None , name : str = \"l1_regularization\" , ** kwargs ) -> GlobalL1L2 : r \"\"\" Create a regularizer that applies an L1 regularization penalty. The L1 regularization penalty is computed as: $$\\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i|$$ Usage: ```python model = elegy.Model( module_fn, loss=[ elegy.losses.SparseCategoricalCrossentropy(), elegy.regularizers.GlobalL1(l=1e-5) ], metrics=lambda: elegy.metrics.SparseCategoricalAccuracy(), ) ``` Arguments: l: L1 regularization factor. kwargs: Additional keyword arguments passed to Module. Returns: An L1 Regularizer with the given regularization factor. \"\"\" return GlobalL1L2 ( l1 = l , reduction = reduction , name = name , ** kwargs )","title":"elegy.regularizers.global_l1.GlobalL1"},{"location":"api/regularizers/GlobalL1L2/","text":"elegy.regularizers.GlobalL1L2 A regularizer that applies both L1 and L2 regularization penalties. The L1 regularization penalty is computed as: \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| The L2 regularization penalty is computed as \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 Usage: model = elegy . Model( module_fn, loss = [ elegy . losses . SparseCategoricalCrossentropy(), elegy . regularizers . GlobalL1L2(l1 =1e-5 , l2 =1e-4 ), ], metrics = lambda : elegy . metrics . SparseCategoricalAccuracy(), ) Attributes: Name Type Description l1 L1 regularization factor. l2 L2 regularization factor. call ( self , states ) Computes the L1 and L2 regularization penalty simultaneously. Parameters: Name Type Description Default net_params A structure with all the parameters of the model. required Source code in elegy/regularizers/global_l1l2.py def call ( self , states : types . States ) -> jnp . ndarray : \"\"\" Computes the L1 and L2 regularization penalty simultaneously. Arguments: net_params: A structure with all the parameters of the model. \"\"\" net_params = states . net_params regularization : jnp . ndarray = jnp . array ( 0.0 ) if not self . l1 and not self . l2 : return regularization if self . l1 : regularization += self . l1 * sum ( jnp . sum ( jnp . abs ( p )) for p in jax . tree_leaves ( net_params ) ) if self . l2 : regularization += self . l2 * sum ( jnp . sum ( jnp . square ( p )) for p in jax . tree_leaves ( net_params ) ) return regularization","title":"GlobalL1L2"},{"location":"api/regularizers/GlobalL1L2/#elegyregularizersgloball1l2","text":"","title":"elegy.regularizers.GlobalL1L2"},{"location":"api/regularizers/GlobalL1L2/#elegy.regularizers.global_l1l2.GlobalL1L2","text":"A regularizer that applies both L1 and L2 regularization penalties. The L1 regularization penalty is computed as: \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| \\ell_1\\,\\,penalty =\\ell_1\\sum_{i=0}^n|x_i| The L2 regularization penalty is computed as \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 Usage: model = elegy . Model( module_fn, loss = [ elegy . losses . SparseCategoricalCrossentropy(), elegy . regularizers . GlobalL1L2(l1 =1e-5 , l2 =1e-4 ), ], metrics = lambda : elegy . metrics . SparseCategoricalAccuracy(), ) Attributes: Name Type Description l1 L1 regularization factor. l2 L2 regularization factor.","title":"elegy.regularizers.global_l1l2.GlobalL1L2"},{"location":"api/regularizers/GlobalL1L2/#elegy.regularizers.global_l1l2.GlobalL1L2.call","text":"Computes the L1 and L2 regularization penalty simultaneously. Parameters: Name Type Description Default net_params A structure with all the parameters of the model. required Source code in elegy/regularizers/global_l1l2.py def call ( self , states : types . States ) -> jnp . ndarray : \"\"\" Computes the L1 and L2 regularization penalty simultaneously. Arguments: net_params: A structure with all the parameters of the model. \"\"\" net_params = states . net_params regularization : jnp . ndarray = jnp . array ( 0.0 ) if not self . l1 and not self . l2 : return regularization if self . l1 : regularization += self . l1 * sum ( jnp . sum ( jnp . abs ( p )) for p in jax . tree_leaves ( net_params ) ) if self . l2 : regularization += self . l2 * sum ( jnp . sum ( jnp . square ( p )) for p in jax . tree_leaves ( net_params ) ) return regularization","title":"call()"},{"location":"api/regularizers/GlobalL2/","text":"elegy.regularizers.GlobalL2 Create a regularizer that applies an L2 regularization penalty. The L2 regularization penalty is computed as: \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 Usage: model = elegy . Model( module_fn, loss = [ elegy . losses . SparseCategoricalCrossentropy(), elegy . losses . GlobalL2Regularization(l =1e-4 ), ], metrics = lambda : elegy . metrics . SparseCategoricalAccuracy(), ) Parameters: Name Type Description Default l float L2 regularization factor. 0.01 Returns: Type Description GlobalL1L2 An L2 Regularizer with the given regularization factor. Source code in elegy/regularizers/global_l2.py def GlobalL2 ( l : float = 0.01 , reduction : tp . Optional [ Reduction ] = None , name : str = \"l2_regularization\" , ) -> GlobalL1L2 : r \"\"\" Create a regularizer that applies an L2 regularization penalty. The L2 regularization penalty is computed as: $$\\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2$$ Usage: ```python model = elegy.Model( module_fn, loss=[ elegy.losses.SparseCategoricalCrossentropy(), elegy.losses.GlobalL2Regularization(l=1e-4), ], metrics=lambda: elegy.metrics.SparseCategoricalAccuracy(), ) ``` Arguments: l: L2 regularization factor. Returns: An L2 Regularizer with the given regularization factor. \"\"\" return GlobalL1L2 ( l2 = l , reduction = reduction , name = name )","title":"GlobalL2"},{"location":"api/regularizers/GlobalL2/#elegyregularizersgloball2","text":"","title":"elegy.regularizers.GlobalL2"},{"location":"api/regularizers/GlobalL2/#elegy.regularizers.global_l2.GlobalL2","text":"Create a regularizer that applies an L2 regularization penalty. The L2 regularization penalty is computed as: \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 \\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2 Usage: model = elegy . Model( module_fn, loss = [ elegy . losses . SparseCategoricalCrossentropy(), elegy . losses . GlobalL2Regularization(l =1e-4 ), ], metrics = lambda : elegy . metrics . SparseCategoricalAccuracy(), ) Parameters: Name Type Description Default l float L2 regularization factor. 0.01 Returns: Type Description GlobalL1L2 An L2 Regularizer with the given regularization factor. Source code in elegy/regularizers/global_l2.py def GlobalL2 ( l : float = 0.01 , reduction : tp . Optional [ Reduction ] = None , name : str = \"l2_regularization\" , ) -> GlobalL1L2 : r \"\"\" Create a regularizer that applies an L2 regularization penalty. The L2 regularization penalty is computed as: $$\\ell_2\\,\\,penalty =\\ell_2\\sum_{i=0}^nx_i^2$$ Usage: ```python model = elegy.Model( module_fn, loss=[ elegy.losses.SparseCategoricalCrossentropy(), elegy.losses.GlobalL2Regularization(l=1e-4), ], metrics=lambda: elegy.metrics.SparseCategoricalAccuracy(), ) ``` Arguments: l: L2 regularization factor. Returns: An L2 Regularizer with the given regularization factor. \"\"\" return GlobalL1L2 ( l2 = l , reduction = reduction , name = name )","title":"elegy.regularizers.global_l2.GlobalL2"},{"location":"basic-api/modules-losses-metrics/","text":"Modules, Losses, and Metrics This guide goes into depth on how modules, losses and metrics work in Elegy when used with an elegy.Model . For more in-depth explanation on how they work internally check out the Module System guide. Keras Limitations One of our goals with Elegy was to solve Keras restrictions around the type of losses and metrics you can define. When creating a complex model with multiple outputs in Keras, say output_a and output_b , you are forced to define losses and metrics per-output only: model . compile( loss = { \"output_a\" : keras . losses . BinaryCrossentropy(from_logits = True ), \"output_b\" : keras . losses . CategoricalCrossentropy(from_logits = True ), }, metrics = { \"output_a\" : keras . losses . BinaryAccuracy(from_logits = True ), \"output_b\" : keras . losses . CategoricalAccuracy(from_logits = True ), }, ... ) This is very restrictive, in particular it doesn't allow the following: Losses and metrics that combine multiple outputs with multiple labels. A single loss/metric based on multiple outputs (a especial case of the previous). Losses and metrics that depend on other variables such as inputs, parameters, states, etc. Most of these are usually solvable by tricks such as: Concatenating the outputs / labels Passing the inputs and other kind of information as labels. Using the functional API which is more flexible (but it only runs on graph mode making it very painful to debug). It is clear that these solution are hacky. Sometimes they are non-obvious, and depending on the problem they can be insufficient. Dependency Injection Elegy solves the previous problems by introducing a dependency injection mechanism that allows the user to express complex functions by simply declaring the variables to use by their name . The following parameters are available for the different callables you pass to Elegy: parameter Description Module Metric Loss x Inputs of the model corresponding to the x argument of fit * x x y_true The input labels corresponding to the y argument of fit x x y_pred Outputs of the model x x sample_weight Importance of each sample x x class_weight Importance of each class x x training Whether training is currently in progress x x x parameters The learnable parameters of the model x x states The non-learnable parameters of the model x x Note The content of x is technically passed to the model's Module but the parameter name \"x\" will bare no special meaning in that context. Modules Modules define the architecture of the network, their primary task (in Elegy terms) is transforming the inputs x into outputs y_pred . To make it easy to consume the content of x , Elegy has some special but very simple rules on how the signature of any Module can be structured: 1. If x is simply an array it will be passed directly: class SomeModule : def call ( self , m): ... model = elegy . Model(SomeModule(), ... ) a = get_inputs() model . fit( x = a, ... ) In this case a is passed as m . 2. If x is a tuple , then x will be expanded positional arguments a.k.a. *args , this means that the module will have define exactly as many arguments as there are inputs. For example: class SomeModule : def call ( self , m, n): ... model = elegy . Model(SomeModule(), ... ) a, b = get_inputs() model . fit( x = (a, b), ... ) In this case a is passed as m and b is passed as n . 3. If x is a dict , then x will be expanded as keyword arguments a.k.a. **kwargs , in this case the module can optionally request any key defined in x as an argument. For example: class SomeModule : def call ( self , n, o): ... model = elegy . Model(SomeModule(), ... ) a, b, c = get_inputs() model . fit( x = { \"m\" : a, \"n\" : b, \"o\" : c}, ... ) Here only n and o are requested by name, and you get as input its values b and c , the variable m with the content of a is safely ignored. If you want to request all the available inputs you can use **kwargs . Losses Losses can request all the available parameters that Elegy provides for dependency injection. A typical loss will request the y_true and y_pred values (as its common / enforced in Keras). The Mean Squared Error loss for example is easily defined in these terms: class MSE (elegy . Loss): def call ( self , y_true, y_pred): return jnp . mean(jnp . square(y_true - y_pred), axis =-1 ) ... X_train, y_train = get_inputs() model . fit( x = X_train, y = y_train, loss = MSE(), ) Here the input y is passed as y_true to MSE . For an auto-encoder however, it makes perfect sense to define the loss only in terms of x (according to the math) and Elegy lets you do exactly that: class AutoEncoderLoss (elegy . Loss): def call ( self , x, y_pred): return jnp . mean(jnp . square(x - y_pred), axis =-1 ) ... X_train, _ = get_inputs() model . fit( x = X_train, loss = AutoEncoderLoss(), ) Notice thanks to this we didn't have to define y on the fit method. Note An alternative here is to just use the previous definition of MSE and define y=X_train . However, avoiding the creation of redundant information is good in general and being explicit about dependencies helps documenting the behaviour of the model. Partitioning a loss If you have a complex loss function that is just a sum of different parts that have to be compute together you might define something like this: class SomeComplexFunction (elegy . Loss): def call ( self , x, y_true, y_pred, parameters, ... ): ... return a + b + c Elegy lets you return a dict specifying the name of each part: class SomeComplexFunction (elegy . Loss): def call ( self , x, y_true, y_pred, parameters, ... ): ... return { \"a\" : a, \"b\" : b, \"c\" : c, } Elegy will use this information to show you each loss separate in the logs / Tensorboard / History with the names: some_complex_function_loss/a some_complex_function_loss/b some_complex_function_loss/c Each individual loss will still be subject to the sample_weight and reduction behavior as specified to SomeComplexFunction . Multiple Outputs + Labels The Model 's constructor loss argument can accept a single Loss , a list or dict of losses, and even nested structures of the previous. However, in Elegy the form of loss is not strictly related to structure of input labels and outputs of the model. This is very different to Keras where each loss has to be matched with exactly one (label, output) pair. Elegy's method of dealing with multiple outputs and labels is super simple: Quote y_true will contain the entire structure passed to y . y_pred will contain the entire structure output by the Module . This means there are no restrictions on how you structure the loss function. According to this rule Keras and Elegy behave the same when there is only one output and one label because there is no structure. Both frameworks will allow you to define something like: model = Model( ... loss = elegy . losses . CategoricalCrossentropy(from_logits = True ) ) However, if you have many outputs and many labels, Elegy will just pass their structures to your loss and you will be able to do whatever you want by e.g. indexing these structures: class MyLoss (Elegy . Loss): def call ( self , y_true, y_pred): return some_function( y_true[ \"label_a\" ], y_pred[ \"output_a\" ], y_true[ \"label_b\" ] ) model = Model( ... loss = elegy . losses . MyLoss() ) This example assumes the y_true and y_pred are dictionaries but they can also be tuples or nested structures. This strategy gives you maximal flexibility but come with the additional cost of having to implement your own loss function. Keras-like behavior While having this flexibility is good, there is a common scenario that Keras covers really well: what if you really just need one loss per (label, output) pair? In other words, how can we achieve equivalent behaviour of the following Keras code? class MyModel (keras . Model): def call ( self , x): ... return { \"key_a\" : key_a, \"key_b\" : key_b, ... } ... model . compile( loss = { \"key_a\" : keras . losses . BinaryCrossentropy(), \"key_b\" : keras . losses . MeanSquaredError(), ... }, loss_weights = { \"key_a\" : 10.0 , \"key_b\" : 1.0 , ... }, ) To do this Elegy lets each Loss optionally filter / index the y_true and y_pred arguments based on a string key (for dict s) or integer key (for tuple s) in the constructor's on parameter: class MyModule : def call ( self , x): ... return { \"key_a\" : key_a, \"key_b\" : key_b, ... } ... model = elegy . Model( module = MyModule(), loss = [ elegy . losses . BinaryCrossentropy(on = \"key_a\" , weight =10.0 ), elegy . losses . MeanSquaredError(on = \"key_b\" , weight =1.0 ), ... ] ) This is almost exactly how Keras behaves except each loss is explicitly aware of which part of the output / label its supposed to attend to. The previous is roughly equivalent to manually indexing y_true and y_pred and passing the resulting value to the loss in question like this: model = elegy . Model( module = MyModule(), loss = [ lambda y_true, y_pred: elegy . losses . BinaryCrossentropy(weight =10.0 )( y_true = y_true[ \"key_a\" ], y_pred = y_pred[ \"key_a\" ], ), lambda y_true, y_pred: elegy . losses . MeanSquaredError(weight =1.0 )( y_true = y_true[ \"key_b\" ], y_pred = y_pred[ \"key_b\" ], ), ... ] ) Note For the same reasons Elegy doesn't support the loss_weights parameter as defined in keras.compile . Nonetheless, each loss accepts a weight argument directly, as seen in the examples above, which you can use to recover this behavior. Metrics Metrics behave exactly like losses except for one thing: Quote Metrics can hold state. As in Keras, Elegy metrics are cumulative so they update their internal state on every step. From a user's perspective this means that you should use the self.add_parameter and self.update_parameter hooks when implementing your own metrics. Here is an example of a simple cumulative implementation of Accuracy which uses state hooks: class Accuracy (elegy . Metric): def call ( self , y_true, y_pred): total = self . add_parameter( \"total\" , initializer =0 , trainable = False ) count = self . add_parameter( \"count\" , initializer =0 , trainable = False ) total += jnp . sum(y_true == y_pred) count += jnp . prod(y_true . shape) self . update_parameter( \"total\" , total) self . update_parameter( \"count\" , count) return total / count For a more in-depth description of how Elegy's hook system works check out the Module System guide.","title":"Intro"},{"location":"basic-api/modules-losses-metrics/#modules-losses-and-metrics","text":"This guide goes into depth on how modules, losses and metrics work in Elegy when used with an elegy.Model . For more in-depth explanation on how they work internally check out the Module System guide.","title":"Modules, Losses, and Metrics"},{"location":"basic-api/modules-losses-metrics/#keras-limitations","text":"One of our goals with Elegy was to solve Keras restrictions around the type of losses and metrics you can define. When creating a complex model with multiple outputs in Keras, say output_a and output_b , you are forced to define losses and metrics per-output only: model . compile( loss = { \"output_a\" : keras . losses . BinaryCrossentropy(from_logits = True ), \"output_b\" : keras . losses . CategoricalCrossentropy(from_logits = True ), }, metrics = { \"output_a\" : keras . losses . BinaryAccuracy(from_logits = True ), \"output_b\" : keras . losses . CategoricalAccuracy(from_logits = True ), }, ... ) This is very restrictive, in particular it doesn't allow the following: Losses and metrics that combine multiple outputs with multiple labels. A single loss/metric based on multiple outputs (a especial case of the previous). Losses and metrics that depend on other variables such as inputs, parameters, states, etc. Most of these are usually solvable by tricks such as: Concatenating the outputs / labels Passing the inputs and other kind of information as labels. Using the functional API which is more flexible (but it only runs on graph mode making it very painful to debug). It is clear that these solution are hacky. Sometimes they are non-obvious, and depending on the problem they can be insufficient.","title":"Keras Limitations"},{"location":"basic-api/modules-losses-metrics/#dependency-injection","text":"Elegy solves the previous problems by introducing a dependency injection mechanism that allows the user to express complex functions by simply declaring the variables to use by their name . The following parameters are available for the different callables you pass to Elegy: parameter Description Module Metric Loss x Inputs of the model corresponding to the x argument of fit * x x y_true The input labels corresponding to the y argument of fit x x y_pred Outputs of the model x x sample_weight Importance of each sample x x class_weight Importance of each class x x training Whether training is currently in progress x x x parameters The learnable parameters of the model x x states The non-learnable parameters of the model x x Note The content of x is technically passed to the model's Module but the parameter name \"x\" will bare no special meaning in that context.","title":"Dependency Injection"},{"location":"basic-api/modules-losses-metrics/#modules","text":"Modules define the architecture of the network, their primary task (in Elegy terms) is transforming the inputs x into outputs y_pred . To make it easy to consume the content of x , Elegy has some special but very simple rules on how the signature of any Module can be structured: 1. If x is simply an array it will be passed directly: class SomeModule : def call ( self , m): ... model = elegy . Model(SomeModule(), ... ) a = get_inputs() model . fit( x = a, ... ) In this case a is passed as m . 2. If x is a tuple , then x will be expanded positional arguments a.k.a. *args , this means that the module will have define exactly as many arguments as there are inputs. For example: class SomeModule : def call ( self , m, n): ... model = elegy . Model(SomeModule(), ... ) a, b = get_inputs() model . fit( x = (a, b), ... ) In this case a is passed as m and b is passed as n . 3. If x is a dict , then x will be expanded as keyword arguments a.k.a. **kwargs , in this case the module can optionally request any key defined in x as an argument. For example: class SomeModule : def call ( self , n, o): ... model = elegy . Model(SomeModule(), ... ) a, b, c = get_inputs() model . fit( x = { \"m\" : a, \"n\" : b, \"o\" : c}, ... ) Here only n and o are requested by name, and you get as input its values b and c , the variable m with the content of a is safely ignored. If you want to request all the available inputs you can use **kwargs .","title":"Modules"},{"location":"basic-api/modules-losses-metrics/#losses","text":"Losses can request all the available parameters that Elegy provides for dependency injection. A typical loss will request the y_true and y_pred values (as its common / enforced in Keras). The Mean Squared Error loss for example is easily defined in these terms: class MSE (elegy . Loss): def call ( self , y_true, y_pred): return jnp . mean(jnp . square(y_true - y_pred), axis =-1 ) ... X_train, y_train = get_inputs() model . fit( x = X_train, y = y_train, loss = MSE(), ) Here the input y is passed as y_true to MSE . For an auto-encoder however, it makes perfect sense to define the loss only in terms of x (according to the math) and Elegy lets you do exactly that: class AutoEncoderLoss (elegy . Loss): def call ( self , x, y_pred): return jnp . mean(jnp . square(x - y_pred), axis =-1 ) ... X_train, _ = get_inputs() model . fit( x = X_train, loss = AutoEncoderLoss(), ) Notice thanks to this we didn't have to define y on the fit method. Note An alternative here is to just use the previous definition of MSE and define y=X_train . However, avoiding the creation of redundant information is good in general and being explicit about dependencies helps documenting the behaviour of the model.","title":"Losses"},{"location":"basic-api/modules-losses-metrics/#partitioning-a-loss","text":"If you have a complex loss function that is just a sum of different parts that have to be compute together you might define something like this: class SomeComplexFunction (elegy . Loss): def call ( self , x, y_true, y_pred, parameters, ... ): ... return a + b + c Elegy lets you return a dict specifying the name of each part: class SomeComplexFunction (elegy . Loss): def call ( self , x, y_true, y_pred, parameters, ... ): ... return { \"a\" : a, \"b\" : b, \"c\" : c, } Elegy will use this information to show you each loss separate in the logs / Tensorboard / History with the names: some_complex_function_loss/a some_complex_function_loss/b some_complex_function_loss/c Each individual loss will still be subject to the sample_weight and reduction behavior as specified to SomeComplexFunction .","title":"Partitioning a loss"},{"location":"basic-api/modules-losses-metrics/#multiple-outputs-labels","text":"The Model 's constructor loss argument can accept a single Loss , a list or dict of losses, and even nested structures of the previous. However, in Elegy the form of loss is not strictly related to structure of input labels and outputs of the model. This is very different to Keras where each loss has to be matched with exactly one (label, output) pair. Elegy's method of dealing with multiple outputs and labels is super simple: Quote y_true will contain the entire structure passed to y . y_pred will contain the entire structure output by the Module . This means there are no restrictions on how you structure the loss function. According to this rule Keras and Elegy behave the same when there is only one output and one label because there is no structure. Both frameworks will allow you to define something like: model = Model( ... loss = elegy . losses . CategoricalCrossentropy(from_logits = True ) ) However, if you have many outputs and many labels, Elegy will just pass their structures to your loss and you will be able to do whatever you want by e.g. indexing these structures: class MyLoss (Elegy . Loss): def call ( self , y_true, y_pred): return some_function( y_true[ \"label_a\" ], y_pred[ \"output_a\" ], y_true[ \"label_b\" ] ) model = Model( ... loss = elegy . losses . MyLoss() ) This example assumes the y_true and y_pred are dictionaries but they can also be tuples or nested structures. This strategy gives you maximal flexibility but come with the additional cost of having to implement your own loss function.","title":"Multiple Outputs + Labels"},{"location":"basic-api/modules-losses-metrics/#keras-like-behavior","text":"While having this flexibility is good, there is a common scenario that Keras covers really well: what if you really just need one loss per (label, output) pair? In other words, how can we achieve equivalent behaviour of the following Keras code? class MyModel (keras . Model): def call ( self , x): ... return { \"key_a\" : key_a, \"key_b\" : key_b, ... } ... model . compile( loss = { \"key_a\" : keras . losses . BinaryCrossentropy(), \"key_b\" : keras . losses . MeanSquaredError(), ... }, loss_weights = { \"key_a\" : 10.0 , \"key_b\" : 1.0 , ... }, ) To do this Elegy lets each Loss optionally filter / index the y_true and y_pred arguments based on a string key (for dict s) or integer key (for tuple s) in the constructor's on parameter: class MyModule : def call ( self , x): ... return { \"key_a\" : key_a, \"key_b\" : key_b, ... } ... model = elegy . Model( module = MyModule(), loss = [ elegy . losses . BinaryCrossentropy(on = \"key_a\" , weight =10.0 ), elegy . losses . MeanSquaredError(on = \"key_b\" , weight =1.0 ), ... ] ) This is almost exactly how Keras behaves except each loss is explicitly aware of which part of the output / label its supposed to attend to. The previous is roughly equivalent to manually indexing y_true and y_pred and passing the resulting value to the loss in question like this: model = elegy . Model( module = MyModule(), loss = [ lambda y_true, y_pred: elegy . losses . BinaryCrossentropy(weight =10.0 )( y_true = y_true[ \"key_a\" ], y_pred = y_pred[ \"key_a\" ], ), lambda y_true, y_pred: elegy . losses . MeanSquaredError(weight =1.0 )( y_true = y_true[ \"key_b\" ], y_pred = y_pred[ \"key_b\" ], ), ... ] ) Note For the same reasons Elegy doesn't support the loss_weights parameter as defined in keras.compile . Nonetheless, each loss accepts a weight argument directly, as seen in the examples above, which you can use to recover this behavior.","title":"Keras-like behavior"},{"location":"basic-api/modules-losses-metrics/#metrics","text":"Metrics behave exactly like losses except for one thing: Quote Metrics can hold state. As in Keras, Elegy metrics are cumulative so they update their internal state on every step. From a user's perspective this means that you should use the self.add_parameter and self.update_parameter hooks when implementing your own metrics. Here is an example of a simple cumulative implementation of Accuracy which uses state hooks: class Accuracy (elegy . Metric): def call ( self , y_true, y_pred): total = self . add_parameter( \"total\" , initializer =0 , trainable = False ) count = self . add_parameter( \"count\" , initializer =0 , trainable = False ) total += jnp . sum(y_true == y_pred) count += jnp . prod(y_true . shape) self . update_parameter( \"total\" , total) self . update_parameter( \"count\" , count) return total / count For a more in-depth description of how Elegy's hook system works check out the Module System guide.","title":"Metrics"},{"location":"getting-started/high-level-api/","text":"pre { line-height: 125%; } td.linenos pre { color: #000000; background-color: #f0f0f0; padding-left: 5px; padding-right: 5px; } span.linenos { color: #000000; background-color: #f0f0f0; padding-left: 5px; padding-right: 5px; } td.linenos pre.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; } span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; } .highlight .hll { background-color: var(--jp-cell-editor-active-background) } .highlight { background: var(--jp-cell-editor-background); color: var(--jp-mirror-editor-variable-color) } .highlight .c { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment */ .highlight .err { color: var(--jp-mirror-editor-error-color) } /* Error */ .highlight .k { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword */ .highlight .o { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator */ .highlight .p { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation */ .highlight .ch { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Multiline */ .highlight .cp { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Preproc */ .highlight .cpf { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Single */ .highlight .cs { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Special */ .highlight .kc { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Pseudo */ .highlight .kr { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Type */ .highlight .m { color: var(--jp-mirror-editor-number-color) } /* Literal.Number */ .highlight .s { color: var(--jp-mirror-editor-string-color) } /* Literal.String */ .highlight .ow { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator.Word */ .highlight .w { color: var(--jp-mirror-editor-variable-color) } /* Text.Whitespace */ .highlight .mb { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Bin */ .highlight .mf { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Float */ .highlight .mh { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Hex */ .highlight .mi { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer */ .highlight .mo { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Oct */ .highlight .sa { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Affix */ .highlight .sb { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Backtick */ .highlight .sc { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Char */ .highlight .dl { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Delimiter */ .highlight .sd { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Doc */ .highlight .s2 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Double */ .highlight .se { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Escape */ .highlight .sh { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Heredoc */ .highlight .si { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Interpol */ .highlight .sx { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Other */ .highlight .sr { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Regex */ .highlight .s1 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Single */ .highlight .ss { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Symbol */ .highlight .il { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer.Long */ /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /* * Mozilla scrollbar styling */ /* use standard opaque scrollbars for most nodes */ [data-jp-theme-scrollbars='true'] { scrollbar-color: rgb(var(--jp-scrollbar-thumb-color)) var(--jp-scrollbar-background-color); } /* for code nodes, use a transparent style of scrollbar. These selectors * will match lower in the tree, and so will override the above */ [data-jp-theme-scrollbars='true'] .CodeMirror-hscrollbar, [data-jp-theme-scrollbars='true'] .CodeMirror-vscrollbar { scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent; } /* * Webkit scrollbar styling */ /* use standard opaque scrollbars for most nodes */ [data-jp-theme-scrollbars='true'] ::-webkit-scrollbar, [data-jp-theme-scrollbars='true'] ::-webkit-scrollbar-corner { background: var(--jp-scrollbar-background-color); } [data-jp-theme-scrollbars='true'] ::-webkit-scrollbar-thumb { background: rgb(var(--jp-scrollbar-thumb-color)); border: var(--jp-scrollbar-thumb-margin) solid transparent; background-clip: content-box; border-radius: var(--jp-scrollbar-thumb-radius); } [data-jp-theme-scrollbars='true'] ::-webkit-scrollbar-track:horizontal { border-left: var(--jp-scrollbar-endpad) solid var(--jp-scrollbar-background-color); border-right: var(--jp-scrollbar-endpad) solid var(--jp-scrollbar-background-color); } [data-jp-theme-scrollbars='true'] ::-webkit-scrollbar-track:vertical { border-top: var(--jp-scrollbar-endpad) solid var(--jp-scrollbar-background-color); border-bottom: var(--jp-scrollbar-endpad) solid var(--jp-scrollbar-background-color); } /* for code nodes, use a transparent style of scrollbar */ [data-jp-theme-scrollbars='true'] .CodeMirror-hscrollbar::-webkit-scrollbar, [data-jp-theme-scrollbars='true'] .CodeMirror-vscrollbar::-webkit-scrollbar, [data-jp-theme-scrollbars='true'] .CodeMirror-hscrollbar::-webkit-scrollbar-corner, [data-jp-theme-scrollbars='true'] .CodeMirror-vscrollbar::-webkit-scrollbar-corner { background-color: transparent; } [data-jp-theme-scrollbars='true'] .CodeMirror-hscrollbar::-webkit-scrollbar-thumb, [data-jp-theme-scrollbars='true'] .CodeMirror-vscrollbar::-webkit-scrollbar-thumb { background: rgba(var(--jp-scrollbar-thumb-color), 0.5); border: var(--jp-scrollbar-thumb-margin) solid transparent; background-clip: content-box; border-radius: var(--jp-scrollbar-thumb-radius); } [data-jp-theme-scrollbars='true'] .CodeMirror-hscrollbar::-webkit-scrollbar-track:horizontal { border-left: var(--jp-scrollbar-endpad) solid transparent; border-right: var(--jp-scrollbar-endpad) solid transparent; } [data-jp-theme-scrollbars='true'] .CodeMirror-vscrollbar::-webkit-scrollbar-track:vertical { border-top: var(--jp-scrollbar-endpad) solid transparent; border-bottom: var(--jp-scrollbar-endpad) solid transparent; } /* * Phosphor */ .lm-ScrollBar[data-orientation='horizontal'] { min-height: 16px; max-height: 16px; min-width: 45px; border-top: 1px solid #a0a0a0; } .lm-ScrollBar[data-orientation='vertical'] { min-width: 16px; max-width: 16px; min-height: 45px; border-left: 1px solid #a0a0a0; } .lm-ScrollBar-button { background-color: #f0f0f0; background-position: center center; min-height: 15px; max-height: 15px; min-width: 15px; max-width: 15px; } .lm-ScrollBar-button:hover { background-color: #dadada; } .lm-ScrollBar-button.lm-mod-active { background-color: #cdcdcd; } .lm-ScrollBar-track { background: #f0f0f0; } .lm-ScrollBar-thumb { background: #cdcdcd; } .lm-ScrollBar-thumb:hover { background: #bababa; } .lm-ScrollBar-thumb.lm-mod-active { background: #a0a0a0; } .lm-ScrollBar[data-orientation='horizontal'] .lm-ScrollBar-thumb { height: 100%; min-width: 15px; border-left: 1px solid #a0a0a0; border-right: 1px solid #a0a0a0; } .lm-ScrollBar[data-orientation='vertical'] .lm-ScrollBar-thumb { width: 100%; min-height: 15px; border-top: 1px solid #a0a0a0; border-bottom: 1px solid #a0a0a0; } .lm-ScrollBar[data-orientation='horizontal'] .lm-ScrollBar-button[data-action='decrement'] { background-image: var(--jp-icon-caret-left); background-size: 17px; } .lm-ScrollBar[data-orientation='horizontal'] .lm-ScrollBar-button[data-action='increment'] { background-image: var(--jp-icon-caret-right); background-size: 17px; } .lm-ScrollBar[data-orientation='vertical'] .lm-ScrollBar-button[data-action='decrement'] { background-image: var(--jp-icon-caret-up); background-size: 17px; } .lm-ScrollBar[data-orientation='vertical'] .lm-ScrollBar-button[data-action='increment'] { background-image: var(--jp-icon-caret-down); background-size: 17px; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Copyright (c) 2014-2017, PhosphorJS Contributors | | Distributed under the terms of the BSD 3-Clause License. | | The full license is in the file LICENSE, distributed with this software. |----------------------------------------------------------------------------*/ /* <DEPRECATED> */ .p-Widget, /* </DEPRECATED> */ .lm-Widget { box-sizing: border-box; position: relative; overflow: hidden; cursor: default; } /* <DEPRECATED> */ .p-Widget.p-mod-hidden, /* </DEPRECATED> */ .lm-Widget.lm-mod-hidden { display: none !important; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Copyright (c) 2014-2017, PhosphorJS Contributors | | Distributed under the terms of the BSD 3-Clause License. | | The full license is in the file LICENSE, distributed with this software. |----------------------------------------------------------------------------*/ /* <DEPRECATED> */ .p-CommandPalette, /* </DEPRECATED> */ .lm-CommandPalette { display: flex; flex-direction: column; -webkit-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; } /* <DEPRECATED> */ .p-CommandPalette-search, /* </DEPRECATED> */ .lm-CommandPalette-search { flex: 0 0 auto; } /* <DEPRECATED> */ .p-CommandPalette-content, /* </DEPRECATED> */ .lm-CommandPalette-content { flex: 1 1 auto; margin: 0; padding: 0; min-height: 0; overflow: auto; list-style-type: none; } /* <DEPRECATED> */ .p-CommandPalette-header, /* </DEPRECATED> */ .lm-CommandPalette-header { overflow: hidden; white-space: nowrap; text-overflow: ellipsis; } /* <DEPRECATED> */ .p-CommandPalette-item, /* </DEPRECATED> */ .lm-CommandPalette-item { display: flex; flex-direction: row; } /* <DEPRECATED> */ .p-CommandPalette-itemIcon, /* </DEPRECATED> */ .lm-CommandPalette-itemIcon { flex: 0 0 auto; } /* <DEPRECATED> */ .p-CommandPalette-itemContent, /* </DEPRECATED> */ .lm-CommandPalette-itemContent { flex: 1 1 auto; overflow: hidden; } /* <DEPRECATED> */ .p-CommandPalette-itemShortcut, /* </DEPRECATED> */ .lm-CommandPalette-itemShortcut { flex: 0 0 auto; } /* <DEPRECATED> */ .p-CommandPalette-itemLabel, /* </DEPRECATED> */ .lm-CommandPalette-itemLabel { overflow: hidden; white-space: nowrap; text-overflow: ellipsis; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Copyright (c) 2014-2017, PhosphorJS Contributors | | Distributed under the terms of the BSD 3-Clause License. | | The full license is in the file LICENSE, distributed with this software. |----------------------------------------------------------------------------*/ /* <DEPRECATED> */ .p-DockPanel, /* </DEPRECATED> */ .lm-DockPanel { z-index: 0; } /* <DEPRECATED> */ .p-DockPanel-widget, /* </DEPRECATED> */ .lm-DockPanel-widget { z-index: 0; } /* <DEPRECATED> */ .p-DockPanel-tabBar, /* </DEPRECATED> */ .lm-DockPanel-tabBar { z-index: 1; } /* <DEPRECATED> */ .p-DockPanel-handle, /* </DEPRECATED> */ .lm-DockPanel-handle { z-index: 2; } /* <DEPRECATED> */ .p-DockPanel-handle.p-mod-hidden, /* </DEPRECATED> */ .lm-DockPanel-handle.lm-mod-hidden { display: none !important; } /* <DEPRECATED> */ .p-DockPanel-handle:after, /* </DEPRECATED> */ .lm-DockPanel-handle:after { position: absolute; top: 0; left: 0; width: 100%; height: 100%; content: ''; } /* <DEPRECATED> */ .p-DockPanel-handle[data-orientation='horizontal'], /* </DEPRECATED> */ .lm-DockPanel-handle[data-orientation='horizontal'] { cursor: ew-resize; } /* <DEPRECATED> */ .p-DockPanel-handle[data-orientation='vertical'], /* </DEPRECATED> */ .lm-DockPanel-handle[data-orientation='vertical'] { cursor: ns-resize; } /* <DEPRECATED> */ .p-DockPanel-handle[data-orientation='horizontal']:after, /* </DEPRECATED> */ .lm-DockPanel-handle[data-orientation='horizontal']:after { left: 50%; min-width: 8px; transform: translateX(-50%); } /* <DEPRECATED> */ .p-DockPanel-handle[data-orientation='vertical']:after, /* </DEPRECATED> */ .lm-DockPanel-handle[data-orientation='vertical']:after { top: 50%; min-height: 8px; transform: translateY(-50%); } /* <DEPRECATED> */ .p-DockPanel-overlay, /* </DEPRECATED> */ .lm-DockPanel-overlay { z-index: 3; box-sizing: border-box; pointer-events: none; } /* <DEPRECATED> */ .p-DockPanel-overlay.p-mod-hidden, /* </DEPRECATED> */ .lm-DockPanel-overlay.lm-mod-hidden { display: none !important; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Copyright (c) 2014-2017, PhosphorJS Contributors | | Distributed under the terms of the BSD 3-Clause License. | | The full license is in the file LICENSE, distributed with this software. |----------------------------------------------------------------------------*/ /* <DEPRECATED> */ .p-Menu, /* </DEPRECATED> */ .lm-Menu { z-index: 10000; position: absolute; white-space: nowrap; overflow-x: hidden; overflow-y: auto; outline: none; -webkit-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; } /* <DEPRECATED> */ .p-Menu-content, /* </DEPRECATED> */ .lm-Menu-content { margin: 0; padding: 0; display: table; list-style-type: none; } /* <DEPRECATED> */ .p-Menu-item, /* </DEPRECATED> */ .lm-Menu-item { display: table-row; } /* <DEPRECATED> */ .p-Menu-item.p-mod-hidden, .p-Menu-item.p-mod-collapsed, /* </DEPRECATED> */ .lm-Menu-item.lm-mod-hidden, .lm-Menu-item.lm-mod-collapsed { display: none !important; } /* <DEPRECATED> */ .p-Menu-itemIcon, .p-Menu-itemSubmenuIcon, /* </DEPRECATED> */ .lm-Menu-itemIcon, .lm-Menu-itemSubmenuIcon { display: table-cell; text-align: center; } /* <DEPRECATED> */ .p-Menu-itemLabel, /* </DEPRECATED> */ .lm-Menu-itemLabel { display: table-cell; text-align: left; } /* <DEPRECATED> */ .p-Menu-itemShortcut, /* </DEPRECATED> */ .lm-Menu-itemShortcut { display: table-cell; text-align: right; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Copyright (c) 2014-2017, PhosphorJS Contributors | | Distributed under the terms of the BSD 3-Clause License. | | The full license is in the file LICENSE, distributed with this software. |----------------------------------------------------------------------------*/ /* <DEPRECATED> */ .p-MenuBar, /* </DEPRECATED> */ .lm-MenuBar { outline: none; -webkit-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; } /* <DEPRECATED> */ .p-MenuBar-content, /* </DEPRECATED> */ .lm-MenuBar-content { margin: 0; padding: 0; display: flex; flex-direction: row; list-style-type: none; } /* <DEPRECATED> */ .p--MenuBar-item, /* </DEPRECATED> */ .lm-MenuBar-item { box-sizing: border-box; } /* <DEPRECATED> */ .p-MenuBar-itemIcon, .p-MenuBar-itemLabel, /* </DEPRECATED> */ .lm-MenuBar-itemIcon, .lm-MenuBar-itemLabel { display: inline-block; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Copyright (c) 2014-2017, PhosphorJS Contributors | | Distributed under the terms of the BSD 3-Clause License. | | The full license is in the file LICENSE, distributed with this software. |----------------------------------------------------------------------------*/ /* <DEPRECATED> */ .p-ScrollBar, /* </DEPRECATED> */ .lm-ScrollBar { display: flex; -webkit-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; } /* <DEPRECATED> */ .p-ScrollBar[data-orientation='horizontal'], /* </DEPRECATED> */ .lm-ScrollBar[data-orientation='horizontal'] { flex-direction: row; } /* <DEPRECATED> */ .p-ScrollBar[data-orientation='vertical'], /* </DEPRECATED> */ .lm-ScrollBar[data-orientation='vertical'] { flex-direction: column; } /* <DEPRECATED> */ .p-ScrollBar-button, /* </DEPRECATED> */ .lm-ScrollBar-button { box-sizing: border-box; flex: 0 0 auto; } /* <DEPRECATED> */ .p-ScrollBar-track, /* </DEPRECATED> */ .lm-ScrollBar-track { box-sizing: border-box; position: relative; overflow: hidden; flex: 1 1 auto; } /* <DEPRECATED> */ .p-ScrollBar-thumb, /* </DEPRECATED> */ .lm-ScrollBar-thumb { box-sizing: border-box; position: absolute; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Copyright (c) 2014-2017, PhosphorJS Contributors | | Distributed under the terms of the BSD 3-Clause License. | | The full license is in the file LICENSE, distributed with this software. |----------------------------------------------------------------------------*/ /* <DEPRECATED> */ .p-SplitPanel-child, /* </DEPRECATED> */ .lm-SplitPanel-child { z-index: 0; } /* <DEPRECATED> */ .p-SplitPanel-handle, /* </DEPRECATED> */ .lm-SplitPanel-handle { z-index: 1; } /* <DEPRECATED> */ .p-SplitPanel-handle.p-mod-hidden, /* </DEPRECATED> */ .lm-SplitPanel-handle.lm-mod-hidden { display: none !important; } /* <DEPRECATED> */ .p-SplitPanel-handle:after, /* </DEPRECATED> */ .lm-SplitPanel-handle:after { position: absolute; top: 0; left: 0; width: 100%; height: 100%; content: ''; } /* <DEPRECATED> */ .p-SplitPanel[data-orientation='horizontal'] > .p-SplitPanel-handle, /* </DEPRECATED> */ .lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle { cursor: ew-resize; } /* <DEPRECATED> */ .p-SplitPanel[data-orientation='vertical'] > .p-SplitPanel-handle, /* </DEPRECATED> */ .lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle { cursor: ns-resize; } /* <DEPRECATED> */ .p-SplitPanel[data-orientation='horizontal'] > .p-SplitPanel-handle:after, /* </DEPRECATED> */ .lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle:after { left: 50%; min-width: 8px; transform: translateX(-50%); } /* <DEPRECATED> */ .p-SplitPanel[data-orientation='vertical'] > .p-SplitPanel-handle:after, /* </DEPRECATED> */ .lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle:after { top: 50%; min-height: 8px; transform: translateY(-50%); } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Copyright (c) 2014-2017, PhosphorJS Contributors | | Distributed under the terms of the BSD 3-Clause License. | | The full license is in the file LICENSE, distributed with this software. |----------------------------------------------------------------------------*/ /* <DEPRECATED> */ .p-TabBar, /* </DEPRECATED> */ .lm-TabBar { display: flex; -webkit-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; } /* <DEPRECATED> */ .p-TabBar[data-orientation='horizontal'], /* </DEPRECATED> */ .lm-TabBar[data-orientation='horizontal'] { flex-direction: row; } /* <DEPRECATED> */ .p-TabBar[data-orientation='vertical'], /* </DEPRECATED> */ .lm-TabBar[data-orientation='vertical'] { flex-direction: column; } /* <DEPRECATED> */ .p-TabBar-content, /* </DEPRECATED> */ .lm-TabBar-content { margin: 0; padding: 0; display: flex; flex: 1 1 auto; list-style-type: none; } /* <DEPRECATED> */ .p-TabBar[data-orientation='horizontal'] > .p-TabBar-content, /* </DEPRECATED> */ .lm-TabBar[data-orientation='horizontal'] > .lm-TabBar-content { flex-direction: row; } /* <DEPRECATED> */ .p-TabBar[data-orientation='vertical'] > .p-TabBar-content, /* </DEPRECATED> */ .lm-TabBar[data-orientation='vertical'] > .lm-TabBar-content { flex-direction: column; } /* <DEPRECATED> */ .p-TabBar-tab, /* </DEPRECATED> */ .lm-TabBar-tab { display: flex; flex-direction: row; box-sizing: border-box; overflow: hidden; } /* <DEPRECATED> */ .p-TabBar-tabIcon, .p-TabBar-tabCloseIcon, /* </DEPRECATED> */ .lm-TabBar-tabIcon, .lm-TabBar-tabCloseIcon { flex: 0 0 auto; } /* <DEPRECATED> */ .p-TabBar-tabLabel, /* </DEPRECATED> */ .lm-TabBar-tabLabel { flex: 1 1 auto; overflow: hidden; white-space: nowrap; } /* <DEPRECATED> */ .p-TabBar-tab.p-mod-hidden, /* </DEPRECATED> */ .lm-TabBar-tab.lm-mod-hidden { display: none !important; } /* <DEPRECATED> */ .p-TabBar.p-mod-dragging .p-TabBar-tab, /* </DEPRECATED> */ .lm-TabBar.lm-mod-dragging .lm-TabBar-tab { position: relative; } /* <DEPRECATED> */ .p-TabBar.p-mod-dragging[data-orientation='horizontal'] .p-TabBar-tab, /* </DEPRECATED> */ .lm-TabBar.lm-mod-dragging[data-orientation='horizontal'] .lm-TabBar-tab { left: 0; transition: left 150ms ease; } /* <DEPRECATED> */ .p-TabBar.p-mod-dragging[data-orientation='vertical'] .p-TabBar-tab, /* </DEPRECATED> */ .lm-TabBar.lm-mod-dragging[data-orientation='vertical'] .lm-TabBar-tab { top: 0; transition: top 150ms ease; } /* <DEPRECATED> */ .p-TabBar.p-mod-dragging .p-TabBar-tab.p-mod-dragging /* </DEPRECATED> */ .lm-TabBar.lm-mod-dragging .lm-TabBar-tab.lm-mod-dragging { transition: none; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Copyright (c) 2014-2017, PhosphorJS Contributors | | Distributed under the terms of the BSD 3-Clause License. | | The full license is in the file LICENSE, distributed with this software. |----------------------------------------------------------------------------*/ /* <DEPRECATED> */ .p-TabPanel-tabBar, /* </DEPRECATED> */ .lm-TabPanel-tabBar { z-index: 1; } /* <DEPRECATED> */ .p-TabPanel-stackedPanel, /* </DEPRECATED> */ .lm-TabPanel-stackedPanel { z-index: 0; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Copyright (c) 2014-2017, PhosphorJS Contributors | | Distributed under the terms of the BSD 3-Clause License. | | The full license is in the file LICENSE, distributed with this software. |----------------------------------------------------------------------------*/ @charset \"UTF-8\"; /*! Copyright 2015-present Palantir Technologies, Inc. All rights reserved. Licensed under the Apache License, Version 2.0. */ html{ -webkit-box-sizing:border-box; box-sizing:border-box; } *, *::before, *::after{ -webkit-box-sizing:inherit; box-sizing:inherit; } body{ text-transform:none; line-height:1.28581; letter-spacing:0; font-size:14px; font-weight:400; color:#182026; font-family:-apple-system, \"BlinkMacSystemFont\", \"Segoe UI\", \"Roboto\", \"Oxygen\", \"Ubuntu\", \"Cantarell\", \"Open Sans\", \"Helvetica Neue\", \"Icons16\", sans-serif; } p{ margin-top:0; margin-bottom:10px; } small{ font-size:12px; } strong{ font-weight:600; } ::-moz-selection{ background:rgba(125, 188, 255, 0.6); } ::selection{ background:rgba(125, 188, 255, 0.6); } .bp3-heading{ color:#182026; font-weight:600; margin:0 0 10px; padding:0; } .bp3-dark .bp3-heading{ color:#f5f8fa; } h1.bp3-heading, .bp3-running-text h1{ line-height:40px; font-size:36px; } h2.bp3-heading, .bp3-running-text h2{ line-height:32px; font-size:28px; } h3.bp3-heading, .bp3-running-text h3{ line-height:25px; font-size:22px; } h4.bp3-heading, .bp3-running-text h4{ line-height:21px; font-size:18px; } h5.bp3-heading, .bp3-running-text h5{ line-height:19px; font-size:16px; } h6.bp3-heading, .bp3-running-text h6{ line-height:16px; font-size:14px; } .bp3-ui-text{ text-transform:none; line-height:1.28581; letter-spacing:0; font-size:14px; font-weight:400; } .bp3-monospace-text{ text-transform:none; font-family:monospace; } .bp3-text-muted{ color:#5c7080; } .bp3-dark .bp3-text-muted{ color:#a7b6c2; } .bp3-text-disabled{ color:rgba(92, 112, 128, 0.6); } .bp3-dark .bp3-text-disabled{ color:rgba(167, 182, 194, 0.6); } .bp3-text-overflow-ellipsis{ overflow:hidden; text-overflow:ellipsis; white-space:nowrap; word-wrap:normal; } .bp3-running-text{ line-height:1.5; font-size:14px; } .bp3-running-text h1{ color:#182026; font-weight:600; margin-top:40px; margin-bottom:20px; } .bp3-dark .bp3-running-text h1{ color:#f5f8fa; } .bp3-running-text h2{ color:#182026; font-weight:600; margin-top:40px; margin-bottom:20px; } .bp3-dark .bp3-running-text h2{ color:#f5f8fa; } .bp3-running-text h3{ color:#182026; font-weight:600; margin-top:40px; margin-bottom:20px; } .bp3-dark .bp3-running-text h3{ color:#f5f8fa; } .bp3-running-text h4{ color:#182026; font-weight:600; margin-top:40px; margin-bottom:20px; } .bp3-dark .bp3-running-text h4{ color:#f5f8fa; } .bp3-running-text h5{ color:#182026; font-weight:600; margin-top:40px; margin-bottom:20px; } .bp3-dark .bp3-running-text h5{ color:#f5f8fa; } .bp3-running-text h6{ color:#182026; font-weight:600; margin-top:40px; margin-bottom:20px; } .bp3-dark .bp3-running-text h6{ color:#f5f8fa; } .bp3-running-text hr{ margin:20px 0; border:none; border-bottom:1px solid rgba(16, 22, 26, 0.15); } .bp3-dark .bp3-running-text hr{ border-color:rgba(255, 255, 255, 0.15); } .bp3-running-text p{ margin:0 0 10px; padding:0; } .bp3-text-large{ font-size:16px; } .bp3-text-small{ font-size:12px; } a{ text-decoration:none; color:#106ba3; } a:hover{ cursor:pointer; text-decoration:underline; color:#106ba3; } a .bp3-icon, a .bp3-icon-standard, a .bp3-icon-large{ color:inherit; } a code, .bp3-dark a code{ color:inherit; } .bp3-dark a, .bp3-dark a:hover{ color:#48aff0; } .bp3-dark a .bp3-icon, .bp3-dark a .bp3-icon-standard, .bp3-dark a .bp3-icon-large, .bp3-dark a:hover .bp3-icon, .bp3-dark a:hover .bp3-icon-standard, .bp3-dark a:hover .bp3-icon-large{ color:inherit; } .bp3-running-text code, .bp3-code{ text-transform:none; font-family:monospace; border-radius:3px; -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2); background:rgba(255, 255, 255, 0.7); padding:2px 5px; color:#5c7080; font-size:smaller; } .bp3-dark .bp3-running-text code, .bp3-running-text .bp3-dark code, .bp3-dark .bp3-code{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4); background:rgba(16, 22, 26, 0.3); color:#a7b6c2; } .bp3-running-text a > code, a > .bp3-code{ color:#137cbd; } .bp3-dark .bp3-running-text a > code, .bp3-running-text .bp3-dark a > code, .bp3-dark a > .bp3-code{ color:inherit; } .bp3-running-text pre, .bp3-code-block{ text-transform:none; font-family:monospace; display:block; margin:10px 0; border-radius:3px; -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.15); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.15); background:rgba(255, 255, 255, 0.7); padding:13px 15px 12px; line-height:1.4; color:#182026; font-size:13px; word-break:break-all; word-wrap:break-word; } .bp3-dark .bp3-running-text pre, .bp3-running-text .bp3-dark pre, .bp3-dark .bp3-code-block{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4); background:rgba(16, 22, 26, 0.3); color:#f5f8fa; } .bp3-running-text pre > code, .bp3-code-block > code{ -webkit-box-shadow:none; box-shadow:none; background:none; padding:0; color:inherit; font-size:inherit; } .bp3-running-text kbd, .bp3-key{ display:-webkit-inline-box; display:-ms-inline-flexbox; display:inline-flex; -webkit-box-align:center; -ms-flex-align:center; align-items:center; -webkit-box-pack:center; -ms-flex-pack:center; justify-content:center; border-radius:3px; -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.2); background:#ffffff; min-width:24px; height:24px; padding:3px 6px; vertical-align:middle; line-height:24px; color:#5c7080; font-family:inherit; font-size:12px; } .bp3-running-text kbd .bp3-icon, .bp3-key .bp3-icon, .bp3-running-text kbd .bp3-icon-standard, .bp3-key .bp3-icon-standard, .bp3-running-text kbd .bp3-icon-large, .bp3-key .bp3-icon-large{ margin-right:5px; } .bp3-dark .bp3-running-text kbd, .bp3-running-text .bp3-dark kbd, .bp3-dark .bp3-key{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4); background:#394b59; color:#a7b6c2; } .bp3-running-text blockquote, .bp3-blockquote{ margin:0 0 10px; border-left:solid 4px rgba(167, 182, 194, 0.5); padding:0 20px; } .bp3-dark .bp3-running-text blockquote, .bp3-running-text .bp3-dark blockquote, .bp3-dark .bp3-blockquote{ border-color:rgba(115, 134, 148, 0.5); } .bp3-running-text ul, .bp3-running-text ol, .bp3-list{ margin:10px 0; padding-left:30px; } .bp3-running-text ul li:not(:last-child), .bp3-running-text ol li:not(:last-child), .bp3-list li:not(:last-child){ margin-bottom:5px; } .bp3-running-text ul ol, .bp3-running-text ol ol, .bp3-list ol, .bp3-running-text ul ul, .bp3-running-text ol ul, .bp3-list ul{ margin-top:5px; } .bp3-list-unstyled{ margin:0; padding:0; list-style:none; } .bp3-list-unstyled li{ padding:0; } .bp3-rtl{ text-align:right; } .bp3-dark{ color:#f5f8fa; } :focus{ outline:rgba(19, 124, 189, 0.6) auto 2px; outline-offset:2px; -moz-outline-radius:6px; } .bp3-focus-disabled :focus{ outline:none !important; } .bp3-focus-disabled :focus ~ .bp3-control-indicator{ outline:none !important; } .bp3-alert{ max-width:400px; padding:20px; } .bp3-alert-body{ display:-webkit-box; display:-ms-flexbox; display:flex; } .bp3-alert-body .bp3-icon{ margin-top:0; margin-right:20px; font-size:40px; } .bp3-alert-footer{ display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-orient:horizontal; -webkit-box-direction:reverse; -ms-flex-direction:row-reverse; flex-direction:row-reverse; margin-top:10px; } .bp3-alert-footer .bp3-button{ margin-left:10px; } .bp3-breadcrumbs{ display:-webkit-box; display:-ms-flexbox; display:flex; -ms-flex-wrap:wrap; flex-wrap:wrap; -webkit-box-align:center; -ms-flex-align:center; align-items:center; margin:0; cursor:default; height:30px; padding:0; list-style:none; } .bp3-breadcrumbs > li{ display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-align:center; -ms-flex-align:center; align-items:center; } .bp3-breadcrumbs > li::after{ display:block; margin:0 5px; background:url(\"data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16'%3e%3cpath fill-rule='evenodd' clip-rule='evenodd' d='M10.71 7.29l-4-4a1.003 1.003 0 0 0-1.42 1.42L8.59 8 5.3 11.29c-.19.18-.3.43-.3.71a1.003 1.003 0 0 0 1.71.71l4-4c.18-.18.29-.43.29-.71 0-.28-.11-.53-.29-.71z' fill='%235C7080'/%3e%3c/svg%3e\"); width:16px; height:16px; content:\"\"; } .bp3-breadcrumbs > li:last-of-type::after{ display:none; } .bp3-breadcrumb, .bp3-breadcrumb-current, .bp3-breadcrumbs-collapsed{ display:-webkit-inline-box; display:-ms-inline-flexbox; display:inline-flex; -webkit-box-align:center; -ms-flex-align:center; align-items:center; font-size:16px; } .bp3-breadcrumb, .bp3-breadcrumbs-collapsed{ color:#5c7080; } .bp3-breadcrumb:hover{ text-decoration:none; } .bp3-breadcrumb.bp3-disabled{ cursor:not-allowed; color:rgba(92, 112, 128, 0.6); } .bp3-breadcrumb .bp3-icon{ margin-right:5px; } .bp3-breadcrumb-current{ color:inherit; font-weight:600; } .bp3-breadcrumb-current .bp3-input{ vertical-align:baseline; font-size:inherit; font-weight:inherit; } .bp3-breadcrumbs-collapsed{ margin-right:2px; border:none; border-radius:3px; background:#ced9e0; cursor:pointer; padding:1px 5px; vertical-align:text-bottom; } .bp3-breadcrumbs-collapsed::before{ display:block; background:url(\"data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16'%3e%3cg fill='%235C7080'%3e%3ccircle cx='2' cy='8.03' r='2'/%3e%3ccircle cx='14' cy='8.03' r='2'/%3e%3ccircle cx='8' cy='8.03' r='2'/%3e%3c/g%3e%3c/svg%3e\") center no-repeat; width:16px; height:16px; content:\"\"; } .bp3-breadcrumbs-collapsed:hover{ background:#bfccd6; text-decoration:none; color:#182026; } .bp3-dark .bp3-breadcrumb, .bp3-dark .bp3-breadcrumbs-collapsed{ color:#a7b6c2; } .bp3-dark .bp3-breadcrumbs > li::after{ color:#a7b6c2; } .bp3-dark .bp3-breadcrumb.bp3-disabled{ color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-breadcrumb-current{ color:#f5f8fa; } .bp3-dark .bp3-breadcrumbs-collapsed{ background:rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-breadcrumbs-collapsed:hover{ background:rgba(16, 22, 26, 0.6); color:#f5f8fa; } .bp3-button{ display:-webkit-inline-box; display:-ms-inline-flexbox; display:inline-flex; -webkit-box-orient:horizontal; -webkit-box-direction:normal; -ms-flex-direction:row; flex-direction:row; -webkit-box-align:center; -ms-flex-align:center; align-items:center; -webkit-box-pack:center; -ms-flex-pack:center; justify-content:center; border:none; border-radius:3px; cursor:pointer; padding:5px 10px; vertical-align:middle; text-align:left; font-size:14px; min-width:30px; min-height:30px; } .bp3-button > *{ -webkit-box-flex:0; -ms-flex-positive:0; flex-grow:0; -ms-flex-negative:0; flex-shrink:0; } .bp3-button > .bp3-fill{ -webkit-box-flex:1; -ms-flex-positive:1; flex-grow:1; -ms-flex-negative:1; flex-shrink:1; } .bp3-button::before, .bp3-button > *{ margin-right:7px; } .bp3-button:empty::before, .bp3-button > :last-child{ margin-right:0; } .bp3-button:empty{ padding:0 !important; } .bp3-button:disabled, .bp3-button.bp3-disabled{ cursor:not-allowed; } .bp3-button.bp3-fill{ display:-webkit-box; display:-ms-flexbox; display:flex; width:100%; } .bp3-button.bp3-align-right, .bp3-align-right .bp3-button{ text-align:right; } .bp3-button.bp3-align-left, .bp3-align-left .bp3-button{ text-align:left; } .bp3-button:not([class*=\"bp3-intent-\"]){ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); background-color:#f5f8fa; background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0))); background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0)); color:#182026; } .bp3-button:not([class*=\"bp3-intent-\"]):hover{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); background-clip:padding-box; background-color:#ebf1f5; } .bp3-button:not([class*=\"bp3-intent-\"]):active, .bp3-button:not([class*=\"bp3-intent-\"]).bp3-active{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2); background-color:#d8e1e8; background-image:none; } .bp3-button:not([class*=\"bp3-intent-\"]):disabled, .bp3-button:not([class*=\"bp3-intent-\"]).bp3-disabled{ outline:none; -webkit-box-shadow:none; box-shadow:none; background-color:rgba(206, 217, 224, 0.5); background-image:none; cursor:not-allowed; color:rgba(92, 112, 128, 0.6); } .bp3-button:not([class*=\"bp3-intent-\"]):disabled.bp3-active, .bp3-button:not([class*=\"bp3-intent-\"]):disabled.bp3-active:hover, .bp3-button:not([class*=\"bp3-intent-\"]).bp3-disabled.bp3-active, .bp3-button:not([class*=\"bp3-intent-\"]).bp3-disabled.bp3-active:hover{ background:rgba(206, 217, 224, 0.7); } .bp3-button.bp3-intent-primary{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); background-color:#137cbd; background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0))); background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0)); color:#ffffff; } .bp3-button.bp3-intent-primary:hover, .bp3-button.bp3-intent-primary:active, .bp3-button.bp3-intent-primary.bp3-active{ color:#ffffff; } .bp3-button.bp3-intent-primary:hover{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); background-color:#106ba3; } .bp3-button.bp3-intent-primary:active, .bp3-button.bp3-intent-primary.bp3-active{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); background-color:#0e5a8a; background-image:none; } .bp3-button.bp3-intent-primary:disabled, .bp3-button.bp3-intent-primary.bp3-disabled{ border-color:transparent; -webkit-box-shadow:none; box-shadow:none; background-color:rgba(19, 124, 189, 0.5); background-image:none; color:rgba(255, 255, 255, 0.6); } .bp3-button.bp3-intent-success{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); background-color:#0f9960; background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0))); background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0)); color:#ffffff; } .bp3-button.bp3-intent-success:hover, .bp3-button.bp3-intent-success:active, .bp3-button.bp3-intent-success.bp3-active{ color:#ffffff; } .bp3-button.bp3-intent-success:hover{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); background-color:#0d8050; } .bp3-button.bp3-intent-success:active, .bp3-button.bp3-intent-success.bp3-active{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); background-color:#0a6640; background-image:none; } .bp3-button.bp3-intent-success:disabled, .bp3-button.bp3-intent-success.bp3-disabled{ border-color:transparent; -webkit-box-shadow:none; box-shadow:none; background-color:rgba(15, 153, 96, 0.5); background-image:none; color:rgba(255, 255, 255, 0.6); } .bp3-button.bp3-intent-warning{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); background-color:#d9822b; background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0))); background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0)); color:#ffffff; } .bp3-button.bp3-intent-warning:hover, .bp3-button.bp3-intent-warning:active, .bp3-button.bp3-intent-warning.bp3-active{ color:#ffffff; } .bp3-button.bp3-intent-warning:hover{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); background-color:#bf7326; } .bp3-button.bp3-intent-warning:active, .bp3-button.bp3-intent-warning.bp3-active{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); background-color:#a66321; background-image:none; } .bp3-button.bp3-intent-warning:disabled, .bp3-button.bp3-intent-warning.bp3-disabled{ border-color:transparent; -webkit-box-shadow:none; box-shadow:none; background-color:rgba(217, 130, 43, 0.5); background-image:none; color:rgba(255, 255, 255, 0.6); } .bp3-button.bp3-intent-danger{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); background-color:#db3737; background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0))); background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0)); color:#ffffff; } .bp3-button.bp3-intent-danger:hover, .bp3-button.bp3-intent-danger:active, .bp3-button.bp3-intent-danger.bp3-active{ color:#ffffff; } .bp3-button.bp3-intent-danger:hover{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); background-color:#c23030; } .bp3-button.bp3-intent-danger:active, .bp3-button.bp3-intent-danger.bp3-active{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); background-color:#a82a2a; background-image:none; } .bp3-button.bp3-intent-danger:disabled, .bp3-button.bp3-intent-danger.bp3-disabled{ border-color:transparent; -webkit-box-shadow:none; box-shadow:none; background-color:rgba(219, 55, 55, 0.5); background-image:none; color:rgba(255, 255, 255, 0.6); } .bp3-button[class*=\"bp3-intent-\"] .bp3-button-spinner .bp3-spinner-head{ stroke:#ffffff; } .bp3-button.bp3-large, .bp3-large .bp3-button{ min-width:40px; min-height:40px; padding:5px 15px; font-size:16px; } .bp3-button.bp3-large::before, .bp3-button.bp3-large > *, .bp3-large .bp3-button::before, .bp3-large .bp3-button > *{ margin-right:10px; } .bp3-button.bp3-large:empty::before, .bp3-button.bp3-large > :last-child, .bp3-large .bp3-button:empty::before, .bp3-large .bp3-button > :last-child{ margin-right:0; } .bp3-button.bp3-small, .bp3-small .bp3-button{ min-width:24px; min-height:24px; padding:0 7px; } .bp3-button.bp3-loading{ position:relative; } .bp3-button.bp3-loading[class*=\"bp3-icon-\"]::before{ visibility:hidden; } .bp3-button.bp3-loading .bp3-button-spinner{ position:absolute; margin:0; } .bp3-button.bp3-loading > :not(.bp3-button-spinner){ visibility:hidden; } .bp3-button[class*=\"bp3-icon-\"]::before{ line-height:1; font-family:\"Icons16\", sans-serif; font-size:16px; font-weight:400; font-style:normal; -moz-osx-font-smoothing:grayscale; -webkit-font-smoothing:antialiased; color:#5c7080; } .bp3-button .bp3-icon, .bp3-button .bp3-icon-standard, .bp3-button .bp3-icon-large{ color:#5c7080; } .bp3-button .bp3-icon.bp3-align-right, .bp3-button .bp3-icon-standard.bp3-align-right, .bp3-button .bp3-icon-large.bp3-align-right{ margin-left:7px; } .bp3-button .bp3-icon:first-child:last-child, .bp3-button .bp3-spinner + .bp3-icon:last-child{ margin:0 -7px; } .bp3-dark .bp3-button:not([class*=\"bp3-intent-\"]){ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); background-color:#394b59; background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0))); background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0)); color:#f5f8fa; } .bp3-dark .bp3-button:not([class*=\"bp3-intent-\"]):hover, .bp3-dark .bp3-button:not([class*=\"bp3-intent-\"]):active, .bp3-dark .bp3-button:not([class*=\"bp3-intent-\"]).bp3-active{ color:#f5f8fa; } .bp3-dark .bp3-button:not([class*=\"bp3-intent-\"]):hover{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); background-color:#30404d; } .bp3-dark .bp3-button:not([class*=\"bp3-intent-\"]):active, .bp3-dark .bp3-button:not([class*=\"bp3-intent-\"]).bp3-active{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2); background-color:#202b33; background-image:none; } .bp3-dark .bp3-button:not([class*=\"bp3-intent-\"]):disabled, .bp3-dark .bp3-button:not([class*=\"bp3-intent-\"]).bp3-disabled{ -webkit-box-shadow:none; box-shadow:none; background-color:rgba(57, 75, 89, 0.5); background-image:none; color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-button:not([class*=\"bp3-intent-\"]):disabled.bp3-active, .bp3-dark .bp3-button:not([class*=\"bp3-intent-\"]).bp3-disabled.bp3-active{ background:rgba(57, 75, 89, 0.7); } .bp3-dark .bp3-button:not([class*=\"bp3-intent-\"]) .bp3-button-spinner .bp3-spinner-head{ background:rgba(16, 22, 26, 0.5); stroke:#8a9ba8; } .bp3-dark .bp3-button:not([class*=\"bp3-intent-\"])[class*=\"bp3-icon-\"]::before{ color:#a7b6c2; } .bp3-dark .bp3-button:not([class*=\"bp3-intent-\"]) .bp3-icon, .bp3-dark .bp3-button:not([class*=\"bp3-intent-\"]) .bp3-icon-standard, .bp3-dark .bp3-button:not([class*=\"bp3-intent-\"]) .bp3-icon-large{ color:#a7b6c2; } .bp3-dark .bp3-button[class*=\"bp3-intent-\"]{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-button[class*=\"bp3-intent-\"]:hover{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-button[class*=\"bp3-intent-\"]:active, .bp3-dark .bp3-button[class*=\"bp3-intent-\"].bp3-active{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); } .bp3-dark .bp3-button[class*=\"bp3-intent-\"]:disabled, .bp3-dark .bp3-button[class*=\"bp3-intent-\"].bp3-disabled{ -webkit-box-shadow:none; box-shadow:none; background-image:none; color:rgba(255, 255, 255, 0.3); } .bp3-dark .bp3-button[class*=\"bp3-intent-\"] .bp3-button-spinner .bp3-spinner-head{ stroke:#8a9ba8; } .bp3-button:disabled::before, .bp3-button:disabled .bp3-icon, .bp3-button:disabled .bp3-icon-standard, .bp3-button:disabled .bp3-icon-large, .bp3-button.bp3-disabled::before, .bp3-button.bp3-disabled .bp3-icon, .bp3-button.bp3-disabled .bp3-icon-standard, .bp3-button.bp3-disabled .bp3-icon-large, .bp3-button[class*=\"bp3-intent-\"]::before, .bp3-button[class*=\"bp3-intent-\"] .bp3-icon, .bp3-button[class*=\"bp3-intent-\"] .bp3-icon-standard, .bp3-button[class*=\"bp3-intent-\"] .bp3-icon-large{ color:inherit !important; } .bp3-button.bp3-minimal{ -webkit-box-shadow:none; box-shadow:none; background:none; } .bp3-button.bp3-minimal:hover{ -webkit-box-shadow:none; box-shadow:none; background:rgba(167, 182, 194, 0.3); text-decoration:none; color:#182026; } .bp3-button.bp3-minimal:active, .bp3-button.bp3-minimal.bp3-active{ -webkit-box-shadow:none; box-shadow:none; background:rgba(115, 134, 148, 0.3); color:#182026; } .bp3-button.bp3-minimal:disabled, .bp3-button.bp3-minimal:disabled:hover, .bp3-button.bp3-minimal.bp3-disabled, .bp3-button.bp3-minimal.bp3-disabled:hover{ background:none; cursor:not-allowed; color:rgba(92, 112, 128, 0.6); } .bp3-button.bp3-minimal:disabled.bp3-active, .bp3-button.bp3-minimal:disabled:hover.bp3-active, .bp3-button.bp3-minimal.bp3-disabled.bp3-active, .bp3-button.bp3-minimal.bp3-disabled:hover.bp3-active{ background:rgba(115, 134, 148, 0.3); } .bp3-dark .bp3-button.bp3-minimal{ -webkit-box-shadow:none; box-shadow:none; background:none; color:inherit; } .bp3-dark .bp3-button.bp3-minimal:hover, .bp3-dark .bp3-button.bp3-minimal:active, .bp3-dark .bp3-button.bp3-minimal.bp3-active{ -webkit-box-shadow:none; box-shadow:none; background:none; } .bp3-dark .bp3-button.bp3-minimal:hover{ background:rgba(138, 155, 168, 0.15); } .bp3-dark .bp3-button.bp3-minimal:active, .bp3-dark .bp3-button.bp3-minimal.bp3-active{ background:rgba(138, 155, 168, 0.3); color:#f5f8fa; } .bp3-dark .bp3-button.bp3-minimal:disabled, .bp3-dark .bp3-button.bp3-minimal:disabled:hover, .bp3-dark .bp3-button.bp3-minimal.bp3-disabled, .bp3-dark .bp3-button.bp3-minimal.bp3-disabled:hover{ background:none; cursor:not-allowed; color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-button.bp3-minimal:disabled.bp3-active, .bp3-dark .bp3-button.bp3-minimal:disabled:hover.bp3-active, .bp3-dark .bp3-button.bp3-minimal.bp3-disabled.bp3-active, .bp3-dark .bp3-button.bp3-minimal.bp3-disabled:hover.bp3-active{ background:rgba(138, 155, 168, 0.3); } .bp3-button.bp3-minimal.bp3-intent-primary{ color:#106ba3; } .bp3-button.bp3-minimal.bp3-intent-primary:hover, .bp3-button.bp3-minimal.bp3-intent-primary:active, .bp3-button.bp3-minimal.bp3-intent-primary.bp3-active{ -webkit-box-shadow:none; box-shadow:none; background:none; color:#106ba3; } .bp3-button.bp3-minimal.bp3-intent-primary:hover{ background:rgba(19, 124, 189, 0.15); color:#106ba3; } .bp3-button.bp3-minimal.bp3-intent-primary:active, .bp3-button.bp3-minimal.bp3-intent-primary.bp3-active{ background:rgba(19, 124, 189, 0.3); color:#106ba3; } .bp3-button.bp3-minimal.bp3-intent-primary:disabled, .bp3-button.bp3-minimal.bp3-intent-primary.bp3-disabled{ background:none; color:rgba(16, 107, 163, 0.5); } .bp3-button.bp3-minimal.bp3-intent-primary:disabled.bp3-active, .bp3-button.bp3-minimal.bp3-intent-primary.bp3-disabled.bp3-active{ background:rgba(19, 124, 189, 0.3); } .bp3-button.bp3-minimal.bp3-intent-primary .bp3-button-spinner .bp3-spinner-head{ stroke:#106ba3; } .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary{ color:#48aff0; } .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary:hover{ background:rgba(19, 124, 189, 0.2); color:#48aff0; } .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary:active, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary.bp3-active{ background:rgba(19, 124, 189, 0.3); color:#48aff0; } .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary:disabled, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary.bp3-disabled{ background:none; color:rgba(72, 175, 240, 0.5); } .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary:disabled.bp3-active, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary.bp3-disabled.bp3-active{ background:rgba(19, 124, 189, 0.3); } .bp3-button.bp3-minimal.bp3-intent-success{ color:#0d8050; } .bp3-button.bp3-minimal.bp3-intent-success:hover, .bp3-button.bp3-minimal.bp3-intent-success:active, .bp3-button.bp3-minimal.bp3-intent-success.bp3-active{ -webkit-box-shadow:none; box-shadow:none; background:none; color:#0d8050; } .bp3-button.bp3-minimal.bp3-intent-success:hover{ background:rgba(15, 153, 96, 0.15); color:#0d8050; } .bp3-button.bp3-minimal.bp3-intent-success:active, .bp3-button.bp3-minimal.bp3-intent-success.bp3-active{ background:rgba(15, 153, 96, 0.3); color:#0d8050; } .bp3-button.bp3-minimal.bp3-intent-success:disabled, .bp3-button.bp3-minimal.bp3-intent-success.bp3-disabled{ background:none; color:rgba(13, 128, 80, 0.5); } .bp3-button.bp3-minimal.bp3-intent-success:disabled.bp3-active, .bp3-button.bp3-minimal.bp3-intent-success.bp3-disabled.bp3-active{ background:rgba(15, 153, 96, 0.3); } .bp3-button.bp3-minimal.bp3-intent-success .bp3-button-spinner .bp3-spinner-head{ stroke:#0d8050; } .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success{ color:#3dcc91; } .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success:hover{ background:rgba(15, 153, 96, 0.2); color:#3dcc91; } .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success:active, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success.bp3-active{ background:rgba(15, 153, 96, 0.3); color:#3dcc91; } .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success:disabled, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success.bp3-disabled{ background:none; color:rgba(61, 204, 145, 0.5); } .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success:disabled.bp3-active, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success.bp3-disabled.bp3-active{ background:rgba(15, 153, 96, 0.3); } .bp3-button.bp3-minimal.bp3-intent-warning{ color:#bf7326; } .bp3-button.bp3-minimal.bp3-intent-warning:hover, .bp3-button.bp3-minimal.bp3-intent-warning:active, .bp3-button.bp3-minimal.bp3-intent-warning.bp3-active{ -webkit-box-shadow:none; box-shadow:none; background:none; color:#bf7326; } .bp3-button.bp3-minimal.bp3-intent-warning:hover{ background:rgba(217, 130, 43, 0.15); color:#bf7326; } .bp3-button.bp3-minimal.bp3-intent-warning:active, .bp3-button.bp3-minimal.bp3-intent-warning.bp3-active{ background:rgba(217, 130, 43, 0.3); color:#bf7326; } .bp3-button.bp3-minimal.bp3-intent-warning:disabled, .bp3-button.bp3-minimal.bp3-intent-warning.bp3-disabled{ background:none; color:rgba(191, 115, 38, 0.5); } .bp3-button.bp3-minimal.bp3-intent-warning:disabled.bp3-active, .bp3-button.bp3-minimal.bp3-intent-warning.bp3-disabled.bp3-active{ background:rgba(217, 130, 43, 0.3); } .bp3-button.bp3-minimal.bp3-intent-warning .bp3-button-spinner .bp3-spinner-head{ stroke:#bf7326; } .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning{ color:#ffb366; } .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning:hover{ background:rgba(217, 130, 43, 0.2); color:#ffb366; } .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning:active, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning.bp3-active{ background:rgba(217, 130, 43, 0.3); color:#ffb366; } .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning:disabled, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning.bp3-disabled{ background:none; color:rgba(255, 179, 102, 0.5); } .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning:disabled.bp3-active, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning.bp3-disabled.bp3-active{ background:rgba(217, 130, 43, 0.3); } .bp3-button.bp3-minimal.bp3-intent-danger{ color:#c23030; } .bp3-button.bp3-minimal.bp3-intent-danger:hover, .bp3-button.bp3-minimal.bp3-intent-danger:active, .bp3-button.bp3-minimal.bp3-intent-danger.bp3-active{ -webkit-box-shadow:none; box-shadow:none; background:none; color:#c23030; } .bp3-button.bp3-minimal.bp3-intent-danger:hover{ background:rgba(219, 55, 55, 0.15); color:#c23030; } .bp3-button.bp3-minimal.bp3-intent-danger:active, .bp3-button.bp3-minimal.bp3-intent-danger.bp3-active{ background:rgba(219, 55, 55, 0.3); color:#c23030; } .bp3-button.bp3-minimal.bp3-intent-danger:disabled, .bp3-button.bp3-minimal.bp3-intent-danger.bp3-disabled{ background:none; color:rgba(194, 48, 48, 0.5); } .bp3-button.bp3-minimal.bp3-intent-danger:disabled.bp3-active, .bp3-button.bp3-minimal.bp3-intent-danger.bp3-disabled.bp3-active{ background:rgba(219, 55, 55, 0.3); } .bp3-button.bp3-minimal.bp3-intent-danger .bp3-button-spinner .bp3-spinner-head{ stroke:#c23030; } .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger{ color:#ff7373; } .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger:hover{ background:rgba(219, 55, 55, 0.2); color:#ff7373; } .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger:active, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger.bp3-active{ background:rgba(219, 55, 55, 0.3); color:#ff7373; } .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger:disabled, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger.bp3-disabled{ background:none; color:rgba(255, 115, 115, 0.5); } .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger:disabled.bp3-active, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger.bp3-disabled.bp3-active{ background:rgba(219, 55, 55, 0.3); } a.bp3-button{ text-align:center; text-decoration:none; -webkit-transition:none; transition:none; } a.bp3-button, a.bp3-button:hover, a.bp3-button:active{ color:#182026; } a.bp3-button.bp3-disabled{ color:rgba(92, 112, 128, 0.6); } .bp3-button-text{ -webkit-box-flex:0; -ms-flex:0 1 auto; flex:0 1 auto; } .bp3-button.bp3-align-left .bp3-button-text, .bp3-button.bp3-align-right .bp3-button-text, .bp3-button-group.bp3-align-left .bp3-button-text, .bp3-button-group.bp3-align-right .bp3-button-text{ -webkit-box-flex:1; -ms-flex:1 1 auto; flex:1 1 auto; } .bp3-button-group{ display:-webkit-inline-box; display:-ms-inline-flexbox; display:inline-flex; } .bp3-button-group .bp3-button{ -webkit-box-flex:0; -ms-flex:0 0 auto; flex:0 0 auto; position:relative; z-index:4; } .bp3-button-group .bp3-button:focus{ z-index:5; } .bp3-button-group .bp3-button:hover{ z-index:6; } .bp3-button-group .bp3-button:active, .bp3-button-group .bp3-button.bp3-active{ z-index:7; } .bp3-button-group .bp3-button:disabled, .bp3-button-group .bp3-button.bp3-disabled{ z-index:3; } .bp3-button-group .bp3-button[class*=\"bp3-intent-\"]{ z-index:9; } .bp3-button-group .bp3-button[class*=\"bp3-intent-\"]:focus{ z-index:10; } .bp3-button-group .bp3-button[class*=\"bp3-intent-\"]:hover{ z-index:11; } .bp3-button-group .bp3-button[class*=\"bp3-intent-\"]:active, .bp3-button-group .bp3-button[class*=\"bp3-intent-\"].bp3-active{ z-index:12; } .bp3-button-group .bp3-button[class*=\"bp3-intent-\"]:disabled, .bp3-button-group .bp3-button[class*=\"bp3-intent-\"].bp3-disabled{ z-index:8; } .bp3-button-group:not(.bp3-minimal) > .bp3-popover-wrapper:not(:first-child) .bp3-button, .bp3-button-group:not(.bp3-minimal) > .bp3-button:not(:first-child){ border-top-left-radius:0; border-bottom-left-radius:0; } .bp3-button-group:not(.bp3-minimal) > .bp3-popover-wrapper:not(:last-child) .bp3-button, .bp3-button-group:not(.bp3-minimal) > .bp3-button:not(:last-child){ margin-right:-1px; border-top-right-radius:0; border-bottom-right-radius:0; } .bp3-button-group.bp3-minimal .bp3-button{ -webkit-box-shadow:none; box-shadow:none; background:none; } .bp3-button-group.bp3-minimal .bp3-button:hover{ -webkit-box-shadow:none; box-shadow:none; background:rgba(167, 182, 194, 0.3); text-decoration:none; color:#182026; } .bp3-button-group.bp3-minimal .bp3-button:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-active{ -webkit-box-shadow:none; box-shadow:none; background:rgba(115, 134, 148, 0.3); color:#182026; } .bp3-button-group.bp3-minimal .bp3-button:disabled, .bp3-button-group.bp3-minimal .bp3-button:disabled:hover, .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled, .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled:hover{ background:none; cursor:not-allowed; color:rgba(92, 112, 128, 0.6); } .bp3-button-group.bp3-minimal .bp3-button:disabled.bp3-active, .bp3-button-group.bp3-minimal .bp3-button:disabled:hover.bp3-active, .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled.bp3-active, .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled:hover.bp3-active{ background:rgba(115, 134, 148, 0.3); } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button{ -webkit-box-shadow:none; box-shadow:none; background:none; color:inherit; } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:hover, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-active{ -webkit-box-shadow:none; box-shadow:none; background:none; } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:hover{ background:rgba(138, 155, 168, 0.15); } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-active{ background:rgba(138, 155, 168, 0.3); color:#f5f8fa; } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:disabled, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:disabled:hover, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled:hover{ background:none; cursor:not-allowed; color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:disabled.bp3-active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:disabled:hover.bp3-active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled.bp3-active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled:hover.bp3-active{ background:rgba(138, 155, 168, 0.3); } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary{ color:#106ba3; } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:hover, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-active{ -webkit-box-shadow:none; box-shadow:none; background:none; color:#106ba3; } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:hover{ background:rgba(19, 124, 189, 0.15); color:#106ba3; } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-active{ background:rgba(19, 124, 189, 0.3); color:#106ba3; } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:disabled, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-disabled{ background:none; color:rgba(16, 107, 163, 0.5); } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:disabled.bp3-active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-disabled.bp3-active{ background:rgba(19, 124, 189, 0.3); } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary .bp3-button-spinner .bp3-spinner-head{ stroke:#106ba3; } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary{ color:#48aff0; } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:hover{ background:rgba(19, 124, 189, 0.2); color:#48aff0; } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-active{ background:rgba(19, 124, 189, 0.3); color:#48aff0; } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:disabled, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-disabled{ background:none; color:rgba(72, 175, 240, 0.5); } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:disabled.bp3-active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-disabled.bp3-active{ background:rgba(19, 124, 189, 0.3); } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success{ color:#0d8050; } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:hover, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-active{ -webkit-box-shadow:none; box-shadow:none; background:none; color:#0d8050; } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:hover{ background:rgba(15, 153, 96, 0.15); color:#0d8050; } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-active{ background:rgba(15, 153, 96, 0.3); color:#0d8050; } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:disabled, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-disabled{ background:none; color:rgba(13, 128, 80, 0.5); } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:disabled.bp3-active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-disabled.bp3-active{ background:rgba(15, 153, 96, 0.3); } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success .bp3-button-spinner .bp3-spinner-head{ stroke:#0d8050; } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success{ color:#3dcc91; } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:hover{ background:rgba(15, 153, 96, 0.2); color:#3dcc91; } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-active{ background:rgba(15, 153, 96, 0.3); color:#3dcc91; } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:disabled, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-disabled{ background:none; color:rgba(61, 204, 145, 0.5); } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:disabled.bp3-active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-disabled.bp3-active{ background:rgba(15, 153, 96, 0.3); } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning{ color:#bf7326; } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:hover, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-active{ -webkit-box-shadow:none; box-shadow:none; background:none; color:#bf7326; } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:hover{ background:rgba(217, 130, 43, 0.15); color:#bf7326; } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-active{ background:rgba(217, 130, 43, 0.3); color:#bf7326; } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:disabled, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-disabled{ background:none; color:rgba(191, 115, 38, 0.5); } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:disabled.bp3-active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-disabled.bp3-active{ background:rgba(217, 130, 43, 0.3); } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning .bp3-button-spinner .bp3-spinner-head{ stroke:#bf7326; } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning{ color:#ffb366; } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:hover{ background:rgba(217, 130, 43, 0.2); color:#ffb366; } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-active{ background:rgba(217, 130, 43, 0.3); color:#ffb366; } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:disabled, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-disabled{ background:none; color:rgba(255, 179, 102, 0.5); } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:disabled.bp3-active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-disabled.bp3-active{ background:rgba(217, 130, 43, 0.3); } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger{ color:#c23030; } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:hover, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-active{ -webkit-box-shadow:none; box-shadow:none; background:none; color:#c23030; } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:hover{ background:rgba(219, 55, 55, 0.15); color:#c23030; } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-active{ background:rgba(219, 55, 55, 0.3); color:#c23030; } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:disabled, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-disabled{ background:none; color:rgba(194, 48, 48, 0.5); } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:disabled.bp3-active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-disabled.bp3-active{ background:rgba(219, 55, 55, 0.3); } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger .bp3-button-spinner .bp3-spinner-head{ stroke:#c23030; } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger{ color:#ff7373; } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:hover{ background:rgba(219, 55, 55, 0.2); color:#ff7373; } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-active{ background:rgba(219, 55, 55, 0.3); color:#ff7373; } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:disabled, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-disabled{ background:none; color:rgba(255, 115, 115, 0.5); } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:disabled.bp3-active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-disabled.bp3-active{ background:rgba(219, 55, 55, 0.3); } .bp3-button-group .bp3-popover-wrapper, .bp3-button-group .bp3-popover-target{ display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-flex:1; -ms-flex:1 1 auto; flex:1 1 auto; } .bp3-button-group.bp3-fill{ display:-webkit-box; display:-ms-flexbox; display:flex; width:100%; } .bp3-button-group .bp3-button.bp3-fill, .bp3-button-group.bp3-fill .bp3-button:not(.bp3-fixed){ -webkit-box-flex:1; -ms-flex:1 1 auto; flex:1 1 auto; } .bp3-button-group.bp3-vertical{ -webkit-box-orient:vertical; -webkit-box-direction:normal; -ms-flex-direction:column; flex-direction:column; -webkit-box-align:stretch; -ms-flex-align:stretch; align-items:stretch; vertical-align:top; } .bp3-button-group.bp3-vertical.bp3-fill{ width:unset; height:100%; } .bp3-button-group.bp3-vertical .bp3-button{ margin-right:0 !important; width:100%; } .bp3-button-group.bp3-vertical:not(.bp3-minimal) > .bp3-popover-wrapper:first-child .bp3-button, .bp3-button-group.bp3-vertical:not(.bp3-minimal) > .bp3-button:first-child{ border-radius:3px 3px 0 0; } .bp3-button-group.bp3-vertical:not(.bp3-minimal) > .bp3-popover-wrapper:last-child .bp3-button, .bp3-button-group.bp3-vertical:not(.bp3-minimal) > .bp3-button:last-child{ border-radius:0 0 3px 3px; } .bp3-button-group.bp3-vertical:not(.bp3-minimal) > .bp3-popover-wrapper:not(:last-child) .bp3-button, .bp3-button-group.bp3-vertical:not(.bp3-minimal) > .bp3-button:not(:last-child){ margin-bottom:-1px; } .bp3-button-group.bp3-align-left .bp3-button{ text-align:left; } .bp3-dark .bp3-button-group:not(.bp3-minimal) > .bp3-popover-wrapper:not(:last-child) .bp3-button, .bp3-dark .bp3-button-group:not(.bp3-minimal) > .bp3-button:not(:last-child){ margin-right:1px; } .bp3-dark .bp3-button-group.bp3-vertical > .bp3-popover-wrapper:not(:last-child) .bp3-button, .bp3-dark .bp3-button-group.bp3-vertical > .bp3-button:not(:last-child){ margin-bottom:1px; } .bp3-callout{ line-height:1.5; font-size:14px; position:relative; border-radius:3px; background-color:rgba(138, 155, 168, 0.15); width:100%; padding:10px 12px 9px; } .bp3-callout[class*=\"bp3-icon-\"]{ padding-left:40px; } .bp3-callout[class*=\"bp3-icon-\"]::before{ line-height:1; font-family:\"Icons20\", sans-serif; font-size:20px; font-weight:400; font-style:normal; -moz-osx-font-smoothing:grayscale; -webkit-font-smoothing:antialiased; position:absolute; top:10px; left:10px; color:#5c7080; } .bp3-callout.bp3-callout-icon{ padding-left:40px; } .bp3-callout.bp3-callout-icon > .bp3-icon:first-child{ position:absolute; top:10px; left:10px; color:#5c7080; } .bp3-callout .bp3-heading{ margin-top:0; margin-bottom:5px; line-height:20px; } .bp3-callout .bp3-heading:last-child{ margin-bottom:0; } .bp3-dark .bp3-callout{ background-color:rgba(138, 155, 168, 0.2); } .bp3-dark .bp3-callout[class*=\"bp3-icon-\"]::before{ color:#a7b6c2; } .bp3-callout.bp3-intent-primary{ background-color:rgba(19, 124, 189, 0.15); } .bp3-callout.bp3-intent-primary[class*=\"bp3-icon-\"]::before, .bp3-callout.bp3-intent-primary > .bp3-icon:first-child, .bp3-callout.bp3-intent-primary .bp3-heading{ color:#106ba3; } .bp3-dark .bp3-callout.bp3-intent-primary{ background-color:rgba(19, 124, 189, 0.25); } .bp3-dark .bp3-callout.bp3-intent-primary[class*=\"bp3-icon-\"]::before, .bp3-dark .bp3-callout.bp3-intent-primary > .bp3-icon:first-child, .bp3-dark .bp3-callout.bp3-intent-primary .bp3-heading{ color:#48aff0; } .bp3-callout.bp3-intent-success{ background-color:rgba(15, 153, 96, 0.15); } .bp3-callout.bp3-intent-success[class*=\"bp3-icon-\"]::before, .bp3-callout.bp3-intent-success > .bp3-icon:first-child, .bp3-callout.bp3-intent-success .bp3-heading{ color:#0d8050; } .bp3-dark .bp3-callout.bp3-intent-success{ background-color:rgba(15, 153, 96, 0.25); } .bp3-dark .bp3-callout.bp3-intent-success[class*=\"bp3-icon-\"]::before, .bp3-dark .bp3-callout.bp3-intent-success > .bp3-icon:first-child, .bp3-dark .bp3-callout.bp3-intent-success .bp3-heading{ color:#3dcc91; } .bp3-callout.bp3-intent-warning{ background-color:rgba(217, 130, 43, 0.15); } .bp3-callout.bp3-intent-warning[class*=\"bp3-icon-\"]::before, .bp3-callout.bp3-intent-warning > .bp3-icon:first-child, .bp3-callout.bp3-intent-warning .bp3-heading{ color:#bf7326; } .bp3-dark .bp3-callout.bp3-intent-warning{ background-color:rgba(217, 130, 43, 0.25); } .bp3-dark .bp3-callout.bp3-intent-warning[class*=\"bp3-icon-\"]::before, .bp3-dark .bp3-callout.bp3-intent-warning > .bp3-icon:first-child, .bp3-dark .bp3-callout.bp3-intent-warning .bp3-heading{ color:#ffb366; } .bp3-callout.bp3-intent-danger{ background-color:rgba(219, 55, 55, 0.15); } .bp3-callout.bp3-intent-danger[class*=\"bp3-icon-\"]::before, .bp3-callout.bp3-intent-danger > .bp3-icon:first-child, .bp3-callout.bp3-intent-danger .bp3-heading{ color:#c23030; } .bp3-dark .bp3-callout.bp3-intent-danger{ background-color:rgba(219, 55, 55, 0.25); } .bp3-dark .bp3-callout.bp3-intent-danger[class*=\"bp3-icon-\"]::before, .bp3-dark .bp3-callout.bp3-intent-danger > .bp3-icon:first-child, .bp3-dark .bp3-callout.bp3-intent-danger .bp3-heading{ color:#ff7373; } .bp3-running-text .bp3-callout{ margin:20px 0; } .bp3-card{ border-radius:3px; -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.15), 0 0 0 rgba(16, 22, 26, 0), 0 0 0 rgba(16, 22, 26, 0); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.15), 0 0 0 rgba(16, 22, 26, 0), 0 0 0 rgba(16, 22, 26, 0); background-color:#ffffff; padding:20px; -webkit-transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9); transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9); transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9), box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9); transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9), box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9); } .bp3-card.bp3-dark, .bp3-dark .bp3-card{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), 0 0 0 rgba(16, 22, 26, 0), 0 0 0 rgba(16, 22, 26, 0); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), 0 0 0 rgba(16, 22, 26, 0), 0 0 0 rgba(16, 22, 26, 0); background-color:#30404d; } .bp3-elevation-0{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.15), 0 0 0 rgba(16, 22, 26, 0), 0 0 0 rgba(16, 22, 26, 0); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.15), 0 0 0 rgba(16, 22, 26, 0), 0 0 0 rgba(16, 22, 26, 0); } .bp3-elevation-0.bp3-dark, .bp3-dark .bp3-elevation-0{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), 0 0 0 rgba(16, 22, 26, 0), 0 0 0 rgba(16, 22, 26, 0); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), 0 0 0 rgba(16, 22, 26, 0), 0 0 0 rgba(16, 22, 26, 0); } .bp3-elevation-1{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-elevation-1.bp3-dark, .bp3-dark .bp3-elevation-1{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4); } .bp3-elevation-2{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 1px 1px rgba(16, 22, 26, 0.2), 0 2px 6px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 1px 1px rgba(16, 22, 26, 0.2), 0 2px 6px rgba(16, 22, 26, 0.2); } .bp3-elevation-2.bp3-dark, .bp3-dark .bp3-elevation-2{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 1px 1px rgba(16, 22, 26, 0.4), 0 2px 6px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 1px 1px rgba(16, 22, 26, 0.4), 0 2px 6px rgba(16, 22, 26, 0.4); } .bp3-elevation-3{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2); } .bp3-elevation-3.bp3-dark, .bp3-dark .bp3-elevation-3{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); } .bp3-elevation-4{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 4px 8px rgba(16, 22, 26, 0.2), 0 18px 46px 6px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 4px 8px rgba(16, 22, 26, 0.2), 0 18px 46px 6px rgba(16, 22, 26, 0.2); } .bp3-elevation-4.bp3-dark, .bp3-dark .bp3-elevation-4{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 4px 8px rgba(16, 22, 26, 0.4), 0 18px 46px 6px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 4px 8px rgba(16, 22, 26, 0.4), 0 18px 46px 6px rgba(16, 22, 26, 0.4); } .bp3-card.bp3-interactive:hover{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2); cursor:pointer; } .bp3-card.bp3-interactive:hover.bp3-dark, .bp3-dark .bp3-card.bp3-interactive:hover{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); } .bp3-card.bp3-interactive:active{ opacity:0.9; -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.2); -webkit-transition-duration:0; transition-duration:0; } .bp3-card.bp3-interactive:active.bp3-dark, .bp3-dark .bp3-card.bp3-interactive:active{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4); } .bp3-collapse{ height:0; overflow-y:hidden; -webkit-transition:height 200ms cubic-bezier(0.4, 1, 0.75, 0.9); transition:height 200ms cubic-bezier(0.4, 1, 0.75, 0.9); } .bp3-collapse .bp3-collapse-body{ -webkit-transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9); transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9); transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9); transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9); } .bp3-collapse .bp3-collapse-body[aria-hidden=\"true\"]{ display:none; } .bp3-context-menu .bp3-popover-target{ display:block; } .bp3-context-menu-popover-target{ position:fixed; } .bp3-divider{ margin:5px; border-right:1px solid rgba(16, 22, 26, 0.15); border-bottom:1px solid rgba(16, 22, 26, 0.15); } .bp3-dark .bp3-divider{ border-color:rgba(16, 22, 26, 0.4); } .bp3-dialog-container{ opacity:1; -webkit-transform:scale(1); transform:scale(1); display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-align:center; -ms-flex-align:center; align-items:center; -webkit-box-pack:center; -ms-flex-pack:center; justify-content:center; width:100%; min-height:100%; pointer-events:none; -webkit-user-select:none; -moz-user-select:none; -ms-user-select:none; user-select:none; } .bp3-dialog-container.bp3-overlay-enter > .bp3-dialog, .bp3-dialog-container.bp3-overlay-appear > .bp3-dialog{ opacity:0; -webkit-transform:scale(0.5); transform:scale(0.5); } .bp3-dialog-container.bp3-overlay-enter-active > .bp3-dialog, .bp3-dialog-container.bp3-overlay-appear-active > .bp3-dialog{ opacity:1; -webkit-transform:scale(1); transform:scale(1); -webkit-transition-property:opacity, -webkit-transform; transition-property:opacity, -webkit-transform; transition-property:opacity, transform; transition-property:opacity, transform, -webkit-transform; -webkit-transition-duration:300ms; transition-duration:300ms; -webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11); transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11); -webkit-transition-delay:0; transition-delay:0; } .bp3-dialog-container.bp3-overlay-exit > .bp3-dialog{ opacity:1; -webkit-transform:scale(1); transform:scale(1); } .bp3-dialog-container.bp3-overlay-exit-active > .bp3-dialog{ opacity:0; -webkit-transform:scale(0.5); transform:scale(0.5); -webkit-transition-property:opacity, -webkit-transform; transition-property:opacity, -webkit-transform; transition-property:opacity, transform; transition-property:opacity, transform, -webkit-transform; -webkit-transition-duration:300ms; transition-duration:300ms; -webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11); transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11); -webkit-transition-delay:0; transition-delay:0; } .bp3-dialog{ display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-orient:vertical; -webkit-box-direction:normal; -ms-flex-direction:column; flex-direction:column; margin:30px 0; border-radius:6px; -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 4px 8px rgba(16, 22, 26, 0.2), 0 18px 46px 6px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 4px 8px rgba(16, 22, 26, 0.2), 0 18px 46px 6px rgba(16, 22, 26, 0.2); background:#ebf1f5; width:500px; padding-bottom:20px; pointer-events:all; -webkit-user-select:text; -moz-user-select:text; -ms-user-select:text; user-select:text; } .bp3-dialog:focus{ outline:0; } .bp3-dialog.bp3-dark, .bp3-dark .bp3-dialog{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 4px 8px rgba(16, 22, 26, 0.4), 0 18px 46px 6px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 4px 8px rgba(16, 22, 26, 0.4), 0 18px 46px 6px rgba(16, 22, 26, 0.4); background:#293742; color:#f5f8fa; } .bp3-dialog-header{ display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-flex:0; -ms-flex:0 0 auto; flex:0 0 auto; -webkit-box-align:center; -ms-flex-align:center; align-items:center; border-radius:6px 6px 0 0; -webkit-box-shadow:0 1px 0 rgba(16, 22, 26, 0.15); box-shadow:0 1px 0 rgba(16, 22, 26, 0.15); background:#ffffff; min-height:40px; padding-right:5px; padding-left:20px; } .bp3-dialog-header .bp3-icon-large, .bp3-dialog-header .bp3-icon{ -webkit-box-flex:0; -ms-flex:0 0 auto; flex:0 0 auto; margin-right:10px; color:#5c7080; } .bp3-dialog-header .bp3-heading{ overflow:hidden; text-overflow:ellipsis; white-space:nowrap; word-wrap:normal; -webkit-box-flex:1; -ms-flex:1 1 auto; flex:1 1 auto; margin:0; line-height:inherit; } .bp3-dialog-header .bp3-heading:last-child{ margin-right:20px; } .bp3-dark .bp3-dialog-header{ -webkit-box-shadow:0 1px 0 rgba(16, 22, 26, 0.4); box-shadow:0 1px 0 rgba(16, 22, 26, 0.4); background:#30404d; } .bp3-dark .bp3-dialog-header .bp3-icon-large, .bp3-dark .bp3-dialog-header .bp3-icon{ color:#a7b6c2; } .bp3-dialog-body{ -webkit-box-flex:1; -ms-flex:1 1 auto; flex:1 1 auto; margin:20px; line-height:18px; } .bp3-dialog-footer{ -webkit-box-flex:0; -ms-flex:0 0 auto; flex:0 0 auto; margin:0 20px; } .bp3-dialog-footer-actions{ display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-pack:end; -ms-flex-pack:end; justify-content:flex-end; } .bp3-dialog-footer-actions .bp3-button{ margin-left:10px; } .bp3-drawer{ display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-orient:vertical; -webkit-box-direction:normal; -ms-flex-direction:column; flex-direction:column; margin:0; -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 4px 8px rgba(16, 22, 26, 0.2), 0 18px 46px 6px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 4px 8px rgba(16, 22, 26, 0.2), 0 18px 46px 6px rgba(16, 22, 26, 0.2); background:#ffffff; padding:0; } .bp3-drawer:focus{ outline:0; } .bp3-drawer.bp3-position-top{ top:0; right:0; left:0; height:50%; } .bp3-drawer.bp3-position-top.bp3-overlay-enter, .bp3-drawer.bp3-position-top.bp3-overlay-appear{ -webkit-transform:translateY(-100%); transform:translateY(-100%); } .bp3-drawer.bp3-position-top.bp3-overlay-enter-active, .bp3-drawer.bp3-position-top.bp3-overlay-appear-active{ -webkit-transform:translateY(0); transform:translateY(0); -webkit-transition-property:-webkit-transform; transition-property:-webkit-transform; transition-property:transform; transition-property:transform, -webkit-transform; -webkit-transition-duration:200ms; transition-duration:200ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-transition-delay:0; transition-delay:0; } .bp3-drawer.bp3-position-top.bp3-overlay-exit{ -webkit-transform:translateY(0); transform:translateY(0); } .bp3-drawer.bp3-position-top.bp3-overlay-exit-active{ -webkit-transform:translateY(-100%); transform:translateY(-100%); -webkit-transition-property:-webkit-transform; transition-property:-webkit-transform; transition-property:transform; transition-property:transform, -webkit-transform; -webkit-transition-duration:100ms; transition-duration:100ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-transition-delay:0; transition-delay:0; } .bp3-drawer.bp3-position-bottom{ right:0; bottom:0; left:0; height:50%; } .bp3-drawer.bp3-position-bottom.bp3-overlay-enter, .bp3-drawer.bp3-position-bottom.bp3-overlay-appear{ -webkit-transform:translateY(100%); transform:translateY(100%); } .bp3-drawer.bp3-position-bottom.bp3-overlay-enter-active, .bp3-drawer.bp3-position-bottom.bp3-overlay-appear-active{ -webkit-transform:translateY(0); transform:translateY(0); -webkit-transition-property:-webkit-transform; transition-property:-webkit-transform; transition-property:transform; transition-property:transform, -webkit-transform; -webkit-transition-duration:200ms; transition-duration:200ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-transition-delay:0; transition-delay:0; } .bp3-drawer.bp3-position-bottom.bp3-overlay-exit{ -webkit-transform:translateY(0); transform:translateY(0); } .bp3-drawer.bp3-position-bottom.bp3-overlay-exit-active{ -webkit-transform:translateY(100%); transform:translateY(100%); -webkit-transition-property:-webkit-transform; transition-property:-webkit-transform; transition-property:transform; transition-property:transform, -webkit-transform; -webkit-transition-duration:100ms; transition-duration:100ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-transition-delay:0; transition-delay:0; } .bp3-drawer.bp3-position-left{ top:0; bottom:0; left:0; width:50%; } .bp3-drawer.bp3-position-left.bp3-overlay-enter, .bp3-drawer.bp3-position-left.bp3-overlay-appear{ -webkit-transform:translateX(-100%); transform:translateX(-100%); } .bp3-drawer.bp3-position-left.bp3-overlay-enter-active, .bp3-drawer.bp3-position-left.bp3-overlay-appear-active{ -webkit-transform:translateX(0); transform:translateX(0); -webkit-transition-property:-webkit-transform; transition-property:-webkit-transform; transition-property:transform; transition-property:transform, -webkit-transform; -webkit-transition-duration:200ms; transition-duration:200ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-transition-delay:0; transition-delay:0; } .bp3-drawer.bp3-position-left.bp3-overlay-exit{ -webkit-transform:translateX(0); transform:translateX(0); } .bp3-drawer.bp3-position-left.bp3-overlay-exit-active{ -webkit-transform:translateX(-100%); transform:translateX(-100%); -webkit-transition-property:-webkit-transform; transition-property:-webkit-transform; transition-property:transform; transition-property:transform, -webkit-transform; -webkit-transition-duration:100ms; transition-duration:100ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-transition-delay:0; transition-delay:0; } .bp3-drawer.bp3-position-right{ top:0; right:0; bottom:0; width:50%; } .bp3-drawer.bp3-position-right.bp3-overlay-enter, .bp3-drawer.bp3-position-right.bp3-overlay-appear{ -webkit-transform:translateX(100%); transform:translateX(100%); } .bp3-drawer.bp3-position-right.bp3-overlay-enter-active, .bp3-drawer.bp3-position-right.bp3-overlay-appear-active{ -webkit-transform:translateX(0); transform:translateX(0); -webkit-transition-property:-webkit-transform; transition-property:-webkit-transform; transition-property:transform; transition-property:transform, -webkit-transform; -webkit-transition-duration:200ms; transition-duration:200ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-transition-delay:0; transition-delay:0; } .bp3-drawer.bp3-position-right.bp3-overlay-exit{ -webkit-transform:translateX(0); transform:translateX(0); } .bp3-drawer.bp3-position-right.bp3-overlay-exit-active{ -webkit-transform:translateX(100%); transform:translateX(100%); -webkit-transition-property:-webkit-transform; transition-property:-webkit-transform; transition-property:transform; transition-property:transform, -webkit-transform; -webkit-transition-duration:100ms; transition-duration:100ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-transition-delay:0; transition-delay:0; } .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not( .bp3-position-right):not(.bp3-vertical){ top:0; right:0; bottom:0; width:50%; } .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not( .bp3-position-right):not(.bp3-vertical).bp3-overlay-enter, .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not( .bp3-position-right):not(.bp3-vertical).bp3-overlay-appear{ -webkit-transform:translateX(100%); transform:translateX(100%); } .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not( .bp3-position-right):not(.bp3-vertical).bp3-overlay-enter-active, .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not( .bp3-position-right):not(.bp3-vertical).bp3-overlay-appear-active{ -webkit-transform:translateX(0); transform:translateX(0); -webkit-transition-property:-webkit-transform; transition-property:-webkit-transform; transition-property:transform; transition-property:transform, -webkit-transform; -webkit-transition-duration:200ms; transition-duration:200ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-transition-delay:0; transition-delay:0; } .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not( .bp3-position-right):not(.bp3-vertical).bp3-overlay-exit{ -webkit-transform:translateX(0); transform:translateX(0); } .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not( .bp3-position-right):not(.bp3-vertical).bp3-overlay-exit-active{ -webkit-transform:translateX(100%); transform:translateX(100%); -webkit-transition-property:-webkit-transform; transition-property:-webkit-transform; transition-property:transform; transition-property:transform, -webkit-transform; -webkit-transition-duration:100ms; transition-duration:100ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-transition-delay:0; transition-delay:0; } .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not( .bp3-position-right).bp3-vertical{ right:0; bottom:0; left:0; height:50%; } .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not( .bp3-position-right).bp3-vertical.bp3-overlay-enter, .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not( .bp3-position-right).bp3-vertical.bp3-overlay-appear{ -webkit-transform:translateY(100%); transform:translateY(100%); } .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not( .bp3-position-right).bp3-vertical.bp3-overlay-enter-active, .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not( .bp3-position-right).bp3-vertical.bp3-overlay-appear-active{ -webkit-transform:translateY(0); transform:translateY(0); -webkit-transition-property:-webkit-transform; transition-property:-webkit-transform; transition-property:transform; transition-property:transform, -webkit-transform; -webkit-transition-duration:200ms; transition-duration:200ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-transition-delay:0; transition-delay:0; } .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not( .bp3-position-right).bp3-vertical.bp3-overlay-exit{ -webkit-transform:translateY(0); transform:translateY(0); } .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not( .bp3-position-right).bp3-vertical.bp3-overlay-exit-active{ -webkit-transform:translateY(100%); transform:translateY(100%); -webkit-transition-property:-webkit-transform; transition-property:-webkit-transform; transition-property:transform; transition-property:transform, -webkit-transform; -webkit-transition-duration:100ms; transition-duration:100ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-transition-delay:0; transition-delay:0; } .bp3-drawer.bp3-dark, .bp3-dark .bp3-drawer{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 4px 8px rgba(16, 22, 26, 0.4), 0 18px 46px 6px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 4px 8px rgba(16, 22, 26, 0.4), 0 18px 46px 6px rgba(16, 22, 26, 0.4); background:#30404d; color:#f5f8fa; } .bp3-drawer-header{ display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-flex:0; -ms-flex:0 0 auto; flex:0 0 auto; -webkit-box-align:center; -ms-flex-align:center; align-items:center; position:relative; border-radius:0; -webkit-box-shadow:0 1px 0 rgba(16, 22, 26, 0.15); box-shadow:0 1px 0 rgba(16, 22, 26, 0.15); min-height:40px; padding:5px; padding-left:20px; } .bp3-drawer-header .bp3-icon-large, .bp3-drawer-header .bp3-icon{ -webkit-box-flex:0; -ms-flex:0 0 auto; flex:0 0 auto; margin-right:10px; color:#5c7080; } .bp3-drawer-header .bp3-heading{ overflow:hidden; text-overflow:ellipsis; white-space:nowrap; word-wrap:normal; -webkit-box-flex:1; -ms-flex:1 1 auto; flex:1 1 auto; margin:0; line-height:inherit; } .bp3-drawer-header .bp3-heading:last-child{ margin-right:20px; } .bp3-dark .bp3-drawer-header{ -webkit-box-shadow:0 1px 0 rgba(16, 22, 26, 0.4); box-shadow:0 1px 0 rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-drawer-header .bp3-icon-large, .bp3-dark .bp3-drawer-header .bp3-icon{ color:#a7b6c2; } .bp3-drawer-body{ -webkit-box-flex:1; -ms-flex:1 1 auto; flex:1 1 auto; overflow:auto; line-height:18px; } .bp3-drawer-footer{ -webkit-box-flex:0; -ms-flex:0 0 auto; flex:0 0 auto; position:relative; -webkit-box-shadow:inset 0 1px 0 rgba(16, 22, 26, 0.15); box-shadow:inset 0 1px 0 rgba(16, 22, 26, 0.15); padding:10px 20px; } .bp3-dark .bp3-drawer-footer{ -webkit-box-shadow:inset 0 1px 0 rgba(16, 22, 26, 0.4); box-shadow:inset 0 1px 0 rgba(16, 22, 26, 0.4); } .bp3-editable-text{ display:inline-block; position:relative; cursor:text; max-width:100%; vertical-align:top; white-space:nowrap; } .bp3-editable-text::before{ position:absolute; top:-3px; right:-3px; bottom:-3px; left:-3px; border-radius:3px; content:\"\"; -webkit-transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9); transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9); transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9), box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9); transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9), box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9); } .bp3-editable-text:hover::before{ -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.15); box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.15); } .bp3-editable-text.bp3-editable-text-editing::before{ -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); background-color:#ffffff; } .bp3-editable-text.bp3-disabled::before{ -webkit-box-shadow:none; box-shadow:none; } .bp3-editable-text.bp3-intent-primary .bp3-editable-text-input, .bp3-editable-text.bp3-intent-primary .bp3-editable-text-content{ color:#137cbd; } .bp3-editable-text.bp3-intent-primary:hover::before{ -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(19, 124, 189, 0.4); box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(19, 124, 189, 0.4); } .bp3-editable-text.bp3-intent-primary.bp3-editable-text-editing::before{ -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-editable-text.bp3-intent-success .bp3-editable-text-input, .bp3-editable-text.bp3-intent-success .bp3-editable-text-content{ color:#0f9960; } .bp3-editable-text.bp3-intent-success:hover::before{ -webkit-box-shadow:0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), inset 0 0 0 1px rgba(15, 153, 96, 0.4); box-shadow:0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), inset 0 0 0 1px rgba(15, 153, 96, 0.4); } .bp3-editable-text.bp3-intent-success.bp3-editable-text-editing::before{ -webkit-box-shadow:0 0 0 1px #0f9960, 0 0 0 3px rgba(15, 153, 96, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px #0f9960, 0 0 0 3px rgba(15, 153, 96, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-editable-text.bp3-intent-warning .bp3-editable-text-input, .bp3-editable-text.bp3-intent-warning .bp3-editable-text-content{ color:#d9822b; } .bp3-editable-text.bp3-intent-warning:hover::before{ -webkit-box-shadow:0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), inset 0 0 0 1px rgba(217, 130, 43, 0.4); box-shadow:0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), inset 0 0 0 1px rgba(217, 130, 43, 0.4); } .bp3-editable-text.bp3-intent-warning.bp3-editable-text-editing::before{ -webkit-box-shadow:0 0 0 1px #d9822b, 0 0 0 3px rgba(217, 130, 43, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px #d9822b, 0 0 0 3px rgba(217, 130, 43, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-editable-text.bp3-intent-danger .bp3-editable-text-input, .bp3-editable-text.bp3-intent-danger .bp3-editable-text-content{ color:#db3737; } .bp3-editable-text.bp3-intent-danger:hover::before{ -webkit-box-shadow:0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), inset 0 0 0 1px rgba(219, 55, 55, 0.4); box-shadow:0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), inset 0 0 0 1px rgba(219, 55, 55, 0.4); } .bp3-editable-text.bp3-intent-danger.bp3-editable-text-editing::before{ -webkit-box-shadow:0 0 0 1px #db3737, 0 0 0 3px rgba(219, 55, 55, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px #db3737, 0 0 0 3px rgba(219, 55, 55, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-dark .bp3-editable-text:hover::before{ -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(255, 255, 255, 0.15); box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(255, 255, 255, 0.15); } .bp3-dark .bp3-editable-text.bp3-editable-text-editing::before{ -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); background-color:rgba(16, 22, 26, 0.3); } .bp3-dark .bp3-editable-text.bp3-disabled::before{ -webkit-box-shadow:none; box-shadow:none; } .bp3-dark .bp3-editable-text.bp3-intent-primary .bp3-editable-text-content{ color:#48aff0; } .bp3-dark .bp3-editable-text.bp3-intent-primary:hover::before{ -webkit-box-shadow:0 0 0 0 rgba(72, 175, 240, 0), 0 0 0 0 rgba(72, 175, 240, 0), inset 0 0 0 1px rgba(72, 175, 240, 0.4); box-shadow:0 0 0 0 rgba(72, 175, 240, 0), 0 0 0 0 rgba(72, 175, 240, 0), inset 0 0 0 1px rgba(72, 175, 240, 0.4); } .bp3-dark .bp3-editable-text.bp3-intent-primary.bp3-editable-text-editing::before{ -webkit-box-shadow:0 0 0 1px #48aff0, 0 0 0 3px rgba(72, 175, 240, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px #48aff0, 0 0 0 3px rgba(72, 175, 240, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-editable-text.bp3-intent-success .bp3-editable-text-content{ color:#3dcc91; } .bp3-dark .bp3-editable-text.bp3-intent-success:hover::before{ -webkit-box-shadow:0 0 0 0 rgba(61, 204, 145, 0), 0 0 0 0 rgba(61, 204, 145, 0), inset 0 0 0 1px rgba(61, 204, 145, 0.4); box-shadow:0 0 0 0 rgba(61, 204, 145, 0), 0 0 0 0 rgba(61, 204, 145, 0), inset 0 0 0 1px rgba(61, 204, 145, 0.4); } .bp3-dark .bp3-editable-text.bp3-intent-success.bp3-editable-text-editing::before{ -webkit-box-shadow:0 0 0 1px #3dcc91, 0 0 0 3px rgba(61, 204, 145, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px #3dcc91, 0 0 0 3px rgba(61, 204, 145, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-editable-text.bp3-intent-warning .bp3-editable-text-content{ color:#ffb366; } .bp3-dark .bp3-editable-text.bp3-intent-warning:hover::before{ -webkit-box-shadow:0 0 0 0 rgba(255, 179, 102, 0), 0 0 0 0 rgba(255, 179, 102, 0), inset 0 0 0 1px rgba(255, 179, 102, 0.4); box-shadow:0 0 0 0 rgba(255, 179, 102, 0), 0 0 0 0 rgba(255, 179, 102, 0), inset 0 0 0 1px rgba(255, 179, 102, 0.4); } .bp3-dark .bp3-editable-text.bp3-intent-warning.bp3-editable-text-editing::before{ -webkit-box-shadow:0 0 0 1px #ffb366, 0 0 0 3px rgba(255, 179, 102, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px #ffb366, 0 0 0 3px rgba(255, 179, 102, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-editable-text.bp3-intent-danger .bp3-editable-text-content{ color:#ff7373; } .bp3-dark .bp3-editable-text.bp3-intent-danger:hover::before{ -webkit-box-shadow:0 0 0 0 rgba(255, 115, 115, 0), 0 0 0 0 rgba(255, 115, 115, 0), inset 0 0 0 1px rgba(255, 115, 115, 0.4); box-shadow:0 0 0 0 rgba(255, 115, 115, 0), 0 0 0 0 rgba(255, 115, 115, 0), inset 0 0 0 1px rgba(255, 115, 115, 0.4); } .bp3-dark .bp3-editable-text.bp3-intent-danger.bp3-editable-text-editing::before{ -webkit-box-shadow:0 0 0 1px #ff7373, 0 0 0 3px rgba(255, 115, 115, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px #ff7373, 0 0 0 3px rgba(255, 115, 115, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); } .bp3-editable-text-input, .bp3-editable-text-content{ display:inherit; position:relative; min-width:inherit; max-width:inherit; vertical-align:top; text-transform:inherit; letter-spacing:inherit; color:inherit; font:inherit; resize:none; } .bp3-editable-text-input{ border:none; -webkit-box-shadow:none; box-shadow:none; background:none; width:100%; padding:0; white-space:pre-wrap; } .bp3-editable-text-input::-webkit-input-placeholder{ opacity:1; color:rgba(92, 112, 128, 0.6); } .bp3-editable-text-input::-moz-placeholder{ opacity:1; color:rgba(92, 112, 128, 0.6); } .bp3-editable-text-input:-ms-input-placeholder{ opacity:1; color:rgba(92, 112, 128, 0.6); } .bp3-editable-text-input::-ms-input-placeholder{ opacity:1; color:rgba(92, 112, 128, 0.6); } .bp3-editable-text-input::placeholder{ opacity:1; color:rgba(92, 112, 128, 0.6); } .bp3-editable-text-input:focus{ outline:none; } .bp3-editable-text-input::-ms-clear{ display:none; } .bp3-editable-text-content{ overflow:hidden; padding-right:2px; text-overflow:ellipsis; white-space:pre; } .bp3-editable-text-editing > .bp3-editable-text-content{ position:absolute; left:0; visibility:hidden; } .bp3-editable-text-placeholder > .bp3-editable-text-content{ color:rgba(92, 112, 128, 0.6); } .bp3-dark .bp3-editable-text-placeholder > .bp3-editable-text-content{ color:rgba(167, 182, 194, 0.6); } .bp3-editable-text.bp3-multiline{ display:block; } .bp3-editable-text.bp3-multiline .bp3-editable-text-content{ overflow:auto; white-space:pre-wrap; word-wrap:break-word; } .bp3-control-group{ -webkit-transform:translateZ(0); transform:translateZ(0); display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-orient:horizontal; -webkit-box-direction:normal; -ms-flex-direction:row; flex-direction:row; -webkit-box-align:stretch; -ms-flex-align:stretch; align-items:stretch; } .bp3-control-group > *{ -webkit-box-flex:0; -ms-flex-positive:0; flex-grow:0; -ms-flex-negative:0; flex-shrink:0; } .bp3-control-group > .bp3-fill{ -webkit-box-flex:1; -ms-flex-positive:1; flex-grow:1; -ms-flex-negative:1; flex-shrink:1; } .bp3-control-group .bp3-button, .bp3-control-group .bp3-html-select, .bp3-control-group .bp3-input, .bp3-control-group .bp3-select{ position:relative; } .bp3-control-group .bp3-input{ z-index:2; border-radius:inherit; } .bp3-control-group .bp3-input:focus{ z-index:14; border-radius:3px; } .bp3-control-group .bp3-input[class*=\"bp3-intent\"]{ z-index:13; } .bp3-control-group .bp3-input[class*=\"bp3-intent\"]:focus{ z-index:15; } .bp3-control-group .bp3-input[readonly], .bp3-control-group .bp3-input:disabled, .bp3-control-group .bp3-input.bp3-disabled{ z-index:1; } .bp3-control-group .bp3-input-group[class*=\"bp3-intent\"] .bp3-input{ z-index:13; } .bp3-control-group .bp3-input-group[class*=\"bp3-intent\"] .bp3-input:focus{ z-index:15; } .bp3-control-group .bp3-button, .bp3-control-group .bp3-html-select select, .bp3-control-group .bp3-select select{ -webkit-transform:translateZ(0); transform:translateZ(0); z-index:4; border-radius:inherit; } .bp3-control-group .bp3-button:focus, .bp3-control-group .bp3-html-select select:focus, .bp3-control-group .bp3-select select:focus{ z-index:5; } .bp3-control-group .bp3-button:hover, .bp3-control-group .bp3-html-select select:hover, .bp3-control-group .bp3-select select:hover{ z-index:6; } .bp3-control-group .bp3-button:active, .bp3-control-group .bp3-html-select select:active, .bp3-control-group .bp3-select select:active{ z-index:7; } .bp3-control-group .bp3-button[readonly], .bp3-control-group .bp3-button:disabled, .bp3-control-group .bp3-button.bp3-disabled, .bp3-control-group .bp3-html-select select[readonly], .bp3-control-group .bp3-html-select select:disabled, .bp3-control-group .bp3-html-select select.bp3-disabled, .bp3-control-group .bp3-select select[readonly], .bp3-control-group .bp3-select select:disabled, .bp3-control-group .bp3-select select.bp3-disabled{ z-index:3; } .bp3-control-group .bp3-button[class*=\"bp3-intent\"], .bp3-control-group .bp3-html-select select[class*=\"bp3-intent\"], .bp3-control-group .bp3-select select[class*=\"bp3-intent\"]{ z-index:9; } .bp3-control-group .bp3-button[class*=\"bp3-intent\"]:focus, .bp3-control-group .bp3-html-select select[class*=\"bp3-intent\"]:focus, .bp3-control-group .bp3-select select[class*=\"bp3-intent\"]:focus{ z-index:10; } .bp3-control-group .bp3-button[class*=\"bp3-intent\"]:hover, .bp3-control-group .bp3-html-select select[class*=\"bp3-intent\"]:hover, .bp3-control-group .bp3-select select[class*=\"bp3-intent\"]:hover{ z-index:11; } .bp3-control-group .bp3-button[class*=\"bp3-intent\"]:active, .bp3-control-group .bp3-html-select select[class*=\"bp3-intent\"]:active, .bp3-control-group .bp3-select select[class*=\"bp3-intent\"]:active{ z-index:12; } .bp3-control-group .bp3-button[class*=\"bp3-intent\"][readonly], .bp3-control-group .bp3-button[class*=\"bp3-intent\"]:disabled, .bp3-control-group .bp3-button[class*=\"bp3-intent\"].bp3-disabled, .bp3-control-group .bp3-html-select select[class*=\"bp3-intent\"][readonly], .bp3-control-group .bp3-html-select select[class*=\"bp3-intent\"]:disabled, .bp3-control-group .bp3-html-select select[class*=\"bp3-intent\"].bp3-disabled, .bp3-control-group .bp3-select select[class*=\"bp3-intent\"][readonly], .bp3-control-group .bp3-select select[class*=\"bp3-intent\"]:disabled, .bp3-control-group .bp3-select select[class*=\"bp3-intent\"].bp3-disabled{ z-index:8; } .bp3-control-group .bp3-input-group > .bp3-icon, .bp3-control-group .bp3-input-group > .bp3-button, .bp3-control-group .bp3-input-group > .bp3-input-action{ z-index:16; } .bp3-control-group .bp3-select::after, .bp3-control-group .bp3-html-select::after, .bp3-control-group .bp3-select > .bp3-icon, .bp3-control-group .bp3-html-select > .bp3-icon{ z-index:17; } .bp3-control-group:not(.bp3-vertical) > *{ margin-right:-1px; } .bp3-dark .bp3-control-group:not(.bp3-vertical) > *{ margin-right:0; } .bp3-dark .bp3-control-group:not(.bp3-vertical) > .bp3-button + .bp3-button{ margin-left:1px; } .bp3-control-group .bp3-popover-wrapper, .bp3-control-group .bp3-popover-target{ border-radius:inherit; } .bp3-control-group > :first-child{ border-radius:3px 0 0 3px; } .bp3-control-group > :last-child{ margin-right:0; border-radius:0 3px 3px 0; } .bp3-control-group > :only-child{ margin-right:0; border-radius:3px; } .bp3-control-group .bp3-input-group .bp3-button{ border-radius:3px; } .bp3-control-group > .bp3-fill{ -webkit-box-flex:1; -ms-flex:1 1 auto; flex:1 1 auto; } .bp3-control-group.bp3-fill > *:not(.bp3-fixed){ -webkit-box-flex:1; -ms-flex:1 1 auto; flex:1 1 auto; } .bp3-control-group.bp3-vertical{ -webkit-box-orient:vertical; -webkit-box-direction:normal; -ms-flex-direction:column; flex-direction:column; } .bp3-control-group.bp3-vertical > *{ margin-top:-1px; } .bp3-control-group.bp3-vertical > :first-child{ margin-top:0; border-radius:3px 3px 0 0; } .bp3-control-group.bp3-vertical > :last-child{ border-radius:0 0 3px 3px; } .bp3-control{ display:block; position:relative; margin-bottom:10px; cursor:pointer; text-transform:none; } .bp3-control input:checked ~ .bp3-control-indicator{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); background-color:#137cbd; background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0))); background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0)); color:#ffffff; } .bp3-control:hover input:checked ~ .bp3-control-indicator{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); background-color:#106ba3; } .bp3-control input:not(:disabled):active:checked ~ .bp3-control-indicator{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); background:#0e5a8a; } .bp3-control input:disabled:checked ~ .bp3-control-indicator{ -webkit-box-shadow:none; box-shadow:none; background:rgba(19, 124, 189, 0.5); } .bp3-dark .bp3-control input:checked ~ .bp3-control-indicator{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-control:hover input:checked ~ .bp3-control-indicator{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); background-color:#106ba3; } .bp3-dark .bp3-control input:not(:disabled):active:checked ~ .bp3-control-indicator{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); background-color:#0e5a8a; } .bp3-dark .bp3-control input:disabled:checked ~ .bp3-control-indicator{ -webkit-box-shadow:none; box-shadow:none; background:rgba(14, 90, 138, 0.5); } .bp3-control:not(.bp3-align-right){ padding-left:26px; } .bp3-control:not(.bp3-align-right) .bp3-control-indicator{ margin-left:-26px; } .bp3-control.bp3-align-right{ padding-right:26px; } .bp3-control.bp3-align-right .bp3-control-indicator{ margin-right:-26px; } .bp3-control.bp3-disabled{ cursor:not-allowed; color:rgba(92, 112, 128, 0.6); } .bp3-control.bp3-inline{ display:inline-block; margin-right:20px; } .bp3-control input{ position:absolute; top:0; left:0; opacity:0; z-index:-1; } .bp3-control .bp3-control-indicator{ display:inline-block; position:relative; margin-top:-3px; margin-right:10px; border:none; -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); background-clip:padding-box; background-color:#f5f8fa; background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0))); background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0)); cursor:pointer; width:1em; height:1em; vertical-align:middle; font-size:16px; -webkit-user-select:none; -moz-user-select:none; -ms-user-select:none; user-select:none; } .bp3-control .bp3-control-indicator::before{ display:block; width:1em; height:1em; content:\"\"; } .bp3-control:hover .bp3-control-indicator{ background-color:#ebf1f5; } .bp3-control input:not(:disabled):active ~ .bp3-control-indicator{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2); background:#d8e1e8; } .bp3-control input:disabled ~ .bp3-control-indicator{ -webkit-box-shadow:none; box-shadow:none; background:rgba(206, 217, 224, 0.5); cursor:not-allowed; } .bp3-control input:focus ~ .bp3-control-indicator{ outline:rgba(19, 124, 189, 0.6) auto 2px; outline-offset:2px; -moz-outline-radius:6px; } .bp3-control.bp3-align-right .bp3-control-indicator{ float:right; margin-top:1px; margin-left:10px; } .bp3-control.bp3-large{ font-size:16px; } .bp3-control.bp3-large:not(.bp3-align-right){ padding-left:30px; } .bp3-control.bp3-large:not(.bp3-align-right) .bp3-control-indicator{ margin-left:-30px; } .bp3-control.bp3-large.bp3-align-right{ padding-right:30px; } .bp3-control.bp3-large.bp3-align-right .bp3-control-indicator{ margin-right:-30px; } .bp3-control.bp3-large .bp3-control-indicator{ font-size:20px; } .bp3-control.bp3-large.bp3-align-right .bp3-control-indicator{ margin-top:0; } .bp3-control.bp3-checkbox input:indeterminate ~ .bp3-control-indicator{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); background-color:#137cbd; background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0))); background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0)); color:#ffffff; } .bp3-control.bp3-checkbox:hover input:indeterminate ~ .bp3-control-indicator{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); background-color:#106ba3; } .bp3-control.bp3-checkbox input:not(:disabled):active:indeterminate ~ .bp3-control-indicator{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); background:#0e5a8a; } .bp3-control.bp3-checkbox input:disabled:indeterminate ~ .bp3-control-indicator{ -webkit-box-shadow:none; box-shadow:none; background:rgba(19, 124, 189, 0.5); } .bp3-dark .bp3-control.bp3-checkbox input:indeterminate ~ .bp3-control-indicator{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-control.bp3-checkbox:hover input:indeterminate ~ .bp3-control-indicator{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); background-color:#106ba3; } .bp3-dark .bp3-control.bp3-checkbox input:not(:disabled):active:indeterminate ~ .bp3-control-indicator{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); background-color:#0e5a8a; } .bp3-dark .bp3-control.bp3-checkbox input:disabled:indeterminate ~ .bp3-control-indicator{ -webkit-box-shadow:none; box-shadow:none; background:rgba(14, 90, 138, 0.5); } .bp3-control.bp3-checkbox .bp3-control-indicator{ border-radius:3px; } .bp3-control.bp3-checkbox input:checked ~ .bp3-control-indicator::before{ background-image:url(\"data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16'%3e%3cpath fill-rule='evenodd' clip-rule='evenodd' d='M12 5c-.28 0-.53.11-.71.29L7 9.59l-2.29-2.3a1.003 1.003 0 0 0-1.42 1.42l3 3c.18.18.43.29.71.29s.53-.11.71-.29l5-5A1.003 1.003 0 0 0 12 5z' fill='white'/%3e%3c/svg%3e\"); } .bp3-control.bp3-checkbox input:indeterminate ~ .bp3-control-indicator::before{ background-image:url(\"data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16'%3e%3cpath fill-rule='evenodd' clip-rule='evenodd' d='M11 7H5c-.55 0-1 .45-1 1s.45 1 1 1h6c.55 0 1-.45 1-1s-.45-1-1-1z' fill='white'/%3e%3c/svg%3e\"); } .bp3-control.bp3-radio .bp3-control-indicator{ border-radius:50%; } .bp3-control.bp3-radio input:checked ~ .bp3-control-indicator::before{ background-image:radial-gradient(#ffffff, #ffffff 28%, transparent 32%); } .bp3-control.bp3-radio input:checked:disabled ~ .bp3-control-indicator::before{ opacity:0.5; } .bp3-control.bp3-radio input:focus ~ .bp3-control-indicator{ -moz-outline-radius:16px; } .bp3-control.bp3-switch input ~ .bp3-control-indicator{ background:rgba(167, 182, 194, 0.5); } .bp3-control.bp3-switch:hover input ~ .bp3-control-indicator{ background:rgba(115, 134, 148, 0.5); } .bp3-control.bp3-switch input:not(:disabled):active ~ .bp3-control-indicator{ background:rgba(92, 112, 128, 0.5); } .bp3-control.bp3-switch input:disabled ~ .bp3-control-indicator{ background:rgba(206, 217, 224, 0.5); } .bp3-control.bp3-switch input:disabled ~ .bp3-control-indicator::before{ background:rgba(255, 255, 255, 0.8); } .bp3-control.bp3-switch input:checked ~ .bp3-control-indicator{ background:#137cbd; } .bp3-control.bp3-switch:hover input:checked ~ .bp3-control-indicator{ background:#106ba3; } .bp3-control.bp3-switch input:checked:not(:disabled):active ~ .bp3-control-indicator{ background:#0e5a8a; } .bp3-control.bp3-switch input:checked:disabled ~ .bp3-control-indicator{ background:rgba(19, 124, 189, 0.5); } .bp3-control.bp3-switch input:checked:disabled ~ .bp3-control-indicator::before{ background:rgba(255, 255, 255, 0.8); } .bp3-control.bp3-switch:not(.bp3-align-right){ padding-left:38px; } .bp3-control.bp3-switch:not(.bp3-align-right) .bp3-control-indicator{ margin-left:-38px; } .bp3-control.bp3-switch.bp3-align-right{ padding-right:38px; } .bp3-control.bp3-switch.bp3-align-right .bp3-control-indicator{ margin-right:-38px; } .bp3-control.bp3-switch .bp3-control-indicator{ border:none; border-radius:1.75em; -webkit-box-shadow:none !important; box-shadow:none !important; width:auto; min-width:1.75em; -webkit-transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9); transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9); } .bp3-control.bp3-switch .bp3-control-indicator::before{ position:absolute; left:0; margin:2px; border-radius:50%; -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 1px 1px rgba(16, 22, 26, 0.2); background:#ffffff; width:calc(1em - 4px); height:calc(1em - 4px); -webkit-transition:left 100ms cubic-bezier(0.4, 1, 0.75, 0.9); transition:left 100ms cubic-bezier(0.4, 1, 0.75, 0.9); } .bp3-control.bp3-switch input:checked ~ .bp3-control-indicator::before{ left:calc(100% - 1em); } .bp3-control.bp3-switch.bp3-large:not(.bp3-align-right){ padding-left:45px; } .bp3-control.bp3-switch.bp3-large:not(.bp3-align-right) .bp3-control-indicator{ margin-left:-45px; } .bp3-control.bp3-switch.bp3-large.bp3-align-right{ padding-right:45px; } .bp3-control.bp3-switch.bp3-large.bp3-align-right .bp3-control-indicator{ margin-right:-45px; } .bp3-dark .bp3-control.bp3-switch input ~ .bp3-control-indicator{ background:rgba(16, 22, 26, 0.5); } .bp3-dark .bp3-control.bp3-switch:hover input ~ .bp3-control-indicator{ background:rgba(16, 22, 26, 0.7); } .bp3-dark .bp3-control.bp3-switch input:not(:disabled):active ~ .bp3-control-indicator{ background:rgba(16, 22, 26, 0.9); } .bp3-dark .bp3-control.bp3-switch input:disabled ~ .bp3-control-indicator{ background:rgba(57, 75, 89, 0.5); } .bp3-dark .bp3-control.bp3-switch input:disabled ~ .bp3-control-indicator::before{ background:rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-control.bp3-switch input:checked ~ .bp3-control-indicator{ background:#137cbd; } .bp3-dark .bp3-control.bp3-switch:hover input:checked ~ .bp3-control-indicator{ background:#106ba3; } .bp3-dark .bp3-control.bp3-switch input:checked:not(:disabled):active ~ .bp3-control-indicator{ background:#0e5a8a; } .bp3-dark .bp3-control.bp3-switch input:checked:disabled ~ .bp3-control-indicator{ background:rgba(14, 90, 138, 0.5); } .bp3-dark .bp3-control.bp3-switch input:checked:disabled ~ .bp3-control-indicator::before{ background:rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-control.bp3-switch .bp3-control-indicator::before{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); background:#394b59; } .bp3-dark .bp3-control.bp3-switch input:checked ~ .bp3-control-indicator::before{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4); } .bp3-control.bp3-switch .bp3-switch-inner-text{ text-align:center; font-size:0.7em; } .bp3-control.bp3-switch .bp3-control-indicator-child:first-child{ visibility:hidden; margin-right:1.2em; margin-left:0.5em; line-height:0; } .bp3-control.bp3-switch .bp3-control-indicator-child:last-child{ visibility:visible; margin-right:0.5em; margin-left:1.2em; line-height:1em; } .bp3-control.bp3-switch input:checked ~ .bp3-control-indicator .bp3-control-indicator-child:first-child{ visibility:visible; line-height:1em; } .bp3-control.bp3-switch input:checked ~ .bp3-control-indicator .bp3-control-indicator-child:last-child{ visibility:hidden; line-height:0; } .bp3-dark .bp3-control{ color:#f5f8fa; } .bp3-dark .bp3-control.bp3-disabled{ color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-control .bp3-control-indicator{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); background-color:#394b59; background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0))); background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0)); } .bp3-dark .bp3-control:hover .bp3-control-indicator{ background-color:#30404d; } .bp3-dark .bp3-control input:not(:disabled):active ~ .bp3-control-indicator{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2); background:#202b33; } .bp3-dark .bp3-control input:disabled ~ .bp3-control-indicator{ -webkit-box-shadow:none; box-shadow:none; background:rgba(57, 75, 89, 0.5); cursor:not-allowed; } .bp3-dark .bp3-control.bp3-checkbox input:disabled:checked ~ .bp3-control-indicator, .bp3-dark .bp3-control.bp3-checkbox input:disabled:indeterminate ~ .bp3-control-indicator{ color:rgba(167, 182, 194, 0.6); } .bp3-file-input{ display:inline-block; position:relative; cursor:pointer; height:30px; } .bp3-file-input input{ opacity:0; margin:0; min-width:200px; } .bp3-file-input input:disabled + .bp3-file-upload-input, .bp3-file-input input.bp3-disabled + .bp3-file-upload-input{ -webkit-box-shadow:none; box-shadow:none; background:rgba(206, 217, 224, 0.5); cursor:not-allowed; color:rgba(92, 112, 128, 0.6); resize:none; } .bp3-file-input input:disabled + .bp3-file-upload-input::after, .bp3-file-input input.bp3-disabled + .bp3-file-upload-input::after{ outline:none; -webkit-box-shadow:none; box-shadow:none; background-color:rgba(206, 217, 224, 0.5); background-image:none; cursor:not-allowed; color:rgba(92, 112, 128, 0.6); } .bp3-file-input input:disabled + .bp3-file-upload-input::after.bp3-active, .bp3-file-input input:disabled + .bp3-file-upload-input::after.bp3-active:hover, .bp3-file-input input.bp3-disabled + .bp3-file-upload-input::after.bp3-active, .bp3-file-input input.bp3-disabled + .bp3-file-upload-input::after.bp3-active:hover{ background:rgba(206, 217, 224, 0.7); } .bp3-dark .bp3-file-input input:disabled + .bp3-file-upload-input, .bp3-dark .bp3-file-input input.bp3-disabled + .bp3-file-upload-input{ -webkit-box-shadow:none; box-shadow:none; background:rgba(57, 75, 89, 0.5); color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-file-input input:disabled + .bp3-file-upload-input::after, .bp3-dark .bp3-file-input input.bp3-disabled + .bp3-file-upload-input::after{ -webkit-box-shadow:none; box-shadow:none; background-color:rgba(57, 75, 89, 0.5); background-image:none; color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-file-input input:disabled + .bp3-file-upload-input::after.bp3-active, .bp3-dark .bp3-file-input input.bp3-disabled + .bp3-file-upload-input::after.bp3-active{ background:rgba(57, 75, 89, 0.7); } .bp3-file-input.bp3-file-input-has-selection .bp3-file-upload-input{ color:#182026; } .bp3-dark .bp3-file-input.bp3-file-input-has-selection .bp3-file-upload-input{ color:#f5f8fa; } .bp3-file-input.bp3-fill{ width:100%; } .bp3-file-input.bp3-large, .bp3-large .bp3-file-input{ height:40px; } .bp3-file-input .bp3-file-upload-input-custom-text::after{ content:attr(bp3-button-text); } .bp3-file-upload-input{ outline:none; border:none; border-radius:3px; -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); background:#ffffff; height:30px; padding:0 10px; vertical-align:middle; line-height:30px; color:#182026; font-size:14px; font-weight:400; -webkit-transition:-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9); transition:-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9); transition:box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9); transition:box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-appearance:none; -moz-appearance:none; appearance:none; overflow:hidden; text-overflow:ellipsis; white-space:nowrap; word-wrap:normal; position:absolute; top:0; right:0; left:0; padding-right:80px; color:rgba(92, 112, 128, 0.6); -webkit-user-select:none; -moz-user-select:none; -ms-user-select:none; user-select:none; } .bp3-file-upload-input::-webkit-input-placeholder{ opacity:1; color:rgba(92, 112, 128, 0.6); } .bp3-file-upload-input::-moz-placeholder{ opacity:1; color:rgba(92, 112, 128, 0.6); } .bp3-file-upload-input:-ms-input-placeholder{ opacity:1; color:rgba(92, 112, 128, 0.6); } .bp3-file-upload-input::-ms-input-placeholder{ opacity:1; color:rgba(92, 112, 128, 0.6); } .bp3-file-upload-input::placeholder{ opacity:1; color:rgba(92, 112, 128, 0.6); } .bp3-file-upload-input:focus, .bp3-file-upload-input.bp3-active{ -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-file-upload-input[type=\"search\"], .bp3-file-upload-input.bp3-round{ border-radius:30px; -webkit-box-sizing:border-box; box-sizing:border-box; padding-left:10px; } .bp3-file-upload-input[readonly]{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.15); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.15); } .bp3-file-upload-input:disabled, .bp3-file-upload-input.bp3-disabled{ -webkit-box-shadow:none; box-shadow:none; background:rgba(206, 217, 224, 0.5); cursor:not-allowed; color:rgba(92, 112, 128, 0.6); resize:none; } .bp3-file-upload-input::after{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); background-color:#f5f8fa; background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0))); background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0)); color:#182026; min-width:24px; min-height:24px; overflow:hidden; text-overflow:ellipsis; white-space:nowrap; word-wrap:normal; position:absolute; top:0; right:0; margin:3px; border-radius:3px; width:70px; text-align:center; line-height:24px; content:\"Browse\"; } .bp3-file-upload-input::after:hover{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); background-clip:padding-box; background-color:#ebf1f5; } .bp3-file-upload-input::after:active, .bp3-file-upload-input::after.bp3-active{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2); background-color:#d8e1e8; background-image:none; } .bp3-file-upload-input::after:disabled, .bp3-file-upload-input::after.bp3-disabled{ outline:none; -webkit-box-shadow:none; box-shadow:none; background-color:rgba(206, 217, 224, 0.5); background-image:none; cursor:not-allowed; color:rgba(92, 112, 128, 0.6); } .bp3-file-upload-input::after:disabled.bp3-active, .bp3-file-upload-input::after:disabled.bp3-active:hover, .bp3-file-upload-input::after.bp3-disabled.bp3-active, .bp3-file-upload-input::after.bp3-disabled.bp3-active:hover{ background:rgba(206, 217, 224, 0.7); } .bp3-file-upload-input:hover::after{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); background-clip:padding-box; background-color:#ebf1f5; } .bp3-file-upload-input:active::after{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2); background-color:#d8e1e8; background-image:none; } .bp3-large .bp3-file-upload-input{ height:40px; line-height:40px; font-size:16px; padding-right:95px; } .bp3-large .bp3-file-upload-input[type=\"search\"], .bp3-large .bp3-file-upload-input.bp3-round{ padding:0 15px; } .bp3-large .bp3-file-upload-input::after{ min-width:30px; min-height:30px; margin:5px; width:85px; line-height:30px; } .bp3-dark .bp3-file-upload-input{ -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); background:rgba(16, 22, 26, 0.3); color:#f5f8fa; color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-file-upload-input::-webkit-input-placeholder{ color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-file-upload-input::-moz-placeholder{ color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-file-upload-input:-ms-input-placeholder{ color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-file-upload-input::-ms-input-placeholder{ color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-file-upload-input::placeholder{ color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-file-upload-input:focus{ -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-file-upload-input[readonly]{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-file-upload-input:disabled, .bp3-dark .bp3-file-upload-input.bp3-disabled{ -webkit-box-shadow:none; box-shadow:none; background:rgba(57, 75, 89, 0.5); color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-file-upload-input::after{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); background-color:#394b59; background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0))); background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0)); color:#f5f8fa; } .bp3-dark .bp3-file-upload-input::after:hover, .bp3-dark .bp3-file-upload-input::after:active, .bp3-dark .bp3-file-upload-input::after.bp3-active{ color:#f5f8fa; } .bp3-dark .bp3-file-upload-input::after:hover{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); background-color:#30404d; } .bp3-dark .bp3-file-upload-input::after:active, .bp3-dark .bp3-file-upload-input::after.bp3-active{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2); background-color:#202b33; background-image:none; } .bp3-dark .bp3-file-upload-input::after:disabled, .bp3-dark .bp3-file-upload-input::after.bp3-disabled{ -webkit-box-shadow:none; box-shadow:none; background-color:rgba(57, 75, 89, 0.5); background-image:none; color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-file-upload-input::after:disabled.bp3-active, .bp3-dark .bp3-file-upload-input::after.bp3-disabled.bp3-active{ background:rgba(57, 75, 89, 0.7); } .bp3-dark .bp3-file-upload-input::after .bp3-button-spinner .bp3-spinner-head{ background:rgba(16, 22, 26, 0.5); stroke:#8a9ba8; } .bp3-dark .bp3-file-upload-input:hover::after{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); background-color:#30404d; } .bp3-dark .bp3-file-upload-input:active::after{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2); background-color:#202b33; background-image:none; } .bp3-file-upload-input::after{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); } .bp3-form-group{ display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-orient:vertical; -webkit-box-direction:normal; -ms-flex-direction:column; flex-direction:column; margin:0 0 15px; } .bp3-form-group label.bp3-label{ margin-bottom:5px; } .bp3-form-group .bp3-control{ margin-top:7px; } .bp3-form-group .bp3-form-helper-text{ margin-top:5px; color:#5c7080; font-size:12px; } .bp3-form-group.bp3-intent-primary .bp3-form-helper-text{ color:#106ba3; } .bp3-form-group.bp3-intent-success .bp3-form-helper-text{ color:#0d8050; } .bp3-form-group.bp3-intent-warning .bp3-form-helper-text{ color:#bf7326; } .bp3-form-group.bp3-intent-danger .bp3-form-helper-text{ color:#c23030; } .bp3-form-group.bp3-inline{ -webkit-box-orient:horizontal; -webkit-box-direction:normal; -ms-flex-direction:row; flex-direction:row; -webkit-box-align:start; -ms-flex-align:start; align-items:flex-start; } .bp3-form-group.bp3-inline.bp3-large label.bp3-label{ margin:0 10px 0 0; line-height:40px; } .bp3-form-group.bp3-inline label.bp3-label{ margin:0 10px 0 0; line-height:30px; } .bp3-form-group.bp3-disabled .bp3-label, .bp3-form-group.bp3-disabled .bp3-text-muted, .bp3-form-group.bp3-disabled .bp3-form-helper-text{ color:rgba(92, 112, 128, 0.6) !important; } .bp3-dark .bp3-form-group.bp3-intent-primary .bp3-form-helper-text{ color:#48aff0; } .bp3-dark .bp3-form-group.bp3-intent-success .bp3-form-helper-text{ color:#3dcc91; } .bp3-dark .bp3-form-group.bp3-intent-warning .bp3-form-helper-text{ color:#ffb366; } .bp3-dark .bp3-form-group.bp3-intent-danger .bp3-form-helper-text{ color:#ff7373; } .bp3-dark .bp3-form-group .bp3-form-helper-text{ color:#a7b6c2; } .bp3-dark .bp3-form-group.bp3-disabled .bp3-label, .bp3-dark .bp3-form-group.bp3-disabled .bp3-text-muted, .bp3-dark .bp3-form-group.bp3-disabled .bp3-form-helper-text{ color:rgba(167, 182, 194, 0.6) !important; } .bp3-input-group{ display:block; position:relative; } .bp3-input-group .bp3-input{ position:relative; width:100%; } .bp3-input-group .bp3-input:not(:first-child){ padding-left:30px; } .bp3-input-group .bp3-input:not(:last-child){ padding-right:30px; } .bp3-input-group .bp3-input-action, .bp3-input-group > .bp3-button, .bp3-input-group > .bp3-icon{ position:absolute; top:0; } .bp3-input-group .bp3-input-action:first-child, .bp3-input-group > .bp3-button:first-child, .bp3-input-group > .bp3-icon:first-child{ left:0; } .bp3-input-group .bp3-input-action:last-child, .bp3-input-group > .bp3-button:last-child, .bp3-input-group > .bp3-icon:last-child{ right:0; } .bp3-input-group .bp3-button{ min-width:24px; min-height:24px; margin:3px; padding:0 7px; } .bp3-input-group .bp3-button:empty{ padding:0; } .bp3-input-group > .bp3-icon{ z-index:1; color:#5c7080; } .bp3-input-group > .bp3-icon:empty{ line-height:1; font-family:\"Icons16\", sans-serif; font-size:16px; font-weight:400; font-style:normal; -moz-osx-font-smoothing:grayscale; -webkit-font-smoothing:antialiased; } .bp3-input-group > .bp3-icon, .bp3-input-group .bp3-input-action > .bp3-spinner{ margin:7px; } .bp3-input-group .bp3-tag{ margin:5px; } .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:not(:hover):not(:focus), .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus){ color:#5c7080; } .bp3-dark .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:not(:hover):not(:focus), .bp3-dark .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus){ color:#a7b6c2; } .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon, .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon-standard, .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon-large, .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon, .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon-standard, .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon-large{ color:#5c7080; } .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:disabled, .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:disabled{ color:rgba(92, 112, 128, 0.6) !important; } .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:disabled .bp3-icon, .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:disabled .bp3-icon-standard, .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:disabled .bp3-icon-large, .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:disabled .bp3-icon, .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:disabled .bp3-icon-standard, .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:disabled .bp3-icon-large{ color:rgba(92, 112, 128, 0.6) !important; } .bp3-input-group.bp3-disabled{ cursor:not-allowed; } .bp3-input-group.bp3-disabled .bp3-icon{ color:rgba(92, 112, 128, 0.6); } .bp3-input-group.bp3-large .bp3-button{ min-width:30px; min-height:30px; margin:5px; } .bp3-input-group.bp3-large > .bp3-icon, .bp3-input-group.bp3-large .bp3-input-action > .bp3-spinner{ margin:12px; } .bp3-input-group.bp3-large .bp3-input{ height:40px; line-height:40px; font-size:16px; } .bp3-input-group.bp3-large .bp3-input[type=\"search\"], .bp3-input-group.bp3-large .bp3-input.bp3-round{ padding:0 15px; } .bp3-input-group.bp3-large .bp3-input:not(:first-child){ padding-left:40px; } .bp3-input-group.bp3-large .bp3-input:not(:last-child){ padding-right:40px; } .bp3-input-group.bp3-small .bp3-button{ min-width:20px; min-height:20px; margin:2px; } .bp3-input-group.bp3-small .bp3-tag{ min-width:20px; min-height:20px; margin:2px; } .bp3-input-group.bp3-small > .bp3-icon, .bp3-input-group.bp3-small .bp3-input-action > .bp3-spinner{ margin:4px; } .bp3-input-group.bp3-small .bp3-input{ height:24px; padding-right:8px; padding-left:8px; line-height:24px; font-size:12px; } .bp3-input-group.bp3-small .bp3-input[type=\"search\"], .bp3-input-group.bp3-small .bp3-input.bp3-round{ padding:0 12px; } .bp3-input-group.bp3-small .bp3-input:not(:first-child){ padding-left:24px; } .bp3-input-group.bp3-small .bp3-input:not(:last-child){ padding-right:24px; } .bp3-input-group.bp3-fill{ -webkit-box-flex:1; -ms-flex:1 1 auto; flex:1 1 auto; width:100%; } .bp3-input-group.bp3-round .bp3-button, .bp3-input-group.bp3-round .bp3-input, .bp3-input-group.bp3-round .bp3-tag{ border-radius:30px; } .bp3-dark .bp3-input-group .bp3-icon{ color:#a7b6c2; } .bp3-dark .bp3-input-group.bp3-disabled .bp3-icon{ color:rgba(167, 182, 194, 0.6); } .bp3-input-group.bp3-intent-primary .bp3-input{ -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px #137cbd, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px #137cbd, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-input-group.bp3-intent-primary .bp3-input:focus{ -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-input-group.bp3-intent-primary .bp3-input[readonly]{ -webkit-box-shadow:inset 0 0 0 1px #137cbd; box-shadow:inset 0 0 0 1px #137cbd; } .bp3-input-group.bp3-intent-primary .bp3-input:disabled, .bp3-input-group.bp3-intent-primary .bp3-input.bp3-disabled{ -webkit-box-shadow:none; box-shadow:none; } .bp3-input-group.bp3-intent-primary > .bp3-icon{ color:#106ba3; } .bp3-dark .bp3-input-group.bp3-intent-primary > .bp3-icon{ color:#48aff0; } .bp3-input-group.bp3-intent-success .bp3-input{ -webkit-box-shadow:0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), inset 0 0 0 1px #0f9960, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), inset 0 0 0 1px #0f9960, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-input-group.bp3-intent-success .bp3-input:focus{ -webkit-box-shadow:0 0 0 1px #0f9960, 0 0 0 3px rgba(15, 153, 96, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px #0f9960, 0 0 0 3px rgba(15, 153, 96, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-input-group.bp3-intent-success .bp3-input[readonly]{ -webkit-box-shadow:inset 0 0 0 1px #0f9960; box-shadow:inset 0 0 0 1px #0f9960; } .bp3-input-group.bp3-intent-success .bp3-input:disabled, .bp3-input-group.bp3-intent-success .bp3-input.bp3-disabled{ -webkit-box-shadow:none; box-shadow:none; } .bp3-input-group.bp3-intent-success > .bp3-icon{ color:#0d8050; } .bp3-dark .bp3-input-group.bp3-intent-success > .bp3-icon{ color:#3dcc91; } .bp3-input-group.bp3-intent-warning .bp3-input{ -webkit-box-shadow:0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), inset 0 0 0 1px #d9822b, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), inset 0 0 0 1px #d9822b, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-input-group.bp3-intent-warning .bp3-input:focus{ -webkit-box-shadow:0 0 0 1px #d9822b, 0 0 0 3px rgba(217, 130, 43, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px #d9822b, 0 0 0 3px rgba(217, 130, 43, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-input-group.bp3-intent-warning .bp3-input[readonly]{ -webkit-box-shadow:inset 0 0 0 1px #d9822b; box-shadow:inset 0 0 0 1px #d9822b; } .bp3-input-group.bp3-intent-warning .bp3-input:disabled, .bp3-input-group.bp3-intent-warning .bp3-input.bp3-disabled{ -webkit-box-shadow:none; box-shadow:none; } .bp3-input-group.bp3-intent-warning > .bp3-icon{ color:#bf7326; } .bp3-dark .bp3-input-group.bp3-intent-warning > .bp3-icon{ color:#ffb366; } .bp3-input-group.bp3-intent-danger .bp3-input{ -webkit-box-shadow:0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), inset 0 0 0 1px #db3737, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), inset 0 0 0 1px #db3737, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-input-group.bp3-intent-danger .bp3-input:focus{ -webkit-box-shadow:0 0 0 1px #db3737, 0 0 0 3px rgba(219, 55, 55, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px #db3737, 0 0 0 3px rgba(219, 55, 55, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-input-group.bp3-intent-danger .bp3-input[readonly]{ -webkit-box-shadow:inset 0 0 0 1px #db3737; box-shadow:inset 0 0 0 1px #db3737; } .bp3-input-group.bp3-intent-danger .bp3-input:disabled, .bp3-input-group.bp3-intent-danger .bp3-input.bp3-disabled{ -webkit-box-shadow:none; box-shadow:none; } .bp3-input-group.bp3-intent-danger > .bp3-icon{ color:#c23030; } .bp3-dark .bp3-input-group.bp3-intent-danger > .bp3-icon{ color:#ff7373; } .bp3-input{ outline:none; border:none; border-radius:3px; -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); background:#ffffff; height:30px; padding:0 10px; vertical-align:middle; line-height:30px; color:#182026; font-size:14px; font-weight:400; -webkit-transition:-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9); transition:-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9); transition:box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9); transition:box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-appearance:none; -moz-appearance:none; appearance:none; } .bp3-input::-webkit-input-placeholder{ opacity:1; color:rgba(92, 112, 128, 0.6); } .bp3-input::-moz-placeholder{ opacity:1; color:rgba(92, 112, 128, 0.6); } .bp3-input:-ms-input-placeholder{ opacity:1; color:rgba(92, 112, 128, 0.6); } .bp3-input::-ms-input-placeholder{ opacity:1; color:rgba(92, 112, 128, 0.6); } .bp3-input::placeholder{ opacity:1; color:rgba(92, 112, 128, 0.6); } .bp3-input:focus, .bp3-input.bp3-active{ -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-input[type=\"search\"], .bp3-input.bp3-round{ border-radius:30px; -webkit-box-sizing:border-box; box-sizing:border-box; padding-left:10px; } .bp3-input[readonly]{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.15); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.15); } .bp3-input:disabled, .bp3-input.bp3-disabled{ -webkit-box-shadow:none; box-shadow:none; background:rgba(206, 217, 224, 0.5); cursor:not-allowed; color:rgba(92, 112, 128, 0.6); resize:none; } .bp3-input.bp3-large{ height:40px; line-height:40px; font-size:16px; } .bp3-input.bp3-large[type=\"search\"], .bp3-input.bp3-large.bp3-round{ padding:0 15px; } .bp3-input.bp3-small{ height:24px; padding-right:8px; padding-left:8px; line-height:24px; font-size:12px; } .bp3-input.bp3-small[type=\"search\"], .bp3-input.bp3-small.bp3-round{ padding:0 12px; } .bp3-input.bp3-fill{ -webkit-box-flex:1; -ms-flex:1 1 auto; flex:1 1 auto; width:100%; } .bp3-dark .bp3-input{ -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); background:rgba(16, 22, 26, 0.3); color:#f5f8fa; } .bp3-dark .bp3-input::-webkit-input-placeholder{ color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-input::-moz-placeholder{ color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-input:-ms-input-placeholder{ color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-input::-ms-input-placeholder{ color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-input::placeholder{ color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-input:focus{ -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-input[readonly]{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-input:disabled, .bp3-dark .bp3-input.bp3-disabled{ -webkit-box-shadow:none; box-shadow:none; background:rgba(57, 75, 89, 0.5); color:rgba(167, 182, 194, 0.6); } .bp3-input.bp3-intent-primary{ -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px #137cbd, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px #137cbd, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-input.bp3-intent-primary:focus{ -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-input.bp3-intent-primary[readonly]{ -webkit-box-shadow:inset 0 0 0 1px #137cbd; box-shadow:inset 0 0 0 1px #137cbd; } .bp3-input.bp3-intent-primary:disabled, .bp3-input.bp3-intent-primary.bp3-disabled{ -webkit-box-shadow:none; box-shadow:none; } .bp3-dark .bp3-input.bp3-intent-primary{ -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px #137cbd, inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px #137cbd, inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-input.bp3-intent-primary:focus{ -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-input.bp3-intent-primary[readonly]{ -webkit-box-shadow:inset 0 0 0 1px #137cbd; box-shadow:inset 0 0 0 1px #137cbd; } .bp3-dark .bp3-input.bp3-intent-primary:disabled, .bp3-dark .bp3-input.bp3-intent-primary.bp3-disabled{ -webkit-box-shadow:none; box-shadow:none; } .bp3-input.bp3-intent-success{ -webkit-box-shadow:0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), inset 0 0 0 1px #0f9960, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), inset 0 0 0 1px #0f9960, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-input.bp3-intent-success:focus{ -webkit-box-shadow:0 0 0 1px #0f9960, 0 0 0 3px rgba(15, 153, 96, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px #0f9960, 0 0 0 3px rgba(15, 153, 96, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-input.bp3-intent-success[readonly]{ -webkit-box-shadow:inset 0 0 0 1px #0f9960; box-shadow:inset 0 0 0 1px #0f9960; } .bp3-input.bp3-intent-success:disabled, .bp3-input.bp3-intent-success.bp3-disabled{ -webkit-box-shadow:none; box-shadow:none; } .bp3-dark .bp3-input.bp3-intent-success{ -webkit-box-shadow:0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), inset 0 0 0 1px #0f9960, inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), inset 0 0 0 1px #0f9960, inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-input.bp3-intent-success:focus{ -webkit-box-shadow:0 0 0 1px #0f9960, 0 0 0 1px #0f9960, 0 0 0 3px rgba(15, 153, 96, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px #0f9960, 0 0 0 1px #0f9960, 0 0 0 3px rgba(15, 153, 96, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-input.bp3-intent-success[readonly]{ -webkit-box-shadow:inset 0 0 0 1px #0f9960; box-shadow:inset 0 0 0 1px #0f9960; } .bp3-dark .bp3-input.bp3-intent-success:disabled, .bp3-dark .bp3-input.bp3-intent-success.bp3-disabled{ -webkit-box-shadow:none; box-shadow:none; } .bp3-input.bp3-intent-warning{ -webkit-box-shadow:0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), inset 0 0 0 1px #d9822b, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), inset 0 0 0 1px #d9822b, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-input.bp3-intent-warning:focus{ -webkit-box-shadow:0 0 0 1px #d9822b, 0 0 0 3px rgba(217, 130, 43, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px #d9822b, 0 0 0 3px rgba(217, 130, 43, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-input.bp3-intent-warning[readonly]{ -webkit-box-shadow:inset 0 0 0 1px #d9822b; box-shadow:inset 0 0 0 1px #d9822b; } .bp3-input.bp3-intent-warning:disabled, .bp3-input.bp3-intent-warning.bp3-disabled{ -webkit-box-shadow:none; box-shadow:none; } .bp3-dark .bp3-input.bp3-intent-warning{ -webkit-box-shadow:0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), inset 0 0 0 1px #d9822b, inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), inset 0 0 0 1px #d9822b, inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-input.bp3-intent-warning:focus{ -webkit-box-shadow:0 0 0 1px #d9822b, 0 0 0 1px #d9822b, 0 0 0 3px rgba(217, 130, 43, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px #d9822b, 0 0 0 1px #d9822b, 0 0 0 3px rgba(217, 130, 43, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-input.bp3-intent-warning[readonly]{ -webkit-box-shadow:inset 0 0 0 1px #d9822b; box-shadow:inset 0 0 0 1px #d9822b; } .bp3-dark .bp3-input.bp3-intent-warning:disabled, .bp3-dark .bp3-input.bp3-intent-warning.bp3-disabled{ -webkit-box-shadow:none; box-shadow:none; } .bp3-input.bp3-intent-danger{ -webkit-box-shadow:0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), inset 0 0 0 1px #db3737, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), inset 0 0 0 1px #db3737, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-input.bp3-intent-danger:focus{ -webkit-box-shadow:0 0 0 1px #db3737, 0 0 0 3px rgba(219, 55, 55, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px #db3737, 0 0 0 3px rgba(219, 55, 55, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-input.bp3-intent-danger[readonly]{ -webkit-box-shadow:inset 0 0 0 1px #db3737; box-shadow:inset 0 0 0 1px #db3737; } .bp3-input.bp3-intent-danger:disabled, .bp3-input.bp3-intent-danger.bp3-disabled{ -webkit-box-shadow:none; box-shadow:none; } .bp3-dark .bp3-input.bp3-intent-danger{ -webkit-box-shadow:0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), inset 0 0 0 1px #db3737, inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), inset 0 0 0 1px #db3737, inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-input.bp3-intent-danger:focus{ -webkit-box-shadow:0 0 0 1px #db3737, 0 0 0 1px #db3737, 0 0 0 3px rgba(219, 55, 55, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px #db3737, 0 0 0 1px #db3737, 0 0 0 3px rgba(219, 55, 55, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-input.bp3-intent-danger[readonly]{ -webkit-box-shadow:inset 0 0 0 1px #db3737; box-shadow:inset 0 0 0 1px #db3737; } .bp3-dark .bp3-input.bp3-intent-danger:disabled, .bp3-dark .bp3-input.bp3-intent-danger.bp3-disabled{ -webkit-box-shadow:none; box-shadow:none; } .bp3-input::-ms-clear{ display:none; } textarea.bp3-input{ max-width:100%; padding:10px; } textarea.bp3-input, textarea.bp3-input.bp3-large, textarea.bp3-input.bp3-small{ height:auto; line-height:inherit; } textarea.bp3-input.bp3-small{ padding:8px; } .bp3-dark textarea.bp3-input{ -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); background:rgba(16, 22, 26, 0.3); color:#f5f8fa; } .bp3-dark textarea.bp3-input::-webkit-input-placeholder{ color:rgba(167, 182, 194, 0.6); } .bp3-dark textarea.bp3-input::-moz-placeholder{ color:rgba(167, 182, 194, 0.6); } .bp3-dark textarea.bp3-input:-ms-input-placeholder{ color:rgba(167, 182, 194, 0.6); } .bp3-dark textarea.bp3-input::-ms-input-placeholder{ color:rgba(167, 182, 194, 0.6); } .bp3-dark textarea.bp3-input::placeholder{ color:rgba(167, 182, 194, 0.6); } .bp3-dark textarea.bp3-input:focus{ -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); } .bp3-dark textarea.bp3-input[readonly]{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4); } .bp3-dark textarea.bp3-input:disabled, .bp3-dark textarea.bp3-input.bp3-disabled{ -webkit-box-shadow:none; box-shadow:none; background:rgba(57, 75, 89, 0.5); color:rgba(167, 182, 194, 0.6); } label.bp3-label{ display:block; margin-top:0; margin-bottom:15px; } label.bp3-label .bp3-html-select, label.bp3-label .bp3-input, label.bp3-label .bp3-select, label.bp3-label .bp3-slider, label.bp3-label .bp3-popover-wrapper{ display:block; margin-top:5px; text-transform:none; } label.bp3-label .bp3-button-group{ margin-top:5px; } label.bp3-label .bp3-select select, label.bp3-label .bp3-html-select select{ width:100%; vertical-align:top; font-weight:400; } label.bp3-label.bp3-disabled, label.bp3-label.bp3-disabled .bp3-text-muted{ color:rgba(92, 112, 128, 0.6); } label.bp3-label.bp3-inline{ line-height:30px; } label.bp3-label.bp3-inline .bp3-html-select, label.bp3-label.bp3-inline .bp3-input, label.bp3-label.bp3-inline .bp3-input-group, label.bp3-label.bp3-inline .bp3-select, label.bp3-label.bp3-inline .bp3-popover-wrapper{ display:inline-block; margin:0 0 0 5px; vertical-align:top; } label.bp3-label.bp3-inline .bp3-button-group{ margin:0 0 0 5px; } label.bp3-label.bp3-inline .bp3-input-group .bp3-input{ margin-left:0; } label.bp3-label.bp3-inline.bp3-large{ line-height:40px; } label.bp3-label:not(.bp3-inline) .bp3-popover-target{ display:block; } .bp3-dark label.bp3-label{ color:#f5f8fa; } .bp3-dark label.bp3-label.bp3-disabled, .bp3-dark label.bp3-label.bp3-disabled .bp3-text-muted{ color:rgba(167, 182, 194, 0.6); } .bp3-numeric-input .bp3-button-group.bp3-vertical > .bp3-button{ -webkit-box-flex:1; -ms-flex:1 1 14px; flex:1 1 14px; width:30px; min-height:0; padding:0; } .bp3-numeric-input .bp3-button-group.bp3-vertical > .bp3-button:first-child{ border-radius:0 3px 0 0; } .bp3-numeric-input .bp3-button-group.bp3-vertical > .bp3-button:last-child{ border-radius:0 0 3px 0; } .bp3-numeric-input .bp3-button-group.bp3-vertical:first-child > .bp3-button:first-child{ border-radius:3px 0 0 0; } .bp3-numeric-input .bp3-button-group.bp3-vertical:first-child > .bp3-button:last-child{ border-radius:0 0 0 3px; } .bp3-numeric-input.bp3-large .bp3-button-group.bp3-vertical > .bp3-button{ width:40px; } form{ display:block; } .bp3-html-select select, .bp3-select select{ display:-webkit-inline-box; display:-ms-inline-flexbox; display:inline-flex; -webkit-box-orient:horizontal; -webkit-box-direction:normal; -ms-flex-direction:row; flex-direction:row; -webkit-box-align:center; -ms-flex-align:center; align-items:center; -webkit-box-pack:center; -ms-flex-pack:center; justify-content:center; border:none; border-radius:3px; cursor:pointer; padding:5px 10px; vertical-align:middle; text-align:left; font-size:14px; -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); background-color:#f5f8fa; background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0))); background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0)); color:#182026; border-radius:3px; width:100%; height:30px; padding:0 25px 0 10px; -moz-appearance:none; -webkit-appearance:none; } .bp3-html-select select > *, .bp3-select select > *{ -webkit-box-flex:0; -ms-flex-positive:0; flex-grow:0; -ms-flex-negative:0; flex-shrink:0; } .bp3-html-select select > .bp3-fill, .bp3-select select > .bp3-fill{ -webkit-box-flex:1; -ms-flex-positive:1; flex-grow:1; -ms-flex-negative:1; flex-shrink:1; } .bp3-html-select select::before, .bp3-select select::before, .bp3-html-select select > *, .bp3-select select > *{ margin-right:7px; } .bp3-html-select select:empty::before, .bp3-select select:empty::before, .bp3-html-select select > :last-child, .bp3-select select > :last-child{ margin-right:0; } .bp3-html-select select:hover, .bp3-select select:hover{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); background-clip:padding-box; background-color:#ebf1f5; } .bp3-html-select select:active, .bp3-select select:active, .bp3-html-select select.bp3-active, .bp3-select select.bp3-active{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2); background-color:#d8e1e8; background-image:none; } .bp3-html-select select:disabled, .bp3-select select:disabled, .bp3-html-select select.bp3-disabled, .bp3-select select.bp3-disabled{ outline:none; -webkit-box-shadow:none; box-shadow:none; background-color:rgba(206, 217, 224, 0.5); background-image:none; cursor:not-allowed; color:rgba(92, 112, 128, 0.6); } .bp3-html-select select:disabled.bp3-active, .bp3-select select:disabled.bp3-active, .bp3-html-select select:disabled.bp3-active:hover, .bp3-select select:disabled.bp3-active:hover, .bp3-html-select select.bp3-disabled.bp3-active, .bp3-select select.bp3-disabled.bp3-active, .bp3-html-select select.bp3-disabled.bp3-active:hover, .bp3-select select.bp3-disabled.bp3-active:hover{ background:rgba(206, 217, 224, 0.7); } .bp3-html-select.bp3-minimal select, .bp3-select.bp3-minimal select{ -webkit-box-shadow:none; box-shadow:none; background:none; } .bp3-html-select.bp3-minimal select:hover, .bp3-select.bp3-minimal select:hover{ -webkit-box-shadow:none; box-shadow:none; background:rgba(167, 182, 194, 0.3); text-decoration:none; color:#182026; } .bp3-html-select.bp3-minimal select:active, .bp3-select.bp3-minimal select:active, .bp3-html-select.bp3-minimal select.bp3-active, .bp3-select.bp3-minimal select.bp3-active{ -webkit-box-shadow:none; box-shadow:none; background:rgba(115, 134, 148, 0.3); color:#182026; } .bp3-html-select.bp3-minimal select:disabled, .bp3-select.bp3-minimal select:disabled, .bp3-html-select.bp3-minimal select:disabled:hover, .bp3-select.bp3-minimal select:disabled:hover, .bp3-html-select.bp3-minimal select.bp3-disabled, .bp3-select.bp3-minimal select.bp3-disabled, .bp3-html-select.bp3-minimal select.bp3-disabled:hover, .bp3-select.bp3-minimal select.bp3-disabled:hover{ background:none; cursor:not-allowed; color:rgba(92, 112, 128, 0.6); } .bp3-html-select.bp3-minimal select:disabled.bp3-active, .bp3-select.bp3-minimal select:disabled.bp3-active, .bp3-html-select.bp3-minimal select:disabled:hover.bp3-active, .bp3-select.bp3-minimal select:disabled:hover.bp3-active, .bp3-html-select.bp3-minimal select.bp3-disabled.bp3-active, .bp3-select.bp3-minimal select.bp3-disabled.bp3-active, .bp3-html-select.bp3-minimal select.bp3-disabled:hover.bp3-active, .bp3-select.bp3-minimal select.bp3-disabled:hover.bp3-active{ background:rgba(115, 134, 148, 0.3); } .bp3-dark .bp3-html-select.bp3-minimal select, .bp3-html-select.bp3-minimal .bp3-dark select, .bp3-dark .bp3-select.bp3-minimal select, .bp3-select.bp3-minimal .bp3-dark select{ -webkit-box-shadow:none; box-shadow:none; background:none; color:inherit; } .bp3-dark .bp3-html-select.bp3-minimal select:hover, .bp3-html-select.bp3-minimal .bp3-dark select:hover, .bp3-dark .bp3-select.bp3-minimal select:hover, .bp3-select.bp3-minimal .bp3-dark select:hover, .bp3-dark .bp3-html-select.bp3-minimal select:active, .bp3-html-select.bp3-minimal .bp3-dark select:active, .bp3-dark .bp3-select.bp3-minimal select:active, .bp3-select.bp3-minimal .bp3-dark select:active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-active, .bp3-dark .bp3-select.bp3-minimal select.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-active{ -webkit-box-shadow:none; box-shadow:none; background:none; } .bp3-dark .bp3-html-select.bp3-minimal select:hover, .bp3-html-select.bp3-minimal .bp3-dark select:hover, .bp3-dark .bp3-select.bp3-minimal select:hover, .bp3-select.bp3-minimal .bp3-dark select:hover{ background:rgba(138, 155, 168, 0.15); } .bp3-dark .bp3-html-select.bp3-minimal select:active, .bp3-html-select.bp3-minimal .bp3-dark select:active, .bp3-dark .bp3-select.bp3-minimal select:active, .bp3-select.bp3-minimal .bp3-dark select:active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-active, .bp3-dark .bp3-select.bp3-minimal select.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-active{ background:rgba(138, 155, 168, 0.3); color:#f5f8fa; } .bp3-dark .bp3-html-select.bp3-minimal select:disabled, .bp3-html-select.bp3-minimal .bp3-dark select:disabled, .bp3-dark .bp3-select.bp3-minimal select:disabled, .bp3-select.bp3-minimal .bp3-dark select:disabled, .bp3-dark .bp3-html-select.bp3-minimal select:disabled:hover, .bp3-html-select.bp3-minimal .bp3-dark select:disabled:hover, .bp3-dark .bp3-select.bp3-minimal select:disabled:hover, .bp3-select.bp3-minimal .bp3-dark select:disabled:hover, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-disabled, .bp3-dark .bp3-select.bp3-minimal select.bp3-disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-disabled, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-disabled:hover, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-disabled:hover, .bp3-dark .bp3-select.bp3-minimal select.bp3-disabled:hover, .bp3-select.bp3-minimal .bp3-dark select.bp3-disabled:hover{ background:none; cursor:not-allowed; color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-html-select.bp3-minimal select:disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select:disabled.bp3-active, .bp3-dark .bp3-select.bp3-minimal select:disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select:disabled.bp3-active, .bp3-dark .bp3-html-select.bp3-minimal select:disabled:hover.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select:disabled:hover.bp3-active, .bp3-dark .bp3-select.bp3-minimal select:disabled:hover.bp3-active, .bp3-select.bp3-minimal .bp3-dark select:disabled:hover.bp3-active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-disabled.bp3-active, .bp3-dark .bp3-select.bp3-minimal select.bp3-disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-disabled.bp3-active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-disabled:hover.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-disabled:hover.bp3-active, .bp3-dark .bp3-select.bp3-minimal select.bp3-disabled:hover.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-disabled:hover.bp3-active{ background:rgba(138, 155, 168, 0.3); } .bp3-html-select.bp3-minimal select.bp3-intent-primary, .bp3-select.bp3-minimal select.bp3-intent-primary{ color:#106ba3; } .bp3-html-select.bp3-minimal select.bp3-intent-primary:hover, .bp3-select.bp3-minimal select.bp3-intent-primary:hover, .bp3-html-select.bp3-minimal select.bp3-intent-primary:active, .bp3-select.bp3-minimal select.bp3-intent-primary:active, .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-active, .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-active{ -webkit-box-shadow:none; box-shadow:none; background:none; color:#106ba3; } .bp3-html-select.bp3-minimal select.bp3-intent-primary:hover, .bp3-select.bp3-minimal select.bp3-intent-primary:hover{ background:rgba(19, 124, 189, 0.15); color:#106ba3; } .bp3-html-select.bp3-minimal select.bp3-intent-primary:active, .bp3-select.bp3-minimal select.bp3-intent-primary:active, .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-active, .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-active{ background:rgba(19, 124, 189, 0.3); color:#106ba3; } .bp3-html-select.bp3-minimal select.bp3-intent-primary:disabled, .bp3-select.bp3-minimal select.bp3-intent-primary:disabled, .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-disabled, .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-disabled{ background:none; color:rgba(16, 107, 163, 0.5); } .bp3-html-select.bp3-minimal select.bp3-intent-primary:disabled.bp3-active, .bp3-select.bp3-minimal select.bp3-intent-primary:disabled.bp3-active, .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-disabled.bp3-active, .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-disabled.bp3-active{ background:rgba(19, 124, 189, 0.3); } .bp3-html-select.bp3-minimal select.bp3-intent-primary .bp3-button-spinner .bp3-spinner-head, .bp3-select.bp3-minimal select.bp3-intent-primary .bp3-button-spinner .bp3-spinner-head{ stroke:#106ba3; } .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary{ color:#48aff0; } .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary:hover, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary:hover, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary:hover, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary:hover{ background:rgba(19, 124, 189, 0.2); color:#48aff0; } .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary:active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary:active, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary:active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary:active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-active, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-active{ background:rgba(19, 124, 189, 0.3); color:#48aff0; } .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary:disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary:disabled, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary:disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary:disabled, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-disabled, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-disabled{ background:none; color:rgba(72, 175, 240, 0.5); } .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary:disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary:disabled.bp3-active, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary:disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary:disabled.bp3-active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-disabled.bp3-active, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-disabled.bp3-active{ background:rgba(19, 124, 189, 0.3); } .bp3-html-select.bp3-minimal select.bp3-intent-success, .bp3-select.bp3-minimal select.bp3-intent-success{ color:#0d8050; } .bp3-html-select.bp3-minimal select.bp3-intent-success:hover, .bp3-select.bp3-minimal select.bp3-intent-success:hover, .bp3-html-select.bp3-minimal select.bp3-intent-success:active, .bp3-select.bp3-minimal select.bp3-intent-success:active, .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-active, .bp3-select.bp3-minimal select.bp3-intent-success.bp3-active{ -webkit-box-shadow:none; box-shadow:none; background:none; color:#0d8050; } .bp3-html-select.bp3-minimal select.bp3-intent-success:hover, .bp3-select.bp3-minimal select.bp3-intent-success:hover{ background:rgba(15, 153, 96, 0.15); color:#0d8050; } .bp3-html-select.bp3-minimal select.bp3-intent-success:active, .bp3-select.bp3-minimal select.bp3-intent-success:active, .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-active, .bp3-select.bp3-minimal select.bp3-intent-success.bp3-active{ background:rgba(15, 153, 96, 0.3); color:#0d8050; } .bp3-html-select.bp3-minimal select.bp3-intent-success:disabled, .bp3-select.bp3-minimal select.bp3-intent-success:disabled, .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-disabled, .bp3-select.bp3-minimal select.bp3-intent-success.bp3-disabled{ background:none; color:rgba(13, 128, 80, 0.5); } .bp3-html-select.bp3-minimal select.bp3-intent-success:disabled.bp3-active, .bp3-select.bp3-minimal select.bp3-intent-success:disabled.bp3-active, .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-disabled.bp3-active, .bp3-select.bp3-minimal select.bp3-intent-success.bp3-disabled.bp3-active{ background:rgba(15, 153, 96, 0.3); } .bp3-html-select.bp3-minimal select.bp3-intent-success .bp3-button-spinner .bp3-spinner-head, .bp3-select.bp3-minimal select.bp3-intent-success .bp3-button-spinner .bp3-spinner-head{ stroke:#0d8050; } .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success{ color:#3dcc91; } .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success:hover, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success:hover, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success:hover, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success:hover{ background:rgba(15, 153, 96, 0.2); color:#3dcc91; } .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success:active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success:active, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success:active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success:active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-active, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-active{ background:rgba(15, 153, 96, 0.3); color:#3dcc91; } .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success:disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success:disabled, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success:disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success:disabled, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-disabled, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success.bp3-disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-disabled{ background:none; color:rgba(61, 204, 145, 0.5); } .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success:disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success:disabled.bp3-active, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success:disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success:disabled.bp3-active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-disabled.bp3-active, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success.bp3-disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-disabled.bp3-active{ background:rgba(15, 153, 96, 0.3); } .bp3-html-select.bp3-minimal select.bp3-intent-warning, .bp3-select.bp3-minimal select.bp3-intent-warning{ color:#bf7326; } .bp3-html-select.bp3-minimal select.bp3-intent-warning:hover, .bp3-select.bp3-minimal select.bp3-intent-warning:hover, .bp3-html-select.bp3-minimal select.bp3-intent-warning:active, .bp3-select.bp3-minimal select.bp3-intent-warning:active, .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-active, .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-active{ -webkit-box-shadow:none; box-shadow:none; background:none; color:#bf7326; } .bp3-html-select.bp3-minimal select.bp3-intent-warning:hover, .bp3-select.bp3-minimal select.bp3-intent-warning:hover{ background:rgba(217, 130, 43, 0.15); color:#bf7326; } .bp3-html-select.bp3-minimal select.bp3-intent-warning:active, .bp3-select.bp3-minimal select.bp3-intent-warning:active, .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-active, .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-active{ background:rgba(217, 130, 43, 0.3); color:#bf7326; } .bp3-html-select.bp3-minimal select.bp3-intent-warning:disabled, .bp3-select.bp3-minimal select.bp3-intent-warning:disabled, .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-disabled, .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-disabled{ background:none; color:rgba(191, 115, 38, 0.5); } .bp3-html-select.bp3-minimal select.bp3-intent-warning:disabled.bp3-active, .bp3-select.bp3-minimal select.bp3-intent-warning:disabled.bp3-active, .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-disabled.bp3-active, .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-disabled.bp3-active{ background:rgba(217, 130, 43, 0.3); } .bp3-html-select.bp3-minimal select.bp3-intent-warning .bp3-button-spinner .bp3-spinner-head, .bp3-select.bp3-minimal select.bp3-intent-warning .bp3-button-spinner .bp3-spinner-head{ stroke:#bf7326; } .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning{ color:#ffb366; } .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning:hover, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning:hover, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning:hover, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning:hover{ background:rgba(217, 130, 43, 0.2); color:#ffb366; } .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning:active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning:active, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning:active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning:active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-active, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-active{ background:rgba(217, 130, 43, 0.3); color:#ffb366; } .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning:disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning:disabled, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning:disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning:disabled, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-disabled, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-disabled{ background:none; color:rgba(255, 179, 102, 0.5); } .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning:disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning:disabled.bp3-active, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning:disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning:disabled.bp3-active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-disabled.bp3-active, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-disabled.bp3-active{ background:rgba(217, 130, 43, 0.3); } .bp3-html-select.bp3-minimal select.bp3-intent-danger, .bp3-select.bp3-minimal select.bp3-intent-danger{ color:#c23030; } .bp3-html-select.bp3-minimal select.bp3-intent-danger:hover, .bp3-select.bp3-minimal select.bp3-intent-danger:hover, .bp3-html-select.bp3-minimal select.bp3-intent-danger:active, .bp3-select.bp3-minimal select.bp3-intent-danger:active, .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-active, .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-active{ -webkit-box-shadow:none; box-shadow:none; background:none; color:#c23030; } .bp3-html-select.bp3-minimal select.bp3-intent-danger:hover, .bp3-select.bp3-minimal select.bp3-intent-danger:hover{ background:rgba(219, 55, 55, 0.15); color:#c23030; } .bp3-html-select.bp3-minimal select.bp3-intent-danger:active, .bp3-select.bp3-minimal select.bp3-intent-danger:active, .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-active, .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-active{ background:rgba(219, 55, 55, 0.3); color:#c23030; } .bp3-html-select.bp3-minimal select.bp3-intent-danger:disabled, .bp3-select.bp3-minimal select.bp3-intent-danger:disabled, .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-disabled, .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-disabled{ background:none; color:rgba(194, 48, 48, 0.5); } .bp3-html-select.bp3-minimal select.bp3-intent-danger:disabled.bp3-active, .bp3-select.bp3-minimal select.bp3-intent-danger:disabled.bp3-active, .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-disabled.bp3-active, .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-disabled.bp3-active{ background:rgba(219, 55, 55, 0.3); } .bp3-html-select.bp3-minimal select.bp3-intent-danger .bp3-button-spinner .bp3-spinner-head, .bp3-select.bp3-minimal select.bp3-intent-danger .bp3-button-spinner .bp3-spinner-head{ stroke:#c23030; } .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger{ color:#ff7373; } .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger:hover, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger:hover, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger:hover, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger:hover{ background:rgba(219, 55, 55, 0.2); color:#ff7373; } .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger:active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger:active, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger:active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger:active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-active, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-active{ background:rgba(219, 55, 55, 0.3); color:#ff7373; } .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger:disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger:disabled, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger:disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger:disabled, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-disabled, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-disabled{ background:none; color:rgba(255, 115, 115, 0.5); } .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger:disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger:disabled.bp3-active, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger:disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger:disabled.bp3-active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-disabled.bp3-active, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-disabled.bp3-active{ background:rgba(219, 55, 55, 0.3); } .bp3-html-select.bp3-large select, .bp3-select.bp3-large select{ height:40px; padding-right:35px; font-size:16px; } .bp3-dark .bp3-html-select select, .bp3-dark .bp3-select select{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); background-color:#394b59; background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0))); background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0)); color:#f5f8fa; } .bp3-dark .bp3-html-select select:hover, .bp3-dark .bp3-select select:hover, .bp3-dark .bp3-html-select select:active, .bp3-dark .bp3-select select:active, .bp3-dark .bp3-html-select select.bp3-active, .bp3-dark .bp3-select select.bp3-active{ color:#f5f8fa; } .bp3-dark .bp3-html-select select:hover, .bp3-dark .bp3-select select:hover{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); background-color:#30404d; } .bp3-dark .bp3-html-select select:active, .bp3-dark .bp3-select select:active, .bp3-dark .bp3-html-select select.bp3-active, .bp3-dark .bp3-select select.bp3-active{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2); background-color:#202b33; background-image:none; } .bp3-dark .bp3-html-select select:disabled, .bp3-dark .bp3-select select:disabled, .bp3-dark .bp3-html-select select.bp3-disabled, .bp3-dark .bp3-select select.bp3-disabled{ -webkit-box-shadow:none; box-shadow:none; background-color:rgba(57, 75, 89, 0.5); background-image:none; color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-html-select select:disabled.bp3-active, .bp3-dark .bp3-select select:disabled.bp3-active, .bp3-dark .bp3-html-select select.bp3-disabled.bp3-active, .bp3-dark .bp3-select select.bp3-disabled.bp3-active{ background:rgba(57, 75, 89, 0.7); } .bp3-dark .bp3-html-select select .bp3-button-spinner .bp3-spinner-head, .bp3-dark .bp3-select select .bp3-button-spinner .bp3-spinner-head{ background:rgba(16, 22, 26, 0.5); stroke:#8a9ba8; } .bp3-html-select select:disabled, .bp3-select select:disabled{ -webkit-box-shadow:none; box-shadow:none; background-color:rgba(206, 217, 224, 0.5); cursor:not-allowed; color:rgba(92, 112, 128, 0.6); } .bp3-html-select .bp3-icon, .bp3-select .bp3-icon, .bp3-select::after{ position:absolute; top:7px; right:7px; color:#5c7080; pointer-events:none; } .bp3-html-select .bp3-disabled.bp3-icon, .bp3-select .bp3-disabled.bp3-icon, .bp3-disabled.bp3-select::after{ color:rgba(92, 112, 128, 0.6); } .bp3-html-select, .bp3-select{ display:inline-block; position:relative; vertical-align:middle; letter-spacing:normal; } .bp3-html-select select::-ms-expand, .bp3-select select::-ms-expand{ display:none; } .bp3-html-select .bp3-icon, .bp3-select .bp3-icon{ color:#5c7080; } .bp3-html-select .bp3-icon:hover, .bp3-select .bp3-icon:hover{ color:#182026; } .bp3-dark .bp3-html-select .bp3-icon, .bp3-dark .bp3-select .bp3-icon{ color:#a7b6c2; } .bp3-dark .bp3-html-select .bp3-icon:hover, .bp3-dark .bp3-select .bp3-icon:hover{ color:#f5f8fa; } .bp3-html-select.bp3-large::after, .bp3-html-select.bp3-large .bp3-icon, .bp3-select.bp3-large::after, .bp3-select.bp3-large .bp3-icon{ top:12px; right:12px; } .bp3-html-select.bp3-fill, .bp3-html-select.bp3-fill select, .bp3-select.bp3-fill, .bp3-select.bp3-fill select{ width:100%; } .bp3-dark .bp3-html-select option, .bp3-dark .bp3-select option{ background-color:#30404d; color:#f5f8fa; } .bp3-dark .bp3-html-select::after, .bp3-dark .bp3-select::after{ color:#a7b6c2; } .bp3-select::after{ line-height:1; font-family:\"Icons16\", sans-serif; font-size:16px; font-weight:400; font-style:normal; -moz-osx-font-smoothing:grayscale; -webkit-font-smoothing:antialiased; content:\"\ue6c6\"; } .bp3-running-text table, table.bp3-html-table{ border-spacing:0; font-size:14px; } .bp3-running-text table th, table.bp3-html-table th, .bp3-running-text table td, table.bp3-html-table td{ padding:11px; vertical-align:top; text-align:left; } .bp3-running-text table th, table.bp3-html-table th{ color:#182026; font-weight:600; } .bp3-running-text table td, table.bp3-html-table td{ color:#182026; } .bp3-running-text table tbody tr:first-child th, table.bp3-html-table tbody tr:first-child th, .bp3-running-text table tbody tr:first-child td, table.bp3-html-table tbody tr:first-child td{ -webkit-box-shadow:inset 0 1px 0 0 rgba(16, 22, 26, 0.15); box-shadow:inset 0 1px 0 0 rgba(16, 22, 26, 0.15); } .bp3-dark .bp3-running-text table th, .bp3-running-text .bp3-dark table th, .bp3-dark table.bp3-html-table th{ color:#f5f8fa; } .bp3-dark .bp3-running-text table td, .bp3-running-text .bp3-dark table td, .bp3-dark table.bp3-html-table td{ color:#f5f8fa; } .bp3-dark .bp3-running-text table tbody tr:first-child th, .bp3-running-text .bp3-dark table tbody tr:first-child th, .bp3-dark table.bp3-html-table tbody tr:first-child th, .bp3-dark .bp3-running-text table tbody tr:first-child td, .bp3-running-text .bp3-dark table tbody tr:first-child td, .bp3-dark table.bp3-html-table tbody tr:first-child td{ -webkit-box-shadow:inset 0 1px 0 0 rgba(255, 255, 255, 0.15); box-shadow:inset 0 1px 0 0 rgba(255, 255, 255, 0.15); } table.bp3-html-table.bp3-html-table-condensed th, table.bp3-html-table.bp3-html-table-condensed td, table.bp3-html-table.bp3-small th, table.bp3-html-table.bp3-small td{ padding-top:6px; padding-bottom:6px; } table.bp3-html-table.bp3-html-table-striped tbody tr:nth-child(odd) td{ background:rgba(191, 204, 214, 0.15); } table.bp3-html-table.bp3-html-table-bordered th:not(:first-child){ -webkit-box-shadow:inset 1px 0 0 0 rgba(16, 22, 26, 0.15); box-shadow:inset 1px 0 0 0 rgba(16, 22, 26, 0.15); } table.bp3-html-table.bp3-html-table-bordered tbody tr td{ -webkit-box-shadow:inset 0 1px 0 0 rgba(16, 22, 26, 0.15); box-shadow:inset 0 1px 0 0 rgba(16, 22, 26, 0.15); } table.bp3-html-table.bp3-html-table-bordered tbody tr td:not(:first-child){ -webkit-box-shadow:inset 1px 1px 0 0 rgba(16, 22, 26, 0.15); box-shadow:inset 1px 1px 0 0 rgba(16, 22, 26, 0.15); } table.bp3-html-table.bp3-html-table-bordered.bp3-html-table-striped tbody tr:not(:first-child) td{ -webkit-box-shadow:none; box-shadow:none; } table.bp3-html-table.bp3-html-table-bordered.bp3-html-table-striped tbody tr:not(:first-child) td:not(:first-child){ -webkit-box-shadow:inset 1px 0 0 0 rgba(16, 22, 26, 0.15); box-shadow:inset 1px 0 0 0 rgba(16, 22, 26, 0.15); } table.bp3-html-table.bp3-interactive tbody tr:hover td{ background-color:rgba(191, 204, 214, 0.3); cursor:pointer; } table.bp3-html-table.bp3-interactive tbody tr:active td{ background-color:rgba(191, 204, 214, 0.4); } .bp3-dark table.bp3-html-table.bp3-html-table-striped tbody tr:nth-child(odd) td{ background:rgba(92, 112, 128, 0.15); } .bp3-dark table.bp3-html-table.bp3-html-table-bordered th:not(:first-child){ -webkit-box-shadow:inset 1px 0 0 0 rgba(255, 255, 255, 0.15); box-shadow:inset 1px 0 0 0 rgba(255, 255, 255, 0.15); } .bp3-dark table.bp3-html-table.bp3-html-table-bordered tbody tr td{ -webkit-box-shadow:inset 0 1px 0 0 rgba(255, 255, 255, 0.15); box-shadow:inset 0 1px 0 0 rgba(255, 255, 255, 0.15); } .bp3-dark table.bp3-html-table.bp3-html-table-bordered tbody tr td:not(:first-child){ -webkit-box-shadow:inset 1px 1px 0 0 rgba(255, 255, 255, 0.15); box-shadow:inset 1px 1px 0 0 rgba(255, 255, 255, 0.15); } .bp3-dark table.bp3-html-table.bp3-html-table-bordered.bp3-html-table-striped tbody tr:not(:first-child) td{ -webkit-box-shadow:inset 1px 0 0 0 rgba(255, 255, 255, 0.15); box-shadow:inset 1px 0 0 0 rgba(255, 255, 255, 0.15); } .bp3-dark table.bp3-html-table.bp3-html-table-bordered.bp3-html-table-striped tbody tr:not(:first-child) td:first-child{ -webkit-box-shadow:none; box-shadow:none; } .bp3-dark table.bp3-html-table.bp3-interactive tbody tr:hover td{ background-color:rgba(92, 112, 128, 0.3); cursor:pointer; } .bp3-dark table.bp3-html-table.bp3-interactive tbody tr:active td{ background-color:rgba(92, 112, 128, 0.4); } .bp3-key-combo{ display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-orient:horizontal; -webkit-box-direction:normal; -ms-flex-direction:row; flex-direction:row; -webkit-box-align:center; -ms-flex-align:center; align-items:center; } .bp3-key-combo > *{ -webkit-box-flex:0; -ms-flex-positive:0; flex-grow:0; -ms-flex-negative:0; flex-shrink:0; } .bp3-key-combo > .bp3-fill{ -webkit-box-flex:1; -ms-flex-positive:1; flex-grow:1; -ms-flex-negative:1; flex-shrink:1; } .bp3-key-combo::before, .bp3-key-combo > *{ margin-right:5px; } .bp3-key-combo:empty::before, .bp3-key-combo > :last-child{ margin-right:0; } .bp3-hotkey-dialog{ top:40px; padding-bottom:0; } .bp3-hotkey-dialog .bp3-dialog-body{ margin:0; padding:0; } .bp3-hotkey-dialog .bp3-hotkey-label{ -webkit-box-flex:1; -ms-flex-positive:1; flex-grow:1; } .bp3-hotkey-column{ margin:auto; max-height:80vh; overflow-y:auto; padding:30px; } .bp3-hotkey-column .bp3-heading{ margin-bottom:20px; } .bp3-hotkey-column .bp3-heading:not(:first-child){ margin-top:40px; } .bp3-hotkey{ display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-align:center; -ms-flex-align:center; align-items:center; -webkit-box-pack:justify; -ms-flex-pack:justify; justify-content:space-between; margin-right:0; margin-left:0; } .bp3-hotkey:not(:last-child){ margin-bottom:10px; } .bp3-icon{ display:inline-block; -webkit-box-flex:0; -ms-flex:0 0 auto; flex:0 0 auto; vertical-align:text-bottom; } .bp3-icon:not(:empty)::before{ content:\"\" !important; content:unset !important; } .bp3-icon > svg{ display:block; } .bp3-icon > svg:not([fill]){ fill:currentColor; } .bp3-icon.bp3-intent-primary, .bp3-icon-standard.bp3-intent-primary, .bp3-icon-large.bp3-intent-primary{ color:#106ba3; } .bp3-dark .bp3-icon.bp3-intent-primary, .bp3-dark .bp3-icon-standard.bp3-intent-primary, .bp3-dark .bp3-icon-large.bp3-intent-primary{ color:#48aff0; } .bp3-icon.bp3-intent-success, .bp3-icon-standard.bp3-intent-success, .bp3-icon-large.bp3-intent-success{ color:#0d8050; } .bp3-dark .bp3-icon.bp3-intent-success, .bp3-dark .bp3-icon-standard.bp3-intent-success, .bp3-dark .bp3-icon-large.bp3-intent-success{ color:#3dcc91; } .bp3-icon.bp3-intent-warning, .bp3-icon-standard.bp3-intent-warning, .bp3-icon-large.bp3-intent-warning{ color:#bf7326; } .bp3-dark .bp3-icon.bp3-intent-warning, .bp3-dark .bp3-icon-standard.bp3-intent-warning, .bp3-dark .bp3-icon-large.bp3-intent-warning{ color:#ffb366; } .bp3-icon.bp3-intent-danger, .bp3-icon-standard.bp3-intent-danger, .bp3-icon-large.bp3-intent-danger{ color:#c23030; } .bp3-dark .bp3-icon.bp3-intent-danger, .bp3-dark .bp3-icon-standard.bp3-intent-danger, .bp3-dark .bp3-icon-large.bp3-intent-danger{ color:#ff7373; } span.bp3-icon-standard{ line-height:1; font-family:\"Icons16\", sans-serif; font-size:16px; font-weight:400; font-style:normal; -moz-osx-font-smoothing:grayscale; -webkit-font-smoothing:antialiased; display:inline-block; } span.bp3-icon-large{ line-height:1; font-family:\"Icons20\", sans-serif; font-size:20px; font-weight:400; font-style:normal; -moz-osx-font-smoothing:grayscale; -webkit-font-smoothing:antialiased; display:inline-block; } span.bp3-icon:empty{ line-height:1; font-family:\"Icons20\"; font-size:inherit; font-weight:400; font-style:normal; } span.bp3-icon:empty::before{ -moz-osx-font-smoothing:grayscale; -webkit-font-smoothing:antialiased; } .bp3-icon-add::before{ content:\"\ue63e\"; } .bp3-icon-add-column-left::before{ content:\"\ue6f9\"; } .bp3-icon-add-column-right::before{ content:\"\ue6fa\"; } .bp3-icon-add-row-bottom::before{ content:\"\ue6f8\"; } .bp3-icon-add-row-top::before{ content:\"\ue6f7\"; } .bp3-icon-add-to-artifact::before{ content:\"\ue67c\"; } .bp3-icon-add-to-folder::before{ content:\"\ue6d2\"; } .bp3-icon-airplane::before{ content:\"\ue74b\"; } .bp3-icon-align-center::before{ content:\"\ue603\"; } .bp3-icon-align-justify::before{ content:\"\ue605\"; } .bp3-icon-align-left::before{ content:\"\ue602\"; } .bp3-icon-align-right::before{ content:\"\ue604\"; } .bp3-icon-alignment-bottom::before{ content:\"\ue727\"; } .bp3-icon-alignment-horizontal-center::before{ content:\"\ue726\"; } .bp3-icon-alignment-left::before{ content:\"\ue722\"; } .bp3-icon-alignment-right::before{ content:\"\ue724\"; } .bp3-icon-alignment-top::before{ content:\"\ue725\"; } .bp3-icon-alignment-vertical-center::before{ content:\"\ue723\"; } .bp3-icon-annotation::before{ content:\"\ue6f0\"; } .bp3-icon-application::before{ content:\"\ue735\"; } .bp3-icon-applications::before{ content:\"\ue621\"; } .bp3-icon-archive::before{ content:\"\ue907\"; } .bp3-icon-arrow-bottom-left::before{ content:\"\u2199\"; } .bp3-icon-arrow-bottom-right::before{ content:\"\u2198\"; } .bp3-icon-arrow-down::before{ content:\"\u2193\"; } .bp3-icon-arrow-left::before{ content:\"\u2190\"; } .bp3-icon-arrow-right::before{ content:\"\u2192\"; } .bp3-icon-arrow-top-left::before{ content:\"\u2196\"; } .bp3-icon-arrow-top-right::before{ content:\"\u2197\"; } .bp3-icon-arrow-up::before{ content:\"\u2191\"; } .bp3-icon-arrows-horizontal::before{ content:\"\u2194\"; } .bp3-icon-arrows-vertical::before{ content:\"\u2195\"; } .bp3-icon-asterisk::before{ content:\"*\"; } .bp3-icon-automatic-updates::before{ content:\"\ue65f\"; } .bp3-icon-badge::before{ content:\"\ue6e3\"; } .bp3-icon-ban-circle::before{ content:\"\ue69d\"; } .bp3-icon-bank-account::before{ content:\"\ue76f\"; } .bp3-icon-barcode::before{ content:\"\ue676\"; } .bp3-icon-blank::before{ content:\"\ue900\"; } .bp3-icon-blocked-person::before{ content:\"\ue768\"; } .bp3-icon-bold::before{ content:\"\ue606\"; } .bp3-icon-book::before{ content:\"\ue6b8\"; } .bp3-icon-bookmark::before{ content:\"\ue61a\"; } .bp3-icon-box::before{ content:\"\ue6bf\"; } .bp3-icon-briefcase::before{ content:\"\ue674\"; } .bp3-icon-bring-data::before{ content:\"\ue90a\"; } .bp3-icon-build::before{ content:\"\ue72d\"; } .bp3-icon-calculator::before{ content:\"\ue70b\"; } .bp3-icon-calendar::before{ content:\"\ue62b\"; } .bp3-icon-camera::before{ content:\"\ue69e\"; } .bp3-icon-caret-down::before{ content:\"\u2304\"; } .bp3-icon-caret-left::before{ content:\"\u2329\"; } .bp3-icon-caret-right::before{ content:\"\u232a\"; } .bp3-icon-caret-up::before{ content:\"\u2303\"; } .bp3-icon-cell-tower::before{ content:\"\ue770\"; } .bp3-icon-changes::before{ content:\"\ue623\"; } .bp3-icon-chart::before{ content:\"\ue67e\"; } .bp3-icon-chat::before{ content:\"\ue689\"; } .bp3-icon-chevron-backward::before{ content:\"\ue6df\"; } .bp3-icon-chevron-down::before{ content:\"\ue697\"; } .bp3-icon-chevron-forward::before{ content:\"\ue6e0\"; } .bp3-icon-chevron-left::before{ content:\"\ue694\"; } .bp3-icon-chevron-right::before{ content:\"\ue695\"; } .bp3-icon-chevron-up::before{ content:\"\ue696\"; } .bp3-icon-circle::before{ content:\"\ue66a\"; } .bp3-icon-circle-arrow-down::before{ content:\"\ue68e\"; } .bp3-icon-circle-arrow-left::before{ content:\"\ue68c\"; } .bp3-icon-circle-arrow-right::before{ content:\"\ue68b\"; } .bp3-icon-circle-arrow-up::before{ content:\"\ue68d\"; } .bp3-icon-citation::before{ content:\"\ue61b\"; } .bp3-icon-clean::before{ content:\"\ue7c5\"; } .bp3-icon-clipboard::before{ content:\"\ue61d\"; } .bp3-icon-cloud::before{ content:\"\u2601\"; } .bp3-icon-cloud-download::before{ content:\"\ue690\"; } .bp3-icon-cloud-upload::before{ content:\"\ue691\"; } .bp3-icon-code::before{ content:\"\ue661\"; } .bp3-icon-code-block::before{ content:\"\ue6c5\"; } .bp3-icon-cog::before{ content:\"\ue645\"; } .bp3-icon-collapse-all::before{ content:\"\ue763\"; } .bp3-icon-column-layout::before{ content:\"\ue6da\"; } .bp3-icon-comment::before{ content:\"\ue68a\"; } .bp3-icon-comparison::before{ content:\"\ue637\"; } .bp3-icon-compass::before{ content:\"\ue79c\"; } .bp3-icon-compressed::before{ content:\"\ue6c0\"; } .bp3-icon-confirm::before{ content:\"\ue639\"; } .bp3-icon-console::before{ content:\"\ue79b\"; } .bp3-icon-contrast::before{ content:\"\ue6cb\"; } .bp3-icon-control::before{ content:\"\ue67f\"; } .bp3-icon-credit-card::before{ content:\"\ue649\"; } .bp3-icon-cross::before{ content:\"\u2717\"; } .bp3-icon-crown::before{ content:\"\ue7b4\"; } .bp3-icon-cube::before{ content:\"\ue7c8\"; } .bp3-icon-cube-add::before{ content:\"\ue7c9\"; } .bp3-icon-cube-remove::before{ content:\"\ue7d0\"; } .bp3-icon-curved-range-chart::before{ content:\"\ue71b\"; } .bp3-icon-cut::before{ content:\"\ue6ef\"; } .bp3-icon-dashboard::before{ content:\"\ue751\"; } .bp3-icon-data-lineage::before{ content:\"\ue908\"; } .bp3-icon-database::before{ content:\"\ue683\"; } .bp3-icon-delete::before{ content:\"\ue644\"; } .bp3-icon-delta::before{ content:\"\u0394\"; } .bp3-icon-derive-column::before{ content:\"\ue739\"; } .bp3-icon-desktop::before{ content:\"\ue6af\"; } .bp3-icon-diagram-tree::before{ content:\"\ue7b3\"; } .bp3-icon-direction-left::before{ content:\"\ue681\"; } .bp3-icon-direction-right::before{ content:\"\ue682\"; } .bp3-icon-disable::before{ content:\"\ue600\"; } .bp3-icon-document::before{ content:\"\ue630\"; } .bp3-icon-document-open::before{ content:\"\ue71e\"; } .bp3-icon-document-share::before{ content:\"\ue71f\"; } .bp3-icon-dollar::before{ content:\"$\"; } .bp3-icon-dot::before{ content:\"\u2022\"; } .bp3-icon-double-caret-horizontal::before{ content:\"\ue6c7\"; } .bp3-icon-double-caret-vertical::before{ content:\"\ue6c6\"; } .bp3-icon-double-chevron-down::before{ content:\"\ue703\"; } .bp3-icon-double-chevron-left::before{ content:\"\ue6ff\"; } .bp3-icon-double-chevron-right::before{ content:\"\ue701\"; } .bp3-icon-double-chevron-up::before{ content:\"\ue702\"; } .bp3-icon-doughnut-chart::before{ content:\"\ue6ce\"; } .bp3-icon-download::before{ content:\"\ue62f\"; } .bp3-icon-drag-handle-horizontal::before{ content:\"\ue716\"; } .bp3-icon-drag-handle-vertical::before{ content:\"\ue715\"; } .bp3-icon-draw::before{ content:\"\ue66b\"; } .bp3-icon-drive-time::before{ content:\"\ue615\"; } .bp3-icon-duplicate::before{ content:\"\ue69c\"; } .bp3-icon-edit::before{ content:\"\u270e\"; } .bp3-icon-eject::before{ content:\"\u23cf\"; } .bp3-icon-endorsed::before{ content:\"\ue75f\"; } .bp3-icon-envelope::before{ content:\"\u2709\"; } .bp3-icon-equals::before{ content:\"\ue7d9\"; } .bp3-icon-eraser::before{ content:\"\ue773\"; } .bp3-icon-error::before{ content:\"\ue648\"; } .bp3-icon-euro::before{ content:\"\u20ac\"; } .bp3-icon-exchange::before{ content:\"\ue636\"; } .bp3-icon-exclude-row::before{ content:\"\ue6ea\"; } .bp3-icon-expand-all::before{ content:\"\ue764\"; } .bp3-icon-export::before{ content:\"\ue633\"; } .bp3-icon-eye-off::before{ content:\"\ue6cc\"; } .bp3-icon-eye-on::before{ content:\"\ue75a\"; } .bp3-icon-eye-open::before{ content:\"\ue66f\"; } .bp3-icon-fast-backward::before{ content:\"\ue6a8\"; } .bp3-icon-fast-forward::before{ content:\"\ue6ac\"; } .bp3-icon-feed::before{ content:\"\ue656\"; } .bp3-icon-feed-subscribed::before{ content:\"\ue78f\"; } .bp3-icon-film::before{ content:\"\ue6a1\"; } .bp3-icon-filter::before{ content:\"\ue638\"; } .bp3-icon-filter-keep::before{ content:\"\ue78c\"; } .bp3-icon-filter-list::before{ content:\"\ue6ee\"; } .bp3-icon-filter-open::before{ content:\"\ue7d7\"; } .bp3-icon-filter-remove::before{ content:\"\ue78d\"; } .bp3-icon-flag::before{ content:\"\u2691\"; } .bp3-icon-flame::before{ content:\"\ue7a9\"; } .bp3-icon-flash::before{ content:\"\ue6b3\"; } .bp3-icon-floppy-disk::before{ content:\"\ue6b7\"; } .bp3-icon-flow-branch::before{ content:\"\ue7c1\"; } .bp3-icon-flow-end::before{ content:\"\ue7c4\"; } .bp3-icon-flow-linear::before{ content:\"\ue7c0\"; } .bp3-icon-flow-review::before{ content:\"\ue7c2\"; } .bp3-icon-flow-review-branch::before{ content:\"\ue7c3\"; } .bp3-icon-flows::before{ content:\"\ue659\"; } .bp3-icon-folder-close::before{ content:\"\ue652\"; } .bp3-icon-folder-new::before{ content:\"\ue7b0\"; } .bp3-icon-folder-open::before{ content:\"\ue651\"; } .bp3-icon-folder-shared::before{ content:\"\ue653\"; } .bp3-icon-folder-shared-open::before{ content:\"\ue670\"; } .bp3-icon-follower::before{ content:\"\ue760\"; } .bp3-icon-following::before{ content:\"\ue761\"; } .bp3-icon-font::before{ content:\"\ue6b4\"; } .bp3-icon-fork::before{ content:\"\ue63a\"; } .bp3-icon-form::before{ content:\"\ue795\"; } .bp3-icon-full-circle::before{ content:\"\ue685\"; } .bp3-icon-full-stacked-chart::before{ content:\"\ue75e\"; } .bp3-icon-fullscreen::before{ content:\"\ue699\"; } .bp3-icon-function::before{ content:\"\ue6e5\"; } .bp3-icon-gantt-chart::before{ content:\"\ue6f4\"; } .bp3-icon-geolocation::before{ content:\"\ue640\"; } .bp3-icon-geosearch::before{ content:\"\ue613\"; } .bp3-icon-git-branch::before{ content:\"\ue72a\"; } .bp3-icon-git-commit::before{ content:\"\ue72b\"; } .bp3-icon-git-merge::before{ content:\"\ue729\"; } .bp3-icon-git-new-branch::before{ content:\"\ue749\"; } .bp3-icon-git-pull::before{ content:\"\ue728\"; } .bp3-icon-git-push::before{ content:\"\ue72c\"; } .bp3-icon-git-repo::before{ content:\"\ue748\"; } .bp3-icon-glass::before{ content:\"\ue6b1\"; } .bp3-icon-globe::before{ content:\"\ue666\"; } .bp3-icon-globe-network::before{ content:\"\ue7b5\"; } .bp3-icon-graph::before{ content:\"\ue673\"; } .bp3-icon-graph-remove::before{ content:\"\ue609\"; } .bp3-icon-greater-than::before{ content:\"\ue7e1\"; } .bp3-icon-greater-than-or-equal-to::before{ content:\"\ue7e2\"; } .bp3-icon-grid::before{ content:\"\ue6d0\"; } .bp3-icon-grid-view::before{ content:\"\ue6e4\"; } .bp3-icon-group-objects::before{ content:\"\ue60a\"; } .bp3-icon-grouped-bar-chart::before{ content:\"\ue75d\"; } .bp3-icon-hand::before{ content:\"\ue6de\"; } .bp3-icon-hand-down::before{ content:\"\ue6bb\"; } .bp3-icon-hand-left::before{ content:\"\ue6bc\"; } .bp3-icon-hand-right::before{ content:\"\ue6b9\"; } .bp3-icon-hand-up::before{ content:\"\ue6ba\"; } .bp3-icon-header::before{ content:\"\ue6b5\"; } .bp3-icon-header-one::before{ content:\"\ue793\"; } .bp3-icon-header-two::before{ content:\"\ue794\"; } .bp3-icon-headset::before{ content:\"\ue6dc\"; } .bp3-icon-heart::before{ content:\"\u2665\"; } .bp3-icon-heart-broken::before{ content:\"\ue7a2\"; } .bp3-icon-heat-grid::before{ content:\"\ue6f3\"; } .bp3-icon-heatmap::before{ content:\"\ue614\"; } .bp3-icon-help::before{ content:\"?\"; } .bp3-icon-helper-management::before{ content:\"\ue66d\"; } .bp3-icon-highlight::before{ content:\"\ue6ed\"; } .bp3-icon-history::before{ content:\"\ue64a\"; } .bp3-icon-home::before{ content:\"\u2302\"; } .bp3-icon-horizontal-bar-chart::before{ content:\"\ue70c\"; } .bp3-icon-horizontal-bar-chart-asc::before{ content:\"\ue75c\"; } .bp3-icon-horizontal-bar-chart-desc::before{ content:\"\ue71d\"; } .bp3-icon-horizontal-distribution::before{ content:\"\ue720\"; } .bp3-icon-id-number::before{ content:\"\ue771\"; } .bp3-icon-image-rotate-left::before{ content:\"\ue73a\"; } .bp3-icon-image-rotate-right::before{ content:\"\ue73b\"; } .bp3-icon-import::before{ content:\"\ue632\"; } .bp3-icon-inbox::before{ content:\"\ue629\"; } .bp3-icon-inbox-filtered::before{ content:\"\ue7d1\"; } .bp3-icon-inbox-geo::before{ content:\"\ue7d2\"; } .bp3-icon-inbox-search::before{ content:\"\ue7d3\"; } .bp3-icon-inbox-update::before{ content:\"\ue7d4\"; } .bp3-icon-info-sign::before{ content:\"\u2139\"; } .bp3-icon-inheritance::before{ content:\"\ue7d5\"; } .bp3-icon-inner-join::before{ content:\"\ue7a3\"; } .bp3-icon-insert::before{ content:\"\ue66c\"; } .bp3-icon-intersection::before{ content:\"\ue765\"; } .bp3-icon-ip-address::before{ content:\"\ue772\"; } .bp3-icon-issue::before{ content:\"\ue774\"; } .bp3-icon-issue-closed::before{ content:\"\ue776\"; } .bp3-icon-issue-new::before{ content:\"\ue775\"; } .bp3-icon-italic::before{ content:\"\ue607\"; } .bp3-icon-join-table::before{ content:\"\ue738\"; } .bp3-icon-key::before{ content:\"\ue78e\"; } .bp3-icon-key-backspace::before{ content:\"\ue707\"; } .bp3-icon-key-command::before{ content:\"\ue705\"; } .bp3-icon-key-control::before{ content:\"\ue704\"; } .bp3-icon-key-delete::before{ content:\"\ue708\"; } .bp3-icon-key-enter::before{ content:\"\ue70a\"; } .bp3-icon-key-escape::before{ content:\"\ue709\"; } .bp3-icon-key-option::before{ content:\"\ue742\"; } .bp3-icon-key-shift::before{ content:\"\ue706\"; } .bp3-icon-key-tab::before{ content:\"\ue757\"; } .bp3-icon-known-vehicle::before{ content:\"\ue73c\"; } .bp3-icon-label::before{ content:\"\ue665\"; } .bp3-icon-layer::before{ content:\"\ue6cf\"; } .bp3-icon-layers::before{ content:\"\ue618\"; } .bp3-icon-layout::before{ content:\"\ue60c\"; } .bp3-icon-layout-auto::before{ content:\"\ue60d\"; } .bp3-icon-layout-balloon::before{ content:\"\ue6d3\"; } .bp3-icon-layout-circle::before{ content:\"\ue60e\"; } .bp3-icon-layout-grid::before{ content:\"\ue610\"; } .bp3-icon-layout-group-by::before{ content:\"\ue611\"; } .bp3-icon-layout-hierarchy::before{ content:\"\ue60f\"; } .bp3-icon-layout-linear::before{ content:\"\ue6c3\"; } .bp3-icon-layout-skew-grid::before{ content:\"\ue612\"; } .bp3-icon-layout-sorted-clusters::before{ content:\"\ue6d4\"; } .bp3-icon-learning::before{ content:\"\ue904\"; } .bp3-icon-left-join::before{ content:\"\ue7a4\"; } .bp3-icon-less-than::before{ content:\"\ue7e3\"; } .bp3-icon-less-than-or-equal-to::before{ content:\"\ue7e4\"; } .bp3-icon-lifesaver::before{ content:\"\ue7c7\"; } .bp3-icon-lightbulb::before{ content:\"\ue6b0\"; } .bp3-icon-link::before{ content:\"\ue62d\"; } .bp3-icon-list::before{ content:\"\u2630\"; } .bp3-icon-list-columns::before{ content:\"\ue7b9\"; } .bp3-icon-list-detail-view::before{ content:\"\ue743\"; } .bp3-icon-locate::before{ content:\"\ue619\"; } .bp3-icon-lock::before{ content:\"\ue625\"; } .bp3-icon-log-in::before{ content:\"\ue69a\"; } .bp3-icon-log-out::before{ content:\"\ue64c\"; } .bp3-icon-manual::before{ content:\"\ue6f6\"; } .bp3-icon-manually-entered-data::before{ content:\"\ue74a\"; } .bp3-icon-map::before{ content:\"\ue662\"; } .bp3-icon-map-create::before{ content:\"\ue741\"; } .bp3-icon-map-marker::before{ content:\"\ue67d\"; } .bp3-icon-maximize::before{ content:\"\ue635\"; } .bp3-icon-media::before{ content:\"\ue62c\"; } .bp3-icon-menu::before{ content:\"\ue762\"; } .bp3-icon-menu-closed::before{ content:\"\ue655\"; } .bp3-icon-menu-open::before{ content:\"\ue654\"; } .bp3-icon-merge-columns::before{ content:\"\ue74f\"; } .bp3-icon-merge-links::before{ content:\"\ue60b\"; } .bp3-icon-minimize::before{ content:\"\ue634\"; } .bp3-icon-minus::before{ content:\"\u2212\"; } .bp3-icon-mobile-phone::before{ content:\"\ue717\"; } .bp3-icon-mobile-video::before{ content:\"\ue69f\"; } .bp3-icon-moon::before{ content:\"\ue754\"; } .bp3-icon-more::before{ content:\"\ue62a\"; } .bp3-icon-mountain::before{ content:\"\ue7b1\"; } .bp3-icon-move::before{ content:\"\ue693\"; } .bp3-icon-mugshot::before{ content:\"\ue6db\"; } .bp3-icon-multi-select::before{ content:\"\ue680\"; } .bp3-icon-music::before{ content:\"\ue6a6\"; } .bp3-icon-new-drawing::before{ content:\"\ue905\"; } .bp3-icon-new-grid-item::before{ content:\"\ue747\"; } .bp3-icon-new-layer::before{ content:\"\ue902\"; } .bp3-icon-new-layers::before{ content:\"\ue903\"; } .bp3-icon-new-link::before{ content:\"\ue65c\"; } .bp3-icon-new-object::before{ content:\"\ue65d\"; } .bp3-icon-new-person::before{ content:\"\ue6e9\"; } .bp3-icon-new-prescription::before{ content:\"\ue78b\"; } .bp3-icon-new-text-box::before{ content:\"\ue65b\"; } .bp3-icon-ninja::before{ content:\"\ue675\"; } .bp3-icon-not-equal-to::before{ content:\"\ue7e0\"; } .bp3-icon-notifications::before{ content:\"\ue624\"; } .bp3-icon-notifications-updated::before{ content:\"\ue7b8\"; } .bp3-icon-numbered-list::before{ content:\"\ue746\"; } .bp3-icon-numerical::before{ content:\"\ue756\"; } .bp3-icon-office::before{ content:\"\ue69b\"; } .bp3-icon-offline::before{ content:\"\ue67a\"; } .bp3-icon-oil-field::before{ content:\"\ue73f\"; } .bp3-icon-one-column::before{ content:\"\ue658\"; } .bp3-icon-outdated::before{ content:\"\ue7a8\"; } .bp3-icon-page-layout::before{ content:\"\ue660\"; } .bp3-icon-panel-stats::before{ content:\"\ue777\"; } .bp3-icon-panel-table::before{ content:\"\ue778\"; } .bp3-icon-paperclip::before{ content:\"\ue664\"; } .bp3-icon-paragraph::before{ content:\"\ue76c\"; } .bp3-icon-path::before{ content:\"\ue753\"; } .bp3-icon-path-search::before{ content:\"\ue65e\"; } .bp3-icon-pause::before{ content:\"\ue6a9\"; } .bp3-icon-people::before{ content:\"\ue63d\"; } .bp3-icon-percentage::before{ content:\"\ue76a\"; } .bp3-icon-person::before{ content:\"\ue63c\"; } .bp3-icon-phone::before{ content:\"\u260e\"; } .bp3-icon-pie-chart::before{ content:\"\ue684\"; } .bp3-icon-pin::before{ content:\"\ue646\"; } .bp3-icon-pivot::before{ content:\"\ue6f1\"; } .bp3-icon-pivot-table::before{ content:\"\ue6eb\"; } .bp3-icon-play::before{ content:\"\ue6ab\"; } .bp3-icon-plus::before{ content:\"+\"; } .bp3-icon-polygon-filter::before{ content:\"\ue6d1\"; } .bp3-icon-power::before{ content:\"\ue6d9\"; } .bp3-icon-predictive-analysis::before{ content:\"\ue617\"; } .bp3-icon-prescription::before{ content:\"\ue78a\"; } .bp3-icon-presentation::before{ content:\"\ue687\"; } .bp3-icon-print::before{ content:\"\u2399\"; } .bp3-icon-projects::before{ content:\"\ue622\"; } .bp3-icon-properties::before{ content:\"\ue631\"; } .bp3-icon-property::before{ content:\"\ue65a\"; } .bp3-icon-publish-function::before{ content:\"\ue752\"; } .bp3-icon-pulse::before{ content:\"\ue6e8\"; } .bp3-icon-random::before{ content:\"\ue698\"; } .bp3-icon-record::before{ content:\"\ue6ae\"; } .bp3-icon-redo::before{ content:\"\ue6c4\"; } .bp3-icon-refresh::before{ content:\"\ue643\"; } .bp3-icon-regression-chart::before{ content:\"\ue758\"; } .bp3-icon-remove::before{ content:\"\ue63f\"; } .bp3-icon-remove-column::before{ content:\"\ue755\"; } .bp3-icon-remove-column-left::before{ content:\"\ue6fd\"; } .bp3-icon-remove-column-right::before{ content:\"\ue6fe\"; } .bp3-icon-remove-row-bottom::before{ content:\"\ue6fc\"; } .bp3-icon-remove-row-top::before{ content:\"\ue6fb\"; } .bp3-icon-repeat::before{ content:\"\ue692\"; } .bp3-icon-reset::before{ content:\"\ue7d6\"; } .bp3-icon-resolve::before{ content:\"\ue672\"; } .bp3-icon-rig::before{ content:\"\ue740\"; } .bp3-icon-right-join::before{ content:\"\ue7a5\"; } .bp3-icon-ring::before{ content:\"\ue6f2\"; } .bp3-icon-rotate-document::before{ content:\"\ue6e1\"; } .bp3-icon-rotate-page::before{ content:\"\ue6e2\"; } .bp3-icon-satellite::before{ content:\"\ue76b\"; } .bp3-icon-saved::before{ content:\"\ue6b6\"; } .bp3-icon-scatter-plot::before{ content:\"\ue73e\"; } .bp3-icon-search::before{ content:\"\ue64b\"; } .bp3-icon-search-around::before{ content:\"\ue608\"; } .bp3-icon-search-template::before{ content:\"\ue628\"; } .bp3-icon-search-text::before{ content:\"\ue663\"; } .bp3-icon-segmented-control::before{ content:\"\ue6ec\"; } .bp3-icon-select::before{ content:\"\ue616\"; } .bp3-icon-selection::before{ content:\"\u29bf\"; } .bp3-icon-send-to::before{ content:\"\ue66e\"; } .bp3-icon-send-to-graph::before{ content:\"\ue736\"; } .bp3-icon-send-to-map::before{ content:\"\ue737\"; } .bp3-icon-series-add::before{ content:\"\ue796\"; } .bp3-icon-series-configuration::before{ content:\"\ue79a\"; } .bp3-icon-series-derived::before{ content:\"\ue799\"; } .bp3-icon-series-filtered::before{ content:\"\ue798\"; } .bp3-icon-series-search::before{ content:\"\ue797\"; } .bp3-icon-settings::before{ content:\"\ue6a2\"; } .bp3-icon-share::before{ content:\"\ue62e\"; } .bp3-icon-shield::before{ content:\"\ue7b2\"; } .bp3-icon-shop::before{ content:\"\ue6c2\"; } .bp3-icon-shopping-cart::before{ content:\"\ue6c1\"; } .bp3-icon-signal-search::before{ content:\"\ue909\"; } .bp3-icon-sim-card::before{ content:\"\ue718\"; } .bp3-icon-slash::before{ content:\"\ue769\"; } .bp3-icon-small-cross::before{ content:\"\ue6d7\"; } .bp3-icon-small-minus::before{ content:\"\ue70e\"; } .bp3-icon-small-plus::before{ content:\"\ue70d\"; } .bp3-icon-small-tick::before{ content:\"\ue6d8\"; } .bp3-icon-snowflake::before{ content:\"\ue7b6\"; } .bp3-icon-social-media::before{ content:\"\ue671\"; } .bp3-icon-sort::before{ content:\"\ue64f\"; } .bp3-icon-sort-alphabetical::before{ content:\"\ue64d\"; } .bp3-icon-sort-alphabetical-desc::before{ content:\"\ue6c8\"; } .bp3-icon-sort-asc::before{ content:\"\ue6d5\"; } .bp3-icon-sort-desc::before{ content:\"\ue6d6\"; } .bp3-icon-sort-numerical::before{ content:\"\ue64e\"; } .bp3-icon-sort-numerical-desc::before{ content:\"\ue6c9\"; } .bp3-icon-split-columns::before{ content:\"\ue750\"; } .bp3-icon-square::before{ content:\"\ue686\"; } .bp3-icon-stacked-chart::before{ content:\"\ue6e7\"; } .bp3-icon-star::before{ content:\"\u2605\"; } .bp3-icon-star-empty::before{ content:\"\u2606\"; } .bp3-icon-step-backward::before{ content:\"\ue6a7\"; } .bp3-icon-step-chart::before{ content:\"\ue70f\"; } .bp3-icon-step-forward::before{ content:\"\ue6ad\"; } .bp3-icon-stop::before{ content:\"\ue6aa\"; } .bp3-icon-stopwatch::before{ content:\"\ue901\"; } .bp3-icon-strikethrough::before{ content:\"\ue7a6\"; } .bp3-icon-style::before{ content:\"\ue601\"; } .bp3-icon-swap-horizontal::before{ content:\"\ue745\"; } .bp3-icon-swap-vertical::before{ content:\"\ue744\"; } .bp3-icon-symbol-circle::before{ content:\"\ue72e\"; } .bp3-icon-symbol-cross::before{ content:\"\ue731\"; } .bp3-icon-symbol-diamond::before{ content:\"\ue730\"; } .bp3-icon-symbol-square::before{ content:\"\ue72f\"; } .bp3-icon-symbol-triangle-down::before{ content:\"\ue733\"; } .bp3-icon-symbol-triangle-up::before{ content:\"\ue732\"; } .bp3-icon-tag::before{ content:\"\ue61c\"; } .bp3-icon-take-action::before{ content:\"\ue6ca\"; } .bp3-icon-taxi::before{ content:\"\ue79e\"; } .bp3-icon-text-highlight::before{ content:\"\ue6dd\"; } .bp3-icon-th::before{ content:\"\ue667\"; } .bp3-icon-th-derived::before{ content:\"\ue669\"; } .bp3-icon-th-disconnect::before{ content:\"\ue7d8\"; } .bp3-icon-th-filtered::before{ content:\"\ue7c6\"; } .bp3-icon-th-list::before{ content:\"\ue668\"; } .bp3-icon-thumbs-down::before{ content:\"\ue6be\"; } .bp3-icon-thumbs-up::before{ content:\"\ue6bd\"; } .bp3-icon-tick::before{ content:\"\u2713\"; } .bp3-icon-tick-circle::before{ content:\"\ue779\"; } .bp3-icon-time::before{ content:\"\u23f2\"; } .bp3-icon-timeline-area-chart::before{ content:\"\ue6cd\"; } .bp3-icon-timeline-bar-chart::before{ content:\"\ue620\"; } .bp3-icon-timeline-events::before{ content:\"\ue61e\"; } .bp3-icon-timeline-line-chart::before{ content:\"\ue61f\"; } .bp3-icon-tint::before{ content:\"\ue6b2\"; } .bp3-icon-torch::before{ content:\"\ue677\"; } .bp3-icon-tractor::before{ content:\"\ue90c\"; } .bp3-icon-train::before{ content:\"\ue79f\"; } .bp3-icon-translate::before{ content:\"\ue759\"; } .bp3-icon-trash::before{ content:\"\ue63b\"; } .bp3-icon-tree::before{ content:\"\ue7b7\"; } .bp3-icon-trending-down::before{ content:\"\ue71a\"; } .bp3-icon-trending-up::before{ content:\"\ue719\"; } .bp3-icon-truck::before{ content:\"\ue90b\"; } .bp3-icon-two-columns::before{ content:\"\ue657\"; } .bp3-icon-unarchive::before{ content:\"\ue906\"; } .bp3-icon-underline::before{ content:\"\u2381\"; } .bp3-icon-undo::before{ content:\"\u238c\"; } .bp3-icon-ungroup-objects::before{ content:\"\ue688\"; } .bp3-icon-unknown-vehicle::before{ content:\"\ue73d\"; } .bp3-icon-unlock::before{ content:\"\ue626\"; } .bp3-icon-unpin::before{ content:\"\ue650\"; } .bp3-icon-unresolve::before{ content:\"\ue679\"; } .bp3-icon-updated::before{ content:\"\ue7a7\"; } .bp3-icon-upload::before{ content:\"\ue68f\"; } .bp3-icon-user::before{ content:\"\ue627\"; } .bp3-icon-variable::before{ content:\"\ue6f5\"; } .bp3-icon-vertical-bar-chart-asc::before{ content:\"\ue75b\"; } .bp3-icon-vertical-bar-chart-desc::before{ content:\"\ue71c\"; } .bp3-icon-vertical-distribution::before{ content:\"\ue721\"; } .bp3-icon-video::before{ content:\"\ue6a0\"; } .bp3-icon-volume-down::before{ content:\"\ue6a4\"; } .bp3-icon-volume-off::before{ content:\"\ue6a3\"; } .bp3-icon-volume-up::before{ content:\"\ue6a5\"; } .bp3-icon-walk::before{ content:\"\ue79d\"; } .bp3-icon-warning-sign::before{ content:\"\ue647\"; } .bp3-icon-waterfall-chart::before{ content:\"\ue6e6\"; } .bp3-icon-widget::before{ content:\"\ue678\"; } .bp3-icon-widget-button::before{ content:\"\ue790\"; } .bp3-icon-widget-footer::before{ content:\"\ue792\"; } .bp3-icon-widget-header::before{ content:\"\ue791\"; } .bp3-icon-wrench::before{ content:\"\ue734\"; } .bp3-icon-zoom-in::before{ content:\"\ue641\"; } .bp3-icon-zoom-out::before{ content:\"\ue642\"; } .bp3-icon-zoom-to-fit::before{ content:\"\ue67b\"; } .bp3-submenu > .bp3-popover-wrapper{ display:block; } .bp3-submenu .bp3-popover-target{ display:block; } .bp3-submenu.bp3-popover{ -webkit-box-shadow:none; box-shadow:none; padding:0 5px; } .bp3-submenu.bp3-popover > .bp3-popover-content{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2); } .bp3-dark .bp3-submenu.bp3-popover, .bp3-submenu.bp3-popover.bp3-dark{ -webkit-box-shadow:none; box-shadow:none; } .bp3-dark .bp3-submenu.bp3-popover > .bp3-popover-content, .bp3-submenu.bp3-popover.bp3-dark > .bp3-popover-content{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); } .bp3-menu{ margin:0; border-radius:3px; background:#ffffff; min-width:180px; padding:5px; list-style:none; text-align:left; color:#182026; } .bp3-menu-divider{ display:block; margin:5px; border-top:1px solid rgba(16, 22, 26, 0.15); } .bp3-dark .bp3-menu-divider{ border-top-color:rgba(255, 255, 255, 0.15); } .bp3-menu-item{ display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-orient:horizontal; -webkit-box-direction:normal; -ms-flex-direction:row; flex-direction:row; -webkit-box-align:start; -ms-flex-align:start; align-items:flex-start; border-radius:2px; padding:5px 7px; text-decoration:none; line-height:20px; color:inherit; -webkit-user-select:none; -moz-user-select:none; -ms-user-select:none; user-select:none; } .bp3-menu-item > *{ -webkit-box-flex:0; -ms-flex-positive:0; flex-grow:0; -ms-flex-negative:0; flex-shrink:0; } .bp3-menu-item > .bp3-fill{ -webkit-box-flex:1; -ms-flex-positive:1; flex-grow:1; -ms-flex-negative:1; flex-shrink:1; } .bp3-menu-item::before, .bp3-menu-item > *{ margin-right:7px; } .bp3-menu-item:empty::before, .bp3-menu-item > :last-child{ margin-right:0; } .bp3-menu-item > .bp3-fill{ word-break:break-word; } .bp3-menu-item:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-menu-item{ background-color:rgba(167, 182, 194, 0.3); cursor:pointer; text-decoration:none; } .bp3-menu-item.bp3-disabled{ background-color:inherit; cursor:not-allowed; color:rgba(92, 112, 128, 0.6); } .bp3-dark .bp3-menu-item{ color:inherit; } .bp3-dark .bp3-menu-item:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-menu-item{ background-color:rgba(138, 155, 168, 0.15); color:inherit; } .bp3-dark .bp3-menu-item.bp3-disabled{ background-color:inherit; color:rgba(167, 182, 194, 0.6); } .bp3-menu-item.bp3-intent-primary{ color:#106ba3; } .bp3-menu-item.bp3-intent-primary .bp3-icon{ color:inherit; } .bp3-menu-item.bp3-intent-primary::before, .bp3-menu-item.bp3-intent-primary::after, .bp3-menu-item.bp3-intent-primary .bp3-menu-item-label{ color:#106ba3; } .bp3-menu-item.bp3-intent-primary:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item, .bp3-menu-item.bp3-intent-primary.bp3-active{ background-color:#137cbd; } .bp3-menu-item.bp3-intent-primary:active{ background-color:#106ba3; } .bp3-menu-item.bp3-intent-primary:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item, .bp3-menu-item.bp3-intent-primary:hover::before, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item::before, .bp3-menu-item.bp3-intent-primary:hover::after, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item::after, .bp3-menu-item.bp3-intent-primary:hover .bp3-menu-item-label, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item .bp3-menu-item-label, .bp3-menu-item.bp3-intent-primary:active, .bp3-menu-item.bp3-intent-primary:active::before, .bp3-menu-item.bp3-intent-primary:active::after, .bp3-menu-item.bp3-intent-primary:active .bp3-menu-item-label, .bp3-menu-item.bp3-intent-primary.bp3-active, .bp3-menu-item.bp3-intent-primary.bp3-active::before, .bp3-menu-item.bp3-intent-primary.bp3-active::after, .bp3-menu-item.bp3-intent-primary.bp3-active .bp3-menu-item-label{ color:#ffffff; } .bp3-menu-item.bp3-intent-success{ color:#0d8050; } .bp3-menu-item.bp3-intent-success .bp3-icon{ color:inherit; } .bp3-menu-item.bp3-intent-success::before, .bp3-menu-item.bp3-intent-success::after, .bp3-menu-item.bp3-intent-success .bp3-menu-item-label{ color:#0d8050; } .bp3-menu-item.bp3-intent-success:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item, .bp3-menu-item.bp3-intent-success.bp3-active{ background-color:#0f9960; } .bp3-menu-item.bp3-intent-success:active{ background-color:#0d8050; } .bp3-menu-item.bp3-intent-success:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item, .bp3-menu-item.bp3-intent-success:hover::before, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item::before, .bp3-menu-item.bp3-intent-success:hover::after, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item::after, .bp3-menu-item.bp3-intent-success:hover .bp3-menu-item-label, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item .bp3-menu-item-label, .bp3-menu-item.bp3-intent-success:active, .bp3-menu-item.bp3-intent-success:active::before, .bp3-menu-item.bp3-intent-success:active::after, .bp3-menu-item.bp3-intent-success:active .bp3-menu-item-label, .bp3-menu-item.bp3-intent-success.bp3-active, .bp3-menu-item.bp3-intent-success.bp3-active::before, .bp3-menu-item.bp3-intent-success.bp3-active::after, .bp3-menu-item.bp3-intent-success.bp3-active .bp3-menu-item-label{ color:#ffffff; } .bp3-menu-item.bp3-intent-warning{ color:#bf7326; } .bp3-menu-item.bp3-intent-warning .bp3-icon{ color:inherit; } .bp3-menu-item.bp3-intent-warning::before, .bp3-menu-item.bp3-intent-warning::after, .bp3-menu-item.bp3-intent-warning .bp3-menu-item-label{ color:#bf7326; } .bp3-menu-item.bp3-intent-warning:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item, .bp3-menu-item.bp3-intent-warning.bp3-active{ background-color:#d9822b; } .bp3-menu-item.bp3-intent-warning:active{ background-color:#bf7326; } .bp3-menu-item.bp3-intent-warning:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item, .bp3-menu-item.bp3-intent-warning:hover::before, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item::before, .bp3-menu-item.bp3-intent-warning:hover::after, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item::after, .bp3-menu-item.bp3-intent-warning:hover .bp3-menu-item-label, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item .bp3-menu-item-label, .bp3-menu-item.bp3-intent-warning:active, .bp3-menu-item.bp3-intent-warning:active::before, .bp3-menu-item.bp3-intent-warning:active::after, .bp3-menu-item.bp3-intent-warning:active .bp3-menu-item-label, .bp3-menu-item.bp3-intent-warning.bp3-active, .bp3-menu-item.bp3-intent-warning.bp3-active::before, .bp3-menu-item.bp3-intent-warning.bp3-active::after, .bp3-menu-item.bp3-intent-warning.bp3-active .bp3-menu-item-label{ color:#ffffff; } .bp3-menu-item.bp3-intent-danger{ color:#c23030; } .bp3-menu-item.bp3-intent-danger .bp3-icon{ color:inherit; } .bp3-menu-item.bp3-intent-danger::before, .bp3-menu-item.bp3-intent-danger::after, .bp3-menu-item.bp3-intent-danger .bp3-menu-item-label{ color:#c23030; } .bp3-menu-item.bp3-intent-danger:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item, .bp3-menu-item.bp3-intent-danger.bp3-active{ background-color:#db3737; } .bp3-menu-item.bp3-intent-danger:active{ background-color:#c23030; } .bp3-menu-item.bp3-intent-danger:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item, .bp3-menu-item.bp3-intent-danger:hover::before, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item::before, .bp3-menu-item.bp3-intent-danger:hover::after, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item::after, .bp3-menu-item.bp3-intent-danger:hover .bp3-menu-item-label, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item .bp3-menu-item-label, .bp3-menu-item.bp3-intent-danger:active, .bp3-menu-item.bp3-intent-danger:active::before, .bp3-menu-item.bp3-intent-danger:active::after, .bp3-menu-item.bp3-intent-danger:active .bp3-menu-item-label, .bp3-menu-item.bp3-intent-danger.bp3-active, .bp3-menu-item.bp3-intent-danger.bp3-active::before, .bp3-menu-item.bp3-intent-danger.bp3-active::after, .bp3-menu-item.bp3-intent-danger.bp3-active .bp3-menu-item-label{ color:#ffffff; } .bp3-menu-item::before{ line-height:1; font-family:\"Icons16\", sans-serif; font-size:16px; font-weight:400; font-style:normal; -moz-osx-font-smoothing:grayscale; -webkit-font-smoothing:antialiased; margin-right:7px; } .bp3-menu-item::before, .bp3-menu-item > .bp3-icon{ margin-top:2px; color:#5c7080; } .bp3-menu-item .bp3-menu-item-label{ color:#5c7080; } .bp3-menu-item:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-menu-item{ color:inherit; } .bp3-menu-item.bp3-active, .bp3-menu-item:active{ background-color:rgba(115, 134, 148, 0.3); } .bp3-menu-item.bp3-disabled{ outline:none !important; background-color:inherit !important; cursor:not-allowed !important; color:rgba(92, 112, 128, 0.6) !important; } .bp3-menu-item.bp3-disabled::before, .bp3-menu-item.bp3-disabled > .bp3-icon, .bp3-menu-item.bp3-disabled .bp3-menu-item-label{ color:rgba(92, 112, 128, 0.6) !important; } .bp3-large .bp3-menu-item{ padding:9px 7px; line-height:22px; font-size:16px; } .bp3-large .bp3-menu-item .bp3-icon{ margin-top:3px; } .bp3-large .bp3-menu-item::before{ line-height:1; font-family:\"Icons20\", sans-serif; font-size:20px; font-weight:400; font-style:normal; -moz-osx-font-smoothing:grayscale; -webkit-font-smoothing:antialiased; margin-top:1px; margin-right:10px; } button.bp3-menu-item{ border:none; background:none; width:100%; text-align:left; } .bp3-menu-header{ display:block; margin:5px; border-top:1px solid rgba(16, 22, 26, 0.15); cursor:default; padding-left:2px; } .bp3-dark .bp3-menu-header{ border-top-color:rgba(255, 255, 255, 0.15); } .bp3-menu-header:first-of-type{ border-top:none; } .bp3-menu-header > h6{ color:#182026; font-weight:600; overflow:hidden; text-overflow:ellipsis; white-space:nowrap; word-wrap:normal; margin:0; padding:10px 7px 0 1px; line-height:17px; } .bp3-dark .bp3-menu-header > h6{ color:#f5f8fa; } .bp3-menu-header:first-of-type > h6{ padding-top:0; } .bp3-large .bp3-menu-header > h6{ padding-top:15px; padding-bottom:5px; font-size:18px; } .bp3-large .bp3-menu-header:first-of-type > h6{ padding-top:0; } .bp3-dark .bp3-menu{ background:#30404d; color:#f5f8fa; } .bp3-dark .bp3-menu-item.bp3-intent-primary{ color:#48aff0; } .bp3-dark .bp3-menu-item.bp3-intent-primary .bp3-icon{ color:inherit; } .bp3-dark .bp3-menu-item.bp3-intent-primary::before, .bp3-dark .bp3-menu-item.bp3-intent-primary::after, .bp3-dark .bp3-menu-item.bp3-intent-primary .bp3-menu-item-label{ color:#48aff0; } .bp3-dark .bp3-menu-item.bp3-intent-primary:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item, .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active{ background-color:#137cbd; } .bp3-dark .bp3-menu-item.bp3-intent-primary:active{ background-color:#106ba3; } .bp3-dark .bp3-menu-item.bp3-intent-primary:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item, .bp3-dark .bp3-menu-item.bp3-intent-primary:hover::before, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item::before, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item::before, .bp3-dark .bp3-menu-item.bp3-intent-primary:hover::after, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item::after, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item::after, .bp3-dark .bp3-menu-item.bp3-intent-primary:hover .bp3-menu-item-label, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item .bp3-menu-item-label, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item .bp3-menu-item-label, .bp3-dark .bp3-menu-item.bp3-intent-primary:active, .bp3-dark .bp3-menu-item.bp3-intent-primary:active::before, .bp3-dark .bp3-menu-item.bp3-intent-primary:active::after, .bp3-dark .bp3-menu-item.bp3-intent-primary:active .bp3-menu-item-label, .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active, .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active::before, .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active::after, .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active .bp3-menu-item-label{ color:#ffffff; } .bp3-dark .bp3-menu-item.bp3-intent-success{ color:#3dcc91; } .bp3-dark .bp3-menu-item.bp3-intent-success .bp3-icon{ color:inherit; } .bp3-dark .bp3-menu-item.bp3-intent-success::before, .bp3-dark .bp3-menu-item.bp3-intent-success::after, .bp3-dark .bp3-menu-item.bp3-intent-success .bp3-menu-item-label{ color:#3dcc91; } .bp3-dark .bp3-menu-item.bp3-intent-success:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item, .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active{ background-color:#0f9960; } .bp3-dark .bp3-menu-item.bp3-intent-success:active{ background-color:#0d8050; } .bp3-dark .bp3-menu-item.bp3-intent-success:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item, .bp3-dark .bp3-menu-item.bp3-intent-success:hover::before, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item::before, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item::before, .bp3-dark .bp3-menu-item.bp3-intent-success:hover::after, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item::after, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item::after, .bp3-dark .bp3-menu-item.bp3-intent-success:hover .bp3-menu-item-label, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item .bp3-menu-item-label, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item .bp3-menu-item-label, .bp3-dark .bp3-menu-item.bp3-intent-success:active, .bp3-dark .bp3-menu-item.bp3-intent-success:active::before, .bp3-dark .bp3-menu-item.bp3-intent-success:active::after, .bp3-dark .bp3-menu-item.bp3-intent-success:active .bp3-menu-item-label, .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active, .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active::before, .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active::after, .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active .bp3-menu-item-label{ color:#ffffff; } .bp3-dark .bp3-menu-item.bp3-intent-warning{ color:#ffb366; } .bp3-dark .bp3-menu-item.bp3-intent-warning .bp3-icon{ color:inherit; } .bp3-dark .bp3-menu-item.bp3-intent-warning::before, .bp3-dark .bp3-menu-item.bp3-intent-warning::after, .bp3-dark .bp3-menu-item.bp3-intent-warning .bp3-menu-item-label{ color:#ffb366; } .bp3-dark .bp3-menu-item.bp3-intent-warning:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item, .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active{ background-color:#d9822b; } .bp3-dark .bp3-menu-item.bp3-intent-warning:active{ background-color:#bf7326; } .bp3-dark .bp3-menu-item.bp3-intent-warning:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item, .bp3-dark .bp3-menu-item.bp3-intent-warning:hover::before, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item::before, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item::before, .bp3-dark .bp3-menu-item.bp3-intent-warning:hover::after, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item::after, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item::after, .bp3-dark .bp3-menu-item.bp3-intent-warning:hover .bp3-menu-item-label, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item .bp3-menu-item-label, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item .bp3-menu-item-label, .bp3-dark .bp3-menu-item.bp3-intent-warning:active, .bp3-dark .bp3-menu-item.bp3-intent-warning:active::before, .bp3-dark .bp3-menu-item.bp3-intent-warning:active::after, .bp3-dark .bp3-menu-item.bp3-intent-warning:active .bp3-menu-item-label, .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active, .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active::before, .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active::after, .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active .bp3-menu-item-label{ color:#ffffff; } .bp3-dark .bp3-menu-item.bp3-intent-danger{ color:#ff7373; } .bp3-dark .bp3-menu-item.bp3-intent-danger .bp3-icon{ color:inherit; } .bp3-dark .bp3-menu-item.bp3-intent-danger::before, .bp3-dark .bp3-menu-item.bp3-intent-danger::after, .bp3-dark .bp3-menu-item.bp3-intent-danger .bp3-menu-item-label{ color:#ff7373; } .bp3-dark .bp3-menu-item.bp3-intent-danger:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item, .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active{ background-color:#db3737; } .bp3-dark .bp3-menu-item.bp3-intent-danger:active{ background-color:#c23030; } .bp3-dark .bp3-menu-item.bp3-intent-danger:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item, .bp3-dark .bp3-menu-item.bp3-intent-danger:hover::before, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item::before, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item::before, .bp3-dark .bp3-menu-item.bp3-intent-danger:hover::after, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item::after, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item::after, .bp3-dark .bp3-menu-item.bp3-intent-danger:hover .bp3-menu-item-label, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item .bp3-menu-item-label, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item .bp3-menu-item-label, .bp3-dark .bp3-menu-item.bp3-intent-danger:active, .bp3-dark .bp3-menu-item.bp3-intent-danger:active::before, .bp3-dark .bp3-menu-item.bp3-intent-danger:active::after, .bp3-dark .bp3-menu-item.bp3-intent-danger:active .bp3-menu-item-label, .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active, .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active::before, .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active::after, .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active .bp3-menu-item-label{ color:#ffffff; } .bp3-dark .bp3-menu-item::before, .bp3-dark .bp3-menu-item > .bp3-icon{ color:#a7b6c2; } .bp3-dark .bp3-menu-item .bp3-menu-item-label{ color:#a7b6c2; } .bp3-dark .bp3-menu-item.bp3-active, .bp3-dark .bp3-menu-item:active{ background-color:rgba(138, 155, 168, 0.3); } .bp3-dark .bp3-menu-item.bp3-disabled{ color:rgba(167, 182, 194, 0.6) !important; } .bp3-dark .bp3-menu-item.bp3-disabled::before, .bp3-dark .bp3-menu-item.bp3-disabled > .bp3-icon, .bp3-dark .bp3-menu-item.bp3-disabled .bp3-menu-item-label{ color:rgba(167, 182, 194, 0.6) !important; } .bp3-dark .bp3-menu-divider, .bp3-dark .bp3-menu-header{ border-color:rgba(255, 255, 255, 0.15); } .bp3-dark .bp3-menu-header > h6{ color:#f5f8fa; } .bp3-label .bp3-menu{ margin-top:5px; } .bp3-navbar{ position:relative; z-index:10; -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.2); background-color:#ffffff; width:100%; height:50px; padding:0 15px; } .bp3-navbar.bp3-dark, .bp3-dark .bp3-navbar{ background-color:#394b59; } .bp3-navbar.bp3-dark{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-navbar{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4); } .bp3-navbar.bp3-fixed-top{ position:fixed; top:0; right:0; left:0; } .bp3-navbar-heading{ margin-right:15px; font-size:16px; } .bp3-navbar-group{ display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-align:center; -ms-flex-align:center; align-items:center; height:50px; } .bp3-navbar-group.bp3-align-left{ float:left; } .bp3-navbar-group.bp3-align-right{ float:right; } .bp3-navbar-divider{ margin:0 10px; border-left:1px solid rgba(16, 22, 26, 0.15); height:20px; } .bp3-dark .bp3-navbar-divider{ border-left-color:rgba(255, 255, 255, 0.15); } .bp3-non-ideal-state{ display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-orient:vertical; -webkit-box-direction:normal; -ms-flex-direction:column; flex-direction:column; -webkit-box-align:center; -ms-flex-align:center; align-items:center; -webkit-box-pack:center; -ms-flex-pack:center; justify-content:center; width:100%; height:100%; text-align:center; } .bp3-non-ideal-state > *{ -webkit-box-flex:0; -ms-flex-positive:0; flex-grow:0; -ms-flex-negative:0; flex-shrink:0; } .bp3-non-ideal-state > .bp3-fill{ -webkit-box-flex:1; -ms-flex-positive:1; flex-grow:1; -ms-flex-negative:1; flex-shrink:1; } .bp3-non-ideal-state::before, .bp3-non-ideal-state > *{ margin-bottom:20px; } .bp3-non-ideal-state:empty::before, .bp3-non-ideal-state > :last-child{ margin-bottom:0; } .bp3-non-ideal-state > *{ max-width:400px; } .bp3-non-ideal-state-visual{ color:rgba(92, 112, 128, 0.6); font-size:60px; } .bp3-dark .bp3-non-ideal-state-visual{ color:rgba(167, 182, 194, 0.6); } .bp3-overflow-list{ display:-webkit-box; display:-ms-flexbox; display:flex; -ms-flex-wrap:nowrap; flex-wrap:nowrap; min-width:0; } .bp3-overflow-list-spacer{ -ms-flex-negative:1; flex-shrink:1; width:1px; } body.bp3-overlay-open{ overflow:hidden; } .bp3-overlay{ position:static; top:0; right:0; bottom:0; left:0; z-index:20; } .bp3-overlay:not(.bp3-overlay-open){ pointer-events:none; } .bp3-overlay.bp3-overlay-container{ position:fixed; overflow:hidden; } .bp3-overlay.bp3-overlay-container.bp3-overlay-inline{ position:absolute; } .bp3-overlay.bp3-overlay-scroll-container{ position:fixed; overflow:auto; } .bp3-overlay.bp3-overlay-scroll-container.bp3-overlay-inline{ position:absolute; } .bp3-overlay.bp3-overlay-inline{ display:inline; overflow:visible; } .bp3-overlay-content{ position:fixed; z-index:20; } .bp3-overlay-inline .bp3-overlay-content, .bp3-overlay-scroll-container .bp3-overlay-content{ position:absolute; } .bp3-overlay-backdrop{ position:fixed; top:0; right:0; bottom:0; left:0; opacity:1; z-index:20; background-color:rgba(16, 22, 26, 0.7); overflow:auto; -webkit-user-select:none; -moz-user-select:none; -ms-user-select:none; user-select:none; } .bp3-overlay-backdrop.bp3-overlay-enter, .bp3-overlay-backdrop.bp3-overlay-appear{ opacity:0; } .bp3-overlay-backdrop.bp3-overlay-enter-active, .bp3-overlay-backdrop.bp3-overlay-appear-active{ opacity:1; -webkit-transition-property:opacity; transition-property:opacity; -webkit-transition-duration:200ms; transition-duration:200ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-transition-delay:0; transition-delay:0; } .bp3-overlay-backdrop.bp3-overlay-exit{ opacity:1; } .bp3-overlay-backdrop.bp3-overlay-exit-active{ opacity:0; -webkit-transition-property:opacity; transition-property:opacity; -webkit-transition-duration:200ms; transition-duration:200ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-transition-delay:0; transition-delay:0; } .bp3-overlay-backdrop:focus{ outline:none; } .bp3-overlay-inline .bp3-overlay-backdrop{ position:absolute; } .bp3-panel-stack{ position:relative; overflow:hidden; } .bp3-panel-stack-header{ display:-webkit-box; display:-ms-flexbox; display:flex; -ms-flex-negative:0; flex-shrink:0; -webkit-box-align:center; -ms-flex-align:center; align-items:center; z-index:1; -webkit-box-shadow:0 1px rgba(16, 22, 26, 0.15); box-shadow:0 1px rgba(16, 22, 26, 0.15); height:30px; } .bp3-dark .bp3-panel-stack-header{ -webkit-box-shadow:0 1px rgba(255, 255, 255, 0.15); box-shadow:0 1px rgba(255, 255, 255, 0.15); } .bp3-panel-stack-header > span{ display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-flex:1; -ms-flex:1; flex:1; -webkit-box-align:stretch; -ms-flex-align:stretch; align-items:stretch; } .bp3-panel-stack-header .bp3-heading{ margin:0 5px; } .bp3-button.bp3-panel-stack-header-back{ margin-left:5px; padding-left:0; white-space:nowrap; } .bp3-button.bp3-panel-stack-header-back .bp3-icon{ margin:0 2px; } .bp3-panel-stack-view{ position:absolute; top:0; right:0; bottom:0; left:0; display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-orient:vertical; -webkit-box-direction:normal; -ms-flex-direction:column; flex-direction:column; margin-right:-1px; border-right:1px solid rgba(16, 22, 26, 0.15); background-color:#ffffff; overflow-y:auto; } .bp3-dark .bp3-panel-stack-view{ background-color:#30404d; } .bp3-panel-stack-push .bp3-panel-stack-enter, .bp3-panel-stack-push .bp3-panel-stack-appear{ -webkit-transform:translateX(100%); transform:translateX(100%); opacity:0; } .bp3-panel-stack-push .bp3-panel-stack-enter-active, .bp3-panel-stack-push .bp3-panel-stack-appear-active{ -webkit-transform:translate(0%); transform:translate(0%); opacity:1; -webkit-transition-property:opacity, -webkit-transform; transition-property:opacity, -webkit-transform; transition-property:transform, opacity; transition-property:transform, opacity, -webkit-transform; -webkit-transition-duration:400ms; transition-duration:400ms; -webkit-transition-timing-function:ease; transition-timing-function:ease; -webkit-transition-delay:0; transition-delay:0; } .bp3-panel-stack-push .bp3-panel-stack-exit{ -webkit-transform:translate(0%); transform:translate(0%); opacity:1; } .bp3-panel-stack-push .bp3-panel-stack-exit-active{ -webkit-transform:translateX(-50%); transform:translateX(-50%); opacity:0; -webkit-transition-property:opacity, -webkit-transform; transition-property:opacity, -webkit-transform; transition-property:transform, opacity; transition-property:transform, opacity, -webkit-transform; -webkit-transition-duration:400ms; transition-duration:400ms; -webkit-transition-timing-function:ease; transition-timing-function:ease; -webkit-transition-delay:0; transition-delay:0; } .bp3-panel-stack-pop .bp3-panel-stack-enter, .bp3-panel-stack-pop .bp3-panel-stack-appear{ -webkit-transform:translateX(-50%); transform:translateX(-50%); opacity:0; } .bp3-panel-stack-pop .bp3-panel-stack-enter-active, .bp3-panel-stack-pop .bp3-panel-stack-appear-active{ -webkit-transform:translate(0%); transform:translate(0%); opacity:1; -webkit-transition-property:opacity, -webkit-transform; transition-property:opacity, -webkit-transform; transition-property:transform, opacity; transition-property:transform, opacity, -webkit-transform; -webkit-transition-duration:400ms; transition-duration:400ms; -webkit-transition-timing-function:ease; transition-timing-function:ease; -webkit-transition-delay:0; transition-delay:0; } .bp3-panel-stack-pop .bp3-panel-stack-exit{ -webkit-transform:translate(0%); transform:translate(0%); opacity:1; } .bp3-panel-stack-pop .bp3-panel-stack-exit-active{ -webkit-transform:translateX(100%); transform:translateX(100%); opacity:0; -webkit-transition-property:opacity, -webkit-transform; transition-property:opacity, -webkit-transform; transition-property:transform, opacity; transition-property:transform, opacity, -webkit-transform; -webkit-transition-duration:400ms; transition-duration:400ms; -webkit-transition-timing-function:ease; transition-timing-function:ease; -webkit-transition-delay:0; transition-delay:0; } .bp3-popover{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2); -webkit-transform:scale(1); transform:scale(1); display:inline-block; z-index:20; border-radius:3px; } .bp3-popover .bp3-popover-arrow{ position:absolute; width:30px; height:30px; } .bp3-popover .bp3-popover-arrow::before{ margin:5px; width:20px; height:20px; } .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top > .bp3-popover{ margin-top:-17px; margin-bottom:17px; } .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top > .bp3-popover > .bp3-popover-arrow{ bottom:-11px; } .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top > .bp3-popover > .bp3-popover-arrow svg{ -webkit-transform:rotate(-90deg); transform:rotate(-90deg); } .bp3-tether-element-attached-left.bp3-tether-target-attached-right > .bp3-popover{ margin-left:17px; } .bp3-tether-element-attached-left.bp3-tether-target-attached-right > .bp3-popover > .bp3-popover-arrow{ left:-11px; } .bp3-tether-element-attached-left.bp3-tether-target-attached-right > .bp3-popover > .bp3-popover-arrow svg{ -webkit-transform:rotate(0); transform:rotate(0); } .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom > .bp3-popover{ margin-top:17px; } .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom > .bp3-popover > .bp3-popover-arrow{ top:-11px; } .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom > .bp3-popover > .bp3-popover-arrow svg{ -webkit-transform:rotate(90deg); transform:rotate(90deg); } .bp3-tether-element-attached-right.bp3-tether-target-attached-left > .bp3-popover{ margin-right:17px; margin-left:-17px; } .bp3-tether-element-attached-right.bp3-tether-target-attached-left > .bp3-popover > .bp3-popover-arrow{ right:-11px; } .bp3-tether-element-attached-right.bp3-tether-target-attached-left > .bp3-popover > .bp3-popover-arrow svg{ -webkit-transform:rotate(180deg); transform:rotate(180deg); } .bp3-tether-element-attached-middle > .bp3-popover > .bp3-popover-arrow{ top:50%; -webkit-transform:translateY(-50%); transform:translateY(-50%); } .bp3-tether-element-attached-center > .bp3-popover > .bp3-popover-arrow{ right:50%; -webkit-transform:translateX(50%); transform:translateX(50%); } .bp3-tether-element-attached-top.bp3-tether-target-attached-top > .bp3-popover > .bp3-popover-arrow{ top:-0.3934px; } .bp3-tether-element-attached-right.bp3-tether-target-attached-right > .bp3-popover > .bp3-popover-arrow{ right:-0.3934px; } .bp3-tether-element-attached-left.bp3-tether-target-attached-left > .bp3-popover > .bp3-popover-arrow{ left:-0.3934px; } .bp3-tether-element-attached-bottom.bp3-tether-target-attached-bottom > .bp3-popover > .bp3-popover-arrow{ bottom:-0.3934px; } .bp3-tether-element-attached-top.bp3-tether-element-attached-left > .bp3-popover{ -webkit-transform-origin:top left; transform-origin:top left; } .bp3-tether-element-attached-top.bp3-tether-element-attached-center > .bp3-popover{ -webkit-transform-origin:top center; transform-origin:top center; } .bp3-tether-element-attached-top.bp3-tether-element-attached-right > .bp3-popover{ -webkit-transform-origin:top right; transform-origin:top right; } .bp3-tether-element-attached-middle.bp3-tether-element-attached-left > .bp3-popover{ -webkit-transform-origin:center left; transform-origin:center left; } .bp3-tether-element-attached-middle.bp3-tether-element-attached-center > .bp3-popover{ -webkit-transform-origin:center center; transform-origin:center center; } .bp3-tether-element-attached-middle.bp3-tether-element-attached-right > .bp3-popover{ -webkit-transform-origin:center right; transform-origin:center right; } .bp3-tether-element-attached-bottom.bp3-tether-element-attached-left > .bp3-popover{ -webkit-transform-origin:bottom left; transform-origin:bottom left; } .bp3-tether-element-attached-bottom.bp3-tether-element-attached-center > .bp3-popover{ -webkit-transform-origin:bottom center; transform-origin:bottom center; } .bp3-tether-element-attached-bottom.bp3-tether-element-attached-right > .bp3-popover{ -webkit-transform-origin:bottom right; transform-origin:bottom right; } .bp3-popover .bp3-popover-content{ background:#ffffff; color:inherit; } .bp3-popover .bp3-popover-arrow::before{ -webkit-box-shadow:1px 1px 6px rgba(16, 22, 26, 0.2); box-shadow:1px 1px 6px rgba(16, 22, 26, 0.2); } .bp3-popover .bp3-popover-arrow-border{ fill:#10161a; fill-opacity:0.1; } .bp3-popover .bp3-popover-arrow-fill{ fill:#ffffff; } .bp3-popover-enter > .bp3-popover, .bp3-popover-appear > .bp3-popover{ -webkit-transform:scale(0.3); transform:scale(0.3); } .bp3-popover-enter-active > .bp3-popover, .bp3-popover-appear-active > .bp3-popover{ -webkit-transform:scale(1); transform:scale(1); -webkit-transition-property:-webkit-transform; transition-property:-webkit-transform; transition-property:transform; transition-property:transform, -webkit-transform; -webkit-transition-duration:300ms; transition-duration:300ms; -webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11); transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11); -webkit-transition-delay:0; transition-delay:0; } .bp3-popover-exit > .bp3-popover{ -webkit-transform:scale(1); transform:scale(1); } .bp3-popover-exit-active > .bp3-popover{ -webkit-transform:scale(0.3); transform:scale(0.3); -webkit-transition-property:-webkit-transform; transition-property:-webkit-transform; transition-property:transform; transition-property:transform, -webkit-transform; -webkit-transition-duration:300ms; transition-duration:300ms; -webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11); transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11); -webkit-transition-delay:0; transition-delay:0; } .bp3-popover .bp3-popover-content{ position:relative; border-radius:3px; } .bp3-popover.bp3-popover-content-sizing .bp3-popover-content{ max-width:350px; padding:20px; } .bp3-popover-target + .bp3-overlay .bp3-popover.bp3-popover-content-sizing{ width:350px; } .bp3-popover.bp3-minimal{ margin:0 !important; } .bp3-popover.bp3-minimal .bp3-popover-arrow{ display:none; } .bp3-popover.bp3-minimal.bp3-popover{ -webkit-transform:scale(1); transform:scale(1); } .bp3-popover-enter > .bp3-popover.bp3-minimal.bp3-popover, .bp3-popover-appear > .bp3-popover.bp3-minimal.bp3-popover{ -webkit-transform:scale(1); transform:scale(1); } .bp3-popover-enter-active > .bp3-popover.bp3-minimal.bp3-popover, .bp3-popover-appear-active > .bp3-popover.bp3-minimal.bp3-popover{ -webkit-transform:scale(1); transform:scale(1); -webkit-transition-property:-webkit-transform; transition-property:-webkit-transform; transition-property:transform; transition-property:transform, -webkit-transform; -webkit-transition-duration:100ms; transition-duration:100ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-transition-delay:0; transition-delay:0; } .bp3-popover-exit > .bp3-popover.bp3-minimal.bp3-popover{ -webkit-transform:scale(1); transform:scale(1); } .bp3-popover-exit-active > .bp3-popover.bp3-minimal.bp3-popover{ -webkit-transform:scale(1); transform:scale(1); -webkit-transition-property:-webkit-transform; transition-property:-webkit-transform; transition-property:transform; transition-property:transform, -webkit-transform; -webkit-transition-duration:100ms; transition-duration:100ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-transition-delay:0; transition-delay:0; } .bp3-popover.bp3-dark, .bp3-dark .bp3-popover{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); } .bp3-popover.bp3-dark .bp3-popover-content, .bp3-dark .bp3-popover .bp3-popover-content{ background:#30404d; color:inherit; } .bp3-popover.bp3-dark .bp3-popover-arrow::before, .bp3-dark .bp3-popover .bp3-popover-arrow::before{ -webkit-box-shadow:1px 1px 6px rgba(16, 22, 26, 0.4); box-shadow:1px 1px 6px rgba(16, 22, 26, 0.4); } .bp3-popover.bp3-dark .bp3-popover-arrow-border, .bp3-dark .bp3-popover .bp3-popover-arrow-border{ fill:#10161a; fill-opacity:0.2; } .bp3-popover.bp3-dark .bp3-popover-arrow-fill, .bp3-dark .bp3-popover .bp3-popover-arrow-fill{ fill:#30404d; } .bp3-popover-arrow::before{ display:block; position:absolute; -webkit-transform:rotate(45deg); transform:rotate(45deg); border-radius:2px; content:\"\"; } .bp3-tether-pinned .bp3-popover-arrow{ display:none; } .bp3-popover-backdrop{ background:rgba(255, 255, 255, 0); } .bp3-transition-container{ opacity:1; display:-webkit-box; display:-ms-flexbox; display:flex; z-index:20; } .bp3-transition-container.bp3-popover-enter, .bp3-transition-container.bp3-popover-appear{ opacity:0; } .bp3-transition-container.bp3-popover-enter-active, .bp3-transition-container.bp3-popover-appear-active{ opacity:1; -webkit-transition-property:opacity; transition-property:opacity; -webkit-transition-duration:100ms; transition-duration:100ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-transition-delay:0; transition-delay:0; } .bp3-transition-container.bp3-popover-exit{ opacity:1; } .bp3-transition-container.bp3-popover-exit-active{ opacity:0; -webkit-transition-property:opacity; transition-property:opacity; -webkit-transition-duration:100ms; transition-duration:100ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-transition-delay:0; transition-delay:0; } .bp3-transition-container:focus{ outline:none; } .bp3-transition-container.bp3-popover-leave .bp3-popover-content{ pointer-events:none; } .bp3-transition-container[data-x-out-of-boundaries]{ display:none; } span.bp3-popover-target{ display:inline-block; } .bp3-popover-wrapper.bp3-fill{ width:100%; } .bp3-portal{ position:absolute; top:0; right:0; left:0; } @-webkit-keyframes linear-progress-bar-stripes{ from{ background-position:0 0; } to{ background-position:30px 0; } } @keyframes linear-progress-bar-stripes{ from{ background-position:0 0; } to{ background-position:30px 0; } } .bp3-progress-bar{ display:block; position:relative; border-radius:40px; background:rgba(92, 112, 128, 0.2); width:100%; height:8px; overflow:hidden; } .bp3-progress-bar .bp3-progress-meter{ position:absolute; border-radius:40px; background:linear-gradient(-45deg, rgba(255, 255, 255, 0.2) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.2) 50%, rgba(255, 255, 255, 0.2) 75%, transparent 75%); background-color:rgba(92, 112, 128, 0.8); background-size:30px 30px; width:100%; height:100%; -webkit-transition:width 200ms cubic-bezier(0.4, 1, 0.75, 0.9); transition:width 200ms cubic-bezier(0.4, 1, 0.75, 0.9); } .bp3-progress-bar:not(.bp3-no-animation):not(.bp3-no-stripes) .bp3-progress-meter{ animation:linear-progress-bar-stripes 300ms linear infinite reverse; } .bp3-progress-bar.bp3-no-stripes .bp3-progress-meter{ background-image:none; } .bp3-dark .bp3-progress-bar{ background:rgba(16, 22, 26, 0.5); } .bp3-dark .bp3-progress-bar .bp3-progress-meter{ background-color:#8a9ba8; } .bp3-progress-bar.bp3-intent-primary .bp3-progress-meter{ background-color:#137cbd; } .bp3-progress-bar.bp3-intent-success .bp3-progress-meter{ background-color:#0f9960; } .bp3-progress-bar.bp3-intent-warning .bp3-progress-meter{ background-color:#d9822b; } .bp3-progress-bar.bp3-intent-danger .bp3-progress-meter{ background-color:#db3737; } @-webkit-keyframes skeleton-glow{ from{ border-color:rgba(206, 217, 224, 0.2); background:rgba(206, 217, 224, 0.2); } to{ border-color:rgba(92, 112, 128, 0.2); background:rgba(92, 112, 128, 0.2); } } @keyframes skeleton-glow{ from{ border-color:rgba(206, 217, 224, 0.2); background:rgba(206, 217, 224, 0.2); } to{ border-color:rgba(92, 112, 128, 0.2); background:rgba(92, 112, 128, 0.2); } } .bp3-skeleton{ border-color:rgba(206, 217, 224, 0.2) !important; border-radius:2px; -webkit-box-shadow:none !important; box-shadow:none !important; background:rgba(206, 217, 224, 0.2); background-clip:padding-box !important; cursor:default; color:transparent !important; -webkit-animation:1000ms linear infinite alternate skeleton-glow; animation:1000ms linear infinite alternate skeleton-glow; pointer-events:none; -webkit-user-select:none; -moz-user-select:none; -ms-user-select:none; user-select:none; } .bp3-skeleton::before, .bp3-skeleton::after, .bp3-skeleton *{ visibility:hidden !important; } .bp3-slider{ width:100%; min-width:150px; height:40px; position:relative; outline:none; cursor:default; -webkit-user-select:none; -moz-user-select:none; -ms-user-select:none; user-select:none; } .bp3-slider:hover{ cursor:pointer; } .bp3-slider:active{ cursor:-webkit-grabbing; cursor:grabbing; } .bp3-slider.bp3-disabled{ opacity:0.5; cursor:not-allowed; } .bp3-slider.bp3-slider-unlabeled{ height:16px; } .bp3-slider-track, .bp3-slider-progress{ top:5px; right:0; left:0; height:6px; position:absolute; } .bp3-slider-track{ border-radius:3px; overflow:hidden; } .bp3-slider-progress{ background:rgba(92, 112, 128, 0.2); } .bp3-dark .bp3-slider-progress{ background:rgba(16, 22, 26, 0.5); } .bp3-slider-progress.bp3-intent-primary{ background-color:#137cbd; } .bp3-slider-progress.bp3-intent-success{ background-color:#0f9960; } .bp3-slider-progress.bp3-intent-warning{ background-color:#d9822b; } .bp3-slider-progress.bp3-intent-danger{ background-color:#db3737; } .bp3-slider-handle{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); background-color:#f5f8fa; background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0))); background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0)); color:#182026; position:absolute; top:0; left:0; border-radius:3px; -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 1px 1px rgba(16, 22, 26, 0.2); cursor:pointer; width:16px; height:16px; } .bp3-slider-handle:hover{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); background-clip:padding-box; background-color:#ebf1f5; } .bp3-slider-handle:active, .bp3-slider-handle.bp3-active{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2); background-color:#d8e1e8; background-image:none; } .bp3-slider-handle:disabled, .bp3-slider-handle.bp3-disabled{ outline:none; -webkit-box-shadow:none; box-shadow:none; background-color:rgba(206, 217, 224, 0.5); background-image:none; cursor:not-allowed; color:rgba(92, 112, 128, 0.6); } .bp3-slider-handle:disabled.bp3-active, .bp3-slider-handle:disabled.bp3-active:hover, .bp3-slider-handle.bp3-disabled.bp3-active, .bp3-slider-handle.bp3-disabled.bp3-active:hover{ background:rgba(206, 217, 224, 0.7); } .bp3-slider-handle:focus{ z-index:1; } .bp3-slider-handle:hover{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); background-clip:padding-box; background-color:#ebf1f5; z-index:2; -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 1px 1px rgba(16, 22, 26, 0.2); cursor:-webkit-grab; cursor:grab; } .bp3-slider-handle.bp3-active{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2); background-color:#d8e1e8; background-image:none; -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 1px rgba(16, 22, 26, 0.1); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 1px rgba(16, 22, 26, 0.1); cursor:-webkit-grabbing; cursor:grabbing; } .bp3-disabled .bp3-slider-handle{ -webkit-box-shadow:none; box-shadow:none; background:#bfccd6; pointer-events:none; } .bp3-dark .bp3-slider-handle{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); background-color:#394b59; background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0))); background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0)); color:#f5f8fa; } .bp3-dark .bp3-slider-handle:hover, .bp3-dark .bp3-slider-handle:active, .bp3-dark .bp3-slider-handle.bp3-active{ color:#f5f8fa; } .bp3-dark .bp3-slider-handle:hover{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); background-color:#30404d; } .bp3-dark .bp3-slider-handle:active, .bp3-dark .bp3-slider-handle.bp3-active{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2); background-color:#202b33; background-image:none; } .bp3-dark .bp3-slider-handle:disabled, .bp3-dark .bp3-slider-handle.bp3-disabled{ -webkit-box-shadow:none; box-shadow:none; background-color:rgba(57, 75, 89, 0.5); background-image:none; color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-slider-handle:disabled.bp3-active, .bp3-dark .bp3-slider-handle.bp3-disabled.bp3-active{ background:rgba(57, 75, 89, 0.7); } .bp3-dark .bp3-slider-handle .bp3-button-spinner .bp3-spinner-head{ background:rgba(16, 22, 26, 0.5); stroke:#8a9ba8; } .bp3-dark .bp3-slider-handle, .bp3-dark .bp3-slider-handle:hover{ background-color:#394b59; } .bp3-dark .bp3-slider-handle.bp3-active{ background-color:#293742; } .bp3-dark .bp3-disabled .bp3-slider-handle{ border-color:#5c7080; -webkit-box-shadow:none; box-shadow:none; background:#5c7080; } .bp3-slider-handle .bp3-slider-label{ margin-left:8px; border-radius:3px; -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2); background:#394b59; color:#f5f8fa; } .bp3-dark .bp3-slider-handle .bp3-slider-label{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); background:#e1e8ed; color:#394b59; } .bp3-disabled .bp3-slider-handle .bp3-slider-label{ -webkit-box-shadow:none; box-shadow:none; } .bp3-slider-handle.bp3-start, .bp3-slider-handle.bp3-end{ width:8px; } .bp3-slider-handle.bp3-start{ border-top-right-radius:0; border-bottom-right-radius:0; } .bp3-slider-handle.bp3-end{ margin-left:8px; border-top-left-radius:0; border-bottom-left-radius:0; } .bp3-slider-handle.bp3-end .bp3-slider-label{ margin-left:0; } .bp3-slider-label{ -webkit-transform:translate(-50%, 20px); transform:translate(-50%, 20px); display:inline-block; position:absolute; padding:2px 5px; vertical-align:top; line-height:1; font-size:12px; } .bp3-slider.bp3-vertical{ width:40px; min-width:40px; height:150px; } .bp3-slider.bp3-vertical .bp3-slider-track, .bp3-slider.bp3-vertical .bp3-slider-progress{ top:0; bottom:0; left:5px; width:6px; height:auto; } .bp3-slider.bp3-vertical .bp3-slider-progress{ top:auto; } .bp3-slider.bp3-vertical .bp3-slider-label{ -webkit-transform:translate(20px, 50%); transform:translate(20px, 50%); } .bp3-slider.bp3-vertical .bp3-slider-handle{ top:auto; } .bp3-slider.bp3-vertical .bp3-slider-handle .bp3-slider-label{ margin-top:-8px; margin-left:0; } .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-end, .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-start{ margin-left:0; width:16px; height:8px; } .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-start{ border-top-left-radius:0; border-bottom-right-radius:3px; } .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-start .bp3-slider-label{ -webkit-transform:translate(20px); transform:translate(20px); } .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-end{ margin-bottom:8px; border-top-left-radius:3px; border-bottom-left-radius:0; border-bottom-right-radius:0; } @-webkit-keyframes pt-spinner-animation{ from{ -webkit-transform:rotate(0deg); transform:rotate(0deg); } to{ -webkit-transform:rotate(360deg); transform:rotate(360deg); } } @keyframes pt-spinner-animation{ from{ -webkit-transform:rotate(0deg); transform:rotate(0deg); } to{ -webkit-transform:rotate(360deg); transform:rotate(360deg); } } .bp3-spinner{ display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-align:center; -ms-flex-align:center; align-items:center; -webkit-box-pack:center; -ms-flex-pack:center; justify-content:center; overflow:visible; vertical-align:middle; } .bp3-spinner svg{ display:block; } .bp3-spinner path{ fill-opacity:0; } .bp3-spinner .bp3-spinner-head{ -webkit-transform-origin:center; transform-origin:center; -webkit-transition:stroke-dashoffset 200ms cubic-bezier(0.4, 1, 0.75, 0.9); transition:stroke-dashoffset 200ms cubic-bezier(0.4, 1, 0.75, 0.9); stroke:rgba(92, 112, 128, 0.8); stroke-linecap:round; } .bp3-spinner .bp3-spinner-track{ stroke:rgba(92, 112, 128, 0.2); } .bp3-spinner-animation{ -webkit-animation:pt-spinner-animation 500ms linear infinite; animation:pt-spinner-animation 500ms linear infinite; } .bp3-no-spin > .bp3-spinner-animation{ -webkit-animation:none; animation:none; } .bp3-dark .bp3-spinner .bp3-spinner-head{ stroke:#8a9ba8; } .bp3-dark .bp3-spinner .bp3-spinner-track{ stroke:rgba(16, 22, 26, 0.5); } .bp3-spinner.bp3-intent-primary .bp3-spinner-head{ stroke:#137cbd; } .bp3-spinner.bp3-intent-success .bp3-spinner-head{ stroke:#0f9960; } .bp3-spinner.bp3-intent-warning .bp3-spinner-head{ stroke:#d9822b; } .bp3-spinner.bp3-intent-danger .bp3-spinner-head{ stroke:#db3737; } .bp3-tabs.bp3-vertical{ display:-webkit-box; display:-ms-flexbox; display:flex; } .bp3-tabs.bp3-vertical > .bp3-tab-list{ -webkit-box-orient:vertical; -webkit-box-direction:normal; -ms-flex-direction:column; flex-direction:column; -webkit-box-align:start; -ms-flex-align:start; align-items:flex-start; } .bp3-tabs.bp3-vertical > .bp3-tab-list .bp3-tab{ border-radius:3px; width:100%; padding:0 10px; } .bp3-tabs.bp3-vertical > .bp3-tab-list .bp3-tab[aria-selected=\"true\"]{ -webkit-box-shadow:none; box-shadow:none; background-color:rgba(19, 124, 189, 0.2); } .bp3-tabs.bp3-vertical > .bp3-tab-list .bp3-tab-indicator-wrapper .bp3-tab-indicator{ top:0; right:0; bottom:0; left:0; border-radius:3px; background-color:rgba(19, 124, 189, 0.2); height:auto; } .bp3-tabs.bp3-vertical > .bp3-tab-panel{ margin-top:0; padding-left:20px; } .bp3-tab-list{ display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-flex:0; -ms-flex:0 0 auto; flex:0 0 auto; -webkit-box-align:end; -ms-flex-align:end; align-items:flex-end; position:relative; margin:0; border:none; padding:0; list-style:none; } .bp3-tab-list > *:not(:last-child){ margin-right:20px; } .bp3-tab{ overflow:hidden; text-overflow:ellipsis; white-space:nowrap; word-wrap:normal; -webkit-box-flex:0; -ms-flex:0 0 auto; flex:0 0 auto; position:relative; cursor:pointer; max-width:100%; vertical-align:top; line-height:30px; color:#182026; font-size:14px; } .bp3-tab a{ display:block; text-decoration:none; color:inherit; } .bp3-tab-indicator-wrapper ~ .bp3-tab{ -webkit-box-shadow:none !important; box-shadow:none !important; background-color:transparent !important; } .bp3-tab[aria-disabled=\"true\"]{ cursor:not-allowed; color:rgba(92, 112, 128, 0.6); } .bp3-tab[aria-selected=\"true\"]{ border-radius:0; -webkit-box-shadow:inset 0 -3px 0 #106ba3; box-shadow:inset 0 -3px 0 #106ba3; } .bp3-tab[aria-selected=\"true\"], .bp3-tab:not([aria-disabled=\"true\"]):hover{ color:#106ba3; } .bp3-tab:focus{ -moz-outline-radius:0; } .bp3-large > .bp3-tab{ line-height:40px; font-size:16px; } .bp3-tab-panel{ margin-top:20px; } .bp3-tab-panel[aria-hidden=\"true\"]{ display:none; } .bp3-tab-indicator-wrapper{ position:absolute; top:0; left:0; -webkit-transform:translateX(0), translateY(0); transform:translateX(0), translateY(0); -webkit-transition:height, width, -webkit-transform; transition:height, width, -webkit-transform; transition:height, transform, width; transition:height, transform, width, -webkit-transform; -webkit-transition-duration:200ms; transition-duration:200ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); pointer-events:none; } .bp3-tab-indicator-wrapper .bp3-tab-indicator{ position:absolute; right:0; bottom:0; left:0; background-color:#106ba3; height:3px; } .bp3-tab-indicator-wrapper.bp3-no-animation{ -webkit-transition:none; transition:none; } .bp3-dark .bp3-tab{ color:#f5f8fa; } .bp3-dark .bp3-tab[aria-disabled=\"true\"]{ color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-tab[aria-selected=\"true\"]{ -webkit-box-shadow:inset 0 -3px 0 #48aff0; box-shadow:inset 0 -3px 0 #48aff0; } .bp3-dark .bp3-tab[aria-selected=\"true\"], .bp3-dark .bp3-tab:not([aria-disabled=\"true\"]):hover{ color:#48aff0; } .bp3-dark .bp3-tab-indicator{ background-color:#48aff0; } .bp3-flex-expander{ -webkit-box-flex:1; -ms-flex:1 1; flex:1 1; } .bp3-tag{ display:-webkit-inline-box; display:-ms-inline-flexbox; display:inline-flex; -webkit-box-orient:horizontal; -webkit-box-direction:normal; -ms-flex-direction:row; flex-direction:row; -webkit-box-align:center; -ms-flex-align:center; align-items:center; position:relative; border:none; border-radius:3px; -webkit-box-shadow:none; box-shadow:none; background-color:#5c7080; min-width:20px; max-width:100%; min-height:20px; padding:2px 6px; line-height:16px; color:#f5f8fa; font-size:12px; } .bp3-tag.bp3-interactive{ cursor:pointer; } .bp3-tag.bp3-interactive:hover{ background-color:rgba(92, 112, 128, 0.85); } .bp3-tag.bp3-interactive.bp3-active, .bp3-tag.bp3-interactive:active{ background-color:rgba(92, 112, 128, 0.7); } .bp3-tag > *{ -webkit-box-flex:0; -ms-flex-positive:0; flex-grow:0; -ms-flex-negative:0; flex-shrink:0; } .bp3-tag > .bp3-fill{ -webkit-box-flex:1; -ms-flex-positive:1; flex-grow:1; -ms-flex-negative:1; flex-shrink:1; } .bp3-tag::before, .bp3-tag > *{ margin-right:4px; } .bp3-tag:empty::before, .bp3-tag > :last-child{ margin-right:0; } .bp3-tag:focus{ outline:rgba(19, 124, 189, 0.6) auto 2px; outline-offset:0; -moz-outline-radius:6px; } .bp3-tag.bp3-round{ border-radius:30px; padding-right:8px; padding-left:8px; } .bp3-dark .bp3-tag{ background-color:#bfccd6; color:#182026; } .bp3-dark .bp3-tag.bp3-interactive{ cursor:pointer; } .bp3-dark .bp3-tag.bp3-interactive:hover{ background-color:rgba(191, 204, 214, 0.85); } .bp3-dark .bp3-tag.bp3-interactive.bp3-active, .bp3-dark .bp3-tag.bp3-interactive:active{ background-color:rgba(191, 204, 214, 0.7); } .bp3-dark .bp3-tag > .bp3-icon, .bp3-dark .bp3-tag .bp3-icon-standard, .bp3-dark .bp3-tag .bp3-icon-large{ fill:currentColor; } .bp3-tag > .bp3-icon, .bp3-tag .bp3-icon-standard, .bp3-tag .bp3-icon-large{ fill:#ffffff; } .bp3-tag.bp3-large, .bp3-large .bp3-tag{ min-width:30px; min-height:30px; padding:0 10px; line-height:20px; font-size:14px; } .bp3-tag.bp3-large::before, .bp3-tag.bp3-large > *, .bp3-large .bp3-tag::before, .bp3-large .bp3-tag > *{ margin-right:7px; } .bp3-tag.bp3-large:empty::before, .bp3-tag.bp3-large > :last-child, .bp3-large .bp3-tag:empty::before, .bp3-large .bp3-tag > :last-child{ margin-right:0; } .bp3-tag.bp3-large.bp3-round, .bp3-large .bp3-tag.bp3-round{ padding-right:12px; padding-left:12px; } .bp3-tag.bp3-intent-primary{ background:#137cbd; color:#ffffff; } .bp3-tag.bp3-intent-primary.bp3-interactive{ cursor:pointer; } .bp3-tag.bp3-intent-primary.bp3-interactive:hover{ background-color:rgba(19, 124, 189, 0.85); } .bp3-tag.bp3-intent-primary.bp3-interactive.bp3-active, .bp3-tag.bp3-intent-primary.bp3-interactive:active{ background-color:rgba(19, 124, 189, 0.7); } .bp3-tag.bp3-intent-success{ background:#0f9960; color:#ffffff; } .bp3-tag.bp3-intent-success.bp3-interactive{ cursor:pointer; } .bp3-tag.bp3-intent-success.bp3-interactive:hover{ background-color:rgba(15, 153, 96, 0.85); } .bp3-tag.bp3-intent-success.bp3-interactive.bp3-active, .bp3-tag.bp3-intent-success.bp3-interactive:active{ background-color:rgba(15, 153, 96, 0.7); } .bp3-tag.bp3-intent-warning{ background:#d9822b; color:#ffffff; } .bp3-tag.bp3-intent-warning.bp3-interactive{ cursor:pointer; } .bp3-tag.bp3-intent-warning.bp3-interactive:hover{ background-color:rgba(217, 130, 43, 0.85); } .bp3-tag.bp3-intent-warning.bp3-interactive.bp3-active, .bp3-tag.bp3-intent-warning.bp3-interactive:active{ background-color:rgba(217, 130, 43, 0.7); } .bp3-tag.bp3-intent-danger{ background:#db3737; color:#ffffff; } .bp3-tag.bp3-intent-danger.bp3-interactive{ cursor:pointer; } .bp3-tag.bp3-intent-danger.bp3-interactive:hover{ background-color:rgba(219, 55, 55, 0.85); } .bp3-tag.bp3-intent-danger.bp3-interactive.bp3-active, .bp3-tag.bp3-intent-danger.bp3-interactive:active{ background-color:rgba(219, 55, 55, 0.7); } .bp3-tag.bp3-fill{ display:-webkit-box; display:-ms-flexbox; display:flex; width:100%; } .bp3-tag.bp3-minimal > .bp3-icon, .bp3-tag.bp3-minimal .bp3-icon-standard, .bp3-tag.bp3-minimal .bp3-icon-large{ fill:#5c7080; } .bp3-tag.bp3-minimal:not([class*=\"bp3-intent-\"]){ background-color:rgba(138, 155, 168, 0.2); color:#182026; } .bp3-tag.bp3-minimal:not([class*=\"bp3-intent-\"]).bp3-interactive{ cursor:pointer; } .bp3-tag.bp3-minimal:not([class*=\"bp3-intent-\"]).bp3-interactive:hover{ background-color:rgba(92, 112, 128, 0.3); } .bp3-tag.bp3-minimal:not([class*=\"bp3-intent-\"]).bp3-interactive.bp3-active, .bp3-tag.bp3-minimal:not([class*=\"bp3-intent-\"]).bp3-interactive:active{ background-color:rgba(92, 112, 128, 0.4); } .bp3-dark .bp3-tag.bp3-minimal:not([class*=\"bp3-intent-\"]){ color:#f5f8fa; } .bp3-dark .bp3-tag.bp3-minimal:not([class*=\"bp3-intent-\"]).bp3-interactive{ cursor:pointer; } .bp3-dark .bp3-tag.bp3-minimal:not([class*=\"bp3-intent-\"]).bp3-interactive:hover{ background-color:rgba(191, 204, 214, 0.3); } .bp3-dark .bp3-tag.bp3-minimal:not([class*=\"bp3-intent-\"]).bp3-interactive.bp3-active, .bp3-dark .bp3-tag.bp3-minimal:not([class*=\"bp3-intent-\"]).bp3-interactive:active{ background-color:rgba(191, 204, 214, 0.4); } .bp3-dark .bp3-tag.bp3-minimal:not([class*=\"bp3-intent-\"]) > .bp3-icon, .bp3-dark .bp3-tag.bp3-minimal:not([class*=\"bp3-intent-\"]) .bp3-icon-standard, .bp3-dark .bp3-tag.bp3-minimal:not([class*=\"bp3-intent-\"]) .bp3-icon-large{ fill:#a7b6c2; } .bp3-tag.bp3-minimal.bp3-intent-primary{ background-color:rgba(19, 124, 189, 0.15); color:#106ba3; } .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive{ cursor:pointer; } .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive:hover{ background-color:rgba(19, 124, 189, 0.25); } .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive.bp3-active, .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive:active{ background-color:rgba(19, 124, 189, 0.35); } .bp3-tag.bp3-minimal.bp3-intent-primary > .bp3-icon, .bp3-tag.bp3-minimal.bp3-intent-primary .bp3-icon-standard, .bp3-tag.bp3-minimal.bp3-intent-primary .bp3-icon-large{ fill:#137cbd; } .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary{ background-color:rgba(19, 124, 189, 0.25); color:#48aff0; } .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive{ cursor:pointer; } .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive:hover{ background-color:rgba(19, 124, 189, 0.35); } .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive.bp3-active, .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive:active{ background-color:rgba(19, 124, 189, 0.45); } .bp3-tag.bp3-minimal.bp3-intent-success{ background-color:rgba(15, 153, 96, 0.15); color:#0d8050; } .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive{ cursor:pointer; } .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive:hover{ background-color:rgba(15, 153, 96, 0.25); } .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive.bp3-active, .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive:active{ background-color:rgba(15, 153, 96, 0.35); } .bp3-tag.bp3-minimal.bp3-intent-success > .bp3-icon, .bp3-tag.bp3-minimal.bp3-intent-success .bp3-icon-standard, .bp3-tag.bp3-minimal.bp3-intent-success .bp3-icon-large{ fill:#0f9960; } .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success{ background-color:rgba(15, 153, 96, 0.25); color:#3dcc91; } .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive{ cursor:pointer; } .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive:hover{ background-color:rgba(15, 153, 96, 0.35); } .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive.bp3-active, .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive:active{ background-color:rgba(15, 153, 96, 0.45); } .bp3-tag.bp3-minimal.bp3-intent-warning{ background-color:rgba(217, 130, 43, 0.15); color:#bf7326; } .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive{ cursor:pointer; } .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive:hover{ background-color:rgba(217, 130, 43, 0.25); } .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive.bp3-active, .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive:active{ background-color:rgba(217, 130, 43, 0.35); } .bp3-tag.bp3-minimal.bp3-intent-warning > .bp3-icon, .bp3-tag.bp3-minimal.bp3-intent-warning .bp3-icon-standard, .bp3-tag.bp3-minimal.bp3-intent-warning .bp3-icon-large{ fill:#d9822b; } .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning{ background-color:rgba(217, 130, 43, 0.25); color:#ffb366; } .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive{ cursor:pointer; } .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive:hover{ background-color:rgba(217, 130, 43, 0.35); } .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive.bp3-active, .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive:active{ background-color:rgba(217, 130, 43, 0.45); } .bp3-tag.bp3-minimal.bp3-intent-danger{ background-color:rgba(219, 55, 55, 0.15); color:#c23030; } .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive{ cursor:pointer; } .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive:hover{ background-color:rgba(219, 55, 55, 0.25); } .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive.bp3-active, .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive:active{ background-color:rgba(219, 55, 55, 0.35); } .bp3-tag.bp3-minimal.bp3-intent-danger > .bp3-icon, .bp3-tag.bp3-minimal.bp3-intent-danger .bp3-icon-standard, .bp3-tag.bp3-minimal.bp3-intent-danger .bp3-icon-large{ fill:#db3737; } .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger{ background-color:rgba(219, 55, 55, 0.25); color:#ff7373; } .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive{ cursor:pointer; } .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive:hover{ background-color:rgba(219, 55, 55, 0.35); } .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive.bp3-active, .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive:active{ background-color:rgba(219, 55, 55, 0.45); } .bp3-tag-remove{ display:-webkit-box; display:-ms-flexbox; display:flex; opacity:0.5; margin-top:-2px; margin-right:-6px !important; margin-bottom:-2px; border:none; background:none; cursor:pointer; padding:2px; padding-left:0; color:inherit; } .bp3-tag-remove:hover{ opacity:0.8; background:none; text-decoration:none; } .bp3-tag-remove:active{ opacity:1; } .bp3-tag-remove:empty::before{ line-height:1; font-family:\"Icons16\", sans-serif; font-size:16px; font-weight:400; font-style:normal; -moz-osx-font-smoothing:grayscale; -webkit-font-smoothing:antialiased; content:\"\ue6d7\"; } .bp3-large .bp3-tag-remove{ margin-right:-10px !important; padding:5px; padding-left:0; } .bp3-large .bp3-tag-remove:empty::before{ line-height:1; font-family:\"Icons20\", sans-serif; font-size:20px; font-weight:400; font-style:normal; } .bp3-tag-input{ display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-orient:horizontal; -webkit-box-direction:normal; -ms-flex-direction:row; flex-direction:row; -webkit-box-align:start; -ms-flex-align:start; align-items:flex-start; cursor:text; height:auto; min-height:30px; padding-right:0; padding-left:5px; line-height:inherit; } .bp3-tag-input > *{ -webkit-box-flex:0; -ms-flex-positive:0; flex-grow:0; -ms-flex-negative:0; flex-shrink:0; } .bp3-tag-input > .bp3-tag-input-values{ -webkit-box-flex:1; -ms-flex-positive:1; flex-grow:1; -ms-flex-negative:1; flex-shrink:1; } .bp3-tag-input .bp3-tag-input-icon{ margin-top:7px; margin-right:7px; margin-left:2px; color:#5c7080; } .bp3-tag-input .bp3-tag-input-values{ display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-orient:horizontal; -webkit-box-direction:normal; -ms-flex-direction:row; flex-direction:row; -ms-flex-wrap:wrap; flex-wrap:wrap; -webkit-box-align:center; -ms-flex-align:center; align-items:center; -ms-flex-item-align:stretch; align-self:stretch; margin-top:5px; margin-right:7px; min-width:0; } .bp3-tag-input .bp3-tag-input-values > *{ -webkit-box-flex:0; -ms-flex-positive:0; flex-grow:0; -ms-flex-negative:0; flex-shrink:0; } .bp3-tag-input .bp3-tag-input-values > .bp3-fill{ -webkit-box-flex:1; -ms-flex-positive:1; flex-grow:1; -ms-flex-negative:1; flex-shrink:1; } .bp3-tag-input .bp3-tag-input-values::before, .bp3-tag-input .bp3-tag-input-values > *{ margin-right:5px; } .bp3-tag-input .bp3-tag-input-values:empty::before, .bp3-tag-input .bp3-tag-input-values > :last-child{ margin-right:0; } .bp3-tag-input .bp3-tag-input-values:first-child .bp3-input-ghost:first-child{ padding-left:5px; } .bp3-tag-input .bp3-tag-input-values > *{ margin-bottom:5px; } .bp3-tag-input .bp3-tag{ overflow-wrap:break-word; } .bp3-tag-input .bp3-tag.bp3-active{ outline:rgba(19, 124, 189, 0.6) auto 2px; outline-offset:0; -moz-outline-radius:6px; } .bp3-tag-input .bp3-input-ghost{ -webkit-box-flex:1; -ms-flex:1 1 auto; flex:1 1 auto; width:80px; line-height:20px; } .bp3-tag-input .bp3-input-ghost:disabled, .bp3-tag-input .bp3-input-ghost.bp3-disabled{ cursor:not-allowed; } .bp3-tag-input .bp3-button, .bp3-tag-input .bp3-spinner{ margin:3px; margin-left:0; } .bp3-tag-input .bp3-button{ min-width:24px; min-height:24px; padding:0 7px; } .bp3-tag-input.bp3-large{ height:auto; min-height:40px; } .bp3-tag-input.bp3-large::before, .bp3-tag-input.bp3-large > *{ margin-right:10px; } .bp3-tag-input.bp3-large:empty::before, .bp3-tag-input.bp3-large > :last-child{ margin-right:0; } .bp3-tag-input.bp3-large .bp3-tag-input-icon{ margin-top:10px; margin-left:5px; } .bp3-tag-input.bp3-large .bp3-input-ghost{ line-height:30px; } .bp3-tag-input.bp3-large .bp3-button{ min-width:30px; min-height:30px; padding:5px 10px; margin:5px; margin-left:0; } .bp3-tag-input.bp3-large .bp3-spinner{ margin:8px; margin-left:0; } .bp3-tag-input.bp3-active{ -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); background-color:#ffffff; } .bp3-tag-input.bp3-active.bp3-intent-primary{ -webkit-box-shadow:0 0 0 1px #106ba3, 0 0 0 3px rgba(16, 107, 163, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px #106ba3, 0 0 0 3px rgba(16, 107, 163, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-tag-input.bp3-active.bp3-intent-success{ -webkit-box-shadow:0 0 0 1px #0d8050, 0 0 0 3px rgba(13, 128, 80, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px #0d8050, 0 0 0 3px rgba(13, 128, 80, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-tag-input.bp3-active.bp3-intent-warning{ -webkit-box-shadow:0 0 0 1px #bf7326, 0 0 0 3px rgba(191, 115, 38, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px #bf7326, 0 0 0 3px rgba(191, 115, 38, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-tag-input.bp3-active.bp3-intent-danger{ -webkit-box-shadow:0 0 0 1px #c23030, 0 0 0 3px rgba(194, 48, 48, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px #c23030, 0 0 0 3px rgba(194, 48, 48, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-dark .bp3-tag-input .bp3-tag-input-icon, .bp3-tag-input.bp3-dark .bp3-tag-input-icon{ color:#a7b6c2; } .bp3-dark .bp3-tag-input .bp3-input-ghost, .bp3-tag-input.bp3-dark .bp3-input-ghost{ color:#f5f8fa; } .bp3-dark .bp3-tag-input .bp3-input-ghost::-webkit-input-placeholder, .bp3-tag-input.bp3-dark .bp3-input-ghost::-webkit-input-placeholder{ color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-tag-input .bp3-input-ghost::-moz-placeholder, .bp3-tag-input.bp3-dark .bp3-input-ghost::-moz-placeholder{ color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-tag-input .bp3-input-ghost:-ms-input-placeholder, .bp3-tag-input.bp3-dark .bp3-input-ghost:-ms-input-placeholder{ color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-tag-input .bp3-input-ghost::-ms-input-placeholder, .bp3-tag-input.bp3-dark .bp3-input-ghost::-ms-input-placeholder{ color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-tag-input .bp3-input-ghost::placeholder, .bp3-tag-input.bp3-dark .bp3-input-ghost::placeholder{ color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-tag-input.bp3-active, .bp3-tag-input.bp3-dark.bp3-active{ -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); background-color:rgba(16, 22, 26, 0.3); } .bp3-dark .bp3-tag-input.bp3-active.bp3-intent-primary, .bp3-tag-input.bp3-dark.bp3-active.bp3-intent-primary{ -webkit-box-shadow:0 0 0 1px #106ba3, 0 0 0 3px rgba(16, 107, 163, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px #106ba3, 0 0 0 3px rgba(16, 107, 163, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-tag-input.bp3-active.bp3-intent-success, .bp3-tag-input.bp3-dark.bp3-active.bp3-intent-success{ -webkit-box-shadow:0 0 0 1px #0d8050, 0 0 0 3px rgba(13, 128, 80, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px #0d8050, 0 0 0 3px rgba(13, 128, 80, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-tag-input.bp3-active.bp3-intent-warning, .bp3-tag-input.bp3-dark.bp3-active.bp3-intent-warning{ -webkit-box-shadow:0 0 0 1px #bf7326, 0 0 0 3px rgba(191, 115, 38, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px #bf7326, 0 0 0 3px rgba(191, 115, 38, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-tag-input.bp3-active.bp3-intent-danger, .bp3-tag-input.bp3-dark.bp3-active.bp3-intent-danger{ -webkit-box-shadow:0 0 0 1px #c23030, 0 0 0 3px rgba(194, 48, 48, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px #c23030, 0 0 0 3px rgba(194, 48, 48, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); } .bp3-input-ghost{ border:none; -webkit-box-shadow:none; box-shadow:none; background:none; padding:0; } .bp3-input-ghost::-webkit-input-placeholder{ opacity:1; color:rgba(92, 112, 128, 0.6); } .bp3-input-ghost::-moz-placeholder{ opacity:1; color:rgba(92, 112, 128, 0.6); } .bp3-input-ghost:-ms-input-placeholder{ opacity:1; color:rgba(92, 112, 128, 0.6); } .bp3-input-ghost::-ms-input-placeholder{ opacity:1; color:rgba(92, 112, 128, 0.6); } .bp3-input-ghost::placeholder{ opacity:1; color:rgba(92, 112, 128, 0.6); } .bp3-input-ghost:focus{ outline:none !important; } .bp3-toast{ display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-align:start; -ms-flex-align:start; align-items:flex-start; position:relative !important; margin:20px 0 0; border-radius:3px; -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2); background-color:#ffffff; min-width:300px; max-width:500px; pointer-events:all; } .bp3-toast.bp3-toast-enter, .bp3-toast.bp3-toast-appear{ -webkit-transform:translateY(-40px); transform:translateY(-40px); } .bp3-toast.bp3-toast-enter-active, .bp3-toast.bp3-toast-appear-active{ -webkit-transform:translateY(0); transform:translateY(0); -webkit-transition-property:-webkit-transform; transition-property:-webkit-transform; transition-property:transform; transition-property:transform, -webkit-transform; -webkit-transition-duration:300ms; transition-duration:300ms; -webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11); transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11); -webkit-transition-delay:0; transition-delay:0; } .bp3-toast.bp3-toast-enter ~ .bp3-toast, .bp3-toast.bp3-toast-appear ~ .bp3-toast{ -webkit-transform:translateY(-40px); transform:translateY(-40px); } .bp3-toast.bp3-toast-enter-active ~ .bp3-toast, .bp3-toast.bp3-toast-appear-active ~ .bp3-toast{ -webkit-transform:translateY(0); transform:translateY(0); -webkit-transition-property:-webkit-transform; transition-property:-webkit-transform; transition-property:transform; transition-property:transform, -webkit-transform; -webkit-transition-duration:300ms; transition-duration:300ms; -webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11); transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11); -webkit-transition-delay:0; transition-delay:0; } .bp3-toast.bp3-toast-exit{ opacity:1; -webkit-filter:blur(0); filter:blur(0); } .bp3-toast.bp3-toast-exit-active{ opacity:0; -webkit-filter:blur(10px); filter:blur(10px); -webkit-transition-property:opacity, -webkit-filter; transition-property:opacity, -webkit-filter; transition-property:opacity, filter; transition-property:opacity, filter, -webkit-filter; -webkit-transition-duration:300ms; transition-duration:300ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-transition-delay:0; transition-delay:0; } .bp3-toast.bp3-toast-exit ~ .bp3-toast{ -webkit-transform:translateY(0); transform:translateY(0); } .bp3-toast.bp3-toast-exit-active ~ .bp3-toast{ -webkit-transform:translateY(-40px); transform:translateY(-40px); -webkit-transition-property:-webkit-transform; transition-property:-webkit-transform; transition-property:transform; transition-property:transform, -webkit-transform; -webkit-transition-duration:100ms; transition-duration:100ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-transition-delay:50ms; transition-delay:50ms; } .bp3-toast .bp3-button-group{ -webkit-box-flex:0; -ms-flex:0 0 auto; flex:0 0 auto; padding:5px; padding-left:0; } .bp3-toast > .bp3-icon{ margin:12px; margin-right:0; color:#5c7080; } .bp3-toast.bp3-dark, .bp3-dark .bp3-toast{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); background-color:#394b59; } .bp3-toast.bp3-dark > .bp3-icon, .bp3-dark .bp3-toast > .bp3-icon{ color:#a7b6c2; } .bp3-toast[class*=\"bp3-intent-\"] a{ color:rgba(255, 255, 255, 0.7); } .bp3-toast[class*=\"bp3-intent-\"] a:hover{ color:#ffffff; } .bp3-toast[class*=\"bp3-intent-\"] > .bp3-icon{ color:#ffffff; } .bp3-toast[class*=\"bp3-intent-\"] .bp3-button, .bp3-toast[class*=\"bp3-intent-\"] .bp3-button::before, .bp3-toast[class*=\"bp3-intent-\"] .bp3-button .bp3-icon, .bp3-toast[class*=\"bp3-intent-\"] .bp3-button:active{ color:rgba(255, 255, 255, 0.7) !important; } .bp3-toast[class*=\"bp3-intent-\"] .bp3-button:focus{ outline-color:rgba(255, 255, 255, 0.5); } .bp3-toast[class*=\"bp3-intent-\"] .bp3-button:hover{ background-color:rgba(255, 255, 255, 0.15) !important; color:#ffffff !important; } .bp3-toast[class*=\"bp3-intent-\"] .bp3-button:active{ background-color:rgba(255, 255, 255, 0.3) !important; color:#ffffff !important; } .bp3-toast[class*=\"bp3-intent-\"] .bp3-button::after{ background:rgba(255, 255, 255, 0.3) !important; } .bp3-toast.bp3-intent-primary{ background-color:#137cbd; color:#ffffff; } .bp3-toast.bp3-intent-success{ background-color:#0f9960; color:#ffffff; } .bp3-toast.bp3-intent-warning{ background-color:#d9822b; color:#ffffff; } .bp3-toast.bp3-intent-danger{ background-color:#db3737; color:#ffffff; } .bp3-toast-message{ -webkit-box-flex:1; -ms-flex:1 1 auto; flex:1 1 auto; padding:11px; word-break:break-word; } .bp3-toast-container{ display:-webkit-box !important; display:-ms-flexbox !important; display:flex !important; -webkit-box-orient:vertical; -webkit-box-direction:normal; -ms-flex-direction:column; flex-direction:column; -webkit-box-align:center; -ms-flex-align:center; align-items:center; position:fixed; right:0; left:0; z-index:40; overflow:hidden; padding:0 20px 20px; pointer-events:none; } .bp3-toast-container.bp3-toast-container-top{ top:0; bottom:auto; } .bp3-toast-container.bp3-toast-container-bottom{ -webkit-box-orient:vertical; -webkit-box-direction:reverse; -ms-flex-direction:column-reverse; flex-direction:column-reverse; top:auto; bottom:0; } .bp3-toast-container.bp3-toast-container-left{ -webkit-box-align:start; -ms-flex-align:start; align-items:flex-start; } .bp3-toast-container.bp3-toast-container-right{ -webkit-box-align:end; -ms-flex-align:end; align-items:flex-end; } .bp3-toast-container-bottom .bp3-toast.bp3-toast-enter:not(.bp3-toast-enter-active), .bp3-toast-container-bottom .bp3-toast.bp3-toast-enter:not(.bp3-toast-enter-active) ~ .bp3-toast, .bp3-toast-container-bottom .bp3-toast.bp3-toast-appear:not(.bp3-toast-appear-active), .bp3-toast-container-bottom .bp3-toast.bp3-toast-appear:not(.bp3-toast-appear-active) ~ .bp3-toast, .bp3-toast-container-bottom .bp3-toast.bp3-toast-leave-active ~ .bp3-toast{ -webkit-transform:translateY(60px); transform:translateY(60px); } .bp3-tooltip{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2); -webkit-transform:scale(1); transform:scale(1); } .bp3-tooltip .bp3-popover-arrow{ position:absolute; width:22px; height:22px; } .bp3-tooltip .bp3-popover-arrow::before{ margin:4px; width:14px; height:14px; } .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top > .bp3-tooltip{ margin-top:-11px; margin-bottom:11px; } .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top > .bp3-tooltip > .bp3-popover-arrow{ bottom:-8px; } .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top > .bp3-tooltip > .bp3-popover-arrow svg{ -webkit-transform:rotate(-90deg); transform:rotate(-90deg); } .bp3-tether-element-attached-left.bp3-tether-target-attached-right > .bp3-tooltip{ margin-left:11px; } .bp3-tether-element-attached-left.bp3-tether-target-attached-right > .bp3-tooltip > .bp3-popover-arrow{ left:-8px; } .bp3-tether-element-attached-left.bp3-tether-target-attached-right > .bp3-tooltip > .bp3-popover-arrow svg{ -webkit-transform:rotate(0); transform:rotate(0); } .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom > .bp3-tooltip{ margin-top:11px; } .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom > .bp3-tooltip > .bp3-popover-arrow{ top:-8px; } .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom > .bp3-tooltip > .bp3-popover-arrow svg{ -webkit-transform:rotate(90deg); transform:rotate(90deg); } .bp3-tether-element-attached-right.bp3-tether-target-attached-left > .bp3-tooltip{ margin-right:11px; margin-left:-11px; } .bp3-tether-element-attached-right.bp3-tether-target-attached-left > .bp3-tooltip > .bp3-popover-arrow{ right:-8px; } .bp3-tether-element-attached-right.bp3-tether-target-attached-left > .bp3-tooltip > .bp3-popover-arrow svg{ -webkit-transform:rotate(180deg); transform:rotate(180deg); } .bp3-tether-element-attached-middle > .bp3-tooltip > .bp3-popover-arrow{ top:50%; -webkit-transform:translateY(-50%); transform:translateY(-50%); } .bp3-tether-element-attached-center > .bp3-tooltip > .bp3-popover-arrow{ right:50%; -webkit-transform:translateX(50%); transform:translateX(50%); } .bp3-tether-element-attached-top.bp3-tether-target-attached-top > .bp3-tooltip > .bp3-popover-arrow{ top:-0.22183px; } .bp3-tether-element-attached-right.bp3-tether-target-attached-right > .bp3-tooltip > .bp3-popover-arrow{ right:-0.22183px; } .bp3-tether-element-attached-left.bp3-tether-target-attached-left > .bp3-tooltip > .bp3-popover-arrow{ left:-0.22183px; } .bp3-tether-element-attached-bottom.bp3-tether-target-attached-bottom > .bp3-tooltip > .bp3-popover-arrow{ bottom:-0.22183px; } .bp3-tether-element-attached-top.bp3-tether-element-attached-left > .bp3-tooltip{ -webkit-transform-origin:top left; transform-origin:top left; } .bp3-tether-element-attached-top.bp3-tether-element-attached-center > .bp3-tooltip{ -webkit-transform-origin:top center; transform-origin:top center; } .bp3-tether-element-attached-top.bp3-tether-element-attached-right > .bp3-tooltip{ -webkit-transform-origin:top right; transform-origin:top right; } .bp3-tether-element-attached-middle.bp3-tether-element-attached-left > .bp3-tooltip{ -webkit-transform-origin:center left; transform-origin:center left; } .bp3-tether-element-attached-middle.bp3-tether-element-attached-center > .bp3-tooltip{ -webkit-transform-origin:center center; transform-origin:center center; } .bp3-tether-element-attached-middle.bp3-tether-element-attached-right > .bp3-tooltip{ -webkit-transform-origin:center right; transform-origin:center right; } .bp3-tether-element-attached-bottom.bp3-tether-element-attached-left > .bp3-tooltip{ -webkit-transform-origin:bottom left; transform-origin:bottom left; } .bp3-tether-element-attached-bottom.bp3-tether-element-attached-center > .bp3-tooltip{ -webkit-transform-origin:bottom center; transform-origin:bottom center; } .bp3-tether-element-attached-bottom.bp3-tether-element-attached-right > .bp3-tooltip{ -webkit-transform-origin:bottom right; transform-origin:bottom right; } .bp3-tooltip .bp3-popover-content{ background:#394b59; color:#f5f8fa; } .bp3-tooltip .bp3-popover-arrow::before{ -webkit-box-shadow:1px 1px 6px rgba(16, 22, 26, 0.2); box-shadow:1px 1px 6px rgba(16, 22, 26, 0.2); } .bp3-tooltip .bp3-popover-arrow-border{ fill:#10161a; fill-opacity:0.1; } .bp3-tooltip .bp3-popover-arrow-fill{ fill:#394b59; } .bp3-popover-enter > .bp3-tooltip, .bp3-popover-appear > .bp3-tooltip{ -webkit-transform:scale(0.8); transform:scale(0.8); } .bp3-popover-enter-active > .bp3-tooltip, .bp3-popover-appear-active > .bp3-tooltip{ -webkit-transform:scale(1); transform:scale(1); -webkit-transition-property:-webkit-transform; transition-property:-webkit-transform; transition-property:transform; transition-property:transform, -webkit-transform; -webkit-transition-duration:100ms; transition-duration:100ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-transition-delay:0; transition-delay:0; } .bp3-popover-exit > .bp3-tooltip{ -webkit-transform:scale(1); transform:scale(1); } .bp3-popover-exit-active > .bp3-tooltip{ -webkit-transform:scale(0.8); transform:scale(0.8); -webkit-transition-property:-webkit-transform; transition-property:-webkit-transform; transition-property:transform; transition-property:transform, -webkit-transform; -webkit-transition-duration:100ms; transition-duration:100ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-transition-delay:0; transition-delay:0; } .bp3-tooltip .bp3-popover-content{ padding:10px 12px; } .bp3-tooltip.bp3-dark, .bp3-dark .bp3-tooltip{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); } .bp3-tooltip.bp3-dark .bp3-popover-content, .bp3-dark .bp3-tooltip .bp3-popover-content{ background:#e1e8ed; color:#394b59; } .bp3-tooltip.bp3-dark .bp3-popover-arrow::before, .bp3-dark .bp3-tooltip .bp3-popover-arrow::before{ -webkit-box-shadow:1px 1px 6px rgba(16, 22, 26, 0.4); box-shadow:1px 1px 6px rgba(16, 22, 26, 0.4); } .bp3-tooltip.bp3-dark .bp3-popover-arrow-border, .bp3-dark .bp3-tooltip .bp3-popover-arrow-border{ fill:#10161a; fill-opacity:0.2; } .bp3-tooltip.bp3-dark .bp3-popover-arrow-fill, .bp3-dark .bp3-tooltip .bp3-popover-arrow-fill{ fill:#e1e8ed; } .bp3-tooltip.bp3-intent-primary .bp3-popover-content{ background:#137cbd; color:#ffffff; } .bp3-tooltip.bp3-intent-primary .bp3-popover-arrow-fill{ fill:#137cbd; } .bp3-tooltip.bp3-intent-success .bp3-popover-content{ background:#0f9960; color:#ffffff; } .bp3-tooltip.bp3-intent-success .bp3-popover-arrow-fill{ fill:#0f9960; } .bp3-tooltip.bp3-intent-warning .bp3-popover-content{ background:#d9822b; color:#ffffff; } .bp3-tooltip.bp3-intent-warning .bp3-popover-arrow-fill{ fill:#d9822b; } .bp3-tooltip.bp3-intent-danger .bp3-popover-content{ background:#db3737; color:#ffffff; } .bp3-tooltip.bp3-intent-danger .bp3-popover-arrow-fill{ fill:#db3737; } .bp3-tooltip-indicator{ border-bottom:dotted 1px; cursor:help; } .bp3-tree .bp3-icon, .bp3-tree .bp3-icon-standard, .bp3-tree .bp3-icon-large{ color:#5c7080; } .bp3-tree .bp3-icon.bp3-intent-primary, .bp3-tree .bp3-icon-standard.bp3-intent-primary, .bp3-tree .bp3-icon-large.bp3-intent-primary{ color:#137cbd; } .bp3-tree .bp3-icon.bp3-intent-success, .bp3-tree .bp3-icon-standard.bp3-intent-success, .bp3-tree .bp3-icon-large.bp3-intent-success{ color:#0f9960; } .bp3-tree .bp3-icon.bp3-intent-warning, .bp3-tree .bp3-icon-standard.bp3-intent-warning, .bp3-tree .bp3-icon-large.bp3-intent-warning{ color:#d9822b; } .bp3-tree .bp3-icon.bp3-intent-danger, .bp3-tree .bp3-icon-standard.bp3-intent-danger, .bp3-tree .bp3-icon-large.bp3-intent-danger{ color:#db3737; } .bp3-tree-node-list{ margin:0; padding-left:0; list-style:none; } .bp3-tree-root{ position:relative; background-color:transparent; cursor:default; padding-left:0; } .bp3-tree-node-content-0{ padding-left:0px; } .bp3-tree-node-content-1{ padding-left:23px; } .bp3-tree-node-content-2{ padding-left:46px; } .bp3-tree-node-content-3{ padding-left:69px; } .bp3-tree-node-content-4{ padding-left:92px; } .bp3-tree-node-content-5{ padding-left:115px; } .bp3-tree-node-content-6{ padding-left:138px; } .bp3-tree-node-content-7{ padding-left:161px; } .bp3-tree-node-content-8{ padding-left:184px; } .bp3-tree-node-content-9{ padding-left:207px; } .bp3-tree-node-content-10{ padding-left:230px; } .bp3-tree-node-content-11{ padding-left:253px; } .bp3-tree-node-content-12{ padding-left:276px; } .bp3-tree-node-content-13{ padding-left:299px; } .bp3-tree-node-content-14{ padding-left:322px; } .bp3-tree-node-content-15{ padding-left:345px; } .bp3-tree-node-content-16{ padding-left:368px; } .bp3-tree-node-content-17{ padding-left:391px; } .bp3-tree-node-content-18{ padding-left:414px; } .bp3-tree-node-content-19{ padding-left:437px; } .bp3-tree-node-content-20{ padding-left:460px; } .bp3-tree-node-content{ display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-align:center; -ms-flex-align:center; align-items:center; width:100%; height:30px; padding-right:5px; } .bp3-tree-node-content:hover{ background-color:rgba(191, 204, 214, 0.4); } .bp3-tree-node-caret, .bp3-tree-node-caret-none{ min-width:30px; } .bp3-tree-node-caret{ color:#5c7080; -webkit-transform:rotate(0deg); transform:rotate(0deg); cursor:pointer; padding:7px; -webkit-transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9); transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9); transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9); transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9); } .bp3-tree-node-caret:hover{ color:#182026; } .bp3-dark .bp3-tree-node-caret{ color:#a7b6c2; } .bp3-dark .bp3-tree-node-caret:hover{ color:#f5f8fa; } .bp3-tree-node-caret.bp3-tree-node-caret-open{ -webkit-transform:rotate(90deg); transform:rotate(90deg); } .bp3-tree-node-caret.bp3-icon-standard::before{ content:\"\ue695\"; } .bp3-tree-node-icon{ position:relative; margin-right:7px; } .bp3-tree-node-label{ overflow:hidden; text-overflow:ellipsis; white-space:nowrap; word-wrap:normal; -webkit-box-flex:1; -ms-flex:1 1 auto; flex:1 1 auto; position:relative; -webkit-user-select:none; -moz-user-select:none; -ms-user-select:none; user-select:none; } .bp3-tree-node-label span{ display:inline; } .bp3-tree-node-secondary-label{ padding:0 5px; -webkit-user-select:none; -moz-user-select:none; -ms-user-select:none; user-select:none; } .bp3-tree-node-secondary-label .bp3-popover-wrapper, .bp3-tree-node-secondary-label .bp3-popover-target{ display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-align:center; -ms-flex-align:center; align-items:center; } .bp3-tree-node.bp3-disabled .bp3-tree-node-content{ background-color:inherit; cursor:not-allowed; color:rgba(92, 112, 128, 0.6); } .bp3-tree-node.bp3-disabled .bp3-tree-node-caret, .bp3-tree-node.bp3-disabled .bp3-tree-node-icon{ cursor:not-allowed; color:rgba(92, 112, 128, 0.6); } .bp3-tree-node.bp3-tree-node-selected > .bp3-tree-node-content{ background-color:#137cbd; } .bp3-tree-node.bp3-tree-node-selected > .bp3-tree-node-content, .bp3-tree-node.bp3-tree-node-selected > .bp3-tree-node-content .bp3-icon, .bp3-tree-node.bp3-tree-node-selected > .bp3-tree-node-content .bp3-icon-standard, .bp3-tree-node.bp3-tree-node-selected > .bp3-tree-node-content .bp3-icon-large{ color:#ffffff; } .bp3-tree-node.bp3-tree-node-selected > .bp3-tree-node-content .bp3-tree-node-caret::before{ color:rgba(255, 255, 255, 0.7); } .bp3-tree-node.bp3-tree-node-selected > .bp3-tree-node-content .bp3-tree-node-caret:hover::before{ color:#ffffff; } .bp3-dark .bp3-tree-node-content:hover{ background-color:rgba(92, 112, 128, 0.3); } .bp3-dark .bp3-tree .bp3-icon, .bp3-dark .bp3-tree .bp3-icon-standard, .bp3-dark .bp3-tree .bp3-icon-large{ color:#a7b6c2; } .bp3-dark .bp3-tree .bp3-icon.bp3-intent-primary, .bp3-dark .bp3-tree .bp3-icon-standard.bp3-intent-primary, .bp3-dark .bp3-tree .bp3-icon-large.bp3-intent-primary{ color:#137cbd; } .bp3-dark .bp3-tree .bp3-icon.bp3-intent-success, .bp3-dark .bp3-tree .bp3-icon-standard.bp3-intent-success, .bp3-dark .bp3-tree .bp3-icon-large.bp3-intent-success{ color:#0f9960; } .bp3-dark .bp3-tree .bp3-icon.bp3-intent-warning, .bp3-dark .bp3-tree .bp3-icon-standard.bp3-intent-warning, .bp3-dark .bp3-tree .bp3-icon-large.bp3-intent-warning{ color:#d9822b; } .bp3-dark .bp3-tree .bp3-icon.bp3-intent-danger, .bp3-dark .bp3-tree .bp3-icon-standard.bp3-intent-danger, .bp3-dark .bp3-tree .bp3-icon-large.bp3-intent-danger{ color:#db3737; } .bp3-dark .bp3-tree-node.bp3-tree-node-selected > .bp3-tree-node-content{ background-color:#137cbd; } /*! Copyright 2017-present Palantir Technologies, Inc. All rights reserved. Licensed under the Apache License, Version 2.0. */ .bp3-omnibar{ -webkit-filter:blur(0); filter:blur(0); opacity:1; top:20vh; left:calc(50% - 250px); z-index:21; border-radius:3px; -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 4px 8px rgba(16, 22, 26, 0.2), 0 18px 46px 6px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 4px 8px rgba(16, 22, 26, 0.2), 0 18px 46px 6px rgba(16, 22, 26, 0.2); background-color:#ffffff; width:500px; } .bp3-omnibar.bp3-overlay-enter, .bp3-omnibar.bp3-overlay-appear{ -webkit-filter:blur(20px); filter:blur(20px); opacity:0.2; } .bp3-omnibar.bp3-overlay-enter-active, .bp3-omnibar.bp3-overlay-appear-active{ -webkit-filter:blur(0); filter:blur(0); opacity:1; -webkit-transition-property:opacity, -webkit-filter; transition-property:opacity, -webkit-filter; transition-property:filter, opacity; transition-property:filter, opacity, -webkit-filter; -webkit-transition-duration:200ms; transition-duration:200ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-transition-delay:0; transition-delay:0; } .bp3-omnibar.bp3-overlay-exit{ -webkit-filter:blur(0); filter:blur(0); opacity:1; } .bp3-omnibar.bp3-overlay-exit-active{ -webkit-filter:blur(20px); filter:blur(20px); opacity:0.2; -webkit-transition-property:opacity, -webkit-filter; transition-property:opacity, -webkit-filter; transition-property:filter, opacity; transition-property:filter, opacity, -webkit-filter; -webkit-transition-duration:200ms; transition-duration:200ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-transition-delay:0; transition-delay:0; } .bp3-omnibar .bp3-input{ border-radius:0; background-color:transparent; } .bp3-omnibar .bp3-input, .bp3-omnibar .bp3-input:focus{ -webkit-box-shadow:none; box-shadow:none; } .bp3-omnibar .bp3-menu{ border-radius:0; -webkit-box-shadow:inset 0 1px 0 rgba(16, 22, 26, 0.15); box-shadow:inset 0 1px 0 rgba(16, 22, 26, 0.15); background-color:transparent; max-height:calc(60vh - 40px); overflow:auto; } .bp3-omnibar .bp3-menu:empty{ display:none; } .bp3-dark .bp3-omnibar, .bp3-omnibar.bp3-dark{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 4px 8px rgba(16, 22, 26, 0.4), 0 18px 46px 6px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 4px 8px rgba(16, 22, 26, 0.4), 0 18px 46px 6px rgba(16, 22, 26, 0.4); background-color:#30404d; } .bp3-omnibar-overlay .bp3-overlay-backdrop{ background-color:rgba(16, 22, 26, 0.2); } .bp3-select-popover .bp3-popover-content{ padding:5px; } .bp3-select-popover .bp3-input-group{ margin-bottom:0; } .bp3-select-popover .bp3-menu{ max-width:400px; max-height:300px; overflow:auto; padding:0; } .bp3-select-popover .bp3-menu:not(:first-child){ padding-top:5px; } .bp3-multi-select{ min-width:150px; } .bp3-multi-select-popover .bp3-menu{ max-width:400px; max-height:300px; overflow:auto; } .bp3-select-popover .bp3-popover-content{ padding:5px; } .bp3-select-popover .bp3-input-group{ margin-bottom:0; } .bp3-select-popover .bp3-menu{ max-width:400px; max-height:300px; overflow:auto; padding:0; } .bp3-select-popover .bp3-menu:not(:first-child){ padding-top:5px; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /* This file was auto-generated by ensureUiComponents() in @jupyterlab/buildutils */ /** * (DEPRECATED) Support for consuming icons as CSS background images */ /* Icons urls */ :root { --jp-icon-add: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDEzaC02djZoLTJ2LTZINXYtMmg2VjVoMnY2aDZ2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=); --jp-icon-bug: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwIDhoLTIuODFjLS40NS0uNzgtMS4wNy0xLjQ1LTEuODItMS45NkwxNyA0LjQxIDE1LjU5IDNsLTIuMTcgMi4xN0MxMi45NiA1LjA2IDEyLjQ5IDUgMTIgNWMtLjQ5IDAtLjk2LjA2LTEuNDEuMTdMOC40MSAzIDcgNC40MWwxLjYyIDEuNjNDNy44OCA2LjU1IDcuMjYgNy4yMiA2LjgxIDhINHYyaDIuMDljLS4wNS4zMy0uMDkuNjYtLjA5IDF2MUg0djJoMnYxYzAgLjM0LjA0LjY3LjA5IDFINHYyaDIuODFjMS4wNCAxLjc5IDIuOTcgMyA1LjE5IDNzNC4xNS0xLjIxIDUuMTktM0gyMHYtMmgtMi4wOWMuMDUtLjMzLjA5LS42Ni4wOS0xdi0xaDJ2LTJoLTJ2LTFjMC0uMzQtLjA0LS42Ny0uMDktMUgyMFY4em0tNiA4aC00di0yaDR2MnptMC00aC00di0yaDR2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=); --jp-icon-build: url(data:image/svg+xml;base64,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); --jp-icon-caret-down-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iOS45LDEzLjYgMy42LDcuNCA0LjQsNi42IDkuOSwxMi4yIDE1LjQsNi43IDE2LjEsNy40ICIvPgoJPC9nPgo8L3N2Zz4K); --jp-icon-caret-down-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNS45TDksOS43bDMuOC0zLjhsMS4yLDEuMmwtNC45LDVsLTQuOS01TDUuMiw1Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K); --jp-icon-caret-down: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNy41TDksMTEuMmwzLjgtMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=); --jp-icon-caret-left: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik0xMC44LDEyLjhMNy4xLDlsMy44LTMuOGwwLDcuNkgxMC44eiIvPgogIDwvZz4KPC9zdmc+Cg==); --jp-icon-caret-right: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik03LjIsNS4yTDEwLjksOWwtMy44LDMuOFY1LjJINy4yeiIvPgogIDwvZz4KPC9zdmc+Cg==); --jp-icon-caret-up-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iMTUuNCwxMy4zIDkuOSw3LjcgNC40LDEzLjIgMy42LDEyLjUgOS45LDYuMyAxNi4xLDEyLjYgIi8+Cgk8L2c+Cjwvc3ZnPgo=); --jp-icon-caret-up: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik01LjIsMTAuNUw5LDYuOGwzLjgsMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=); --jp-icon-case-sensitive: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWFjY2VudDIiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTcuNiw4aDAuOWwzLjUsOGgtMS4xTDEwLDE0SDZsLTAuOSwySDRMNy42LDh6IE04LDkuMUw2LjQsMTNoMy4yTDgsOS4xeiIvPgogICAgPHBhdGggZD0iTTE2LjYsOS44Yy0wLjIsMC4xLTAuNCwwLjEtMC43LDAuMWMtMC4yLDAtMC40LTAuMS0wLjYtMC4yYy0wLjEtMC4xLTAuMi0wLjQtMC4yLTAuNyBjLTAuMywwLjMtMC42LDAuNS0wLjksMC43Yy0wLjMsMC4xLTAuNywwLjItMS4xLDAuMmMtMC4zLDAtMC41LDAtMC43LTAuMWMtMC4yLTAuMS0wLjQtMC4yLTAuNi0wLjNjLTAuMi0wLjEtMC4zLTAuMy0wLjQtMC41IGMtMC4xLTAuMi0wLjEtMC40LTAuMS0wLjdjMC0wLjMsMC4xLTAuNiwwLjItMC44YzAuMS0wLjIsMC4zLTAuNCwwLjQtMC41QzEyLDcsMTIuMiw2LjksMTIuNSw2LjhjMC4yLTAuMSwwLjUtMC4xLDAuNy0wLjIgYzAuMy0wLjEsMC41LTAuMSwwLjctMC4xYzAuMiwwLDAuNC0wLjEsMC42LTAuMWMwLjIsMCwwLjMtMC4xLDAuNC0wLjJjMC4xLTAuMSwwLjItMC4yLDAuMi0wLjRjMC0xLTEuMS0xLTEuMy0xIGMtMC40LDAtMS40LDAtMS40LDEuMmgtMC45YzAtMC40LDAuMS0wLjcsMC4yLTFjMC4xLTAuMiwwLjMtMC40LDAuNS0wLjZjMC4yLTAuMiwwLjUtMC4zLDAuOC0wLjNDMTMuMyw0LDEzLjYsNCwxMy45LDQgYzAuMywwLDAuNSwwLDAuOCwwLjFjMC4zLDAsMC41LDAuMSwwLjcsMC4yYzAuMiwwLjEsMC40LDAuMywwLjUsMC41QzE2LDUsMTYsNS4yLDE2LDUuNnYyLjljMCwwLjIsMCwwLjQsMCwwLjUgYzAsMC4xLDAuMSwwLjIsMC4zLDAuMmMwLjEsMCwwLjIsMCwwLjMsMFY5Ljh6IE0xNS4yLDYuOWMtMS4yLDAuNi0zLjEsMC4yLTMuMSwxLjRjMCwxLjQsMy4xLDEsMy4xLTAuNVY2Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K); --jp-icon-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkgMTYuMTdMNC44MyAxMmwtMS40MiAxLjQxTDkgMTkgMjEgN2wtMS40MS0xLjQxeiIvPgogIDwvZz4KPC9zdmc+Cg==); --jp-icon-circle-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDJDNi40NyAyIDIgNi40NyAyIDEyczQuNDcgMTAgMTAgMTAgMTAtNC40NyAxMC0xMFMxNy41MyAyIDEyIDJ6bTAgMThjLTQuNDEgMC04LTMuNTktOC04czMuNTktOCA4LTggOCAzLjU5IDggOC0zLjU5IDgtOCA4eiIvPgogIDwvZz4KPC9zdmc+Cg==); --jp-icon-circle: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iOSIgY3k9IjkiIHI9IjgiLz4KICA8L2c+Cjwvc3ZnPgo=); --jp-icon-clear: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8bWFzayBpZD0iZG9udXRIb2xlIj4KICAgIDxyZWN0IHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgZmlsbD0id2hpdGUiIC8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSI4IiBmaWxsPSJibGFjayIvPgogIDwvbWFzaz4KCiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxyZWN0IGhlaWdodD0iMTgiIHdpZHRoPSIyIiB4PSIxMSIgeT0iMyIgdHJhbnNmb3JtPSJyb3RhdGUoMzE1LCAxMiwgMTIpIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIxMCIgbWFzaz0idXJsKCNkb251dEhvbGUpIi8+CiAgPC9nPgo8L3N2Zz4K); --jp-icon-close: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1ub25lIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIGpwLWljb24zLWhvdmVyIiBmaWxsPSJub25lIj4KICAgIDxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjExIi8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIGpwLWljb24tYWNjZW50Mi1ob3ZlciIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMTkgNi40MUwxNy41OSA1IDEyIDEwLjU5IDYuNDEgNSA1IDYuNDEgMTAuNTkgMTIgNSAxNy41OSA2LjQxIDE5IDEyIDEzLjQxIDE3LjU5IDE5IDE5IDE3LjU5IDEzLjQxIDEyeiIvPgogIDwvZz4KCiAgPGcgY2xhc3M9ImpwLWljb24tbm9uZSBqcC1pY29uLWJ1c3kiIGZpbGw9Im5vbmUiPgogICAgPGNpcmNsZSBjeD0iMTIiIGN5PSIxMiIgcj0iNyIvPgogIDwvZz4KPC9zdmc+Cg==); --jp-icon-console: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwMCAyMDAiPgogIDxnIGNsYXNzPSJqcC1pY29uLWJyYW5kMSBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMjg4RDEiPgogICAgPHBhdGggZD0iTTIwIDE5LjhoMTYwdjE1OS45SDIweiIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNmZmYiPgogICAgPHBhdGggZD0iTTEwNSAxMjcuM2g0MHYxMi44aC00MHpNNTEuMSA3N0w3NCA5OS45bC0yMy4zIDIzLjMgMTAuNSAxMC41IDIzLjMtMjMuM0w5NSA5OS45IDg0LjUgODkuNCA2MS42IDY2LjV6Ii8+CiAgPC9nPgo8L3N2Zz4K); --jp-icon-copy: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTExLjksMUgzLjJDMi40LDEsMS43LDEuNywxLjcsMi41djEwLjJoMS41VjIuNWg4LjdWMXogTTE0LjEsMy45aC04Yy0wLjgsMC0xLjUsMC43LTEuNSwxLjV2MTAuMmMwLDAuOCwwLjcsMS41LDEuNSwxLjVoOCBjMC44LDAsMS41LTAuNywxLjUtMS41VjUuNEMxNS41LDQuNiwxNC45LDMuOSwxNC4xLDMuOXogTTE0LjEsMTUuNWgtOFY1LjRoOFYxNS41eiIvPgogIDwvZz4KPC9zdmc+Cg==); --jp-icon-cut: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkuNjQgNy42NGMuMjMtLjUuMzYtMS4wNS4zNi0xLjY0IDAtMi4yMS0xLjc5LTQtNC00UzIgMy43OSAyIDZzMS43OSA0IDQgNGMuNTkgMCAxLjE0LS4xMyAxLjY0LS4zNkwxMCAxMmwtMi4zNiAyLjM2QzcuMTQgMTQuMTMgNi41OSAxNCA2IDE0Yy0yLjIxIDAtNCAxLjc5LTQgNHMxLjc5IDQgNCA0IDQtMS43OSA0LTRjMC0uNTktLjEzLTEuMTQtLjM2LTEuNjRMMTIgMTRsNyA3aDN2LTFMOS42NCA3LjY0ek02IDhjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTAgMTJjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTYtNy41Yy0uMjggMC0uNS0uMjItLjUtLjVzLjIyLS41LjUtLjUuNS4yMi41LjUtLjIyLjUtLjUuNXpNMTkgM2wtNiA2IDIgMiA3LTdWM3oiLz4KICA8L2c+Cjwvc3ZnPgo=); --jp-icon-download: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDloLTRWM0g5djZINWw3IDcgNy03ek01IDE4djJoMTR2LTJINXoiLz4KICA8L2c+Cjwvc3ZnPgo=); --jp-icon-edit: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMgMTcuMjVWMjFoMy43NUwxNy44MSA5Ljk0bC0zLjc1LTMuNzVMMyAxNy4yNXpNMjAuNzEgNy4wNGMuMzktLjM5LjM5LTEuMDIgMC0xLjQxbC0yLjM0LTIuMzRjLS4zOS0uMzktMS4wMi0uMzktMS40MSAwbC0xLjgzIDEuODMgMy43NSAzLjc1IDEuODMtMS44M3oiLz4KICA8L2c+Cjwvc3ZnPgo=); --jp-icon-ellipses: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iNSIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxOSIgY3k9IjEyIiByPSIyIi8+CiAgPC9nPgo8L3N2Zz4K); --jp-icon-extension: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwLjUgMTFIMTlWN2MwLTEuMS0uOS0yLTItMmgtNFYzLjVDMTMgMi4xMiAxMS44OCAxIDEwLjUgMVM4IDIuMTIgOCAzLjVWNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAydjMuOEgzLjVjMS40OSAwIDIuNyAxLjIxIDIuNyAyLjdzLTEuMjEgMi43LTIuNyAyLjdIMlYyMGMwIDEuMS45IDIgMiAyaDMuOHYtMS41YzAtMS40OSAxLjIxLTIuNyAyLjctMi43IDEuNDkgMCAyLjcgMS4yMSAyLjcgMi43VjIySDE3YzEuMSAwIDItLjkgMi0ydi00aDEuNWMxLjM4IDAgMi41LTEuMTIgMi41LTIuNVMyMS44OCAxMSAyMC41IDExeiIvPgogIDwvZz4KPC9zdmc+Cg==); --jp-icon-fast-forward: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTQgMThsOC41LTZMNCA2djEyem05LTEydjEybDguNS02TDEzIDZ6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=); --jp-icon-file-upload: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkgMTZoNnYtNmg0bC03LTctNyA3aDR6bS00IDJoMTR2Mkg1eiIvPgogIDwvZz4KPC9zdmc+Cg==); --jp-icon-file: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuMyA4LjJsLTUuNS01LjVjLS4zLS4zLS43LS41LTEuMi0uNUgzLjljLS44LjEtMS42LjktMS42IDEuOHYxNC4xYzAgLjkuNyAxLjYgMS42IDEuNmgxNC4yYy45IDAgMS42LS43IDEuNi0xLjZWOS40Yy4xLS41LS4xLS45LS40LTEuMnptLTUuOC0zLjNsMy40IDMuNmgtMy40VjQuOXptMy45IDEyLjdINC43Yy0uMSAwLS4yIDAtLjItLjJWNC43YzAtLjIuMS0uMy4yLS4zaDcuMnY0LjRzMCAuOC4zIDEuMWMuMy4zIDEuMS4zIDEuMS4zaDQuM3Y3LjJzLS4xLjItLjIuMnoiLz4KPC9zdmc+Cg==); --jp-icon-filter-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEwIDE4aDR2LTJoLTR2MnpNMyA2djJoMThWNkgzem0zIDdoMTJ2LTJINnYyeiIvPgogIDwvZz4KPC9zdmc+Cg==); --jp-icon-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY4YzAtMS4xLS45LTItMi0yaC04bC0yLTJ6Ii8+Cjwvc3ZnPgo=); --jp-icon-html5: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uMCBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMDAiIGQ9Ik0xMDguNCAwaDIzdjIyLjhoMjEuMlYwaDIzdjY5aC0yM1Y0NmgtMjF2MjNoLTIzLjJNMjA2IDIzaC0yMC4zVjBoNjMuN3YyM0gyMjl2NDZoLTIzbTUzLjUtNjloMjQuMWwxNC44IDI0LjNMMzEzLjIgMGgyNC4xdjY5aC0yM1YzNC44bC0xNi4xIDI0LjgtMTYuMS0yNC44VjY5aC0yMi42bTg5LjItNjloMjN2NDYuMmgzMi42VjY5aC01NS42Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI2U0NGQyNiIgZD0iTTEwNy42IDQ3MWwtMzMtMzcwLjRoMzYyLjhsLTMzIDM3MC4yTDI1NS43IDUxMiIvPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNmMTY1MjkiIGQ9Ik0yNTYgNDgwLjVWMTMxaDE0OC4zTDM3NiA0NDciLz4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNlYmViZWIiIGQ9Ik0xNDIgMTc2LjNoMTE0djQ1LjRoLTY0LjJsNC4yIDQ2LjVoNjB2NDUuM0gxNTQuNG0yIDIyLjhIMjAybDMuMiAzNi4zIDUwLjggMTMuNnY0Ny40bC05My4yLTI2Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIiBkPSJNMzY5LjYgMTc2LjNIMjU1Ljh2NDUuNGgxMDkuNm0tNC4xIDQ2LjVIMjU1Ljh2NDUuNGg1NmwtNS4zIDU5LTUwLjcgMTMuNnY0Ny4ybDkzLTI1LjgiLz4KPC9zdmc+Cg==); --jp-icon-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1icmFuZDQganAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNGRkYiIGQ9Ik0yLjIgMi4yaDE3LjV2MTcuNUgyLjJ6Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzNGNTFCNSIgZD0iTTIuMiAyLjJ2MTcuNWgxNy41bC4xLTE3LjVIMi4yem0xMi4xIDIuMmMxLjIgMCAyLjIgMSAyLjIgMi4ycy0xIDIuMi0yLjIgMi4yLTIuMi0xLTIuMi0yLjIgMS0yLjIgMi4yLTIuMnpNNC40IDE3LjZsMy4zLTguOCAzLjMgNi42IDIuMi0zLjIgNC40IDUuNEg0LjR6Ii8+Cjwvc3ZnPgo=); --jp-icon-inspector: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY2YzAtMS4xLS45LTItMi0yem0tNSAxNEg0di00aDExdjR6bTAtNUg0VjloMTF2NHptNSA1aC00VjloNHY5eiIvPgo8L3N2Zz4K); --jp-icon-json: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi13YXJuMSBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNGOUE4MjUiPgogICAgPHBhdGggZD0iTTIwLjIgMTEuOGMtMS42IDAtMS43LjUtMS43IDEgMCAuNC4xLjkuMSAxLjMuMS41LjEuOS4xIDEuMyAwIDEuNy0xLjQgMi4zLTMuNSAyLjNoLS45di0xLjloLjVjMS4xIDAgMS40IDAgMS40LS44IDAtLjMgMC0uNi0uMS0xIDAtLjQtLjEtLjgtLjEtMS4yIDAtMS4zIDAtMS44IDEuMy0yLTEuMy0uMi0xLjMtLjctMS4zLTIgMC0uNC4xLS44LjEtMS4yLjEtLjQuMS0uNy4xLTEgMC0uOC0uNC0uNy0xLjQtLjhoLS41VjQuMWguOWMyLjIgMCAzLjUuNyAzLjUgMi4zIDAgLjQtLjEuOS0uMSAxLjMtLjEuNS0uMS45LS4xIDEuMyAwIC41LjIgMSAxLjcgMXYxLjh6TTEuOCAxMC4xYzEuNiAwIDEuNy0uNSAxLjctMSAwLS40LS4xLS45LS4xLTEuMy0uMS0uNS0uMS0uOS0uMS0xLjMgMC0xLjYgMS40LTIuMyAzLjUtMi4zaC45djEuOWgtLjVjLTEgMC0xLjQgMC0xLjQuOCAwIC4zIDAgLjYuMSAxIDAgLjIuMS42LjEgMSAwIDEuMyAwIDEuOC0xLjMgMkM2IDExLjIgNiAxMS43IDYgMTNjMCAuNC0uMS44LS4xIDEuMi0uMS4zLS4xLjctLjEgMSAwIC44LjMuOCAxLjQuOGguNXYxLjloLS45Yy0yLjEgMC0zLjUtLjYtMy41LTIuMyAwLS40LjEtLjkuMS0xLjMuMS0uNS4xLS45LjEtMS4zIDAtLjUtLjItMS0xLjctMXYtMS45eiIvPgogICAgPGNpcmNsZSBjeD0iMTEiIGN5PSIxMy44IiByPSIyLjEiLz4KICAgIDxjaXJjbGUgY3g9IjExIiBjeT0iOC4yIiByPSIyLjEiLz4KICA8L2c+Cjwvc3ZnPgo=); --jp-icon-jupyter-favicon: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTUyIiBoZWlnaHQ9IjE2NSIgdmlld0JveD0iMCAwIDE1MiAxNjUiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbi13YXJuMCIgZmlsbD0iI0YzNzcyNiI+CiAgICA8cGF0aCB0cmFuc2Zvcm09InRyYW5zbGF0ZSgwLjA3ODk0NywgMTEwLjU4MjkyNykiIGQ9Ik03NS45NDIyODQyLDI5LjU4MDQ1NjEgQzQzLjMwMjM5NDcsMjkuNTgwNDU2MSAxNC43OTY3ODMyLDE3LjY1MzQ2MzQgMCwwIEM1LjUxMDgzMjExLDE1Ljg0MDY4MjkgMTUuNzgxNTM4OSwyOS41NjY3NzMyIDI5LjM5MDQ5NDcsMzkuMjc4NDE3MSBDNDIuOTk5Nyw0OC45ODk4NTM3IDU5LjI3MzcsNTQuMjA2NzgwNSA3NS45NjA1Nzg5LDU0LjIwNjc4MDUgQzkyLjY0NzQ1NzksNTQuMjA2NzgwNSAxMDguOTIxNDU4LDQ4Ljk4OTg1MzcgMTIyLjUzMDY2MywzOS4yNzg0MTcxIEMxMzYuMTM5NDUzLDI5LjU2Njc3MzIgMTQ2LjQxMDI4NCwxNS44NDA2ODI5IDE1MS45MjExNTgsMCBDMTM3LjA4Nzg2OCwxNy42NTM0NjM0IDEwOC41ODI1ODksMjkuNTgwNDU2MSA3NS45NDIyODQyLDI5LjU4MDQ1NjEgTDc1Ljk0MjI4NDIsMjkuNTgwNDU2MSBaIiAvPgogICAgPHBhdGggdHJhbnNmb3JtPSJ0cmFuc2xhdGUoMC4wMzczNjgsIDAuNzA0ODc4KSIgZD0iTTc1Ljk3ODQ1NzksMjQuNjI2NDA3MyBDMTA4LjYxODc2MywyNC42MjY0MDczIDEzNy4xMjQ0NTgsMzYuNTUzNDQxNSAxNTEuOTIxMTU4LDU0LjIwNjc4MDUgQzE0Ni40MTAyODQsMzguMzY2MjIyIDEzNi4xMzk0NTMsMjQuNjQwMTMxNyAxMjIuNTMwNjYzLDE0LjkyODQ4NzggQzEwOC45MjE0NTgsNS4yMTY4NDM5IDkyLjY0NzQ1NzksMCA3NS45NjA1Nzg5LDAgQzU5LjI3MzcsMCA0Mi45OTk3LDUuMjE2ODQzOSAyOS4zOTA0OTQ3LDE0LjkyODQ4NzggQzE1Ljc4MTUzODksMjQuNjQwMTMxNyA1LjUxMDgzMjExLDM4LjM2NjIyMiAwLDU0LjIwNjc4MDUgQzE0LjgzMzA4MTYsMzYuNTg5OTI5MyA0My4zMzg1Njg0LDI0LjYyNjQwNzMgNzUuOTc4NDU3OSwyNC42MjY0MDczIEw3NS45Nzg0NTc5LDI0LjYyNjQwNzMgWiIgLz4KICA8L2c+Cjwvc3ZnPgo=); --jp-icon-jupyter: url(data:image/svg+xml;base64,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); --jp-icon-jupyterlab-wordmark: url(data:image/svg+xml;base64,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); --jp-icon-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgZmlsbD0iIzYxNjE2MSIgZD0iTTE1IDlIOXY2aDZWOXptLTIgNGgtMnYtMmgydjJ6bTgtMlY5aC0yVjdjMC0xLjEtLjktMi0yLTJoLTJWM2gtMnYyaC0yVjNIOXYySDdjLTEuMSAwLTIgLjktMiAydjJIM3YyaDJ2MkgzdjJoMnYyYzAgMS4xLjkgMiAyIDJoMnYyaDJ2LTJoMnYyaDJ2LTJoMmMxLjEgMCAyLS45IDItMnYtMmgydi0yaC0ydi0yaDJ6bS00IDZIN1Y3aDEwdjEweiIvPgo8L3N2Zz4K); --jp-icon-keyboard: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMTdjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY3YzAtMS4xLS45LTItMi0yem0tOSAzaDJ2MmgtMlY4em0wIDNoMnYyaC0ydi0yek04IDhoMnYySDhWOHptMCAzaDJ2Mkg4di0yem0tMSAySDV2LTJoMnYyem0wLTNINVY4aDJ2MnptOSA3SDh2LTJoOHYyem0wLTRoLTJ2LTJoMnYyem0wLTNoLTJWOGgydjJ6bTMgM2gtMnYtMmgydjJ6bTAtM2gtMlY4aDJ2MnoiLz4KPC9zdmc+Cg==); --jp-icon-launcher: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkgMTlINVY1aDdWM0g1YTIgMiAwIDAwLTIgMnYxNGEyIDIgMCAwMDIgMmgxNGMxLjEgMCAyLS45IDItMnYtN2gtMnY3ek0xNCAzdjJoMy41OWwtOS44MyA5LjgzIDEuNDEgMS40MUwxOSA2LjQxVjEwaDJWM2gtN3oiLz4KPC9zdmc+Cg==); --jp-icon-line-form: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGZpbGw9IndoaXRlIiBkPSJNNS44OCA0LjEyTDEzLjc2IDEybC03Ljg4IDcuODhMOCAyMmwxMC0xMEw4IDJ6Ii8+Cjwvc3ZnPgo=); --jp-icon-link: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMuOSAxMmMwLTEuNzEgMS4zOS0zLjEgMy4xLTMuMWg0VjdIN2MtMi43NiAwLTUgMi4yNC01IDVzMi4yNCA1IDUgNWg0di0xLjlIN2MtMS43MSAwLTMuMS0xLjM5LTMuMS0zLjF6TTggMTNoOHYtMkg4djJ6bTktNmgtNHYxLjloNGMxLjcxIDAgMy4xIDEuMzkgMy4xIDMuMXMtMS4zOSAzLjEtMy4xIDMuMWgtNFYxN2g0YzIuNzYgMCA1LTIuMjQgNS01cy0yLjI0LTUtNS01eiIvPgogIDwvZz4KPC9zdmc+Cg==); --jp-icon-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xOSA1djE0SDVWNWgxNG0xLjEtMkgzLjljLS41IDAtLjkuNC0uOS45djE2LjJjMCAuNC40LjkuOS45aDE2LjJjLjQgMCAuOS0uNS45LS45VjMuOWMwLS41LS41LS45LS45LS45ek0xMSA3aDZ2MmgtNlY3em0wIDRoNnYyaC02di0yem0wIDRoNnYyaC02ek03IDdoMnYySDd6bTAgNGgydjJIN3ptMCA0aDJ2Mkg3eiIvPgo8L3N2Zz4=); --jp-icon-listings-info: url(data:image/svg+xml;base64,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); --jp-icon-markdown: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjN0IxRkEyIiBkPSJNNSAxNC45aDEybC02LjEgNnptOS40LTYuOGMwLTEuMy0uMS0yLjktLjEtNC41LS40IDEuNC0uOSAyLjktMS4zIDQuM2wtMS4zIDQuM2gtMkw4LjUgNy45Yy0uNC0xLjMtLjctMi45LTEtNC4zLS4xIDEuNi0uMSAzLjItLjIgNC42TDcgMTIuNEg0LjhsLjctMTFoMy4zTDEwIDVjLjQgMS4yLjcgMi43IDEgMy45LjMtMS4yLjctMi42IDEtMy45bDEuMi0zLjdoMy4zbC42IDExaC0yLjRsLS4zLTQuMnoiLz4KPC9zdmc+Cg==); --jp-icon-new-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwIDZoLThsLTItMkg0Yy0xLjExIDAtMS45OS44OS0xLjk5IDJMMiAxOGMwIDEuMTEuODkgMiAyIDJoMTZjMS4xMSAwIDItLjg5IDItMlY4YzAtMS4xMS0uODktMi0yLTJ6bS0xIDhoLTN2M2gtMnYtM2gtM3YtMmgzVjloMnYzaDN2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=); --jp-icon-not-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI1IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDMgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMTkgMTcuMTg0NCAyLjk2OTY4IDE0LjMwMzIgMS44NjA5NCAxMS40NDA5WiIvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24yIiBzdHJva2U9IiMzMzMzMzMiIHN0cm9rZS13aWR0aD0iMiIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOS4zMTU5MiA5LjMyMDMxKSIgZD0iTTcuMzY4NDIgMEwwIDcuMzY0NzkiLz4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDkuMzE1OTIgMTYuNjgzNikgc2NhbGUoMSAtMSkiIGQ9Ik03LjM2ODQyIDBMMCA3LjM2NDc5Ii8+Cjwvc3ZnPgo=); --jp-icon-notebook: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi13YXJuMCBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNFRjZDMDAiPgogICAgPHBhdGggZD0iTTE4LjcgMy4zdjE1LjRIMy4zVjMuM2gxNS40bTEuNS0xLjVIMS44djE4LjNoMTguM2wuMS0xOC4zeiIvPgogICAgPHBhdGggZD0iTTE2LjUgMTYuNWwtNS40LTQuMy01LjYgNC4zdi0xMWgxMXoiLz4KICA8L2c+Cjwvc3ZnPgo=); --jp-icon-palette: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE4IDEzVjIwSDRWNkg5LjAyQzkuMDcgNS4yOSA5LjI0IDQuNjIgOS41IDRINEMyLjkgNCAyIDQuOSAyIDZWMjBDMiAyMS4xIDIuOSAyMiA0IDIySDE4QzE5LjEgMjIgMjAgMjEuMSAyMCAyMFYxNUwxOCAxM1pNMTkuMyA4Ljg5QzE5Ljc0IDguMTkgMjAgNy4zOCAyMCA2LjVDMjAgNC4wMSAxNy45OSAyIDE1LjUgMkMxMy4wMSAyIDExIDQuMDEgMTEgNi41QzExIDguOTkgMTMuMDEgMTEgMTUuNDkgMTFDMTYuMzcgMTEgMTcuMTkgMTAuNzQgMTcuODggMTAuM0wyMSAxMy40MkwyMi40MiAxMkwxOS4zIDguODlaTTE1LjUgOUMxNC4xMiA5IDEzIDcuODggMTMgNi41QzEzIDUuMTIgMTQuMTIgNCAxNS41IDRDMTYuODggNCAxOCA1LjEyIDE4IDYuNUMxOCA3Ljg4IDE2Ljg4IDkgMTUuNSA5WiIvPgogICAgPHBhdGggZmlsbC1ydWxlPSJldmVub2RkIiBjbGlwLXJ1bGU9ImV2ZW5vZGQiIGQ9Ik00IDZIOS4wMTg5NEM5LjAwNjM5IDYuMTY1MDIgOSA2LjMzMTc2IDkgNi41QzkgOC44MTU3NyAxMC4yMTEgMTAuODQ4NyAxMi4wMzQzIDEySDlWMTRIMTZWMTIuOTgxMUMxNi41NzAzIDEyLjkzNzcgMTcuMTIgMTIuODIwNyAxNy42Mzk2IDEyLjYzOTZMMTggMTNWMjBINFY2Wk04IDhINlYxMEg4VjhaTTYgMTJIOFYxNEg2VjEyWk04IDE2SDZWMThIOFYxNlpNOSAxNkgxNlYxOEg5VjE2WiIvPgogIDwvZz4KPC9zdmc+Cg==); --jp-icon-paste: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE5IDJoLTQuMThDMTQuNC44NCAxMy4zIDAgMTIgMGMtMS4zIDAtMi40Ljg0LTIuODIgMkg1Yy0xLjEgMC0yIC45LTIgMnYxNmMwIDEuMS45IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjRjMC0xLjEtLjktMi0yLTJ6bS03IDBjLjU1IDAgMSAuNDUgMSAxcy0uNDUgMS0xIDEtMS0uNDUtMS0xIC40NS0xIDEtMXptNyAxOEg1VjRoMnYzaDEwVjRoMnYxNnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==); --jp-icon-python: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1icmFuZDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMEQ0N0ExIj4KICAgIDxwYXRoIGQ9Ik0xMS4xIDYuOVY1LjhINi45YzAtLjUgMC0xLjMuMi0xLjYuNC0uNy44LTEuMSAxLjctMS40IDEuNy0uMyAyLjUtLjMgMy45LS4xIDEgLjEgMS45LjkgMS45IDEuOXY0LjJjMCAuNS0uOSAxLjYtMiAxLjZIOC44Yy0xLjUgMC0yLjQgMS40LTIuNCAyLjh2Mi4ySDQuN0MzLjUgMTUuMSAzIDE0IDMgMTMuMVY5Yy0uMS0xIC42LTIgMS44LTIgMS41LS4xIDYuMy0uMSA2LjMtLjF6Ii8+CiAgICA8cGF0aCBkPSJNMTAuOSAxNS4xdjEuMWg0LjJjMCAuNSAwIDEuMy0uMiAxLjYtLjQuNy0uOCAxLjEtMS43IDEuNC0xLjcuMy0yLjUuMy0zLjkuMS0xLS4xLTEuOS0uOS0xLjktMS45di00LjJjMC0uNS45LTEuNiAyLTEuNmgzLjhjMS41IDAgMi40LTEuNCAyLjQtMi44VjYuNmgxLjdDMTguNSA2LjkgMTkgOCAxOSA4LjlWMTNjMCAxLS43IDIuMS0xLjkgMi4xaC02LjJ6Ii8+CiAgPC9nPgo8L3N2Zz4K); --jp-icon-r-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjE5NkYzIiBkPSJNNC40IDIuNWMxLjItLjEgMi45LS4zIDQuOS0uMyAyLjUgMCA0LjEuNCA1LjIgMS4zIDEgLjcgMS41IDEuOSAxLjUgMy41IDAgMi0xLjQgMy41LTIuOSA0LjEgMS4yLjQgMS43IDEuNiAyLjIgMyAuNiAxLjkgMSAzLjkgMS4zIDQuNmgtMy44Yy0uMy0uNC0uOC0xLjctMS4yLTMuN3MtMS4yLTIuNi0yLjYtMi42aC0uOXY2LjRINC40VjIuNXptMy43IDYuOWgxLjRjMS45IDAgMi45LS45IDIuOS0yLjNzLTEtMi4zLTIuOC0yLjNjLS43IDAtMS4zIDAtMS42LjJ2NC41aC4xdi0uMXoiLz4KPC9zdmc+Cg==); --jp-icon-react: url(data:image/svg+xml;base64,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); --jp-icon-refresh: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTkgMTMuNWMtMi40OSAwLTQuNS0yLjAxLTQuNS00LjVTNi41MSA0LjUgOSA0LjVjMS4yNCAwIDIuMzYuNTIgMy4xNyAxLjMzTDEwIDhoNVYzbC0xLjc2IDEuNzZDMTIuMTUgMy42OCAxMC42NiAzIDkgMyA1LjY5IDMgMy4wMSA1LjY5IDMuMDEgOVM1LjY5IDE1IDkgMTVjMi45NyAwIDUuNDMtMi4xNiA1LjktNWgtMS41MmMtLjQ2IDItMi4yNCAzLjUtNC4zOCAzLjV6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=); --jp-icon-regex: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbi1hY2NlbnQyIiBmaWxsPSIjRkZGIj4KICAgIDxjaXJjbGUgY2xhc3M9InN0MiIgY3g9IjUuNSIgY3k9IjE0LjUiIHI9IjEuNSIvPgogICAgPHJlY3QgeD0iMTIiIHk9IjQiIGNsYXNzPSJzdDIiIHdpZHRoPSIxIiBoZWlnaHQ9IjgiLz4KICAgIDxyZWN0IHg9IjguNSIgeT0iNy41IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjg2NiAtMC41IDAuNSAwLjg2NiAtMi4zMjU1IDcuMzIxOSkiIGNsYXNzPSJzdDIiIHdpZHRoPSI4IiBoZWlnaHQ9IjEiLz4KICAgIDxyZWN0IHg9IjEyIiB5PSI0IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjUgLTAuODY2IDAuODY2IDAuNSAtMC42Nzc5IDE0LjgyNTIpIiBjbGFzcz0ic3QyIiB3aWR0aD0iMSIgaGVpZ2h0PSI4Ii8+CiAgPC9nPgo8L3N2Zz4K); --jp-icon-run: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTggNXYxNGwxMS03eiIvPgogICAgPC9nPgo8L3N2Zz4K); --jp-icon-running: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMjU2IDhDMTE5IDggOCAxMTkgOCAyNTZzMTExIDI0OCAyNDggMjQ4IDI0OC0xMTEgMjQ4LTI0OFMzOTMgOCAyNTYgOHptOTYgMzI4YzAgOC44LTcuMiAxNi0xNiAxNkgxNzZjLTguOCAwLTE2LTcuMi0xNi0xNlYxNzZjMC04LjggNy4yLTE2IDE2LTE2aDE2MGM4LjggMCAxNiA3LjIgMTYgMTZ2MTYweiIvPgogIDwvZz4KPC9zdmc+Cg==); --jp-icon-save: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE3IDNINWMtMS4xMSAwLTIgLjktMiAydjE0YzAgMS4xLjg5IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjdsLTQtNHptLTUgMTZjLTEuNjYgMC0zLTEuMzQtMy0zczEuMzQtMyAzLTMgMyAxLjM0IDMgMy0xLjM0IDMtMyAzem0zLTEwSDVWNWgxMHY0eiIvPgogICAgPC9nPgo8L3N2Zz4K); --jp-icon-search: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjEsMTAuOWgtMC43bC0wLjItMC4yYzAuOC0wLjksMS4zLTIuMiwxLjMtMy41YzAtMy0yLjQtNS40LTUuNC01LjRTMS44LDQuMiwxLjgsNy4xczIuNCw1LjQsNS40LDUuNCBjMS4zLDAsMi41LTAuNSwzLjUtMS4zbDAuMiwwLjJ2MC43bDQuMSw0LjFsMS4yLTEuMkwxMi4xLDEwLjl6IE03LjEsMTAuOWMtMi4xLDAtMy43LTEuNy0zLjctMy43czEuNy0zLjcsMy43LTMuN3MzLjcsMS43LDMuNywzLjcgUzkuMiwxMC45LDcuMSwxMC45eiIvPgogIDwvZz4KPC9zdmc+Cg==); --jp-icon-settings: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuNDMgMTIuOThjLjA0LS4zMi4wNy0uNjQuMDctLjk4cy0uMDMtLjY2LS4wNy0uOThsMi4xMS0xLjY1Yy4xOS0uMTUuMjQtLjQyLjEyLS42NGwtMi0zLjQ2Yy0uMTItLjIyLS4zOS0uMy0uNjEtLjIybC0yLjQ5IDFjLS41Mi0uNC0xLjA4LS43My0xLjY5LS45OGwtLjM4LTIuNjVBLjQ4OC40ODggMCAwMDE0IDJoLTRjLS4yNSAwLS40Ni4xOC0uNDkuNDJsLS4zOCAyLjY1Yy0uNjEuMjUtMS4xNy41OS0xLjY5Ljk4bC0yLjQ5LTFjLS4yMy0uMDktLjQ5IDAtLjYxLjIybC0yIDMuNDZjLS4xMy4yMi0uMDcuNDkuMTIuNjRsMi4xMSAxLjY1Yy0uMDQuMzItLjA3LjY1LS4wNy45OHMuMDMuNjYuMDcuOThsLTIuMTEgMS42NWMtLjE5LjE1LS4yNC40Mi0uMTIuNjRsMiAzLjQ2Yy4xMi4yMi4zOS4zLjYxLjIybDIuNDktMWMuNTIuNCAxLjA4LjczIDEuNjkuOThsLjM4IDIuNjVjLjAzLjI0LjI0LjQyLjQ5LjQyaDRjLjI1IDAgLjQ2LS4xOC40OS0uNDJsLjM4LTIuNjVjLjYxLS4yNSAxLjE3LS41OSAxLjY5LS45OGwyLjQ5IDFjLjIzLjA5LjQ5IDAgLjYxLS4yMmwyLTMuNDZjLjEyLS4yMi4wNy0uNDktLjEyLS42NGwtMi4xMS0xLjY1ek0xMiAxNS41Yy0xLjkzIDAtMy41LTEuNTctMy41LTMuNXMxLjU3LTMuNSAzLjUtMy41IDMuNSAxLjU3IDMuNSAzLjUtMS41NyAzLjUtMy41IDMuNXoiLz4KPC9zdmc+Cg==); --jp-icon-spreadsheet: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNENBRjUwIiBkPSJNMi4yIDIuMnYxNy42aDE3LjZWMi4ySDIuMnptMTUuNCA3LjdoLTUuNVY0LjRoNS41djUuNXpNOS45IDQuNHY1LjVINC40VjQuNGg1LjV6bS01LjUgNy43aDUuNXY1LjVINC40di01LjV6bTcuNyA1LjV2LTUuNWg1LjV2NS41aC01LjV6Ii8+Cjwvc3ZnPgo=); --jp-icon-stop: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik02IDZoMTJ2MTJINnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==); --jp-icon-tab: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIxIDNIM2MtMS4xIDAtMiAuOS0yIDJ2MTRjMCAxLjEuOSAyIDIgMmgxOGMxLjEgMCAyLS45IDItMlY1YzAtMS4xLS45LTItMi0yem0wIDE2SDNWNWgxMHY0aDh2MTB6Ii8+CiAgPC9nPgo8L3N2Zz4K); --jp-icon-terminal: url(data:image/svg+xml;base64,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); --jp-icon-text-editor: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTUgMTVIM3YyaDEydi0yem0wLThIM3YyaDEyVjd6TTMgMTNoMTh2LTJIM3Yyem0wIDhoMTh2LTJIM3Yyek0zIDN2MmgxOFYzSDN6Ii8+Cjwvc3ZnPgo=); --jp-icon-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDIgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMiAxNy4xODQ0IDIuOTY5NjggMTQuMzAzMiAxLjg2MDk0IDExLjQ0MDlaIi8+CiAgICA8cGF0aCBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiMzMzMzMzMiIHN0cm9rZT0iIzMzMzMzMyIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOCA5Ljg2NzE5KSIgZD0iTTIuODYwMTUgNC44NjUzNUwwLjcyNjU0OSAyLjk5OTU5TDAgMy42MzA0NUwyLjg2MDE1IDYuMTMxNTdMOCAwLjYzMDg3Mkw3LjI3ODU3IDBMMi44NjAxNSA0Ljg2NTM1WiIvPgo8L3N2Zz4K); --jp-icon-undo: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjUgOGMtMi42NSAwLTUuMDUuOTktNi45IDIuNkwyIDd2OWg5bC0zLjYyLTMuNjJjMS4zOS0xLjE2IDMuMTYtMS44OCA1LjEyLTEuODggMy41NCAwIDYuNTUgMi4zMSA3LjYgNS41bDIuMzctLjc4QzIxLjA4IDExLjAzIDE3LjE1IDggMTIuNSA4eiIvPgogIDwvZz4KPC9zdmc+Cg==); --jp-icon-vega: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbjEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjEyMTIxIj4KICAgIDxwYXRoIGQ9Ik0xMC42IDUuNGwyLjItMy4ySDIuMnY3LjNsNC02LjZ6Ii8+CiAgICA8cGF0aCBkPSJNMTUuOCAyLjJsLTQuNCA2LjZMNyA2LjNsLTQuOCA4djUuNWgxNy42VjIuMmgtNHptLTcgMTUuNEg1LjV2LTQuNGgzLjN2NC40em00LjQgMEg5LjhWOS44aDMuNHY3Ljh6bTQuNCAwaC0zLjRWNi41aDMuNHYxMS4xeiIvPgogIDwvZz4KPC9zdmc+Cg==); --jp-icon-yaml: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1jb250cmFzdDIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjRDgxQjYwIj4KICAgIDxwYXRoIGQ9Ik03LjIgMTguNnYtNS40TDMgNS42aDMuM2wxLjQgMy4xYy4zLjkuNiAxLjYgMSAyLjUuMy0uOC42LTEuNiAxLTIuNWwxLjQtMy4xaDMuNGwtNC40IDcuNnY1LjVsLTIuOS0uMXoiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxNi41IiByPSIyLjEiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxMSIgcj0iMi4xIi8+CiAgPC9nPgo8L3N2Zz4K); } /* Icon CSS class declarations */ .jp-AddIcon { background-image: var(--jp-icon-add); } .jp-BugIcon { background-image: var(--jp-icon-bug); } .jp-BuildIcon { background-image: var(--jp-icon-build); } .jp-CaretDownEmptyIcon { background-image: var(--jp-icon-caret-down-empty); } .jp-CaretDownEmptyThinIcon { background-image: var(--jp-icon-caret-down-empty-thin); } .jp-CaretDownIcon { background-image: var(--jp-icon-caret-down); } .jp-CaretLeftIcon { background-image: var(--jp-icon-caret-left); } .jp-CaretRightIcon { background-image: var(--jp-icon-caret-right); } .jp-CaretUpEmptyThinIcon { background-image: var(--jp-icon-caret-up-empty-thin); } .jp-CaretUpIcon { background-image: var(--jp-icon-caret-up); } .jp-CaseSensitiveIcon { background-image: var(--jp-icon-case-sensitive); } .jp-CheckIcon { background-image: var(--jp-icon-check); } .jp-CircleEmptyIcon { background-image: var(--jp-icon-circle-empty); } .jp-CircleIcon { background-image: var(--jp-icon-circle); } .jp-ClearIcon { background-image: var(--jp-icon-clear); } .jp-CloseIcon { background-image: var(--jp-icon-close); } .jp-ConsoleIcon { background-image: var(--jp-icon-console); } .jp-CopyIcon { background-image: var(--jp-icon-copy); } .jp-CutIcon { background-image: var(--jp-icon-cut); } .jp-DownloadIcon { background-image: var(--jp-icon-download); } .jp-EditIcon { background-image: var(--jp-icon-edit); } .jp-EllipsesIcon { background-image: var(--jp-icon-ellipses); } .jp-ExtensionIcon { background-image: var(--jp-icon-extension); } .jp-FastForwardIcon { background-image: var(--jp-icon-fast-forward); } .jp-FileIcon { background-image: var(--jp-icon-file); } .jp-FileUploadIcon { background-image: var(--jp-icon-file-upload); } .jp-FilterListIcon { background-image: var(--jp-icon-filter-list); } .jp-FolderIcon { background-image: var(--jp-icon-folder); } .jp-Html5Icon { background-image: var(--jp-icon-html5); } .jp-ImageIcon { background-image: var(--jp-icon-image); } .jp-InspectorIcon { background-image: var(--jp-icon-inspector); } .jp-JsonIcon { background-image: var(--jp-icon-json); } .jp-JupyterFaviconIcon { background-image: var(--jp-icon-jupyter-favicon); } .jp-JupyterIcon { background-image: var(--jp-icon-jupyter); } .jp-JupyterlabWordmarkIcon { background-image: var(--jp-icon-jupyterlab-wordmark); } .jp-KernelIcon { background-image: var(--jp-icon-kernel); } .jp-KeyboardIcon { background-image: var(--jp-icon-keyboard); } .jp-LauncherIcon { background-image: var(--jp-icon-launcher); } .jp-LineFormIcon { background-image: var(--jp-icon-line-form); } .jp-LinkIcon { background-image: var(--jp-icon-link); } .jp-ListIcon { background-image: var(--jp-icon-list); } .jp-ListingsInfoIcon { background-image: var(--jp-icon-listings-info); } .jp-MarkdownIcon { background-image: var(--jp-icon-markdown); } .jp-NewFolderIcon { background-image: var(--jp-icon-new-folder); } .jp-NotTrustedIcon { background-image: var(--jp-icon-not-trusted); } .jp-NotebookIcon { background-image: var(--jp-icon-notebook); } .jp-PaletteIcon { background-image: var(--jp-icon-palette); } .jp-PasteIcon { background-image: var(--jp-icon-paste); } .jp-PythonIcon { background-image: var(--jp-icon-python); } .jp-RKernelIcon { background-image: var(--jp-icon-r-kernel); } .jp-ReactIcon { background-image: var(--jp-icon-react); } .jp-RefreshIcon { background-image: var(--jp-icon-refresh); } .jp-RegexIcon { background-image: var(--jp-icon-regex); } .jp-RunIcon { background-image: var(--jp-icon-run); } .jp-RunningIcon { background-image: var(--jp-icon-running); } .jp-SaveIcon { background-image: var(--jp-icon-save); } .jp-SearchIcon { background-image: var(--jp-icon-search); } .jp-SettingsIcon { background-image: var(--jp-icon-settings); } .jp-SpreadsheetIcon { background-image: var(--jp-icon-spreadsheet); } .jp-StopIcon { background-image: var(--jp-icon-stop); } .jp-TabIcon { background-image: var(--jp-icon-tab); } .jp-TerminalIcon { background-image: var(--jp-icon-terminal); } .jp-TextEditorIcon { background-image: var(--jp-icon-text-editor); } .jp-TrustedIcon { background-image: var(--jp-icon-trusted); } .jp-UndoIcon { background-image: var(--jp-icon-undo); } .jp-VegaIcon { background-image: var(--jp-icon-vega); } .jp-YamlIcon { background-image: var(--jp-icon-yaml); } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /** * (DEPRECATED) Support for consuming icons as CSS background images */ :root { --jp-icon-search-white: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjEsMTAuOWgtMC43bC0wLjItMC4yYzAuOC0wLjksMS4zLTIuMiwxLjMtMy41YzAtMy0yLjQtNS40LTUuNC01LjRTMS44LDQuMiwxLjgsNy4xczIuNCw1LjQsNS40LDUuNCBjMS4zLDAsMi41LTAuNSwzLjUtMS4zbDAuMiwwLjJ2MC43bDQuMSw0LjFsMS4yLTEuMkwxMi4xLDEwLjl6IE03LjEsMTAuOWMtMi4xLDAtMy43LTEuNy0zLjctMy43czEuNy0zLjcsMy43LTMuN3MzLjcsMS43LDMuNywzLjcgUzkuMiwxMC45LDcuMSwxMC45eiIvPgogIDwvZz4KPC9zdmc+Cg==); } .jp-Icon, .jp-MaterialIcon { background-position: center; background-repeat: no-repeat; background-size: 16px; min-width: 16px; min-height: 16px; } .jp-Icon-cover { background-position: center; background-repeat: no-repeat; background-size: cover; } /** * (DEPRECATED) Support for specific CSS icon sizes */ .jp-Icon-16 { background-size: 16px; min-width: 16px; min-height: 16px; } .jp-Icon-18 { background-size: 18px; min-width: 18px; min-height: 18px; } .jp-Icon-20 { background-size: 20px; min-width: 20px; min-height: 20px; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /** * Support for icons as inline SVG HTMLElements */ /* recolor the primary elements of an icon */ .jp-icon0[fill] { fill: var(--jp-inverse-layout-color0); } .jp-icon1[fill] { fill: var(--jp-inverse-layout-color1); } .jp-icon2[fill] { fill: var(--jp-inverse-layout-color2); } .jp-icon3[fill] { fill: var(--jp-inverse-layout-color3); } .jp-icon4[fill] { fill: var(--jp-inverse-layout-color4); } .jp-icon0[stroke] { stroke: var(--jp-inverse-layout-color0); } .jp-icon1[stroke] { stroke: var(--jp-inverse-layout-color1); } .jp-icon2[stroke] { stroke: var(--jp-inverse-layout-color2); } .jp-icon3[stroke] { stroke: var(--jp-inverse-layout-color3); } .jp-icon4[stroke] { stroke: var(--jp-inverse-layout-color4); } /* recolor the accent elements of an icon */ .jp-icon-accent0[fill] { fill: var(--jp-layout-color0); } .jp-icon-accent1[fill] { fill: var(--jp-layout-color1); } .jp-icon-accent2[fill] { fill: var(--jp-layout-color2); } .jp-icon-accent3[fill] { fill: var(--jp-layout-color3); } .jp-icon-accent4[fill] { fill: var(--jp-layout-color4); } .jp-icon-accent0[stroke] { stroke: var(--jp-layout-color0); } .jp-icon-accent1[stroke] { stroke: var(--jp-layout-color1); } .jp-icon-accent2[stroke] { stroke: var(--jp-layout-color2); } .jp-icon-accent3[stroke] { stroke: var(--jp-layout-color3); } .jp-icon-accent4[stroke] { stroke: var(--jp-layout-color4); } /* set the color of an icon to transparent */ .jp-icon-none[fill] { fill: none; } .jp-icon-none[stroke] { stroke: none; } /* brand icon colors. Same for light and dark */ .jp-icon-brand0[fill] { fill: var(--jp-brand-color0); } .jp-icon-brand1[fill] { fill: var(--jp-brand-color1); } .jp-icon-brand2[fill] { fill: var(--jp-brand-color2); } .jp-icon-brand3[fill] { fill: var(--jp-brand-color3); } .jp-icon-brand4[fill] { fill: var(--jp-brand-color4); } .jp-icon-brand0[stroke] { stroke: var(--jp-brand-color0); } .jp-icon-brand1[stroke] { stroke: var(--jp-brand-color1); } .jp-icon-brand2[stroke] { stroke: var(--jp-brand-color2); } .jp-icon-brand3[stroke] { stroke: var(--jp-brand-color3); } .jp-icon-brand4[stroke] { stroke: var(--jp-brand-color4); } /* warn icon colors. Same for light and dark */ .jp-icon-warn0[fill] { fill: var(--jp-warn-color0); } .jp-icon-warn1[fill] { fill: var(--jp-warn-color1); } .jp-icon-warn2[fill] { fill: var(--jp-warn-color2); } .jp-icon-warn3[fill] { fill: var(--jp-warn-color3); } .jp-icon-warn0[stroke] { stroke: var(--jp-warn-color0); } .jp-icon-warn1[stroke] { stroke: var(--jp-warn-color1); } .jp-icon-warn2[stroke] { stroke: var(--jp-warn-color2); } .jp-icon-warn3[stroke] { stroke: var(--jp-warn-color3); } /* icon colors that contrast well with each other and most backgrounds */ .jp-icon-contrast0[fill] { fill: var(--jp-icon-contrast-color0); } .jp-icon-contrast1[fill] { fill: var(--jp-icon-contrast-color1); } .jp-icon-contrast2[fill] { fill: var(--jp-icon-contrast-color2); } .jp-icon-contrast3[fill] { fill: var(--jp-icon-contrast-color3); } .jp-icon-contrast0[stroke] { stroke: var(--jp-icon-contrast-color0); } .jp-icon-contrast1[stroke] { stroke: var(--jp-icon-contrast-color1); } .jp-icon-contrast2[stroke] { stroke: var(--jp-icon-contrast-color2); } .jp-icon-contrast3[stroke] { stroke: var(--jp-icon-contrast-color3); } /* CSS for icons in selected items in the settings editor */ #setting-editor .jp-PluginList .jp-mod-selected .jp-icon-selectable[fill] { fill: #fff; } #setting-editor .jp-PluginList .jp-mod-selected .jp-icon-selectable-inverse[fill] { fill: var(--jp-brand-color1); } /* CSS for icons in selected filebrowser listing items */ .jp-DirListing-item.jp-mod-selected .jp-icon-selectable[fill] { fill: #fff; } .jp-DirListing-item.jp-mod-selected .jp-icon-selectable-inverse[fill] { fill: var(--jp-brand-color1); } /* CSS for icons in selected tabs in the sidebar tab manager */ #tab-manager .lm-TabBar-tab.jp-mod-active .jp-icon-selectable[fill] { fill: #fff; } #tab-manager .lm-TabBar-tab.jp-mod-active .jp-icon-selectable-inverse[fill] { fill: var(--jp-brand-color1); } #tab-manager .lm-TabBar-tab.jp-mod-active .jp-icon-hover :hover .jp-icon-selectable[fill] { fill: var(--jp-brand-color1); } #tab-manager .lm-TabBar-tab.jp-mod-active .jp-icon-hover :hover .jp-icon-selectable-inverse[fill] { fill: #fff; } /** * TODO: come up with non css-hack solution for showing the busy icon on top * of the close icon * CSS for complex behavior of close icon of tabs in the sidebar tab manager */ #tab-manager .lm-TabBar-tab.jp-mod-dirty > .lm-TabBar-tabCloseIcon > :not(:hover) > .jp-icon3[fill] { fill: none; } #tab-manager .lm-TabBar-tab.jp-mod-dirty > .lm-TabBar-tabCloseIcon > :not(:hover) > .jp-icon-busy[fill] { fill: var(--jp-inverse-layout-color3); } #tab-manager .lm-TabBar-tab.jp-mod-dirty.jp-mod-active > .lm-TabBar-tabCloseIcon > :not(:hover) > .jp-icon-busy[fill] { fill: #fff; } /** * TODO: come up with non css-hack solution for showing the busy icon on top * of the close icon * CSS for complex behavior of close icon of tabs in the main area tabbar */ .lm-DockPanel-tabBar .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty > .lm-TabBar-tabCloseIcon > :not(:hover) > .jp-icon3[fill] { fill: none; } .lm-DockPanel-tabBar .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty > .lm-TabBar-tabCloseIcon > :not(:hover) > .jp-icon-busy[fill] { fill: var(--jp-inverse-layout-color3); } /* CSS for icons in status bar */ #jp-main-statusbar .jp-mod-selected .jp-icon-selectable[fill] { fill: #fff; } #jp-main-statusbar .jp-mod-selected .jp-icon-selectable-inverse[fill] { fill: var(--jp-brand-color1); } /* special handling for splash icon CSS. While the theme CSS reloads during splash, the splash icon can loose theming. To prevent that, we set a default for its color variable */ :root { --jp-warn-color0: var(--md-orange-700); } /* not sure what to do with this one, used in filebrowser listing */ .jp-DragIcon { margin-right: 4px; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /** * Support for alt colors for icons as inline SVG HTMLElements */ /* alt recolor the primary elements of an icon */ .jp-icon-alt .jp-icon0[fill] { fill: var(--jp-layout-color0); } .jp-icon-alt .jp-icon1[fill] { fill: var(--jp-layout-color1); } .jp-icon-alt .jp-icon2[fill] { fill: var(--jp-layout-color2); } .jp-icon-alt .jp-icon3[fill] { fill: var(--jp-layout-color3); } .jp-icon-alt .jp-icon4[fill] { fill: var(--jp-layout-color4); } .jp-icon-alt .jp-icon0[stroke] { stroke: var(--jp-layout-color0); } .jp-icon-alt .jp-icon1[stroke] { stroke: var(--jp-layout-color1); } .jp-icon-alt .jp-icon2[stroke] { stroke: var(--jp-layout-color2); } .jp-icon-alt .jp-icon3[stroke] { stroke: var(--jp-layout-color3); } .jp-icon-alt .jp-icon4[stroke] { stroke: var(--jp-layout-color4); } /* alt recolor the accent elements of an icon */ .jp-icon-alt .jp-icon-accent0[fill] { fill: var(--jp-inverse-layout-color0); } .jp-icon-alt .jp-icon-accent1[fill] { fill: var(--jp-inverse-layout-color1); } .jp-icon-alt .jp-icon-accent2[fill] { fill: var(--jp-inverse-layout-color2); } .jp-icon-alt .jp-icon-accent3[fill] { fill: var(--jp-inverse-layout-color3); } .jp-icon-alt .jp-icon-accent4[fill] { fill: var(--jp-inverse-layout-color4); } .jp-icon-alt .jp-icon-accent0[stroke] { stroke: var(--jp-inverse-layout-color0); } .jp-icon-alt .jp-icon-accent1[stroke] { stroke: var(--jp-inverse-layout-color1); } .jp-icon-alt .jp-icon-accent2[stroke] { stroke: var(--jp-inverse-layout-color2); } .jp-icon-alt .jp-icon-accent3[stroke] { stroke: var(--jp-inverse-layout-color3); } .jp-icon-alt .jp-icon-accent4[stroke] { stroke: var(--jp-inverse-layout-color4); } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ .jp-icon-hoverShow:not(:hover) svg { display: none !important; } /** * Support for hover colors for icons as inline SVG HTMLElements */ /** * regular colors */ /* recolor the primary elements of an icon */ .jp-icon-hover :hover .jp-icon0-hover[fill] { fill: var(--jp-inverse-layout-color0); } .jp-icon-hover :hover .jp-icon1-hover[fill] { fill: var(--jp-inverse-layout-color1); } .jp-icon-hover :hover .jp-icon2-hover[fill] { fill: var(--jp-inverse-layout-color2); } .jp-icon-hover :hover .jp-icon3-hover[fill] { fill: var(--jp-inverse-layout-color3); } .jp-icon-hover :hover .jp-icon4-hover[fill] { fill: var(--jp-inverse-layout-color4); } .jp-icon-hover :hover .jp-icon0-hover[stroke] { stroke: var(--jp-inverse-layout-color0); } .jp-icon-hover :hover .jp-icon1-hover[stroke] { stroke: var(--jp-inverse-layout-color1); } .jp-icon-hover :hover .jp-icon2-hover[stroke] { stroke: var(--jp-inverse-layout-color2); } .jp-icon-hover :hover .jp-icon3-hover[stroke] { stroke: var(--jp-inverse-layout-color3); } .jp-icon-hover :hover .jp-icon4-hover[stroke] { stroke: var(--jp-inverse-layout-color4); } /* recolor the accent elements of an icon */ .jp-icon-hover :hover .jp-icon-accent0-hover[fill] { fill: var(--jp-layout-color0); } .jp-icon-hover :hover .jp-icon-accent1-hover[fill] { fill: var(--jp-layout-color1); } .jp-icon-hover :hover .jp-icon-accent2-hover[fill] { fill: var(--jp-layout-color2); } .jp-icon-hover :hover .jp-icon-accent3-hover[fill] { fill: var(--jp-layout-color3); } .jp-icon-hover :hover .jp-icon-accent4-hover[fill] { fill: var(--jp-layout-color4); } .jp-icon-hover :hover .jp-icon-accent0-hover[stroke] { stroke: var(--jp-layout-color0); } .jp-icon-hover :hover .jp-icon-accent1-hover[stroke] { stroke: var(--jp-layout-color1); } .jp-icon-hover :hover .jp-icon-accent2-hover[stroke] { stroke: var(--jp-layout-color2); } .jp-icon-hover :hover .jp-icon-accent3-hover[stroke] { stroke: var(--jp-layout-color3); } .jp-icon-hover :hover .jp-icon-accent4-hover[stroke] { stroke: var(--jp-layout-color4); } /* set the color of an icon to transparent */ .jp-icon-hover :hover .jp-icon-none-hover[fill] { fill: none; } .jp-icon-hover :hover .jp-icon-none-hover[stroke] { stroke: none; } /** * inverse colors */ /* inverse recolor the primary elements of an icon */ .jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[fill] { fill: var(--jp-layout-color0); } .jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[fill] { fill: var(--jp-layout-color1); } .jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[fill] { fill: var(--jp-layout-color2); } .jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[fill] { fill: var(--jp-layout-color3); } .jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[fill] { fill: var(--jp-layout-color4); } .jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[stroke] { stroke: var(--jp-layout-color0); } .jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[stroke] { stroke: var(--jp-layout-color1); } .jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[stroke] { stroke: var(--jp-layout-color2); } .jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[stroke] { stroke: var(--jp-layout-color3); } .jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[stroke] { stroke: var(--jp-layout-color4); } /* inverse recolor the accent elements of an icon */ .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[fill] { fill: var(--jp-inverse-layout-color0); } .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[fill] { fill: var(--jp-inverse-layout-color1); } .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[fill] { fill: var(--jp-inverse-layout-color2); } .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[fill] { fill: var(--jp-inverse-layout-color3); } .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[fill] { fill: var(--jp-inverse-layout-color4); } .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[stroke] { stroke: var(--jp-inverse-layout-color0); } .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[stroke] { stroke: var(--jp-inverse-layout-color1); } .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[stroke] { stroke: var(--jp-inverse-layout-color2); } .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[stroke] { stroke: var(--jp-inverse-layout-color3); } .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[stroke] { stroke: var(--jp-inverse-layout-color4); } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /* Sibling imports */ /* Override Blueprint's _reset.scss styles */ html { box-sizing: unset; } *, *::before, *::after { box-sizing: unset; } body { color: unset; font-family: var(--jp-ui-font-family); } p { margin-top: unset; margin-bottom: unset; } small { font-size: unset; } strong { font-weight: unset; } /* Override Blueprint's _typography.scss styles */ a { text-decoration: unset; color: unset; } a:hover { text-decoration: unset; color: unset; } /* Override Blueprint's _accessibility.scss styles */ :focus { outline: unset; outline-offset: unset; -moz-outline-radius: unset; } /* Styles for ui-components */ .jp-Button { border-radius: var(--jp-border-radius); padding: 0px 12px; font-size: var(--jp-ui-font-size1); } /* Use our own theme for hover styles */ button.jp-Button.bp3-button.bp3-minimal:hover { background-color: var(--jp-layout-color2); } .jp-Button.minimal { color: unset !important; } .jp-Button.jp-ToolbarButtonComponent { text-transform: none; } .jp-InputGroup input { box-sizing: border-box; border-radius: 0; background-color: transparent; color: var(--jp-ui-font-color0); box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color); } .jp-InputGroup input:focus { box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-active-box-shadow-color), inset 0 0 0 3px var(--jp-input-active-box-shadow-color); } .jp-InputGroup input::placeholder, input::placeholder { color: var(--jp-ui-font-color3); } .jp-BPIcon { display: inline-block; vertical-align: middle; margin: auto; } /* Stop blueprint futzing with our icon fills */ .bp3-icon.jp-BPIcon > svg:not([fill]) { fill: var(--jp-inverse-layout-color3); } .jp-InputGroupAction { padding: 6px; } .jp-HTMLSelect.jp-DefaultStyle select { background-color: initial; border: none; border-radius: 0; box-shadow: none; color: var(--jp-ui-font-color0); display: block; font-size: var(--jp-ui-font-size1); height: 24px; line-height: 14px; padding: 0 25px 0 10px; text-align: left; -moz-appearance: none; -webkit-appearance: none; } /* Use our own theme for hover and option styles */ .jp-HTMLSelect.jp-DefaultStyle select:hover, .jp-HTMLSelect.jp-DefaultStyle select > option { background-color: var(--jp-layout-color2); color: var(--jp-ui-font-color0); } select { box-sizing: border-box; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /* This file was auto-generated by ensurePackage() in @jupyterlab/buildutils */ /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ .jp-Collapse { display: flex; flex-direction: column; align-items: stretch; border-top: 1px solid var(--jp-border-color2); border-bottom: 1px solid var(--jp-border-color2); } .jp-Collapse-header { padding: 1px 12px; color: var(--jp-ui-font-color1); background-color: var(--jp-layout-color1); font-size: var(--jp-ui-font-size2); } .jp-Collapse-header:hover { background-color: var(--jp-layout-color2); } .jp-Collapse-contents { padding: 0px 12px 0px 12px; background-color: var(--jp-layout-color1); color: var(--jp-ui-font-color1); overflow: auto; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /*----------------------------------------------------------------------------- | Variables |----------------------------------------------------------------------------*/ :root { --jp-private-commandpalette-search-height: 28px; } /*----------------------------------------------------------------------------- | Overall styles |----------------------------------------------------------------------------*/ .lm-CommandPalette { padding-bottom: 0px; color: var(--jp-ui-font-color1); background: var(--jp-layout-color1); /* This is needed so that all font sizing of children done in ems is * relative to this base size */ font-size: var(--jp-ui-font-size1); } /*----------------------------------------------------------------------------- | Search |----------------------------------------------------------------------------*/ .lm-CommandPalette-search { padding: 4px; background-color: var(--jp-layout-color1); z-index: 2; } .lm-CommandPalette-wrapper { overflow: overlay; padding: 0px 9px; background-color: var(--jp-input-active-background); height: 30px; box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color); } .lm-CommandPalette.lm-mod-focused .lm-CommandPalette-wrapper { box-shadow: inset 0 0 0 1px var(--jp-input-active-box-shadow-color), inset 0 0 0 3px var(--jp-input-active-box-shadow-color); } .lm-CommandPalette-wrapper::after { content: ' '; color: white; background-color: var(--jp-brand-color1); position: absolute; top: 4px; right: 4px; height: 30px; width: 10px; padding: 0px 10px; background-image: var(--jp-icon-search-white); background-size: 20px; background-repeat: no-repeat; background-position: center; } .lm-CommandPalette-input { background: transparent; width: calc(100% - 18px); float: left; border: none; outline: none; font-size: var(--jp-ui-font-size1); color: var(--jp-ui-font-color0); line-height: var(--jp-private-commandpalette-search-height); } .lm-CommandPalette-input::-webkit-input-placeholder, .lm-CommandPalette-input::-moz-placeholder, .lm-CommandPalette-input:-ms-input-placeholder { color: var(--jp-ui-font-color3); font-size: var(--jp-ui-font-size1); } /*----------------------------------------------------------------------------- | Results |----------------------------------------------------------------------------*/ .lm-CommandPalette-header:first-child { margin-top: 0px; } .lm-CommandPalette-header { border-bottom: solid var(--jp-border-width) var(--jp-border-color2); color: var(--jp-ui-font-color1); cursor: pointer; display: flex; font-size: var(--jp-ui-font-size0); font-weight: 600; letter-spacing: 1px; margin-top: 8px; padding: 8px 0 8px 12px; text-transform: uppercase; } .lm-CommandPalette-header.lm-mod-active { background: var(--jp-layout-color2); } .lm-CommandPalette-header > mark { background-color: transparent; font-weight: bold; color: var(--jp-ui-font-color1); } .lm-CommandPalette-item { padding: 4px 12px 4px 4px; color: var(--jp-ui-font-color1); font-size: var(--jp-ui-font-size1); font-weight: 400; display: flex; } .lm-CommandPalette-item.lm-mod-disabled { color: var(--jp-ui-font-color3); } .lm-CommandPalette-item.lm-mod-active { background: var(--jp-layout-color3); } .lm-CommandPalette-item.lm-mod-active:hover:not(.lm-mod-disabled) { background: var(--jp-layout-color4); } .lm-CommandPalette-item:hover:not(.lm-mod-active):not(.lm-mod-disabled) { background: var(--jp-layout-color2); } .lm-CommandPalette-itemContent { overflow: hidden; } .lm-CommandPalette-itemLabel > mark { color: var(--jp-ui-font-color0); background-color: transparent; font-weight: bold; } .lm-CommandPalette-item.lm-mod-disabled mark { color: var(--jp-ui-font-color3); } .lm-CommandPalette-item .lm-CommandPalette-itemIcon { margin: 0 4px 0 0; position: relative; width: 16px; top: 2px; flex: 0 0 auto; } .lm-CommandPalette-item.lm-mod-disabled .lm-CommandPalette-itemIcon { opacity: 0.4; } .lm-CommandPalette-item .lm-CommandPalette-itemShortcut { flex: 0 0 auto; } .lm-CommandPalette-itemCaption { display: none; } .lm-CommandPalette-content { background-color: var(--jp-layout-color1); } .lm-CommandPalette-content:empty:after { content: 'No results'; margin: auto; margin-top: 20px; width: 100px; display: block; font-size: var(--jp-ui-font-size2); font-family: var(--jp-ui-font-family); font-weight: lighter; } .lm-CommandPalette-emptyMessage { text-align: center; margin-top: 24px; line-height: 1.32; padding: 0px 8px; color: var(--jp-content-font-color3); } /*----------------------------------------------------------------------------- | Copyright (c) 2014-2017, Jupyter Development Team. | | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ .jp-Dialog { position: absolute; z-index: 10000; display: flex; flex-direction: column; align-items: center; justify-content: center; top: 0px; left: 0px; margin: 0; padding: 0; width: 100%; height: 100%; background: var(--jp-dialog-background); } .jp-Dialog-content { display: flex; flex-direction: column; margin-left: auto; margin-right: auto; background: var(--jp-layout-color1); padding: 24px; padding-bottom: 12px; min-width: 300px; min-height: 150px; max-width: 1000px; max-height: 500px; box-sizing: border-box; box-shadow: var(--jp-elevation-z20); word-wrap: break-word; border-radius: var(--jp-border-radius); /* This is needed so that all font sizing of children done in ems is * relative to this base size */ font-size: var(--jp-ui-font-size1); color: var(--jp-ui-font-color1); } .jp-Dialog-button { overflow: visible; } button.jp-Dialog-button:focus { outline: 1px solid var(--jp-brand-color1); outline-offset: 4px; -moz-outline-radius: 0px; } button.jp-Dialog-button:focus::-moz-focus-inner { border: 0; } .jp-Dialog-header { flex: 0 0 auto; padding-bottom: 12px; font-size: var(--jp-ui-font-size3); font-weight: 400; color: var(--jp-ui-font-color0); } .jp-Dialog-body { display: flex; flex-direction: column; flex: 1 1 auto; font-size: var(--jp-ui-font-size1); background: var(--jp-layout-color1); overflow: auto; } .jp-Dialog-footer { display: flex; flex-direction: row; justify-content: flex-end; flex: 0 0 auto; margin-left: -12px; margin-right: -12px; padding: 12px; } .jp-Dialog-title { overflow: hidden; white-space: nowrap; text-overflow: ellipsis; } .jp-Dialog-body > .jp-select-wrapper { width: 100%; } .jp-Dialog-body > button { padding: 0px 16px; } .jp-Dialog-body > label { line-height: 1.4; color: var(--jp-ui-font-color0); } .jp-Dialog-button.jp-mod-styled:not(:last-child) { margin-right: 12px; } /*----------------------------------------------------------------------------- | Copyright (c) 2014-2016, Jupyter Development Team. | | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ .jp-HoverBox { position: fixed; } .jp-HoverBox.jp-mod-outofview { display: none; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ .jp-IFrame { width: 100%; height: 100%; } .jp-IFrame > iframe { border: none; } /* When drag events occur, `p-mod-override-cursor` is added to the body. Because iframes steal all cursor events, the following two rules are necessary to suppress pointer events while resize drags are occurring. There may be a better solution to this problem. */ body.lm-mod-override-cursor .jp-IFrame { position: relative; } body.lm-mod-override-cursor .jp-IFrame:before { content: ''; position: absolute; top: 0; left: 0; right: 0; bottom: 0; background: transparent; } /*----------------------------------------------------------------------------- | Copyright (c) 2014-2016, Jupyter Development Team. | | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ .jp-MainAreaWidget > :focus { outline: none; } /** * google-material-color v1.2.6 * https://github.com/danlevan/google-material-color */ :root { --md-red-50: #ffebee; --md-red-100: #ffcdd2; --md-red-200: #ef9a9a; --md-red-300: #e57373; --md-red-400: #ef5350; --md-red-500: #f44336; --md-red-600: #e53935; --md-red-700: #d32f2f; --md-red-800: #c62828; --md-red-900: #b71c1c; --md-red-A100: #ff8a80; --md-red-A200: #ff5252; --md-red-A400: #ff1744; --md-red-A700: #d50000; --md-pink-50: #fce4ec; --md-pink-100: #f8bbd0; --md-pink-200: #f48fb1; --md-pink-300: #f06292; --md-pink-400: #ec407a; --md-pink-500: #e91e63; --md-pink-600: #d81b60; --md-pink-700: #c2185b; --md-pink-800: #ad1457; --md-pink-900: #880e4f; --md-pink-A100: #ff80ab; --md-pink-A200: #ff4081; --md-pink-A400: #f50057; --md-pink-A700: #c51162; --md-purple-50: #f3e5f5; --md-purple-100: #e1bee7; --md-purple-200: #ce93d8; --md-purple-300: #ba68c8; --md-purple-400: #ab47bc; --md-purple-500: #9c27b0; --md-purple-600: #8e24aa; --md-purple-700: #7b1fa2; --md-purple-800: #6a1b9a; --md-purple-900: #4a148c; --md-purple-A100: #ea80fc; --md-purple-A200: #e040fb; --md-purple-A400: #d500f9; --md-purple-A700: #aa00ff; --md-deep-purple-50: #ede7f6; --md-deep-purple-100: #d1c4e9; --md-deep-purple-200: #b39ddb; --md-deep-purple-300: #9575cd; --md-deep-purple-400: #7e57c2; --md-deep-purple-500: #673ab7; --md-deep-purple-600: #5e35b1; --md-deep-purple-700: #512da8; --md-deep-purple-800: #4527a0; --md-deep-purple-900: #311b92; --md-deep-purple-A100: #b388ff; --md-deep-purple-A200: #7c4dff; --md-deep-purple-A400: #651fff; --md-deep-purple-A700: #6200ea; --md-indigo-50: #e8eaf6; --md-indigo-100: #c5cae9; --md-indigo-200: #9fa8da; --md-indigo-300: #7986cb; --md-indigo-400: #5c6bc0; --md-indigo-500: #3f51b5; --md-indigo-600: #3949ab; --md-indigo-700: #303f9f; --md-indigo-800: #283593; --md-indigo-900: #1a237e; --md-indigo-A100: #8c9eff; --md-indigo-A200: #536dfe; --md-indigo-A400: #3d5afe; --md-indigo-A700: #304ffe; --md-blue-50: #e3f2fd; --md-blue-100: #bbdefb; --md-blue-200: #90caf9; --md-blue-300: #64b5f6; --md-blue-400: #42a5f5; --md-blue-500: #2196f3; --md-blue-600: #1e88e5; --md-blue-700: #1976d2; --md-blue-800: #1565c0; --md-blue-900: #0d47a1; --md-blue-A100: #82b1ff; --md-blue-A200: #448aff; --md-blue-A400: #2979ff; --md-blue-A700: #2962ff; --md-light-blue-50: #e1f5fe; --md-light-blue-100: #b3e5fc; --md-light-blue-200: #81d4fa; --md-light-blue-300: #4fc3f7; --md-light-blue-400: #29b6f6; --md-light-blue-500: #03a9f4; --md-light-blue-600: #039be5; --md-light-blue-700: #0288d1; --md-light-blue-800: #0277bd; --md-light-blue-900: #01579b; --md-light-blue-A100: #80d8ff; --md-light-blue-A200: #40c4ff; --md-light-blue-A400: #00b0ff; --md-light-blue-A700: #0091ea; --md-cyan-50: #e0f7fa; --md-cyan-100: #b2ebf2; --md-cyan-200: #80deea; --md-cyan-300: #4dd0e1; --md-cyan-400: #26c6da; --md-cyan-500: #00bcd4; --md-cyan-600: #00acc1; --md-cyan-700: #0097a7; --md-cyan-800: #00838f; --md-cyan-900: #006064; --md-cyan-A100: #84ffff; --md-cyan-A200: #18ffff; --md-cyan-A400: #00e5ff; --md-cyan-A700: #00b8d4; --md-teal-50: #e0f2f1; --md-teal-100: #b2dfdb; --md-teal-200: #80cbc4; --md-teal-300: #4db6ac; --md-teal-400: #26a69a; --md-teal-500: #009688; --md-teal-600: #00897b; --md-teal-700: #00796b; --md-teal-800: #00695c; --md-teal-900: #004d40; --md-teal-A100: #a7ffeb; --md-teal-A200: #64ffda; --md-teal-A400: #1de9b6; --md-teal-A700: #00bfa5; --md-green-50: #e8f5e9; --md-green-100: #c8e6c9; --md-green-200: #a5d6a7; --md-green-300: #81c784; --md-green-400: #66bb6a; --md-green-500: #4caf50; --md-green-600: #43a047; --md-green-700: #388e3c; --md-green-800: #2e7d32; --md-green-900: #1b5e20; --md-green-A100: #b9f6ca; --md-green-A200: #69f0ae; --md-green-A400: #00e676; --md-green-A700: #00c853; --md-light-green-50: #f1f8e9; --md-light-green-100: #dcedc8; --md-light-green-200: #c5e1a5; --md-light-green-300: #aed581; --md-light-green-400: #9ccc65; --md-light-green-500: #8bc34a; --md-light-green-600: #7cb342; --md-light-green-700: #689f38; --md-light-green-800: #558b2f; --md-light-green-900: #33691e; --md-light-green-A100: #ccff90; --md-light-green-A200: #b2ff59; --md-light-green-A400: #76ff03; --md-light-green-A700: #64dd17; --md-lime-50: #f9fbe7; --md-lime-100: #f0f4c3; --md-lime-200: #e6ee9c; --md-lime-300: #dce775; --md-lime-400: #d4e157; --md-lime-500: #cddc39; --md-lime-600: #c0ca33; --md-lime-700: #afb42b; --md-lime-800: #9e9d24; --md-lime-900: #827717; --md-lime-A100: #f4ff81; --md-lime-A200: #eeff41; --md-lime-A400: #c6ff00; --md-lime-A700: #aeea00; --md-yellow-50: #fffde7; --md-yellow-100: #fff9c4; --md-yellow-200: #fff59d; --md-yellow-300: #fff176; --md-yellow-400: #ffee58; --md-yellow-500: #ffeb3b; --md-yellow-600: #fdd835; --md-yellow-700: #fbc02d; --md-yellow-800: #f9a825; --md-yellow-900: #f57f17; --md-yellow-A100: #ffff8d; --md-yellow-A200: #ffff00; --md-yellow-A400: #ffea00; --md-yellow-A700: #ffd600; --md-amber-50: #fff8e1; --md-amber-100: #ffecb3; --md-amber-200: #ffe082; --md-amber-300: #ffd54f; --md-amber-400: #ffca28; --md-amber-500: #ffc107; --md-amber-600: #ffb300; --md-amber-700: #ffa000; --md-amber-800: #ff8f00; --md-amber-900: #ff6f00; --md-amber-A100: #ffe57f; --md-amber-A200: #ffd740; --md-amber-A400: #ffc400; --md-amber-A700: #ffab00; --md-orange-50: #fff3e0; --md-orange-100: #ffe0b2; --md-orange-200: #ffcc80; --md-orange-300: #ffb74d; --md-orange-400: #ffa726; --md-orange-500: #ff9800; --md-orange-600: #fb8c00; --md-orange-700: #f57c00; --md-orange-800: #ef6c00; --md-orange-900: #e65100; --md-orange-A100: #ffd180; --md-orange-A200: #ffab40; --md-orange-A400: #ff9100; --md-orange-A700: #ff6d00; --md-deep-orange-50: #fbe9e7; --md-deep-orange-100: #ffccbc; --md-deep-orange-200: #ffab91; --md-deep-orange-300: #ff8a65; --md-deep-orange-400: #ff7043; --md-deep-orange-500: #ff5722; --md-deep-orange-600: #f4511e; --md-deep-orange-700: #e64a19; --md-deep-orange-800: #d84315; --md-deep-orange-900: #bf360c; --md-deep-orange-A100: #ff9e80; --md-deep-orange-A200: #ff6e40; --md-deep-orange-A400: #ff3d00; --md-deep-orange-A700: #dd2c00; --md-brown-50: #efebe9; --md-brown-100: #d7ccc8; --md-brown-200: #bcaaa4; --md-brown-300: #a1887f; --md-brown-400: #8d6e63; --md-brown-500: #795548; --md-brown-600: #6d4c41; --md-brown-700: #5d4037; --md-brown-800: #4e342e; --md-brown-900: #3e2723; --md-grey-50: #fafafa; --md-grey-100: #f5f5f5; --md-grey-200: #eeeeee; --md-grey-300: #e0e0e0; --md-grey-400: #bdbdbd; --md-grey-500: #9e9e9e; --md-grey-600: #757575; --md-grey-700: #616161; --md-grey-800: #424242; --md-grey-900: #212121; --md-blue-grey-50: #eceff1; --md-blue-grey-100: #cfd8dc; --md-blue-grey-200: #b0bec5; --md-blue-grey-300: #90a4ae; --md-blue-grey-400: #78909c; --md-blue-grey-500: #607d8b; --md-blue-grey-600: #546e7a; --md-blue-grey-700: #455a64; --md-blue-grey-800: #37474f; --md-blue-grey-900: #263238; } /*----------------------------------------------------------------------------- | Copyright (c) 2017, Jupyter Development Team. | | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ .jp-Spinner { position: absolute; display: flex; justify-content: center; align-items: center; z-index: 10; left: 0; top: 0; width: 100%; height: 100%; background: var(--jp-layout-color0); outline: none; } .jp-SpinnerContent { font-size: 10px; margin: 50px auto; text-indent: -9999em; width: 3em; height: 3em; border-radius: 50%; background: var(--jp-brand-color3); background: linear-gradient( to right, #f37626 10%, rgba(255, 255, 255, 0) 42% ); position: relative; animation: load3 1s infinite linear, fadeIn 1s; } .jp-SpinnerContent:before { width: 50%; height: 50%; background: #f37626; border-radius: 100% 0 0 0; position: absolute; top: 0; left: 0; content: ''; } .jp-SpinnerContent:after { background: var(--jp-layout-color0); width: 75%; height: 75%; border-radius: 50%; content: ''; margin: auto; position: absolute; top: 0; left: 0; bottom: 0; right: 0; } @keyframes fadeIn { 0% { opacity: 0; } 100% { opacity: 1; } } @keyframes load3 { 0% { transform: rotate(0deg); } 100% { transform: rotate(360deg); } } /*----------------------------------------------------------------------------- | Copyright (c) 2014-2017, Jupyter Development Team. | | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ button.jp-mod-styled { font-size: var(--jp-ui-font-size1); color: var(--jp-ui-font-color0); border: none; box-sizing: border-box; text-align: center; line-height: 32px; height: 32px; padding: 0px 12px; letter-spacing: 0.8px; outline: none; appearance: none; -webkit-appearance: none; -moz-appearance: none; } input.jp-mod-styled { background: var(--jp-input-background); height: 28px; box-sizing: border-box; border: var(--jp-border-width) solid var(--jp-border-color1); padding-left: 7px; padding-right: 7px; font-size: var(--jp-ui-font-size2); color: var(--jp-ui-font-color0); outline: none; appearance: none; -webkit-appearance: none; -moz-appearance: none; } input.jp-mod-styled:focus { border: var(--jp-border-width) solid var(--md-blue-500); box-shadow: inset 0 0 4px var(--md-blue-300); } .jp-select-wrapper { display: flex; position: relative; flex-direction: column; padding: 1px; background-color: var(--jp-layout-color1); height: 28px; box-sizing: border-box; margin-bottom: 12px; } .jp-select-wrapper.jp-mod-focused select.jp-mod-styled { border: var(--jp-border-width) solid var(--jp-input-active-border-color); box-shadow: var(--jp-input-box-shadow); background-color: var(--jp-input-active-background); } select.jp-mod-styled:hover { background-color: var(--jp-layout-color1); cursor: pointer; color: var(--jp-ui-font-color0); background-color: var(--jp-input-hover-background); box-shadow: inset 0 0px 1px rgba(0, 0, 0, 0.5); } select.jp-mod-styled { flex: 1 1 auto; height: 32px; width: 100%; font-size: var(--jp-ui-font-size2); background: var(--jp-input-background); color: var(--jp-ui-font-color0); padding: 0 25px 0 8px; border: var(--jp-border-width) solid var(--jp-input-border-color); border-radius: 0px; outline: none; appearance: none; -webkit-appearance: none; -moz-appearance: none; } /*----------------------------------------------------------------------------- | Copyright (c) 2014-2016, Jupyter Development Team. | | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ :root { --jp-private-toolbar-height: calc( 28px + var(--jp-border-width) ); /* leave 28px for content */ } .jp-Toolbar { color: var(--jp-ui-font-color1); flex: 0 0 auto; display: flex; flex-direction: row; border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color); box-shadow: var(--jp-toolbar-box-shadow); background: var(--jp-toolbar-background); min-height: var(--jp-toolbar-micro-height); padding: 2px; z-index: 1; } /* Toolbar items */ .jp-Toolbar > .jp-Toolbar-item.jp-Toolbar-spacer { flex-grow: 1; flex-shrink: 1; } .jp-Toolbar-item.jp-Toolbar-kernelStatus { display: inline-block; width: 32px; background-repeat: no-repeat; background-position: center; background-size: 16px; } .jp-Toolbar > .jp-Toolbar-item { flex: 0 0 auto; display: flex; padding-left: 1px; padding-right: 1px; font-size: var(--jp-ui-font-size1); line-height: var(--jp-private-toolbar-height); height: 100%; } /* Toolbar buttons */ /* This is the div we use to wrap the react component into a Widget */ div.jp-ToolbarButton { color: transparent; border: none; box-sizing: border-box; outline: none; appearance: none; -webkit-appearance: none; -moz-appearance: none; padding: 0px; margin: 0px; } button.jp-ToolbarButtonComponent { background: var(--jp-layout-color1); border: none; box-sizing: border-box; outline: none; appearance: none; -webkit-appearance: none; -moz-appearance: none; padding: 0px 6px; margin: 0px; height: 24px; border-radius: var(--jp-border-radius); display: flex; align-items: center; text-align: center; font-size: 14px; min-width: unset; min-height: unset; } button.jp-ToolbarButtonComponent:disabled { opacity: 0.4; } button.jp-ToolbarButtonComponent span { padding: 0px; flex: 0 0 auto; } button.jp-ToolbarButtonComponent .jp-ToolbarButtonComponent-label { font-size: var(--jp-ui-font-size1); line-height: 100%; padding-left: 2px; color: var(--jp-ui-font-color1); } /*----------------------------------------------------------------------------- | Copyright (c) 2014-2017, Jupyter Development Team. | | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /* This file was auto-generated by ensurePackage() in @jupyterlab/buildutils */ /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Copyright (c) 2014-2017, PhosphorJS Contributors | | Distributed under the terms of the BSD 3-Clause License. | | The full license is in the file LICENSE, distributed with this software. |----------------------------------------------------------------------------*/ /* <DEPRECATED> */ body.p-mod-override-cursor *, /* </DEPRECATED> */ body.lm-mod-override-cursor * { cursor: inherit !important; } /*----------------------------------------------------------------------------- | Copyright (c) 2014-2016, Jupyter Development Team. | | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ .jp-JSONEditor { display: flex; flex-direction: column; width: 100%; } .jp-JSONEditor-host { flex: 1 1 auto; border: var(--jp-border-width) solid var(--jp-input-border-color); border-radius: 0px; background: var(--jp-layout-color0); min-height: 50px; padding: 1px; } .jp-JSONEditor.jp-mod-error .jp-JSONEditor-host { border-color: red; outline-color: red; } .jp-JSONEditor-header { display: flex; flex: 1 0 auto; padding: 0 0 0 12px; } .jp-JSONEditor-header label { flex: 0 0 auto; } .jp-JSONEditor-commitButton { height: 16px; width: 16px; background-size: 18px; background-repeat: no-repeat; background-position: center; } .jp-JSONEditor-host.jp-mod-focused { background-color: var(--jp-input-active-background); border: 1px solid var(--jp-input-active-border-color); box-shadow: var(--jp-input-box-shadow); } .jp-Editor.jp-mod-dropTarget { border: var(--jp-border-width) solid var(--jp-input-active-border-color); box-shadow: var(--jp-input-box-shadow); } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /* This file was auto-generated by ensurePackage() in @jupyterlab/buildutils */ /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /* BASICS */ .CodeMirror { /* Set height, width, borders, and global font properties here */ font-family: monospace; height: 300px; color: black; direction: ltr; } /* PADDING */ .CodeMirror-lines { padding: 4px 0; /* Vertical padding around content */ } .CodeMirror pre.CodeMirror-line, .CodeMirror pre.CodeMirror-line-like { padding: 0 4px; /* Horizontal padding of content */ } .CodeMirror-scrollbar-filler, .CodeMirror-gutter-filler { background-color: white; /* The little square between H and V scrollbars */ } /* GUTTER */ .CodeMirror-gutters { border-right: 1px solid #ddd; background-color: #f7f7f7; white-space: nowrap; } .CodeMirror-linenumbers {} .CodeMirror-linenumber { padding: 0 3px 0 5px; min-width: 20px; text-align: right; color: #999; white-space: nowrap; } .CodeMirror-guttermarker { color: black; } .CodeMirror-guttermarker-subtle { color: #999; } /* CURSOR */ .CodeMirror-cursor { border-left: 1px solid black; border-right: none; width: 0; } /* Shown when moving in bi-directional text */ .CodeMirror div.CodeMirror-secondarycursor { border-left: 1px solid silver; } .cm-fat-cursor .CodeMirror-cursor { width: auto; border: 0 !important; background: #7e7; } .cm-fat-cursor div.CodeMirror-cursors { z-index: 1; } .cm-fat-cursor-mark { background-color: rgba(20, 255, 20, 0.5); -webkit-animation: blink 1.06s steps(1) infinite; -moz-animation: blink 1.06s steps(1) infinite; animation: blink 1.06s steps(1) infinite; } .cm-animate-fat-cursor { width: auto; border: 0; -webkit-animation: blink 1.06s steps(1) infinite; -moz-animation: blink 1.06s steps(1) infinite; animation: blink 1.06s steps(1) infinite; background-color: #7e7; } @-moz-keyframes blink { 0% {} 50% { background-color: transparent; } 100% {} } @-webkit-keyframes blink { 0% {} 50% { background-color: transparent; } 100% {} } @keyframes blink { 0% {} 50% { background-color: transparent; } 100% {} } /* Can style cursor different in overwrite (non-insert) mode */ .CodeMirror-overwrite .CodeMirror-cursor {} .cm-tab { display: inline-block; text-decoration: inherit; } .CodeMirror-rulers { position: absolute; left: 0; right: 0; top: -50px; bottom: 0; overflow: hidden; } .CodeMirror-ruler { border-left: 1px solid #ccc; top: 0; bottom: 0; position: absolute; } /* DEFAULT THEME */ .cm-s-default .cm-header {color: blue;} .cm-s-default .cm-quote {color: #090;} .cm-negative {color: #d44;} .cm-positive {color: #292;} .cm-header, .cm-strong {font-weight: bold;} .cm-em {font-style: italic;} .cm-link {text-decoration: underline;} .cm-strikethrough {text-decoration: line-through;} .cm-s-default .cm-keyword {color: #708;} .cm-s-default .cm-atom {color: #219;} .cm-s-default .cm-number {color: #164;} .cm-s-default .cm-def {color: #00f;} .cm-s-default .cm-variable, .cm-s-default .cm-punctuation, .cm-s-default .cm-property, .cm-s-default .cm-operator {} .cm-s-default .cm-variable-2 {color: #05a;} .cm-s-default .cm-variable-3, .cm-s-default .cm-type {color: #085;} .cm-s-default .cm-comment {color: #a50;} .cm-s-default .cm-string {color: #a11;} .cm-s-default .cm-string-2 {color: #f50;} .cm-s-default .cm-meta {color: #555;} .cm-s-default .cm-qualifier {color: #555;} .cm-s-default .cm-builtin {color: #30a;} .cm-s-default .cm-bracket {color: #997;} .cm-s-default .cm-tag {color: #170;} .cm-s-default .cm-attribute {color: #00c;} .cm-s-default .cm-hr {color: #999;} .cm-s-default .cm-link {color: #00c;} .cm-s-default .cm-error {color: #f00;} .cm-invalidchar {color: #f00;} .CodeMirror-composing { border-bottom: 2px solid; } /* Default styles for common addons */ div.CodeMirror span.CodeMirror-matchingbracket {color: #0b0;} div.CodeMirror span.CodeMirror-nonmatchingbracket {color: #a22;} .CodeMirror-matchingtag { background: rgba(255, 150, 0, .3); } .CodeMirror-activeline-background {background: #e8f2ff;} /* STOP */ /* The rest of this file contains styles related to the mechanics of the editor. You probably shouldn't touch them. */ .CodeMirror { position: relative; overflow: hidden; background: white; } .CodeMirror-scroll { overflow: scroll !important; /* Things will break if this is overridden */ /* 30px is the magic margin used to hide the element's real scrollbars */ /* See overflow: hidden in .CodeMirror */ margin-bottom: -30px; margin-right: -30px; padding-bottom: 30px; height: 100%; outline: none; /* Prevent dragging from highlighting the element */ position: relative; } .CodeMirror-sizer { position: relative; border-right: 30px solid transparent; } /* The fake, visible scrollbars. Used to force redraw during scrolling before actual scrolling happens, thus preventing shaking and flickering artifacts. */ .CodeMirror-vscrollbar, .CodeMirror-hscrollbar, .CodeMirror-scrollbar-filler, .CodeMirror-gutter-filler { position: absolute; z-index: 6; display: none; } .CodeMirror-vscrollbar { right: 0; top: 0; overflow-x: hidden; overflow-y: scroll; } .CodeMirror-hscrollbar { bottom: 0; left: 0; overflow-y: hidden; overflow-x: scroll; } .CodeMirror-scrollbar-filler { right: 0; bottom: 0; } .CodeMirror-gutter-filler { left: 0; bottom: 0; } .CodeMirror-gutters { position: absolute; left: 0; top: 0; min-height: 100%; z-index: 3; } .CodeMirror-gutter { white-space: normal; height: 100%; display: inline-block; vertical-align: top; margin-bottom: -30px; } .CodeMirror-gutter-wrapper { position: absolute; z-index: 4; background: none !important; border: none !important; } .CodeMirror-gutter-background { position: absolute; top: 0; bottom: 0; z-index: 4; } .CodeMirror-gutter-elt { position: absolute; cursor: default; z-index: 4; } .CodeMirror-gutter-wrapper ::selection { background-color: transparent } .CodeMirror-gutter-wrapper ::-moz-selection { background-color: transparent } .CodeMirror-lines { cursor: text; min-height: 1px; /* prevents collapsing before first draw */ } .CodeMirror pre.CodeMirror-line, .CodeMirror pre.CodeMirror-line-like { /* Reset some styles that the rest of the page might have set */ -moz-border-radius: 0; -webkit-border-radius: 0; border-radius: 0; border-width: 0; background: transparent; font-family: inherit; font-size: inherit; margin: 0; white-space: pre; word-wrap: normal; line-height: inherit; color: inherit; z-index: 2; position: relative; overflow: visible; -webkit-tap-highlight-color: transparent; -webkit-font-variant-ligatures: contextual; font-variant-ligatures: contextual; } .CodeMirror-wrap pre.CodeMirror-line, .CodeMirror-wrap pre.CodeMirror-line-like { word-wrap: break-word; white-space: pre-wrap; word-break: normal; } .CodeMirror-linebackground { position: absolute; left: 0; right: 0; top: 0; bottom: 0; z-index: 0; } .CodeMirror-linewidget { position: relative; z-index: 2; padding: 0.1px; /* Force widget margins to stay inside of the container */ } .CodeMirror-widget {} .CodeMirror-rtl pre { direction: rtl; } .CodeMirror-code { outline: none; } /* Force content-box sizing for the elements where we expect it */ .CodeMirror-scroll, .CodeMirror-sizer, .CodeMirror-gutter, .CodeMirror-gutters, .CodeMirror-linenumber { -moz-box-sizing: content-box; box-sizing: content-box; } .CodeMirror-measure { position: absolute; width: 100%; height: 0; overflow: hidden; visibility: hidden; } .CodeMirror-cursor { position: absolute; pointer-events: none; } .CodeMirror-measure pre { position: static; } div.CodeMirror-cursors { visibility: hidden; position: relative; z-index: 3; } div.CodeMirror-dragcursors { visibility: visible; } .CodeMirror-focused div.CodeMirror-cursors { visibility: visible; } .CodeMirror-selected { background: #d9d9d9; } .CodeMirror-focused .CodeMirror-selected { background: #d7d4f0; } .CodeMirror-crosshair { cursor: crosshair; } .CodeMirror-line::selection, .CodeMirror-line > span::selection, .CodeMirror-line > span > span::selection { background: #d7d4f0; } .CodeMirror-line::-moz-selection, .CodeMirror-line > span::-moz-selection, .CodeMirror-line > span > span::-moz-selection { background: #d7d4f0; } .cm-searching { background-color: #ffa; background-color: rgba(255, 255, 0, .4); } /* Used to force a border model for a node */ .cm-force-border { padding-right: .1px; } @media print { /* Hide the cursor when printing */ .CodeMirror div.CodeMirror-cursors { visibility: hidden; } } /* See issue #2901 */ .cm-tab-wrap-hack:after { content: ''; } /* Help users use markselection to safely style text background */ span.CodeMirror-selectedtext { background: none; } .CodeMirror-dialog { position: absolute; left: 0; right: 0; background: inherit; z-index: 15; padding: .1em .8em; overflow: hidden; color: inherit; } .CodeMirror-dialog-top { border-bottom: 1px solid #eee; top: 0; } .CodeMirror-dialog-bottom { border-top: 1px solid #eee; bottom: 0; } .CodeMirror-dialog input { border: none; outline: none; background: transparent; width: 20em; color: inherit; font-family: monospace; } .CodeMirror-dialog button { font-size: 70%; } .CodeMirror-foldmarker { color: blue; text-shadow: #b9f 1px 1px 2px, #b9f -1px -1px 2px, #b9f 1px -1px 2px, #b9f -1px 1px 2px; font-family: arial; line-height: .3; cursor: pointer; } .CodeMirror-foldgutter { width: .7em; } .CodeMirror-foldgutter-open, .CodeMirror-foldgutter-folded { cursor: pointer; } .CodeMirror-foldgutter-open:after { content: \"\\25BE\"; } .CodeMirror-foldgutter-folded:after { content: \"\\25B8\"; } /* Name: material Author: Mattia Astorino (http://github.com/equinusocio) Website: https://material-theme.site/ */ .cm-s-material.CodeMirror { background-color: #263238; color: #EEFFFF; } .cm-s-material .CodeMirror-gutters { background: #263238; color: #546E7A; border: none; } .cm-s-material .CodeMirror-guttermarker, .cm-s-material .CodeMirror-guttermarker-subtle, .cm-s-material .CodeMirror-linenumber { color: #546E7A; } .cm-s-material .CodeMirror-cursor { border-left: 1px solid #FFCC00; } .cm-s-material div.CodeMirror-selected { background: rgba(128, 203, 196, 0.2); } .cm-s-material.CodeMirror-focused div.CodeMirror-selected { background: rgba(128, 203, 196, 0.2); } .cm-s-material .CodeMirror-line::selection, .cm-s-material .CodeMirror-line>span::selection, .cm-s-material .CodeMirror-line>span>span::selection { background: rgba(128, 203, 196, 0.2); } .cm-s-material .CodeMirror-line::-moz-selection, .cm-s-material .CodeMirror-line>span::-moz-selection, .cm-s-material .CodeMirror-line>span>span::-moz-selection { background: rgba(128, 203, 196, 0.2); } .cm-s-material .CodeMirror-activeline-background { background: rgba(0, 0, 0, 0.5); } .cm-s-material .cm-keyword { color: #C792EA; } .cm-s-material .cm-operator { color: #89DDFF; } .cm-s-material .cm-variable-2 { color: #EEFFFF; } .cm-s-material .cm-variable-3, .cm-s-material .cm-type { color: #f07178; } .cm-s-material .cm-builtin { color: #FFCB6B; } .cm-s-material .cm-atom { color: #F78C6C; } .cm-s-material .cm-number { color: #FF5370; } .cm-s-material .cm-def { color: #82AAFF; } .cm-s-material .cm-string { color: #C3E88D; } .cm-s-material .cm-string-2 { color: #f07178; } .cm-s-material .cm-comment { color: #546E7A; } .cm-s-material .cm-variable { color: #f07178; } .cm-s-material .cm-tag { color: #FF5370; } .cm-s-material .cm-meta { color: #FFCB6B; } .cm-s-material .cm-attribute { color: #C792EA; } .cm-s-material .cm-property { color: #C792EA; } .cm-s-material .cm-qualifier { color: #DECB6B; } .cm-s-material .cm-variable-3, .cm-s-material .cm-type { color: #DECB6B; } .cm-s-material .cm-error { color: rgba(255, 255, 255, 1.0); background-color: #FF5370; } .cm-s-material .CodeMirror-matchingbracket { text-decoration: underline; color: white !important; } /** * \" * Using Zenburn color palette from the Emacs Zenburn Theme * https://github.com/bbatsov/zenburn-emacs/blob/master/zenburn-theme.el * * Also using parts of https://github.com/xavi/coderay-lighttable-theme * \" * From: https://github.com/wisenomad/zenburn-lighttable-theme/blob/master/zenburn.css */ .cm-s-zenburn .CodeMirror-gutters { background: #3f3f3f !important; } .cm-s-zenburn .CodeMirror-foldgutter-open, .CodeMirror-foldgutter-folded { color: #999; } .cm-s-zenburn .CodeMirror-cursor { border-left: 1px solid white; } .cm-s-zenburn { background-color: #3f3f3f; color: #dcdccc; } .cm-s-zenburn span.cm-builtin { color: #dcdccc; font-weight: bold; } .cm-s-zenburn span.cm-comment { color: #7f9f7f; } .cm-s-zenburn span.cm-keyword { color: #f0dfaf; font-weight: bold; } .cm-s-zenburn span.cm-atom { color: #bfebbf; } .cm-s-zenburn span.cm-def { color: #dcdccc; } .cm-s-zenburn span.cm-variable { color: #dfaf8f; } .cm-s-zenburn span.cm-variable-2 { color: #dcdccc; } .cm-s-zenburn span.cm-string { color: #cc9393; } .cm-s-zenburn span.cm-string-2 { color: #cc9393; } .cm-s-zenburn span.cm-number { color: #dcdccc; } .cm-s-zenburn span.cm-tag { color: #93e0e3; } .cm-s-zenburn span.cm-property { color: #dfaf8f; } .cm-s-zenburn span.cm-attribute { color: #dfaf8f; } .cm-s-zenburn span.cm-qualifier { color: #7cb8bb; } .cm-s-zenburn span.cm-meta { color: #f0dfaf; } .cm-s-zenburn span.cm-header { color: #f0efd0; } .cm-s-zenburn span.cm-operator { color: #f0efd0; } .cm-s-zenburn span.CodeMirror-matchingbracket { box-sizing: border-box; background: transparent; border-bottom: 1px solid; } .cm-s-zenburn span.CodeMirror-nonmatchingbracket { border-bottom: 1px solid; background: none; } .cm-s-zenburn .CodeMirror-activeline { background: #000000; } .cm-s-zenburn .CodeMirror-activeline-background { background: #000000; } .cm-s-zenburn div.CodeMirror-selected { background: #545454; } .cm-s-zenburn .CodeMirror-focused div.CodeMirror-selected { background: #4f4f4f; } .cm-s-abcdef.CodeMirror { background: #0f0f0f; color: #defdef; } .cm-s-abcdef div.CodeMirror-selected { background: #515151; } .cm-s-abcdef .CodeMirror-line::selection, .cm-s-abcdef .CodeMirror-line > span::selection, .cm-s-abcdef .CodeMirror-line > span > span::selection { background: rgba(56, 56, 56, 0.99); } .cm-s-abcdef .CodeMirror-line::-moz-selection, .cm-s-abcdef .CodeMirror-line > span::-moz-selection, .cm-s-abcdef .CodeMirror-line > span > span::-moz-selection { background: rgba(56, 56, 56, 0.99); } .cm-s-abcdef .CodeMirror-gutters { background: #555; border-right: 2px solid #314151; } .cm-s-abcdef .CodeMirror-guttermarker { color: #222; } .cm-s-abcdef .CodeMirror-guttermarker-subtle { color: azure; } .cm-s-abcdef .CodeMirror-linenumber { color: #FFFFFF; } .cm-s-abcdef .CodeMirror-cursor { border-left: 1px solid #00FF00; } .cm-s-abcdef span.cm-keyword { color: darkgoldenrod; font-weight: bold; } .cm-s-abcdef span.cm-atom { color: #77F; } .cm-s-abcdef span.cm-number { color: violet; } .cm-s-abcdef span.cm-def { color: #fffabc; } .cm-s-abcdef span.cm-variable { color: #abcdef; } .cm-s-abcdef span.cm-variable-2 { color: #cacbcc; } .cm-s-abcdef span.cm-variable-3, .cm-s-abcdef span.cm-type { color: #def; } .cm-s-abcdef span.cm-property { color: #fedcba; } .cm-s-abcdef span.cm-operator { color: #ff0; } .cm-s-abcdef span.cm-comment { color: #7a7b7c; font-style: italic;} .cm-s-abcdef span.cm-string { color: #2b4; } .cm-s-abcdef span.cm-meta { color: #C9F; } .cm-s-abcdef span.cm-qualifier { color: #FFF700; } .cm-s-abcdef span.cm-builtin { color: #30aabc; } .cm-s-abcdef span.cm-bracket { color: #8a8a8a; } .cm-s-abcdef span.cm-tag { color: #FFDD44; } .cm-s-abcdef span.cm-attribute { color: #DDFF00; } .cm-s-abcdef span.cm-error { color: #FF0000; } .cm-s-abcdef span.cm-header { color: aquamarine; font-weight: bold; } .cm-s-abcdef span.cm-link { color: blueviolet; } .cm-s-abcdef .CodeMirror-activeline-background { background: #314151; } /* Name: Base16 Default Light Author: Chris Kempson (http://chriskempson.com) CodeMirror template by Jan T. Sott (https://github.com/idleberg/base16-codemirror) Original Base16 color scheme by Chris Kempson (https://github.com/chriskempson/base16) */ .cm-s-base16-light.CodeMirror { background: #f5f5f5; color: #202020; } .cm-s-base16-light div.CodeMirror-selected { background: #e0e0e0; } .cm-s-base16-light .CodeMirror-line::selection, .cm-s-base16-light .CodeMirror-line > span::selection, .cm-s-base16-light .CodeMirror-line > span > span::selection { background: #e0e0e0; } .cm-s-base16-light .CodeMirror-line::-moz-selection, .cm-s-base16-light .CodeMirror-line > span::-moz-selection, .cm-s-base16-light .CodeMirror-line > span > span::-moz-selection { background: #e0e0e0; } .cm-s-base16-light .CodeMirror-gutters { background: #f5f5f5; border-right: 0px; } .cm-s-base16-light .CodeMirror-guttermarker { color: #ac4142; } .cm-s-base16-light .CodeMirror-guttermarker-subtle { color: #b0b0b0; } .cm-s-base16-light .CodeMirror-linenumber { color: #b0b0b0; } .cm-s-base16-light .CodeMirror-cursor { border-left: 1px solid #505050; } .cm-s-base16-light span.cm-comment { color: #8f5536; } .cm-s-base16-light span.cm-atom { color: #aa759f; } .cm-s-base16-light span.cm-number { color: #aa759f; } .cm-s-base16-light span.cm-property, .cm-s-base16-light span.cm-attribute { color: #90a959; } .cm-s-base16-light span.cm-keyword { color: #ac4142; } .cm-s-base16-light span.cm-string { color: #f4bf75; } .cm-s-base16-light span.cm-variable { color: #90a959; } .cm-s-base16-light span.cm-variable-2 { color: #6a9fb5; } .cm-s-base16-light span.cm-def { color: #d28445; } .cm-s-base16-light span.cm-bracket { color: #202020; } .cm-s-base16-light span.cm-tag { color: #ac4142; } .cm-s-base16-light span.cm-link { color: #aa759f; } .cm-s-base16-light span.cm-error { background: #ac4142; color: #505050; } .cm-s-base16-light .CodeMirror-activeline-background { background: #DDDCDC; } .cm-s-base16-light .CodeMirror-matchingbracket { color: #f5f5f5 !important; background-color: #6A9FB5 !important} /* Name: Base16 Default Dark Author: Chris Kempson (http://chriskempson.com) CodeMirror template by Jan T. Sott (https://github.com/idleberg/base16-codemirror) Original Base16 color scheme by Chris Kempson (https://github.com/chriskempson/base16) */ .cm-s-base16-dark.CodeMirror { background: #151515; color: #e0e0e0; } .cm-s-base16-dark div.CodeMirror-selected { background: #303030; } .cm-s-base16-dark .CodeMirror-line::selection, .cm-s-base16-dark .CodeMirror-line > span::selection, .cm-s-base16-dark .CodeMirror-line > span > span::selection { background: rgba(48, 48, 48, .99); } .cm-s-base16-dark .CodeMirror-line::-moz-selection, .cm-s-base16-dark .CodeMirror-line > span::-moz-selection, .cm-s-base16-dark .CodeMirror-line > span > span::-moz-selection { background: rgba(48, 48, 48, .99); } .cm-s-base16-dark .CodeMirror-gutters { background: #151515; border-right: 0px; } .cm-s-base16-dark .CodeMirror-guttermarker { color: #ac4142; } .cm-s-base16-dark .CodeMirror-guttermarker-subtle { color: #505050; } .cm-s-base16-dark .CodeMirror-linenumber { color: #505050; } .cm-s-base16-dark .CodeMirror-cursor { border-left: 1px solid #b0b0b0; } .cm-s-base16-dark span.cm-comment { color: #8f5536; } .cm-s-base16-dark span.cm-atom { color: #aa759f; } .cm-s-base16-dark span.cm-number { color: #aa759f; } .cm-s-base16-dark span.cm-property, .cm-s-base16-dark span.cm-attribute { color: #90a959; } .cm-s-base16-dark span.cm-keyword { color: #ac4142; } .cm-s-base16-dark span.cm-string { color: #f4bf75; } .cm-s-base16-dark span.cm-variable { color: #90a959; } .cm-s-base16-dark span.cm-variable-2 { color: #6a9fb5; } .cm-s-base16-dark span.cm-def { color: #d28445; } .cm-s-base16-dark span.cm-bracket { color: #e0e0e0; } .cm-s-base16-dark span.cm-tag { color: #ac4142; } .cm-s-base16-dark span.cm-link { color: #aa759f; } .cm-s-base16-dark span.cm-error { background: #ac4142; color: #b0b0b0; } .cm-s-base16-dark .CodeMirror-activeline-background { background: #202020; } .cm-s-base16-dark .CodeMirror-matchingbracket { text-decoration: underline; color: white !important; } /* Name: dracula Author: Michael Kaminsky (http://github.com/mkaminsky11) Original dracula color scheme by Zeno Rocha (https://github.com/zenorocha/dracula-theme) */ .cm-s-dracula.CodeMirror, .cm-s-dracula .CodeMirror-gutters { background-color: #282a36 !important; color: #f8f8f2 !important; border: none; } .cm-s-dracula .CodeMirror-gutters { color: #282a36; } .cm-s-dracula .CodeMirror-cursor { border-left: solid thin #f8f8f0; } .cm-s-dracula .CodeMirror-linenumber { color: #6D8A88; } .cm-s-dracula .CodeMirror-selected { background: rgba(255, 255, 255, 0.10); } .cm-s-dracula .CodeMirror-line::selection, .cm-s-dracula .CodeMirror-line > span::selection, .cm-s-dracula .CodeMirror-line > span > span::selection { background: rgba(255, 255, 255, 0.10); } .cm-s-dracula .CodeMirror-line::-moz-selection, .cm-s-dracula .CodeMirror-line > span::-moz-selection, .cm-s-dracula .CodeMirror-line > span > span::-moz-selection { background: rgba(255, 255, 255, 0.10); } .cm-s-dracula span.cm-comment { color: #6272a4; } .cm-s-dracula span.cm-string, .cm-s-dracula span.cm-string-2 { color: #f1fa8c; } .cm-s-dracula span.cm-number { color: #bd93f9; } .cm-s-dracula span.cm-variable { color: #50fa7b; } .cm-s-dracula span.cm-variable-2 { color: white; } .cm-s-dracula span.cm-def { color: #50fa7b; } .cm-s-dracula span.cm-operator { color: #ff79c6; } .cm-s-dracula span.cm-keyword { color: #ff79c6; } .cm-s-dracula span.cm-atom { color: #bd93f9; } .cm-s-dracula span.cm-meta { color: #f8f8f2; } .cm-s-dracula span.cm-tag { color: #ff79c6; } .cm-s-dracula span.cm-attribute { color: #50fa7b; } .cm-s-dracula span.cm-qualifier { color: #50fa7b; } .cm-s-dracula span.cm-property { color: #66d9ef; } .cm-s-dracula span.cm-builtin { color: #50fa7b; } .cm-s-dracula span.cm-variable-3, .cm-s-dracula span.cm-type { color: #ffb86c; } .cm-s-dracula .CodeMirror-activeline-background { background: rgba(255,255,255,0.1); } .cm-s-dracula .CodeMirror-matchingbracket { text-decoration: underline; color: white !important; } /* Name: Hopscotch Author: Jan T. Sott CodeMirror template by Jan T. Sott (https://github.com/idleberg/base16-codemirror) Original Base16 color scheme by Chris Kempson (https://github.com/chriskempson/base16) */ .cm-s-hopscotch.CodeMirror {background: #322931; color: #d5d3d5;} .cm-s-hopscotch div.CodeMirror-selected {background: #433b42 !important;} .cm-s-hopscotch .CodeMirror-gutters {background: #322931; border-right: 0px;} .cm-s-hopscotch .CodeMirror-linenumber {color: #797379;} .cm-s-hopscotch .CodeMirror-cursor {border-left: 1px solid #989498 !important;} .cm-s-hopscotch span.cm-comment {color: #b33508;} .cm-s-hopscotch span.cm-atom {color: #c85e7c;} .cm-s-hopscotch span.cm-number {color: #c85e7c;} .cm-s-hopscotch span.cm-property, .cm-s-hopscotch span.cm-attribute {color: #8fc13e;} .cm-s-hopscotch span.cm-keyword {color: #dd464c;} .cm-s-hopscotch span.cm-string {color: #fdcc59;} .cm-s-hopscotch span.cm-variable {color: #8fc13e;} .cm-s-hopscotch span.cm-variable-2 {color: #1290bf;} .cm-s-hopscotch span.cm-def {color: #fd8b19;} .cm-s-hopscotch span.cm-error {background: #dd464c; color: #989498;} .cm-s-hopscotch span.cm-bracket {color: #d5d3d5;} .cm-s-hopscotch span.cm-tag {color: #dd464c;} .cm-s-hopscotch span.cm-link {color: #c85e7c;} .cm-s-hopscotch .CodeMirror-matchingbracket { text-decoration: underline; color: white !important;} .cm-s-hopscotch .CodeMirror-activeline-background { background: #302020; } /****************************************************************/ /* Based on mbonaci's Brackets mbo theme */ /* https://github.com/mbonaci/global/blob/master/Mbo.tmTheme */ /* Create your own: http://tmtheme-editor.herokuapp.com */ /****************************************************************/ .cm-s-mbo.CodeMirror { background: #2c2c2c; color: #ffffec; } .cm-s-mbo div.CodeMirror-selected { background: #716C62; } .cm-s-mbo .CodeMirror-line::selection, .cm-s-mbo .CodeMirror-line > span::selection, .cm-s-mbo .CodeMirror-line > span > span::selection { background: rgba(113, 108, 98, .99); } .cm-s-mbo .CodeMirror-line::-moz-selection, .cm-s-mbo .CodeMirror-line > span::-moz-selection, .cm-s-mbo .CodeMirror-line > span > span::-moz-selection { background: rgba(113, 108, 98, .99); } .cm-s-mbo .CodeMirror-gutters { background: #4e4e4e; border-right: 0px; } .cm-s-mbo .CodeMirror-guttermarker { color: white; } .cm-s-mbo .CodeMirror-guttermarker-subtle { color: grey; } .cm-s-mbo .CodeMirror-linenumber { color: #dadada; } .cm-s-mbo .CodeMirror-cursor { border-left: 1px solid #ffffec; } .cm-s-mbo span.cm-comment { color: #95958a; } .cm-s-mbo span.cm-atom { color: #00a8c6; } .cm-s-mbo span.cm-number { color: #00a8c6; } .cm-s-mbo span.cm-property, .cm-s-mbo span.cm-attribute { color: #9ddfe9; } .cm-s-mbo span.cm-keyword { color: #ffb928; } .cm-s-mbo span.cm-string { color: #ffcf6c; } .cm-s-mbo span.cm-string.cm-property { color: #ffffec; } .cm-s-mbo span.cm-variable { color: #ffffec; } .cm-s-mbo span.cm-variable-2 { color: #00a8c6; } .cm-s-mbo span.cm-def { color: #ffffec; } .cm-s-mbo span.cm-bracket { color: #fffffc; font-weight: bold; } .cm-s-mbo span.cm-tag { color: #9ddfe9; } .cm-s-mbo span.cm-link { color: #f54b07; } .cm-s-mbo span.cm-error { border-bottom: #636363; color: #ffffec; } .cm-s-mbo span.cm-qualifier { color: #ffffec; } .cm-s-mbo .CodeMirror-activeline-background { background: #494b41; } .cm-s-mbo .CodeMirror-matchingbracket { color: #ffb928 !important; } .cm-s-mbo .CodeMirror-matchingtag { background: rgba(255, 255, 255, .37); } /* MDN-LIKE Theme - Mozilla Ported to CodeMirror by Peter Kroon <plakroon@gmail.com> Report bugs/issues here: https://github.com/codemirror/CodeMirror/issues GitHub: @peterkroon The mdn-like theme is inspired on the displayed code examples at: https://developer.mozilla.org/en-US/docs/Web/CSS/animation */ .cm-s-mdn-like.CodeMirror { color: #999; background-color: #fff; } .cm-s-mdn-like div.CodeMirror-selected { background: #cfc; } .cm-s-mdn-like .CodeMirror-line::selection, .cm-s-mdn-like .CodeMirror-line > span::selection, .cm-s-mdn-like .CodeMirror-line > span > span::selection { background: #cfc; } .cm-s-mdn-like .CodeMirror-line::-moz-selection, .cm-s-mdn-like .CodeMirror-line > span::-moz-selection, .cm-s-mdn-like .CodeMirror-line > span > span::-moz-selection { background: #cfc; } .cm-s-mdn-like .CodeMirror-gutters { background: #f8f8f8; border-left: 6px solid rgba(0,83,159,0.65); color: #333; } .cm-s-mdn-like .CodeMirror-linenumber { color: #aaa; padding-left: 8px; } .cm-s-mdn-like .CodeMirror-cursor { border-left: 2px solid #222; } .cm-s-mdn-like .cm-keyword { color: #6262FF; } .cm-s-mdn-like .cm-atom { color: #F90; } .cm-s-mdn-like .cm-number { color: #ca7841; } .cm-s-mdn-like .cm-def { color: #8DA6CE; } .cm-s-mdn-like span.cm-variable-2, .cm-s-mdn-like span.cm-tag { color: #690; } .cm-s-mdn-like span.cm-variable-3, .cm-s-mdn-like span.cm-def, .cm-s-mdn-like span.cm-type { color: #07a; } .cm-s-mdn-like .cm-variable { color: #07a; } .cm-s-mdn-like .cm-property { color: #905; } .cm-s-mdn-like .cm-qualifier { color: #690; } .cm-s-mdn-like .cm-operator { color: #cda869; } .cm-s-mdn-like .cm-comment { color:#777; font-weight:normal; } .cm-s-mdn-like .cm-string { color:#07a; font-style:italic; } .cm-s-mdn-like .cm-string-2 { color:#bd6b18; } /*?*/ .cm-s-mdn-like .cm-meta { color: #000; } /*?*/ .cm-s-mdn-like .cm-builtin { color: #9B7536; } /*?*/ .cm-s-mdn-like .cm-tag { color: #997643; } .cm-s-mdn-like .cm-attribute { color: #d6bb6d; } /*?*/ .cm-s-mdn-like .cm-header { color: #FF6400; } .cm-s-mdn-like .cm-hr { color: #AEAEAE; } .cm-s-mdn-like .cm-link { color:#ad9361; font-style:italic; text-decoration:none; } .cm-s-mdn-like .cm-error { border-bottom: 1px solid red; } div.cm-s-mdn-like .CodeMirror-activeline-background { background: #efefff; } div.cm-s-mdn-like span.CodeMirror-matchingbracket { outline:1px solid grey; color: inherit; } .cm-s-mdn-like.CodeMirror { background-image: url(data:image/png;base64,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); } /* Name: seti Author: Michael Kaminsky (http://github.com/mkaminsky11) Original seti color scheme by Jesse Weed (https://github.com/jesseweed/seti-syntax) */ .cm-s-seti.CodeMirror { background-color: #151718 !important; color: #CFD2D1 !important; border: none; } .cm-s-seti .CodeMirror-gutters { color: #404b53; background-color: #0E1112; border: none; } .cm-s-seti .CodeMirror-cursor { border-left: solid thin #f8f8f0; } .cm-s-seti .CodeMirror-linenumber { color: #6D8A88; } .cm-s-seti.CodeMirror-focused div.CodeMirror-selected { background: rgba(255, 255, 255, 0.10); } .cm-s-seti .CodeMirror-line::selection, .cm-s-seti .CodeMirror-line > span::selection, .cm-s-seti .CodeMirror-line > span > span::selection { background: rgba(255, 255, 255, 0.10); } .cm-s-seti .CodeMirror-line::-moz-selection, .cm-s-seti .CodeMirror-line > span::-moz-selection, .cm-s-seti .CodeMirror-line > span > span::-moz-selection { background: rgba(255, 255, 255, 0.10); } .cm-s-seti span.cm-comment { color: #41535b; } .cm-s-seti span.cm-string, .cm-s-seti span.cm-string-2 { color: #55b5db; } .cm-s-seti span.cm-number { color: #cd3f45; } .cm-s-seti span.cm-variable { color: #55b5db; } .cm-s-seti span.cm-variable-2 { color: #a074c4; } .cm-s-seti span.cm-def { color: #55b5db; } .cm-s-seti span.cm-keyword { color: #ff79c6; } .cm-s-seti span.cm-operator { color: #9fca56; } .cm-s-seti span.cm-keyword { color: #e6cd69; } .cm-s-seti span.cm-atom { color: #cd3f45; } .cm-s-seti span.cm-meta { color: #55b5db; } .cm-s-seti span.cm-tag { color: #55b5db; } .cm-s-seti span.cm-attribute { color: #9fca56; } .cm-s-seti span.cm-qualifier { color: #9fca56; } .cm-s-seti span.cm-property { color: #a074c4; } .cm-s-seti span.cm-variable-3, .cm-s-seti span.cm-type { color: #9fca56; } .cm-s-seti span.cm-builtin { color: #9fca56; } .cm-s-seti .CodeMirror-activeline-background { background: #101213; } .cm-s-seti .CodeMirror-matchingbracket { text-decoration: underline; color: white !important; } /* Solarized theme for code-mirror http://ethanschoonover.com/solarized */ /* Solarized color palette http://ethanschoonover.com/solarized/img/solarized-palette.png */ .solarized.base03 { color: #002b36; } .solarized.base02 { color: #073642; } .solarized.base01 { color: #586e75; } .solarized.base00 { color: #657b83; } .solarized.base0 { color: #839496; } .solarized.base1 { color: #93a1a1; } .solarized.base2 { color: #eee8d5; } .solarized.base3 { color: #fdf6e3; } .solarized.solar-yellow { color: #b58900; } .solarized.solar-orange { color: #cb4b16; } .solarized.solar-red { color: #dc322f; } .solarized.solar-magenta { color: #d33682; } .solarized.solar-violet { color: #6c71c4; } .solarized.solar-blue { color: #268bd2; } .solarized.solar-cyan { color: #2aa198; } .solarized.solar-green { color: #859900; } /* Color scheme for code-mirror */ .cm-s-solarized { line-height: 1.45em; color-profile: sRGB; rendering-intent: auto; } .cm-s-solarized.cm-s-dark { color: #839496; background-color: #002b36; text-shadow: #002b36 0 1px; } .cm-s-solarized.cm-s-light { background-color: #fdf6e3; color: #657b83; text-shadow: #eee8d5 0 1px; } .cm-s-solarized .CodeMirror-widget { text-shadow: none; } .cm-s-solarized .cm-header { color: #586e75; } .cm-s-solarized .cm-quote { color: #93a1a1; } .cm-s-solarized .cm-keyword { color: #cb4b16; } .cm-s-solarized .cm-atom { color: #d33682; } .cm-s-solarized .cm-number { color: #d33682; } .cm-s-solarized .cm-def { color: #2aa198; } .cm-s-solarized .cm-variable { color: #839496; } .cm-s-solarized .cm-variable-2 { color: #b58900; } .cm-s-solarized .cm-variable-3, .cm-s-solarized .cm-type { color: #6c71c4; } .cm-s-solarized .cm-property { color: #2aa198; } .cm-s-solarized .cm-operator { color: #6c71c4; } .cm-s-solarized .cm-comment { color: #586e75; font-style:italic; } .cm-s-solarized .cm-string { color: #859900; } .cm-s-solarized .cm-string-2 { color: #b58900; } .cm-s-solarized .cm-meta { color: #859900; } .cm-s-solarized .cm-qualifier { color: #b58900; } .cm-s-solarized .cm-builtin { color: #d33682; } .cm-s-solarized .cm-bracket { color: #cb4b16; } .cm-s-solarized .CodeMirror-matchingbracket { color: #859900; } .cm-s-solarized .CodeMirror-nonmatchingbracket { color: #dc322f; } .cm-s-solarized .cm-tag { color: #93a1a1; } .cm-s-solarized .cm-attribute { color: #2aa198; } .cm-s-solarized .cm-hr { color: transparent; border-top: 1px solid #586e75; display: block; } .cm-s-solarized .cm-link { color: #93a1a1; cursor: pointer; } .cm-s-solarized .cm-special { color: #6c71c4; } .cm-s-solarized .cm-em { color: #999; text-decoration: underline; text-decoration-style: dotted; } .cm-s-solarized .cm-error, .cm-s-solarized .cm-invalidchar { color: #586e75; border-bottom: 1px dotted #dc322f; } .cm-s-solarized.cm-s-dark div.CodeMirror-selected { background: #073642; } .cm-s-solarized.cm-s-dark.CodeMirror ::selection { background: rgba(7, 54, 66, 0.99); } .cm-s-solarized.cm-s-dark .CodeMirror-line::-moz-selection, .cm-s-dark .CodeMirror-line > span::-moz-selection, .cm-s-dark .CodeMirror-line > span > span::-moz-selection { background: rgba(7, 54, 66, 0.99); } .cm-s-solarized.cm-s-light div.CodeMirror-selected { background: #eee8d5; } .cm-s-solarized.cm-s-light .CodeMirror-line::selection, .cm-s-light .CodeMirror-line > span::selection, .cm-s-light .CodeMirror-line > span > span::selection { background: #eee8d5; } .cm-s-solarized.cm-s-light .CodeMirror-line::-moz-selection, .cm-s-ligh .CodeMirror-line > span::-moz-selection, .cm-s-ligh .CodeMirror-line > span > span::-moz-selection { background: #eee8d5; } /* Editor styling */ /* Little shadow on the view-port of the buffer view */ .cm-s-solarized.CodeMirror { -moz-box-shadow: inset 7px 0 12px -6px #000; -webkit-box-shadow: inset 7px 0 12px -6px #000; box-shadow: inset 7px 0 12px -6px #000; } /* Remove gutter border */ .cm-s-solarized .CodeMirror-gutters { border-right: 0; } /* Gutter colors and line number styling based of color scheme (dark / light) */ /* Dark */ .cm-s-solarized.cm-s-dark .CodeMirror-gutters { background-color: #073642; } .cm-s-solarized.cm-s-dark .CodeMirror-linenumber { color: #586e75; text-shadow: #021014 0 -1px; } /* Light */ .cm-s-solarized.cm-s-light .CodeMirror-gutters { background-color: #eee8d5; } .cm-s-solarized.cm-s-light .CodeMirror-linenumber { color: #839496; } /* Common */ .cm-s-solarized .CodeMirror-linenumber { padding: 0 5px; } .cm-s-solarized .CodeMirror-guttermarker-subtle { color: #586e75; } .cm-s-solarized.cm-s-dark .CodeMirror-guttermarker { color: #ddd; } .cm-s-solarized.cm-s-light .CodeMirror-guttermarker { color: #cb4b16; } .cm-s-solarized .CodeMirror-gutter .CodeMirror-gutter-text { color: #586e75; } /* Cursor */ .cm-s-solarized .CodeMirror-cursor { border-left: 1px solid #819090; } /* Fat cursor */ .cm-s-solarized.cm-s-light.cm-fat-cursor .CodeMirror-cursor { background: #77ee77; } .cm-s-solarized.cm-s-light .cm-animate-fat-cursor { background-color: #77ee77; } .cm-s-solarized.cm-s-dark.cm-fat-cursor .CodeMirror-cursor { background: #586e75; } .cm-s-solarized.cm-s-dark .cm-animate-fat-cursor { background-color: #586e75; } /* Active line */ .cm-s-solarized.cm-s-dark .CodeMirror-activeline-background { background: rgba(255, 255, 255, 0.06); } .cm-s-solarized.cm-s-light .CodeMirror-activeline-background { background: rgba(0, 0, 0, 0.06); } .cm-s-the-matrix.CodeMirror { background: #000000; color: #00FF00; } .cm-s-the-matrix div.CodeMirror-selected { background: #2D2D2D; } .cm-s-the-matrix .CodeMirror-line::selection, .cm-s-the-matrix .CodeMirror-line > span::selection, .cm-s-the-matrix .CodeMirror-line > span > span::selection { background: rgba(45, 45, 45, 0.99); } .cm-s-the-matrix .CodeMirror-line::-moz-selection, .cm-s-the-matrix .CodeMirror-line > span::-moz-selection, .cm-s-the-matrix .CodeMirror-line > span > span::-moz-selection { background: rgba(45, 45, 45, 0.99); } .cm-s-the-matrix .CodeMirror-gutters { background: #060; border-right: 2px solid #00FF00; } .cm-s-the-matrix .CodeMirror-guttermarker { color: #0f0; } .cm-s-the-matrix .CodeMirror-guttermarker-subtle { color: white; } .cm-s-the-matrix .CodeMirror-linenumber { color: #FFFFFF; } .cm-s-the-matrix .CodeMirror-cursor { border-left: 1px solid #00FF00; } .cm-s-the-matrix span.cm-keyword { color: #008803; font-weight: bold; } .cm-s-the-matrix span.cm-atom { color: #3FF; } .cm-s-the-matrix span.cm-number { color: #FFB94F; } .cm-s-the-matrix span.cm-def { color: #99C; } .cm-s-the-matrix span.cm-variable { color: #F6C; } .cm-s-the-matrix span.cm-variable-2 { color: #C6F; } .cm-s-the-matrix span.cm-variable-3, .cm-s-the-matrix span.cm-type { color: #96F; } .cm-s-the-matrix span.cm-property { color: #62FFA0; } .cm-s-the-matrix span.cm-operator { color: #999; } .cm-s-the-matrix span.cm-comment { color: #CCCCCC; } .cm-s-the-matrix span.cm-string { color: #39C; } .cm-s-the-matrix span.cm-meta { color: #C9F; } .cm-s-the-matrix span.cm-qualifier { color: #FFF700; } .cm-s-the-matrix span.cm-builtin { color: #30a; } .cm-s-the-matrix span.cm-bracket { color: #cc7; } .cm-s-the-matrix span.cm-tag { color: #FFBD40; } .cm-s-the-matrix span.cm-attribute { color: #FFF700; } .cm-s-the-matrix span.cm-error { color: #FF0000; } .cm-s-the-matrix .CodeMirror-activeline-background { background: #040; } /* Copyright (C) 2011 by MarkLogic Corporation Author: Mike Brevoort <mike@brevoort.com> Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. */ .cm-s-xq-light span.cm-keyword { line-height: 1em; font-weight: bold; color: #5A5CAD; } .cm-s-xq-light span.cm-atom { color: #6C8CD5; } .cm-s-xq-light span.cm-number { color: #164; } .cm-s-xq-light span.cm-def { text-decoration:underline; } .cm-s-xq-light span.cm-variable { color: black; } .cm-s-xq-light span.cm-variable-2 { color:black; } .cm-s-xq-light span.cm-variable-3, .cm-s-xq-light span.cm-type { color: black; } .cm-s-xq-light span.cm-property {} .cm-s-xq-light span.cm-operator {} .cm-s-xq-light span.cm-comment { color: #0080FF; font-style: italic; } .cm-s-xq-light span.cm-string { color: red; } .cm-s-xq-light span.cm-meta { color: yellow; } .cm-s-xq-light span.cm-qualifier { color: grey; } .cm-s-xq-light span.cm-builtin { color: #7EA656; } .cm-s-xq-light span.cm-bracket { color: #cc7; } .cm-s-xq-light span.cm-tag { color: #3F7F7F; } .cm-s-xq-light span.cm-attribute { color: #7F007F; } .cm-s-xq-light span.cm-error { color: #f00; } .cm-s-xq-light .CodeMirror-activeline-background { background: #e8f2ff; } .cm-s-xq-light .CodeMirror-matchingbracket { outline:1px solid grey;color:black !important;background:yellow; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ .CodeMirror { line-height: var(--jp-code-line-height); font-size: var(--jp-code-font-size); font-family: var(--jp-code-font-family); border: 0; border-radius: 0; height: auto; /* Changed to auto to autogrow */ } .CodeMirror pre { padding: 0 var(--jp-code-padding); } .jp-CodeMirrorEditor[data-type='inline'] .CodeMirror-dialog { background-color: var(--jp-layout-color0); color: var(--jp-content-font-color1); } /* This causes https://github.com/jupyter/jupyterlab/issues/522 */ /* May not cause it not because we changed it! */ .CodeMirror-lines { padding: var(--jp-code-padding) 0; } .CodeMirror-linenumber { padding: 0 8px; } .jp-CodeMirrorEditor-static { margin: var(--jp-code-padding); } .jp-CodeMirrorEditor, .jp-CodeMirrorEditor-static { cursor: text; } .jp-CodeMirrorEditor[data-type='inline'] .CodeMirror-cursor { border-left: var(--jp-code-cursor-width0) solid var(--jp-editor-cursor-color); } /* When zoomed out 67% and 33% on a screen of 1440 width x 900 height */ @media screen and (min-width: 2138px) and (max-width: 4319px) { .jp-CodeMirrorEditor[data-type='inline'] .CodeMirror-cursor { border-left: var(--jp-code-cursor-width1) solid var(--jp-editor-cursor-color); } } /* When zoomed out less than 33% */ @media screen and (min-width: 4320px) { .jp-CodeMirrorEditor[data-type='inline'] .CodeMirror-cursor { border-left: var(--jp-code-cursor-width2) solid var(--jp-editor-cursor-color); } } .CodeMirror.jp-mod-readOnly .CodeMirror-cursor { display: none; } .CodeMirror-gutters { border-right: 1px solid var(--jp-border-color2); background-color: var(--jp-layout-color0); } .jp-CollaboratorCursor { border-left: 5px solid transparent; border-right: 5px solid transparent; border-top: none; border-bottom: 3px solid; background-clip: content-box; margin-left: -5px; margin-right: -5px; } .CodeMirror-selectedtext.cm-searching { background-color: var(--jp-search-selected-match-background-color) !important; color: var(--jp-search-selected-match-color) !important; } .cm-searching { background-color: var( --jp-search-unselected-match-background-color ) !important; color: var(--jp-search-unselected-match-color) !important; } .CodeMirror-focused .CodeMirror-selected { background-color: var(--jp-editor-selected-focused-background); } .CodeMirror-selected { background-color: var(--jp-editor-selected-background); } .jp-CollaboratorCursor-hover { position: absolute; z-index: 1; transform: translateX(-50%); color: white; border-radius: 3px; padding-left: 4px; padding-right: 4px; padding-top: 1px; padding-bottom: 1px; text-align: center; font-size: var(--jp-ui-font-size1); white-space: nowrap; } .jp-CodeMirror-ruler { border-left: 1px dashed var(--jp-border-color2); } /** * Here is our jupyter theme for CodeMirror syntax highlighting * This is used in our marked.js syntax highlighting and CodeMirror itself * The string \"jupyter\" is set in ../codemirror/widget.DEFAULT_CODEMIRROR_THEME * This came from the classic notebook, which came form highlight.js/GitHub */ /** * CodeMirror themes are handling the background/color in this way. This works * fine for CodeMirror editors outside the notebook, but the notebook styles * these things differently. */ .CodeMirror.cm-s-jupyter { background: var(--jp-layout-color0); color: var(--jp-content-font-color1); } /* In the notebook, we want this styling to be handled by its container */ .jp-CodeConsole .CodeMirror.cm-s-jupyter, .jp-Notebook .CodeMirror.cm-s-jupyter { background: transparent; } .cm-s-jupyter .CodeMirror-cursor { border-left: var(--jp-code-cursor-width0) solid var(--jp-editor-cursor-color); } .cm-s-jupyter span.cm-keyword { color: var(--jp-mirror-editor-keyword-color); font-weight: bold; } .cm-s-jupyter span.cm-atom { color: var(--jp-mirror-editor-atom-color); } .cm-s-jupyter span.cm-number { color: var(--jp-mirror-editor-number-color); } .cm-s-jupyter span.cm-def { color: var(--jp-mirror-editor-def-color); } .cm-s-jupyter span.cm-variable { color: var(--jp-mirror-editor-variable-color); } .cm-s-jupyter span.cm-variable-2 { color: var(--jp-mirror-editor-variable-2-color); } .cm-s-jupyter span.cm-variable-3 { color: var(--jp-mirror-editor-variable-3-color); } .cm-s-jupyter span.cm-punctuation { color: var(--jp-mirror-editor-punctuation-color); } .cm-s-jupyter span.cm-property { color: var(--jp-mirror-editor-property-color); } .cm-s-jupyter span.cm-operator { color: var(--jp-mirror-editor-operator-color); font-weight: bold; } .cm-s-jupyter span.cm-comment { color: var(--jp-mirror-editor-comment-color); font-style: italic; } .cm-s-jupyter span.cm-string { color: var(--jp-mirror-editor-string-color); } .cm-s-jupyter span.cm-string-2 { color: var(--jp-mirror-editor-string-2-color); } .cm-s-jupyter span.cm-meta { color: var(--jp-mirror-editor-meta-color); } .cm-s-jupyter span.cm-qualifier { color: var(--jp-mirror-editor-qualifier-color); } .cm-s-jupyter span.cm-builtin { color: var(--jp-mirror-editor-builtin-color); } .cm-s-jupyter span.cm-bracket { color: var(--jp-mirror-editor-bracket-color); } .cm-s-jupyter span.cm-tag { color: var(--jp-mirror-editor-tag-color); } .cm-s-jupyter span.cm-attribute { color: var(--jp-mirror-editor-attribute-color); } .cm-s-jupyter span.cm-header { color: var(--jp-mirror-editor-header-color); } .cm-s-jupyter span.cm-quote { color: var(--jp-mirror-editor-quote-color); } .cm-s-jupyter span.cm-link { color: var(--jp-mirror-editor-link-color); } .cm-s-jupyter span.cm-error { color: var(--jp-mirror-editor-error-color); } .cm-s-jupyter span.cm-hr { color: #999; } .cm-s-jupyter span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } .cm-s-jupyter .CodeMirror-activeline-background, .cm-s-jupyter .CodeMirror-gutter { background-color: var(--jp-layout-color2); } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /* This file was auto-generated by ensurePackage() in @jupyterlab/buildutils */ /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /*----------------------------------------------------------------------------- | RenderedText |----------------------------------------------------------------------------*/ .jp-RenderedText { text-align: left; padding-left: var(--jp-code-padding); line-height: var(--jp-code-line-height); font-family: var(--jp-code-font-family); } .jp-RenderedText pre, .jp-RenderedJavaScript pre, .jp-RenderedHTMLCommon pre { color: var(--jp-content-font-color1); font-size: var(--jp-code-font-size); border: none; margin: 0px; padding: 0px; line-height: normal; } .jp-RenderedText pre a:link { text-decoration: none; color: var(--jp-content-link-color); } .jp-RenderedText pre a:hover { text-decoration: underline; color: var(--jp-content-link-color); } .jp-RenderedText pre a:visited { text-decoration: none; color: var(--jp-content-link-color); } /* console foregrounds and backgrounds */ .jp-RenderedText pre .ansi-black-fg { color: #3e424d; } .jp-RenderedText pre .ansi-red-fg { color: #e75c58; } .jp-RenderedText pre .ansi-green-fg { color: #00a250; } .jp-RenderedText pre .ansi-yellow-fg { color: #ddb62b; } .jp-RenderedText pre .ansi-blue-fg { color: #208ffb; } .jp-RenderedText pre .ansi-magenta-fg { color: #d160c4; } .jp-RenderedText pre .ansi-cyan-fg { color: #60c6c8; } .jp-RenderedText pre .ansi-white-fg { color: #c5c1b4; } .jp-RenderedText pre .ansi-black-bg { background-color: #3e424d; } .jp-RenderedText pre .ansi-red-bg { background-color: #e75c58; } .jp-RenderedText pre .ansi-green-bg { background-color: #00a250; } .jp-RenderedText pre .ansi-yellow-bg { background-color: #ddb62b; } .jp-RenderedText pre .ansi-blue-bg { background-color: #208ffb; } .jp-RenderedText pre .ansi-magenta-bg { background-color: #d160c4; } .jp-RenderedText pre .ansi-cyan-bg { background-color: #60c6c8; } .jp-RenderedText pre .ansi-white-bg { background-color: #c5c1b4; } .jp-RenderedText pre .ansi-black-intense-fg { color: #282c36; } .jp-RenderedText pre .ansi-red-intense-fg { color: #b22b31; } .jp-RenderedText pre .ansi-green-intense-fg { color: #007427; } .jp-RenderedText pre .ansi-yellow-intense-fg { color: #b27d12; } .jp-RenderedText pre .ansi-blue-intense-fg { color: #0065ca; } .jp-RenderedText pre .ansi-magenta-intense-fg { color: #a03196; } .jp-RenderedText pre .ansi-cyan-intense-fg { color: #258f8f; } .jp-RenderedText pre .ansi-white-intense-fg { color: #a1a6b2; } .jp-RenderedText pre .ansi-black-intense-bg { background-color: #282c36; } .jp-RenderedText pre .ansi-red-intense-bg { background-color: #b22b31; } .jp-RenderedText pre .ansi-green-intense-bg { background-color: #007427; } .jp-RenderedText pre .ansi-yellow-intense-bg { background-color: #b27d12; } .jp-RenderedText pre .ansi-blue-intense-bg { background-color: #0065ca; } .jp-RenderedText pre .ansi-magenta-intense-bg { background-color: #a03196; } .jp-RenderedText pre .ansi-cyan-intense-bg { background-color: #258f8f; } .jp-RenderedText pre .ansi-white-intense-bg { background-color: #a1a6b2; } .jp-RenderedText pre .ansi-default-inverse-fg { color: var(--jp-ui-inverse-font-color0); } .jp-RenderedText pre .ansi-default-inverse-bg { background-color: var(--jp-inverse-layout-color0); } .jp-RenderedText pre .ansi-bold { font-weight: bold; } .jp-RenderedText pre .ansi-underline { text-decoration: underline; } .jp-RenderedText[data-mime-type='application/vnd.jupyter.stderr'] { background: var(--jp-rendermime-error-background); padding-top: var(--jp-code-padding); } /*----------------------------------------------------------------------------- | RenderedLatex |----------------------------------------------------------------------------*/ .jp-RenderedLatex { color: var(--jp-content-font-color1); font-size: var(--jp-content-font-size1); line-height: var(--jp-content-line-height); } /* Left-justify outputs.*/ .jp-OutputArea-output.jp-RenderedLatex { padding: var(--jp-code-padding); text-align: left; } /*----------------------------------------------------------------------------- | RenderedHTML |----------------------------------------------------------------------------*/ .jp-RenderedHTMLCommon { color: var(--jp-content-font-color1); font-family: var(--jp-content-font-family); font-size: var(--jp-content-font-size1); line-height: var(--jp-content-line-height); /* Give a bit more R padding on Markdown text to keep line lengths reasonable */ padding-right: 20px; } .jp-RenderedHTMLCommon em { font-style: italic; } .jp-RenderedHTMLCommon strong { font-weight: bold; } .jp-RenderedHTMLCommon u { text-decoration: underline; } .jp-RenderedHTMLCommon a:link { text-decoration: none; color: var(--jp-content-link-color); } .jp-RenderedHTMLCommon a:hover { text-decoration: underline; color: var(--jp-content-link-color); } .jp-RenderedHTMLCommon a:visited { text-decoration: none; color: var(--jp-content-link-color); } /* Headings */ .jp-RenderedHTMLCommon h1, .jp-RenderedHTMLCommon h2, .jp-RenderedHTMLCommon h3, .jp-RenderedHTMLCommon h4, .jp-RenderedHTMLCommon h5, .jp-RenderedHTMLCommon h6 { line-height: var(--jp-content-heading-line-height); font-weight: var(--jp-content-heading-font-weight); font-style: normal; margin: var(--jp-content-heading-margin-top) 0 var(--jp-content-heading-margin-bottom) 0; } .jp-RenderedHTMLCommon h1:first-child, .jp-RenderedHTMLCommon h2:first-child, .jp-RenderedHTMLCommon h3:first-child, .jp-RenderedHTMLCommon h4:first-child, .jp-RenderedHTMLCommon h5:first-child, .jp-RenderedHTMLCommon h6:first-child { margin-top: calc(0.5 * var(--jp-content-heading-margin-top)); } .jp-RenderedHTMLCommon h1:last-child, .jp-RenderedHTMLCommon h2:last-child, .jp-RenderedHTMLCommon h3:last-child, .jp-RenderedHTMLCommon h4:last-child, .jp-RenderedHTMLCommon h5:last-child, .jp-RenderedHTMLCommon h6:last-child { margin-bottom: calc(0.5 * var(--jp-content-heading-margin-bottom)); } .jp-RenderedHTMLCommon h1 { font-size: var(--jp-content-font-size5); } .jp-RenderedHTMLCommon h2 { font-size: var(--jp-content-font-size4); } .jp-RenderedHTMLCommon h3 { font-size: var(--jp-content-font-size3); } .jp-RenderedHTMLCommon h4 { font-size: var(--jp-content-font-size2); } .jp-RenderedHTMLCommon h5 { font-size: var(--jp-content-font-size1); } .jp-RenderedHTMLCommon h6 { font-size: var(--jp-content-font-size0); } /* Lists */ .jp-RenderedHTMLCommon ul:not(.list-inline), .jp-RenderedHTMLCommon ol:not(.list-inline) { padding-left: 2em; } .jp-RenderedHTMLCommon ul { list-style: disc; } .jp-RenderedHTMLCommon ul ul { list-style: square; } .jp-RenderedHTMLCommon ul ul ul { list-style: circle; } .jp-RenderedHTMLCommon ol { list-style: decimal; } .jp-RenderedHTMLCommon ol ol { list-style: upper-alpha; } .jp-RenderedHTMLCommon ol ol ol { list-style: lower-alpha; } .jp-RenderedHTMLCommon ol ol ol ol { list-style: lower-roman; } .jp-RenderedHTMLCommon ol ol ol ol ol { list-style: decimal; } .jp-RenderedHTMLCommon ol, .jp-RenderedHTMLCommon ul { margin-bottom: 1em; } .jp-RenderedHTMLCommon ul ul, .jp-RenderedHTMLCommon ul ol, .jp-RenderedHTMLCommon ol ul, .jp-RenderedHTMLCommon ol ol { margin-bottom: 0em; } .jp-RenderedHTMLCommon hr { color: var(--jp-border-color2); background-color: var(--jp-border-color1); margin-top: 1em; margin-bottom: 1em; } .jp-RenderedHTMLCommon > pre { margin: 1.5em 2em; } .jp-RenderedHTMLCommon pre, .jp-RenderedHTMLCommon code { border: 0; background-color: var(--jp-layout-color0); color: var(--jp-content-font-color1); font-family: var(--jp-code-font-family); font-size: inherit; line-height: var(--jp-code-line-height); padding: 0; white-space: pre-wrap; } .jp-RenderedHTMLCommon :not(pre) > code { background-color: var(--jp-layout-color2); padding: 1px 5px; } /* Tables */ .jp-RenderedHTMLCommon table { border-collapse: collapse; border-spacing: 0; border: none; color: var(--jp-ui-font-color1); font-size: 12px; table-layout: fixed; margin-left: auto; margin-right: auto; } .jp-RenderedHTMLCommon thead { border-bottom: var(--jp-border-width) solid var(--jp-border-color1); vertical-align: bottom; } .jp-RenderedHTMLCommon td, .jp-RenderedHTMLCommon th, .jp-RenderedHTMLCommon tr { vertical-align: middle; padding: 0.5em 0.5em; line-height: normal; white-space: normal; max-width: none; border: none; } .jp-RenderedMarkdown.jp-RenderedHTMLCommon td, .jp-RenderedMarkdown.jp-RenderedHTMLCommon th { max-width: none; } :not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon td, :not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon th, :not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon tr { text-align: right; } .jp-RenderedHTMLCommon th { font-weight: bold; } .jp-RenderedHTMLCommon tbody tr:nth-child(odd) { background: var(--jp-layout-color0); } .jp-RenderedHTMLCommon tbody tr:nth-child(even) { background: var(--jp-rendermime-table-row-background); } .jp-RenderedHTMLCommon tbody tr:hover { background: var(--jp-rendermime-table-row-hover-background); } .jp-RenderedHTMLCommon table { margin-bottom: 1em; } .jp-RenderedHTMLCommon p { text-align: left; margin: 0px; } .jp-RenderedHTMLCommon p { margin-bottom: 1em; } .jp-RenderedHTMLCommon img { -moz-force-broken-image-icon: 1; } /* Restrict to direct children as other images could be nested in other content. */ .jp-RenderedHTMLCommon > img { display: block; margin-left: 0; margin-right: 0; margin-bottom: 1em; } /* Change color behind transparent images if they need it... */ [data-jp-theme-light='false'] .jp-RenderedImage img.jp-needs-light-background { background-color: var(--jp-inverse-layout-color1); } [data-jp-theme-light='true'] .jp-RenderedImage img.jp-needs-dark-background { background-color: var(--jp-inverse-layout-color1); } /* ...or leave it untouched if they don't */ [data-jp-theme-light='false'] .jp-RenderedImage img.jp-needs-dark-background { } [data-jp-theme-light='true'] .jp-RenderedImage img.jp-needs-light-background { } .jp-RenderedHTMLCommon img, .jp-RenderedImage img, .jp-RenderedHTMLCommon svg, .jp-RenderedSVG svg { max-width: 100%; height: auto; } .jp-RenderedHTMLCommon img.jp-mod-unconfined, .jp-RenderedImage img.jp-mod-unconfined, .jp-RenderedHTMLCommon svg.jp-mod-unconfined, .jp-RenderedSVG svg.jp-mod-unconfined { max-width: none; } .jp-RenderedHTMLCommon .alert { padding: var(--jp-notebook-padding); border: var(--jp-border-width) solid transparent; border-radius: var(--jp-border-radius); margin-bottom: 1em; } .jp-RenderedHTMLCommon .alert-info { color: var(--jp-info-color0); background-color: var(--jp-info-color3); border-color: var(--jp-info-color2); } .jp-RenderedHTMLCommon .alert-info hr { border-color: var(--jp-info-color3); } .jp-RenderedHTMLCommon .alert-info > p:last-child, .jp-RenderedHTMLCommon .alert-info > ul:last-child { margin-bottom: 0; } .jp-RenderedHTMLCommon .alert-warning { color: var(--jp-warn-color0); background-color: var(--jp-warn-color3); border-color: var(--jp-warn-color2); } .jp-RenderedHTMLCommon .alert-warning hr { border-color: var(--jp-warn-color3); } .jp-RenderedHTMLCommon .alert-warning > p:last-child, .jp-RenderedHTMLCommon .alert-warning > ul:last-child { margin-bottom: 0; } .jp-RenderedHTMLCommon .alert-success { color: var(--jp-success-color0); background-color: var(--jp-success-color3); border-color: var(--jp-success-color2); } .jp-RenderedHTMLCommon .alert-success hr { border-color: var(--jp-success-color3); } .jp-RenderedHTMLCommon .alert-success > p:last-child, .jp-RenderedHTMLCommon .alert-success > ul:last-child { margin-bottom: 0; } .jp-RenderedHTMLCommon .alert-danger { color: var(--jp-error-color0); background-color: var(--jp-error-color3); border-color: var(--jp-error-color2); } .jp-RenderedHTMLCommon .alert-danger hr { border-color: var(--jp-error-color3); } .jp-RenderedHTMLCommon .alert-danger > p:last-child, .jp-RenderedHTMLCommon .alert-danger > ul:last-child { margin-bottom: 0; } .jp-RenderedHTMLCommon blockquote { margin: 1em 2em; padding: 0 1em; border-left: 5px solid var(--jp-border-color2); } a.jp-InternalAnchorLink { visibility: hidden; margin-left: 8px; color: var(--md-blue-800); } h1:hover .jp-InternalAnchorLink, h2:hover .jp-InternalAnchorLink, h3:hover .jp-InternalAnchorLink, h4:hover .jp-InternalAnchorLink, h5:hover .jp-InternalAnchorLink, h6:hover .jp-InternalAnchorLink { visibility: visible; } .jp-RenderedHTMLCommon kbd { background-color: var(--jp-rendermime-table-row-background); border: 1px solid var(--jp-border-color0); border-bottom-color: var(--jp-border-color2); border-radius: 3px; box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.25); display: inline-block; font-size: 0.8em; line-height: 1em; padding: 0.2em 0.5em; } /* Most direct children of .jp-RenderedHTMLCommon have a margin-bottom of 1.0. * At the bottom of cells this is a bit too much as there is also spacing * between cells. Going all the way to 0 gets too tight between markdown and * code cells. */ .jp-RenderedHTMLCommon > *:last-child { margin-bottom: 0.5em; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /* This file was auto-generated by ensurePackage() in @jupyterlab/buildutils */ /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ .jp-MimeDocument { outline: none; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /* This file was auto-generated by ensurePackage() in @jupyterlab/buildutils */ /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /*----------------------------------------------------------------------------- | Variables |----------------------------------------------------------------------------*/ :root { --jp-private-filebrowser-button-height: 28px; --jp-private-filebrowser-button-width: 48px; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ .jp-FileBrowser { display: flex; flex-direction: column; color: var(--jp-ui-font-color1); background: var(--jp-layout-color1); /* This is needed so that all font sizing of children done in ems is * relative to this base size */ font-size: var(--jp-ui-font-size1); } .jp-FileBrowser-toolbar.jp-Toolbar { border-bottom: none; height: auto; margin: var(--jp-toolbar-header-margin); box-shadow: none; } .jp-BreadCrumbs { flex: 0 0 auto; margin: 4px 12px; } .jp-BreadCrumbs-item { margin: 0px 2px; padding: 0px 2px; border-radius: var(--jp-border-radius); cursor: pointer; } .jp-BreadCrumbs-item:hover { background-color: var(--jp-layout-color2); } .jp-BreadCrumbs-item:first-child { margin-left: 0px; } .jp-BreadCrumbs-item.jp-mod-dropTarget { background-color: var(--jp-brand-color2); opacity: 0.7; } /*----------------------------------------------------------------------------- | Buttons |----------------------------------------------------------------------------*/ .jp-FileBrowser-toolbar.jp-Toolbar { padding: 0px; } .jp-FileBrowser-toolbar.jp-Toolbar { justify-content: space-evenly; } .jp-FileBrowser-toolbar.jp-Toolbar .jp-Toolbar-item { flex: 1; } .jp-FileBrowser-toolbar.jp-Toolbar .jp-ToolbarButtonComponent { width: 100%; } /*----------------------------------------------------------------------------- | DirListing |----------------------------------------------------------------------------*/ .jp-DirListing { flex: 1 1 auto; display: flex; flex-direction: column; outline: 0; } .jp-DirListing-header { flex: 0 0 auto; display: flex; flex-direction: row; overflow: hidden; border-top: var(--jp-border-width) solid var(--jp-border-color2); border-bottom: var(--jp-border-width) solid var(--jp-border-color1); box-shadow: var(--jp-toolbar-box-shadow); z-index: 2; } .jp-DirListing-headerItem { padding: 4px 12px 2px 12px; font-weight: 500; } .jp-DirListing-headerItem:hover { background: var(--jp-layout-color2); } .jp-DirListing-headerItem.jp-id-name { flex: 1 0 84px; } .jp-DirListing-headerItem.jp-id-modified { flex: 0 0 112px; border-left: var(--jp-border-width) solid var(--jp-border-color2); text-align: right; } .jp-DirListing-narrow .jp-id-modified, .jp-DirListing-narrow .jp-DirListing-itemModified { display: none; } .jp-DirListing-headerItem.jp-mod-selected { font-weight: 600; } /* increase specificity to override bundled default */ .jp-DirListing-content { flex: 1 1 auto; margin: 0; padding: 0; list-style-type: none; overflow: auto; background-color: var(--jp-layout-color1); } /* Style the directory listing content when a user drops a file to upload */ .jp-DirListing.jp-mod-native-drop .jp-DirListing-content { outline: 5px dashed rgba(128, 128, 128, 0.5); outline-offset: -10px; cursor: copy; } .jp-DirListing-item { display: flex; flex-direction: row; padding: 4px 12px; -webkit-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; } .jp-DirListing-item.jp-mod-selected { color: white; background: var(--jp-brand-color1); } .jp-DirListing-item.jp-mod-dropTarget { background: var(--jp-brand-color3); } .jp-DirListing-item:hover:not(.jp-mod-selected) { background: var(--jp-layout-color2); } .jp-DirListing-itemIcon { flex: 0 0 20px; margin-right: 4px; } .jp-DirListing-itemText { flex: 1 0 64px; white-space: nowrap; overflow: hidden; text-overflow: ellipsis; user-select: none; } .jp-DirListing-itemModified { flex: 0 0 125px; text-align: right; } .jp-DirListing-editor { flex: 1 0 64px; outline: none; border: none; } .jp-DirListing-item.jp-mod-running .jp-DirListing-itemIcon:before { color: limegreen; content: '\\25CF'; font-size: 8px; position: absolute; left: -8px; } .jp-DirListing-item.lm-mod-drag-image, .jp-DirListing-item.jp-mod-selected.lm-mod-drag-image { font-size: var(--jp-ui-font-size1); padding-left: 4px; margin-left: 4px; width: 160px; background-color: var(--jp-ui-inverse-font-color2); box-shadow: var(--jp-elevation-z2); border-radius: 0px; color: var(--jp-ui-font-color1); transform: translateX(-40%) translateY(-58%); } .jp-DirListing-deadSpace { flex: 1 1 auto; margin: 0; padding: 0; list-style-type: none; overflow: auto; background-color: var(--jp-layout-color1); } .jp-Document { min-width: 120px; min-height: 120px; outline: none; } .jp-FileDialog.jp-mod-conflict input { color: red; } .jp-FileDialog .jp-new-name-title { margin-top: 12px; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /* This file was auto-generated by ensurePackage() in @jupyterlab/buildutils */ /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /*----------------------------------------------------------------------------- | Private CSS variables |----------------------------------------------------------------------------*/ :root { } /*----------------------------------------------------------------------------- | Main OutputArea | OutputArea has a list of Outputs |----------------------------------------------------------------------------*/ .jp-OutputArea { overflow-y: auto; } .jp-OutputArea-child { display: flex; flex-direction: row; } .jp-OutputPrompt { flex: 0 0 var(--jp-cell-prompt-width); color: var(--jp-cell-outprompt-font-color); font-family: var(--jp-cell-prompt-font-family); padding: var(--jp-code-padding); letter-spacing: var(--jp-cell-prompt-letter-spacing); line-height: var(--jp-code-line-height); font-size: var(--jp-code-font-size); border: var(--jp-border-width) solid transparent; opacity: var(--jp-cell-prompt-opacity); /* Right align prompt text, don't wrap to handle large prompt numbers */ text-align: right; white-space: nowrap; overflow: hidden; text-overflow: ellipsis; /* Disable text selection */ -webkit-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; } .jp-OutputArea-output { height: auto; overflow: auto; user-select: text; -moz-user-select: text; -webkit-user-select: text; -ms-user-select: text; } .jp-OutputArea-child .jp-OutputArea-output { flex-grow: 1; flex-shrink: 1; } /** * Isolated output. */ .jp-OutputArea-output.jp-mod-isolated { width: 100%; display: block; } /* When drag events occur, `p-mod-override-cursor` is added to the body. Because iframes steal all cursor events, the following two rules are necessary to suppress pointer events while resize drags are occurring. There may be a better solution to this problem. */ body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated { position: relative; } body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated:before { content: ''; position: absolute; top: 0; left: 0; right: 0; bottom: 0; background: transparent; } /* pre */ .jp-OutputArea-output pre { border: none; margin: 0px; padding: 0px; overflow-x: auto; overflow-y: auto; word-break: break-all; word-wrap: break-word; white-space: pre-wrap; } /* tables */ .jp-OutputArea-output.jp-RenderedHTMLCommon table { margin-left: 0; margin-right: 0; } /* description lists */ .jp-OutputArea-output dl, .jp-OutputArea-output dt, .jp-OutputArea-output dd { display: block; } .jp-OutputArea-output dl { width: 100%; overflow: hidden; padding: 0; margin: 0; } .jp-OutputArea-output dt { font-weight: bold; float: left; width: 20%; padding: 0; margin: 0; } .jp-OutputArea-output dd { float: left; width: 80%; padding: 0; margin: 0; } /* Hide the gutter in case of * - nested output areas (e.g. in the case of output widgets) * - mirrored output areas */ .jp-OutputArea .jp-OutputArea .jp-OutputArea-prompt { display: none; } /*----------------------------------------------------------------------------- | executeResult is added to any Output-result for the display of the object | returned by a cell |----------------------------------------------------------------------------*/ .jp-OutputArea-output.jp-OutputArea-executeResult { margin-left: 0px; flex: 1 1 auto; } .jp-OutputArea-executeResult.jp-RenderedText { padding-top: var(--jp-code-padding); } /*----------------------------------------------------------------------------- | The Stdin output |----------------------------------------------------------------------------*/ .jp-OutputArea-stdin { line-height: var(--jp-code-line-height); padding-top: var(--jp-code-padding); display: flex; } .jp-Stdin-prompt { color: var(--jp-content-font-color0); padding-right: var(--jp-code-padding); vertical-align: baseline; flex: 0 0 auto; } .jp-Stdin-input { font-family: var(--jp-code-font-family); font-size: inherit; color: inherit; background-color: inherit; width: 42%; min-width: 200px; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; flex: 0 0 70%; } .jp-Stdin-input:focus { box-shadow: none; } /*----------------------------------------------------------------------------- | Output Area View |----------------------------------------------------------------------------*/ .jp-LinkedOutputView .jp-OutputArea { height: 100%; display: block; } .jp-LinkedOutputView .jp-OutputArea-output:only-child { height: 100%; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /* This file was auto-generated by ensurePackage() in @jupyterlab/buildutils */ /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ .jp-Collapser { flex: 0 0 var(--jp-cell-collapser-width); padding: 0px; margin: 0px; border: none; outline: none; background: transparent; border-radius: var(--jp-border-radius); opacity: 1; } .jp-Collapser-child { display: block; width: 100%; box-sizing: border-box; /* height: 100% doesn't work because the height of its parent is computed from content */ position: absolute; top: 0px; bottom: 0px; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /*----------------------------------------------------------------------------- | Header/Footer |----------------------------------------------------------------------------*/ /* Hidden by zero height by default */ .jp-CellHeader, .jp-CellFooter { height: 0px; width: 100%; padding: 0px; margin: 0px; border: none; outline: none; background: transparent; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /*----------------------------------------------------------------------------- | Input |----------------------------------------------------------------------------*/ /* All input areas */ .jp-InputArea { display: flex; flex-direction: row; } .jp-InputArea-editor { flex: 1 1 auto; } .jp-InputArea-editor { /* This is the non-active, default styling */ border: var(--jp-border-width) solid var(--jp-cell-editor-border-color); border-radius: 0px; background: var(--jp-cell-editor-background); } .jp-InputPrompt { flex: 0 0 var(--jp-cell-prompt-width); color: var(--jp-cell-inprompt-font-color); font-family: var(--jp-cell-prompt-font-family); padding: var(--jp-code-padding); letter-spacing: var(--jp-cell-prompt-letter-spacing); opacity: var(--jp-cell-prompt-opacity); line-height: var(--jp-code-line-height); font-size: var(--jp-code-font-size); border: var(--jp-border-width) solid transparent; opacity: var(--jp-cell-prompt-opacity); /* Right align prompt text, don't wrap to handle large prompt numbers */ text-align: right; white-space: nowrap; overflow: hidden; text-overflow: ellipsis; /* Disable text selection */ -webkit-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /*----------------------------------------------------------------------------- | Placeholder |----------------------------------------------------------------------------*/ .jp-Placeholder { display: flex; flex-direction: row; flex: 1 1 auto; } .jp-Placeholder-prompt { box-sizing: border-box; } .jp-Placeholder-content { flex: 1 1 auto; border: none; background: transparent; height: 20px; box-sizing: border-box; } .jp-Placeholder-content .jp-MoreHorizIcon { width: 32px; height: 16px; border: 1px solid transparent; border-radius: var(--jp-border-radius); } .jp-Placeholder-content .jp-MoreHorizIcon:hover { border: 1px solid var(--jp-border-color1); box-shadow: 0px 0px 2px 0px rgba(0, 0, 0, 0.25); background-color: var(--jp-layout-color0); } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /*----------------------------------------------------------------------------- | Private CSS variables |----------------------------------------------------------------------------*/ :root { --jp-private-cell-scrolling-output-offset: 5px; } /*----------------------------------------------------------------------------- | Cell |----------------------------------------------------------------------------*/ .jp-Cell { padding: var(--jp-cell-padding); margin: 0px; border: none; outline: none; background: transparent; } /*----------------------------------------------------------------------------- | Common input/output |----------------------------------------------------------------------------*/ .jp-Cell-inputWrapper, .jp-Cell-outputWrapper { display: flex; flex-direction: row; padding: 0px; margin: 0px; /* Added to reveal the box-shadow on the input and output collapsers. */ overflow: visible; } /* Only input/output areas inside cells */ .jp-Cell-inputArea, .jp-Cell-outputArea { flex: 1 1 auto; } /*----------------------------------------------------------------------------- | Collapser |----------------------------------------------------------------------------*/ /* Make the output collapser disappear when there is not output, but do so * in a manner that leaves it in the layout and preserves its width. */ .jp-Cell.jp-mod-noOutputs .jp-Cell-outputCollapser { border: none !important; background: transparent !important; } .jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputCollapser { min-height: var(--jp-cell-collapser-min-height); } /*----------------------------------------------------------------------------- | Output |----------------------------------------------------------------------------*/ /* Put a space between input and output when there IS output */ .jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputWrapper { margin-top: 5px; } /* Text output with the Out[] prompt needs a top padding to match the * alignment of the Out[] prompt itself. */ .jp-OutputArea-executeResult .jp-RenderedText.jp-OutputArea-output { padding-top: var(--jp-code-padding); } .jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea { overflow-y: auto; max-height: 200px; box-shadow: inset 0 0 6px 2px rgba(0, 0, 0, 0.3); margin-left: var(--jp-private-cell-scrolling-output-offset); } .jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-prompt { flex: 0 0 calc( var(--jp-cell-prompt-width) - var(--jp-private-cell-scrolling-output-offset) ); } /*----------------------------------------------------------------------------- | CodeCell |----------------------------------------------------------------------------*/ /*----------------------------------------------------------------------------- | MarkdownCell |----------------------------------------------------------------------------*/ .jp-MarkdownOutput { flex: 1 1 auto; margin-top: 0; margin-bottom: 0; padding-left: var(--jp-code-padding); } .jp-MarkdownOutput.jp-RenderedHTMLCommon { overflow: auto; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /* This file was auto-generated by ensurePackage() in @jupyterlab/buildutils */ /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /*----------------------------------------------------------------------------- | Variables |----------------------------------------------------------------------------*/ /*----------------------------------------------------------------------------- /*----------------------------------------------------------------------------- | Styles |----------------------------------------------------------------------------*/ .jp-NotebookPanel-toolbar { padding: 2px; } .jp-Toolbar-item.jp-Notebook-toolbarCellType .jp-select-wrapper.jp-mod-focused { border: none; box-shadow: none; } .jp-Notebook-toolbarCellTypeDropdown select { height: 24px; font-size: var(--jp-ui-font-size1); line-height: 14px; border-radius: 0; display: block; } .jp-Notebook-toolbarCellTypeDropdown span { top: 5px !important; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /*----------------------------------------------------------------------------- | Private CSS variables |----------------------------------------------------------------------------*/ :root { --jp-private-notebook-dragImage-width: 304px; --jp-private-notebook-dragImage-height: 36px; --jp-private-notebook-selected-color: var(--md-blue-400); --jp-private-notebook-active-color: var(--md-green-400); } /*----------------------------------------------------------------------------- | Imports |----------------------------------------------------------------------------*/ /*----------------------------------------------------------------------------- | Notebook |----------------------------------------------------------------------------*/ .jp-NotebookPanel { display: block; height: 100%; } .jp-NotebookPanel.jp-Document { min-width: 240px; min-height: 120px; } .jp-Notebook { padding: var(--jp-notebook-padding); outline: none; overflow: auto; background: var(--jp-layout-color0); } .jp-Notebook.jp-mod-scrollPastEnd::after { display: block; content: ''; min-height: var(--jp-notebook-scroll-padding); } .jp-Notebook .jp-Cell { overflow: visible; } .jp-Notebook .jp-Cell .jp-InputPrompt { cursor: move; } /*----------------------------------------------------------------------------- | Notebook state related styling | | The notebook and cells each have states, here are the possibilities: | | - Notebook | - Command | - Edit | - Cell | - None | - Active (only one can be active) | - Selected (the cells actions are applied to) | - Multiselected (when multiple selected, the cursor) | - No outputs |----------------------------------------------------------------------------*/ /* Command or edit modes */ .jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-InputPrompt { opacity: var(--jp-cell-prompt-not-active-opacity); color: var(--jp-cell-prompt-not-active-font-color); } .jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-OutputPrompt { opacity: var(--jp-cell-prompt-not-active-opacity); color: var(--jp-cell-prompt-not-active-font-color); } /* cell is active */ .jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser { background: var(--jp-brand-color1); } /* collapser is hovered */ .jp-Notebook .jp-Cell .jp-Collapser:hover { box-shadow: var(--jp-elevation-z2); background: var(--jp-brand-color1); opacity: var(--jp-cell-collapser-not-active-hover-opacity); } /* cell is active and collapser is hovered */ .jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser:hover { background: var(--jp-brand-color0); opacity: 1; } /* Command mode */ .jp-Notebook.jp-mod-commandMode .jp-Cell.jp-mod-selected { background: var(--jp-notebook-multiselected-color); } .jp-Notebook.jp-mod-commandMode .jp-Cell.jp-mod-active.jp-mod-selected:not(.jp-mod-multiSelected) { background: transparent; } /* Edit mode */ .jp-Notebook.jp-mod-editMode .jp-Cell.jp-mod-active .jp-InputArea-editor { border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color); box-shadow: var(--jp-input-box-shadow); background-color: var(--jp-cell-editor-active-background); } /*----------------------------------------------------------------------------- | Notebook drag and drop |----------------------------------------------------------------------------*/ .jp-Notebook-cell.jp-mod-dropSource { opacity: 0.5; } .jp-Notebook-cell.jp-mod-dropTarget, .jp-Notebook.jp-mod-commandMode .jp-Notebook-cell.jp-mod-active.jp-mod-selected.jp-mod-dropTarget { border-top-color: var(--jp-private-notebook-selected-color); border-top-style: solid; border-top-width: 2px; } .jp-dragImage { display: flex; flex-direction: row; width: var(--jp-private-notebook-dragImage-width); height: var(--jp-private-notebook-dragImage-height); border: var(--jp-border-width) solid var(--jp-cell-editor-border-color); background: var(--jp-cell-editor-background); overflow: visible; } .jp-dragImage-singlePrompt { box-shadow: 2px 2px 4px 0px rgba(0, 0, 0, 0.12); } .jp-dragImage .jp-dragImage-content { flex: 1 1 auto; z-index: 2; font-size: var(--jp-code-font-size); font-family: var(--jp-code-font-family); line-height: var(--jp-code-line-height); padding: var(--jp-code-padding); border: var(--jp-border-width) solid var(--jp-cell-editor-border-color); background: var(--jp-cell-editor-background-color); color: var(--jp-content-font-color3); text-align: left; margin: 4px 4px 4px 0px; } .jp-dragImage .jp-dragImage-prompt { flex: 0 0 auto; min-width: 36px; color: var(--jp-cell-inprompt-font-color); padding: var(--jp-code-padding); padding-left: 12px; font-family: var(--jp-cell-prompt-font-family); letter-spacing: var(--jp-cell-prompt-letter-spacing); line-height: 1.9; font-size: var(--jp-code-font-size); border: var(--jp-border-width) solid transparent; } .jp-dragImage-multipleBack { z-index: -1; position: absolute; height: 32px; width: 300px; top: 8px; left: 8px; background: var(--jp-layout-color2); border: var(--jp-border-width) solid var(--jp-input-border-color); box-shadow: 2px 2px 4px 0px rgba(0, 0, 0, 0.12); } /*----------------------------------------------------------------------------- | Cell toolbar |----------------------------------------------------------------------------*/ .jp-NotebookTools { display: block; min-width: var(--jp-sidebar-min-width); color: var(--jp-ui-font-color1); background: var(--jp-layout-color1); /* This is needed so that all font sizing of children done in ems is * relative to this base size */ font-size: var(--jp-ui-font-size1); overflow: auto; } .jp-NotebookTools-tool { padding: 0px 12px 0 12px; } .jp-ActiveCellTool { padding: 12px; background-color: var(--jp-layout-color1); border-top: none !important; } .jp-ActiveCellTool .jp-InputArea-prompt { flex: 0 0 auto; padding-left: 0px; } .jp-ActiveCellTool .jp-InputArea-editor { flex: 1 1 auto; background: var(--jp-cell-editor-background); border-color: var(--jp-cell-editor-border-color); } .jp-ActiveCellTool .jp-InputArea-editor .CodeMirror { background: transparent; } .jp-MetadataEditorTool { flex-direction: column; padding: 12px 0px 12px 0px; } .jp-RankedPanel > :not(:first-child) { margin-top: 12px; } .jp-KeySelector select.jp-mod-styled { font-size: var(--jp-ui-font-size1); color: var(--jp-ui-font-color0); border: var(--jp-border-width) solid var(--jp-border-color1); } .jp-KeySelector label, .jp-MetadataEditorTool label { line-height: 1.4; } /*----------------------------------------------------------------------------- | Presentation Mode (.jp-mod-presentationMode) |----------------------------------------------------------------------------*/ .jp-mod-presentationMode .jp-Notebook { --jp-content-font-size1: var(--jp-content-presentation-font-size1); --jp-code-font-size: var(--jp-code-presentation-font-size); } .jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-InputPrompt, .jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-OutputPrompt { flex: 0 0 110px; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /* This file was auto-generated by ensurePackage() in @jupyterlab/buildutils */ /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /* The following CSS variables define the main, public API for styling JupyterLab. These variables should be used by all plugins wherever possible. In other words, plugins should not define custom colors, sizes, etc unless absolutely necessary. This enables users to change the visual theme of JupyterLab by changing these variables. Many variables appear in an ordered sequence (0,1,2,3). These sequences are designed to work well together, so for example, `--jp-border-color1` should be used with `--jp-layout-color1`. The numbers have the following meanings: * 0: super-primary, reserved for special emphasis * 1: primary, most important under normal situations * 2: secondary, next most important under normal situations * 3: tertiary, next most important under normal situations Throughout JupyterLab, we are mostly following principles from Google's Material Design when selecting colors. We are not, however, following all of MD as it is not optimized for dense, information rich UIs. */ :root { /* Elevation * * We style box-shadows using Material Design's idea of elevation. These particular numbers are taken from here: * * https://github.com/material-components/material-components-web * https://material-components-web.appspot.com/elevation.html */ --jp-shadow-base-lightness: 0; --jp-shadow-umbra-color: rgba( var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), 0.2 ); --jp-shadow-penumbra-color: rgba( var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), 0.14 ); --jp-shadow-ambient-color: rgba( var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), 0.12 ); --jp-elevation-z0: none; --jp-elevation-z1: 0px 2px 1px -1px var(--jp-shadow-umbra-color), 0px 1px 1px 0px var(--jp-shadow-penumbra-color), 0px 1px 3px 0px var(--jp-shadow-ambient-color); --jp-elevation-z2: 0px 3px 1px -2px var(--jp-shadow-umbra-color), 0px 2px 2px 0px var(--jp-shadow-penumbra-color), 0px 1px 5px 0px var(--jp-shadow-ambient-color); --jp-elevation-z4: 0px 2px 4px -1px var(--jp-shadow-umbra-color), 0px 4px 5px 0px var(--jp-shadow-penumbra-color), 0px 1px 10px 0px var(--jp-shadow-ambient-color); --jp-elevation-z6: 0px 3px 5px -1px var(--jp-shadow-umbra-color), 0px 6px 10px 0px var(--jp-shadow-penumbra-color), 0px 1px 18px 0px var(--jp-shadow-ambient-color); --jp-elevation-z8: 0px 5px 5px -3px var(--jp-shadow-umbra-color), 0px 8px 10px 1px var(--jp-shadow-penumbra-color), 0px 3px 14px 2px var(--jp-shadow-ambient-color); --jp-elevation-z12: 0px 7px 8px -4px var(--jp-shadow-umbra-color), 0px 12px 17px 2px var(--jp-shadow-penumbra-color), 0px 5px 22px 4px var(--jp-shadow-ambient-color); --jp-elevation-z16: 0px 8px 10px -5px var(--jp-shadow-umbra-color), 0px 16px 24px 2px var(--jp-shadow-penumbra-color), 0px 6px 30px 5px var(--jp-shadow-ambient-color); --jp-elevation-z20: 0px 10px 13px -6px var(--jp-shadow-umbra-color), 0px 20px 31px 3px var(--jp-shadow-penumbra-color), 0px 8px 38px 7px var(--jp-shadow-ambient-color); --jp-elevation-z24: 0px 11px 15px -7px var(--jp-shadow-umbra-color), 0px 24px 38px 3px var(--jp-shadow-penumbra-color), 0px 9px 46px 8px var(--jp-shadow-ambient-color); /* Borders * * The following variables, specify the visual styling of borders in JupyterLab. */ --jp-border-width: 1px; --jp-border-color0: var(--md-grey-400); --jp-border-color1: var(--md-grey-400); --jp-border-color2: var(--md-grey-300); --jp-border-color3: var(--md-grey-200); --jp-border-radius: 2px; /* UI Fonts * * The UI font CSS variables are used for the typography all of the JupyterLab * user interface elements that are not directly user generated content. * * The font sizing here is done assuming that the body font size of --jp-ui-font-size1 * is applied to a parent element. When children elements, such as headings, are sized * in em all things will be computed relative to that body size. */ --jp-ui-font-scale-factor: 1.2; --jp-ui-font-size0: 0.83333em; --jp-ui-font-size1: 13px; /* Base font size */ --jp-ui-font-size2: 1.2em; --jp-ui-font-size3: 1.44em; --jp-ui-font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; /* * Use these font colors against the corresponding main layout colors. * In a light theme, these go from dark to light. */ /* Defaults use Material Design specification */ --jp-ui-font-color0: rgba(0, 0, 0, 1); --jp-ui-font-color1: rgba(0, 0, 0, 0.87); --jp-ui-font-color2: rgba(0, 0, 0, 0.54); --jp-ui-font-color3: rgba(0, 0, 0, 0.38); /* * Use these against the brand/accent/warn/error colors. * These will typically go from light to darker, in both a dark and light theme. */ --jp-ui-inverse-font-color0: rgba(255, 255, 255, 1); --jp-ui-inverse-font-color1: rgba(255, 255, 255, 1); --jp-ui-inverse-font-color2: rgba(255, 255, 255, 0.7); --jp-ui-inverse-font-color3: rgba(255, 255, 255, 0.5); /* Content Fonts * * Content font variables are used for typography of user generated content. * * The font sizing here is done assuming that the body font size of --jp-content-font-size1 * is applied to a parent element. When children elements, such as headings, are sized * in em all things will be computed relative to that body size. */ --jp-content-line-height: 1.6; --jp-content-font-scale-factor: 1.2; --jp-content-font-size0: 0.83333em; --jp-content-font-size1: 14px; /* Base font size */ --jp-content-font-size2: 1.2em; --jp-content-font-size3: 1.44em; --jp-content-font-size4: 1.728em; --jp-content-font-size5: 2.0736em; /* This gives a magnification of about 125% in presentation mode over normal. */ --jp-content-presentation-font-size1: 17px; --jp-content-heading-line-height: 1; --jp-content-heading-margin-top: 1.2em; --jp-content-heading-margin-bottom: 0.8em; --jp-content-heading-font-weight: 500; /* Defaults use Material Design specification */ --jp-content-font-color0: rgba(0, 0, 0, 1); --jp-content-font-color1: rgba(0, 0, 0, 0.87); --jp-content-font-color2: rgba(0, 0, 0, 0.54); --jp-content-font-color3: rgba(0, 0, 0, 0.38); --jp-content-link-color: var(--md-blue-700); --jp-content-font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; /* * Code Fonts * * Code font variables are used for typography of code and other monospaces content. */ --jp-code-font-size: 13px; --jp-code-line-height: 1.3077; /* 17px for 13px base */ --jp-code-padding: 5px; /* 5px for 13px base, codemirror highlighting needs integer px value */ --jp-code-font-family-default: Menlo, Consolas, 'DejaVu Sans Mono', monospace; --jp-code-font-family: var(--jp-code-font-family-default); /* This gives a magnification of about 125% in presentation mode over normal. */ --jp-code-presentation-font-size: 16px; /* may need to tweak cursor width if you change font size */ --jp-code-cursor-width0: 1.4px; --jp-code-cursor-width1: 2px; --jp-code-cursor-width2: 4px; /* Layout * * The following are the main layout colors use in JupyterLab. In a light * theme these would go from light to dark. */ --jp-layout-color0: white; --jp-layout-color1: white; --jp-layout-color2: var(--md-grey-200); --jp-layout-color3: var(--md-grey-400); --jp-layout-color4: var(--md-grey-600); /* Inverse Layout * * The following are the inverse layout colors use in JupyterLab. In a light * theme these would go from dark to light. */ --jp-inverse-layout-color0: #111111; --jp-inverse-layout-color1: var(--md-grey-900); --jp-inverse-layout-color2: var(--md-grey-800); --jp-inverse-layout-color3: var(--md-grey-700); --jp-inverse-layout-color4: var(--md-grey-600); /* Brand/accent */ --jp-brand-color0: var(--md-blue-700); --jp-brand-color1: var(--md-blue-500); --jp-brand-color2: var(--md-blue-300); --jp-brand-color3: var(--md-blue-100); --jp-brand-color4: var(--md-blue-50); --jp-accent-color0: var(--md-green-700); --jp-accent-color1: var(--md-green-500); --jp-accent-color2: var(--md-green-300); --jp-accent-color3: var(--md-green-100); /* State colors (warn, error, success, info) */ --jp-warn-color0: var(--md-orange-700); --jp-warn-color1: var(--md-orange-500); --jp-warn-color2: var(--md-orange-300); --jp-warn-color3: var(--md-orange-100); --jp-error-color0: var(--md-red-700); --jp-error-color1: var(--md-red-500); --jp-error-color2: var(--md-red-300); --jp-error-color3: var(--md-red-100); --jp-success-color0: var(--md-green-700); --jp-success-color1: var(--md-green-500); --jp-success-color2: var(--md-green-300); --jp-success-color3: var(--md-green-100); --jp-info-color0: var(--md-cyan-700); --jp-info-color1: var(--md-cyan-500); --jp-info-color2: var(--md-cyan-300); --jp-info-color3: var(--md-cyan-100); /* Cell specific styles */ --jp-cell-padding: 5px; --jp-cell-collapser-width: 8px; --jp-cell-collapser-min-height: 20px; --jp-cell-collapser-not-active-hover-opacity: 0.6; --jp-cell-editor-background: var(--md-grey-100); --jp-cell-editor-border-color: var(--md-grey-300); --jp-cell-editor-box-shadow: inset 0 0 2px var(--md-blue-300); --jp-cell-editor-active-background: var(--jp-layout-color0); --jp-cell-editor-active-border-color: var(--jp-brand-color1); --jp-cell-prompt-width: 64px; --jp-cell-prompt-font-family: 'Source Code Pro', monospace; --jp-cell-prompt-letter-spacing: 0px; --jp-cell-prompt-opacity: 1; --jp-cell-prompt-not-active-opacity: 0.5; --jp-cell-prompt-not-active-font-color: var(--md-grey-700); /* A custom blend of MD grey and blue 600 * See https://meyerweb.com/eric/tools/color-blend/#546E7A:1E88E5:5:hex */ --jp-cell-inprompt-font-color: #307fc1; /* A custom blend of MD grey and orange 600 * https://meyerweb.com/eric/tools/color-blend/#546E7A:F4511E:5:hex */ --jp-cell-outprompt-font-color: #bf5b3d; /* Notebook specific styles */ --jp-notebook-padding: 10px; --jp-notebook-select-background: var(--jp-layout-color1); --jp-notebook-multiselected-color: var(--md-blue-50); /* The scroll padding is calculated to fill enough space at the bottom of the notebook to show one single-line cell (with appropriate padding) at the top when the notebook is scrolled all the way to the bottom. We also subtract one pixel so that no scrollbar appears if we have just one single-line cell in the notebook. This padding is to enable a 'scroll past end' feature in a notebook. */ --jp-notebook-scroll-padding: calc( 100% - var(--jp-code-font-size) * var(--jp-code-line-height) - var(--jp-code-padding) - var(--jp-cell-padding) - 1px ); /* Rendermime styles */ --jp-rendermime-error-background: #fdd; --jp-rendermime-table-row-background: var(--md-grey-100); --jp-rendermime-table-row-hover-background: var(--md-light-blue-50); /* Dialog specific styles */ --jp-dialog-background: rgba(0, 0, 0, 0.25); /* Console specific styles */ --jp-console-padding: 10px; /* Toolbar specific styles */ --jp-toolbar-border-color: var(--jp-border-color1); --jp-toolbar-micro-height: 8px; --jp-toolbar-background: var(--jp-layout-color1); --jp-toolbar-box-shadow: 0px 0px 2px 0px rgba(0, 0, 0, 0.24); --jp-toolbar-header-margin: 4px 4px 0px 4px; --jp-toolbar-active-background: var(--md-grey-300); /* Input field styles */ --jp-input-box-shadow: inset 0 0 2px var(--md-blue-300); --jp-input-active-background: var(--jp-layout-color1); --jp-input-hover-background: var(--jp-layout-color1); --jp-input-background: var(--md-grey-100); --jp-input-border-color: var(--jp-border-color1); --jp-input-active-border-color: var(--jp-brand-color1); --jp-input-active-box-shadow-color: rgba(19, 124, 189, 0.3); /* General editor styles */ --jp-editor-selected-background: #d9d9d9; --jp-editor-selected-focused-background: #d7d4f0; --jp-editor-cursor-color: var(--jp-ui-font-color0); /* Code mirror specific styles */ --jp-mirror-editor-keyword-color: #008000; --jp-mirror-editor-atom-color: #88f; --jp-mirror-editor-number-color: #080; --jp-mirror-editor-def-color: #00f; --jp-mirror-editor-variable-color: var(--md-grey-900); --jp-mirror-editor-variable-2-color: #05a; --jp-mirror-editor-variable-3-color: #085; --jp-mirror-editor-punctuation-color: #05a; --jp-mirror-editor-property-color: #05a; --jp-mirror-editor-operator-color: #aa22ff; --jp-mirror-editor-comment-color: #408080; --jp-mirror-editor-string-color: #ba2121; --jp-mirror-editor-string-2-color: #708; --jp-mirror-editor-meta-color: #aa22ff; --jp-mirror-editor-qualifier-color: #555; --jp-mirror-editor-builtin-color: #008000; --jp-mirror-editor-bracket-color: #997; --jp-mirror-editor-tag-color: #170; --jp-mirror-editor-attribute-color: #00c; --jp-mirror-editor-header-color: blue; --jp-mirror-editor-quote-color: #090; --jp-mirror-editor-link-color: #00c; --jp-mirror-editor-error-color: #f00; --jp-mirror-editor-hr-color: #999; /* Vega extension styles */ --jp-vega-background: white; /* Sidebar-related styles */ --jp-sidebar-min-width: 180px; /* Search-related styles */ --jp-search-toggle-off-opacity: 0.5; --jp-search-toggle-hover-opacity: 0.8; --jp-search-toggle-on-opacity: 1; --jp-search-selected-match-background-color: rgb(245, 200, 0); --jp-search-selected-match-color: black; --jp-search-unselected-match-background-color: var( --jp-inverse-layout-color0 ); --jp-search-unselected-match-color: var(--jp-ui-inverse-font-color0); /* Icon colors that work well with light or dark backgrounds */ --jp-icon-contrast-color0: var(--md-purple-600); --jp-icon-contrast-color1: var(--md-green-600); --jp-icon-contrast-color2: var(--md-pink-600); --jp-icon-contrast-color3: var(--md-blue-600); } /* Reset */ *, *::before, *::after { -webkit-box-sizing: border-box; box-sizing: border-box; } /* Reset */ .jp-Notebook { padding: unset; /* outline: none; */ /* overflow: auto; */ background: unset; } /* Reset */ .jp-RenderedHTMLCommon { padding: 0; } /* Anchor links */ .anchor-link { display: none; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link { display: inline-block; } /* Markdown cells: Remove prompt */ .jp-Cell-inputWrapper .jp-InputPrompt { display: none; } /* Code cells: Show prompt */ .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt { display: block; } /* Prompt fixes */ .jp-InputPrompt, .jp-OutputPrompt{ overflow: visible; } /* Lab uses move for the input prompts */ .jp-Notebook .jp-Cell .jp-InputPrompt { cursor: default; } /* Editor max-width */ .jp-InputArea-editor { width: 1px; } /* Editor colors */ .jp-Editor .highlight { margin: 0.4em; background: var(--jp-cell-editor-background); color: var(--jp-mirror-editor-variable-color); } /* Editor colors */ .jp-Editor .highlight pre { background: none; border: none; margin: 0; overflow-x: auto; } /* Material mkdocs */ .jp-RenderedHTMLCommon p { font-size: .8rem; line-height: 1.6; } .jp-RenderedHTMLCommon li { font-size: .8rem; line-height: 1.6; } .jp-RenderedHTMLCommon h1 { margin: 0 0 1.25em; color: var(--md-default-fg-color--light); font-weight: 300; font-size: 2em; line-height: 1.3; letter-spacing: -0.01em; } .jp-RenderedHTMLCommon h2 { margin: 1.6em 0 .64em; font-weight: 300; font-size: 1.965em; line-height: 1.4; letter-spacing: -0.01em; } .jp-RenderedHTMLCommon h3 { margin: 1.6em 0 .8em; font-weight: 400; font-size: 1.57em; line-height: 1.5; letter-spacing: -0.01em; } .jp-RenderedHTMLCommon hr { border: none; } /* Tags on cells */ /* Mostly copying this from straight Jupyter */ .celltoolbar { border: none; background: #EEE; border-radius: 2px 2px 0px 0px; width: 100%; height: 29px; padding-right: 4px; box-orient: horizontal; box-align: stretch; display: flex; flex-direction: row; align-items: stretch; box-pack: end; justify-content: flex-start; display: -webkit-flex; } .celltoolbar .tags_button_container { display: flex; } .celltoolbar .tags_button_container .tag-container { display: flex; flex-direction: row; flex-grow: 1; overflow: hidden; position: relative; } .celltoolbar .tags_button_container .tag-container .cell-tag { background-color: #fff; white-space: nowrap; margin: 3px 4px; padding: 0 4px; border-radius: 1px; border: 1px solid #ccc; box-shadow: none; width: inherit; font-size: 11px; font-family: \"Roboto Mono\",SFMono-Regular,Consolas,Menlo,monospace; height: 22px; /* line-height: 22px; */ display: inline-block; } /* Bokeh plots table no hover */ .bk-plot-wrapper tbody tr { background: none !important; } .bk-plot-wrapper tbody tr:hover { background: none !important; } init_mathjax = function() { if (window.MathJax) { // MathJax loaded MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: \"AMS\", useLabelIds: true } }, tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ], processEscapes: true, processEnvironments: true }, displayAlign: 'center', CommonHTML: { linebreaks: { automatic: true } }, \"HTML-CSS\": { linebreaks: { automatic: true } } }); MathJax.Hub.Queue([\"Typeset\", MathJax.Hub]); } } init_mathjax(); Getting Started: High-level API \u00b6 In this tutorial we will explore the basic features of Elegy . If you are a Keras user you should feel at home, if you are currently using Jax things will appear much more streamlined. To get started you will first need to install the following dependencies: In [ ]: ! pip install -- upgrade pip ! pip install elegy dataget matplotlib # For GPU install proper version of your CUDA, following will work in COLAB: # ! pip install --upgrade jax jaxlib==0.1.59+cuda101 -f https://storage.googleapis.com/jax-releases/jax_releases.html Note: that Elegy depends on the jax CPU version hosted on Pypi, if you want to run jax on GPU you will need to install it separately. If you are running this example on Colab, jax is already preinstalled but you can uncomment the last line of the previous cell if you want to update it. Loading the Data \u00b6 In this tutorial we will train a Neural Network on the MNIST dataset, for this we will first need to download and load the data into memory. Here we will use dataget for simplicity but you can use you favorite datasets library. In [1]: import dataget X_train , y_train , X_test , y_test = dataget . image . mnist ( global_cache = True ) . get () print ( \"X_train:\" , X_train . shape , X_train . dtype ) print ( \"y_train:\" , y_train . shape , y_train . dtype ) print ( \"X_test:\" , X_test . shape , X_test . dtype ) print ( \"y_test:\" , y_test . shape , y_test . dtype ) X_train: (60000, 28, 28) uint8 y_train: (60000,) uint8 X_test: (10000, 28, 28) uint8 y_test: (10000,) uint8 In this case dataget loads the data from Yann LeCun's website. Defining the Architecture \u00b6 The first thing we need to do is define our model's architecture, Elegy's Basic API is \"framework-agnostic\" in the sense that it lets you use varios popular Module systems like Flax, Haiku, Elegy, pure Jax functions, and you can give support to other Module system if you wish. For simplicity we are going to use elegy.Module but the code would be very similar in e.g Flax or Haiku. To implement an Elegy Module all we have to create a class that inherits from elegy.Module and defines the call method. In this example we will create a simple 2 layer MLP using standard modules from elegy.nn : In [2]: import jax.numpy as jnp import jax import elegy class MLP ( elegy . Module ): \"\"\"Standard LeNet-300-100 MLP network.\"\"\" def __init__ ( self , n1 : int = 300 , n2 : int = 100 , ** kwargs ): super () . __init__ ( ** kwargs ) self . n1 = n1 self . n2 = n2 def call ( self , image : jnp . ndarray ) -> jnp . ndarray : image = image . astype ( jnp . float32 ) / 255.0 mlp = elegy . nn . sequential ( elegy . nn . Flatten (), elegy . nn . Linear ( self . n1 ), jax . nn . relu , elegy . nn . Linear ( self . n2 ), jax . nn . relu , elegy . nn . Linear ( 10 ), ) return mlp ( image ) This code should feel familiar to most Keras / PyTorch users. The main difference here is that Jax module systems tend to use hook-based patters so you can actually declare the submodules (e.g. Linear) inline, this tends to produce much shorter and more readable code. Creating the Model \u00b6 Now that we have this module we can create an Elegy Model which is Elegy's central API: In [3]: import optax model = elegy . Model ( module = MLP ( n1 = 300 , n2 = 100 ), loss = [ elegy . losses . SparseCategoricalCrossentropy ( from_logits = True ), elegy . regularizers . GlobalL2 ( l = 1e-4 ), ], metrics = elegy . metrics . SparseCategoricalAccuracy (), optimizer = optax . adam ( 1e-3 ), ) An Elegy Model lets you train, evaluate, and perform inference but requires you to define a module, losses, metrics, and an optimizer depending on what you are doing. If you are a Keras user you should feel at home, just be aware that Elegy is a more flexible in the way it handles losses and metrics (for the better) so checkout of the guides on these to see the differences. As in Keras, you can get a rich description of the model by calling Model.summary with a sample input: In [4]: model . summary ( X_train [: 64 ]) \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513 \u2503 Layer \u2503 Outputs Shape \u2503 Trainable \u2503 Non-trainable \u2503 \u2503 \u2503 \u2503 Parameters \u2503 Parameters \u2503 \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529 \u2502 Inputs \u2502 (64, 28, 28) uint8 \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 flatten Flatten \u2502 (64, 784) float32 \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 linear Linear \u2502 (64, 300) float32 \u2502 235,500 942.0 KB \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 relu \u2502 (64, 300) float32 \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 linear_1 Linear \u2502 (64, 100) float32 \u2502 30,100 120.4 KB \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 relu \u2502 (64, 100) float32 \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 linear_2 Linear \u2502 (64, 10) float32 \u2502 1,010 4.0 KB \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 * MLP \u2502 (64, 10) float32 \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 \u2502 Total \u2502 266,610 1.1 MB \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Total Parameters: 266,610 1.1 MB Note: this works on custom Flax and Haiku modules as well but you might see less details since their standard Modules don't support summaries. Check out the Basic API guides on Modules for more information. Training the Model \u00b6 We are now ready to pass our model some data to start training, like in Keras this is done via the fit method which contains more or less the same signature. Elegy support a variety of input data sources like Tensorflow Dataset, Pytorch DataLoader, Elegy DataLoader, and Python Generators, check out the guide on Data Sources for more information. The following code will train our model for 100 epochs while limiting each epoch to 200 steps and using a batch size of 64 : In [ ]: history = model . fit ( x = X_train , y = y_train , epochs = 100 , steps_per_epoch = 200 , batch_size = 64 , validation_data = ( X_test , y_test ), shuffle = True , callbacks = [ elegy . callbacks . ModelCheckpoint ( \"models/high-level\" , save_best_only = True )], ) ... Epoch 99/100 200/200 [==============================] - 1s 4ms/step - l2_regularization_loss: 0.0452 - loss: 0.0662 - sparse_categorical_accuracy: 0.9928 - sparse_categorical_crossentropy_loss: 0.0210 - val_l2_regularization_loss: 0.0451 - val_loss: 0.1259 - val_sparse_categorical_accuracy: 0.9766 - val_sparse_categorical_crossentropy_loss: 0.0808 Epoch 100/100 200/200 [==============================] - 1s 4ms/step - l2_regularization_loss: 0.0450 - loss: 0.0610 - sparse_categorical_accuracy: 0.9953 - sparse_categorical_crossentropy_loss: 0.0161 - val_l2_regularization_loss: 0.0447 - val_loss: 0.1093 - val_sparse_categorical_accuracy: 0.9795 - val_sparse_categorical_crossentropy_loss: 0.0646 The elegy.callbacks.ModelCheckpoint callback will periodicall saves the model a folder called \"models/high-level\" during training which is very useful if we want to load it after the process is finished, we will use it later. fit returns a History object which which contains information the time series for the values of the losses and metrics throughout training, we can use it to generate some nice plots of the evolution of our training: In [7]: import matplotlib.pyplot as plt def plot_history ( history ): n_plots = len ( history . history . keys ()) // 2 plt . figure ( figsize = ( 14 , 24 )) for i , key in enumerate ( list ( history . history . keys ())[: n_plots ]): metric = history . history [ key ] val_metric = history . history [ f \"val_ { key } \" ] plt . subplot ( n_plots , 1 , i + 1 ) plt . plot ( metric , label = f \"Training { key } \" ) plt . plot ( val_metric , label = f \"Validation { key } \" ) plt . legend ( loc = \"lower right\" ) plt . ylabel ( key ) plt . title ( f \"Training and Validation { key } \" ) plt . show () plot_history ( history ) Generating Predictions \u00b6 Having our trained model we can now get some samples from the test set and generate some predictions. First we will just pick some random samples using numpy : In [8]: import numpy as np idxs = np . random . randint ( 0 , 10000 , size = ( 9 ,)) x_sample = X_test [ idxs ] Here we selected 9 random images. Now we can use the predict method to get their labels: In [9]: y_pred = model . predict ( x = x_sample ) Easy right? Finally lets plot the results to see if they are accurate. In [10]: plt . figure ( figsize = ( 12 , 12 )) for i in range ( 3 ): for j in range ( 3 ): k = 3 * i + j plt . subplot ( 3 , 3 , k + 1 ) plt . title ( f \" { np . argmax ( y_pred [ k ]) } \" ) plt . imshow ( x_sample [ k ], cmap = \"gray\" ) Perfect! Serialization \u00b6 To serialize the Model you can use the model.save(...) , this will create a folder with some files that contain the model's code plus all parameters and states, however since we had previously used the ModelCheckpoint callback we can load it using elegy.load . Lets get a new model reference containing the same weights and call its evaluate method to verify it loaded correctly: In [11]: # You can use can use `save` but `ModelCheckpoint already serialized the model # model.save(\"model\") # current model reference print ( \"current model id:\" , id ( model )) # load model from disk model = elegy . load ( \"models/high-level\" ) # new model reference print ( \"new model id: \" , id ( model )) # check that it works! model . evaluate ( x = X_test , y = y_test ) current model id: 140492784455632 new model id: 140490871398304 313/313 [==============================] - 3s 11ms/step - l2_regularization_loss: 0.0454 - loss: 0.1156 - sparse_categorical_accuracy: 0.9785 - sparse_categorical_crossentropy_loss: 0.0702 Out[11]: {'l2_regularization_loss': DeviceArray(0.04541492, dtype=float32), 'loss': DeviceArray(0.10205666, dtype=float32), 'sparse_categorical_accuracy': DeviceArray(0.9816, dtype=float32), 'sparse_categorical_crossentropy_loss': DeviceArray(0.05664176, dtype=float32), 'size': 32} Excellent! We hope you've enjoyed this tutorial. Next Steps \u00b6 Elegy is still in a very early stage, there are probably tons of bugs and missing features but we will get there. If you have some ideas or feedback on the current design we are eager to hear from you, feel free to open an issue.","title":"High Level API"},{"location":"getting-started/high-level-api/#getting-started-high-level-api","text":"In this tutorial we will explore the basic features of Elegy . If you are a Keras user you should feel at home, if you are currently using Jax things will appear much more streamlined. To get started you will first need to install the following dependencies: In [ ]: ! pip install -- upgrade pip ! pip install elegy dataget matplotlib # For GPU install proper version of your CUDA, following will work in COLAB: # ! pip install --upgrade jax jaxlib==0.1.59+cuda101 -f https://storage.googleapis.com/jax-releases/jax_releases.html Note: that Elegy depends on the jax CPU version hosted on Pypi, if you want to run jax on GPU you will need to install it separately. If you are running this example on Colab, jax is already preinstalled but you can uncomment the last line of the previous cell if you want to update it.","title":"Getting Started: High-level API"},{"location":"getting-started/high-level-api/#loading-the-data","text":"In this tutorial we will train a Neural Network on the MNIST dataset, for this we will first need to download and load the data into memory. Here we will use dataget for simplicity but you can use you favorite datasets library. In [1]: import dataget X_train , y_train , X_test , y_test = dataget . image . mnist ( global_cache = True ) . get () print ( \"X_train:\" , X_train . shape , X_train . dtype ) print ( \"y_train:\" , y_train . shape , y_train . dtype ) print ( \"X_test:\" , X_test . shape , X_test . dtype ) print ( \"y_test:\" , y_test . shape , y_test . dtype ) X_train: (60000, 28, 28) uint8 y_train: (60000,) uint8 X_test: (10000, 28, 28) uint8 y_test: (10000,) uint8 In this case dataget loads the data from Yann LeCun's website.","title":"Loading the Data"},{"location":"getting-started/high-level-api/#defining-the-architecture","text":"The first thing we need to do is define our model's architecture, Elegy's Basic API is \"framework-agnostic\" in the sense that it lets you use varios popular Module systems like Flax, Haiku, Elegy, pure Jax functions, and you can give support to other Module system if you wish. For simplicity we are going to use elegy.Module but the code would be very similar in e.g Flax or Haiku. To implement an Elegy Module all we have to create a class that inherits from elegy.Module and defines the call method. In this example we will create a simple 2 layer MLP using standard modules from elegy.nn : In [2]: import jax.numpy as jnp import jax import elegy class MLP ( elegy . Module ): \"\"\"Standard LeNet-300-100 MLP network.\"\"\" def __init__ ( self , n1 : int = 300 , n2 : int = 100 , ** kwargs ): super () . __init__ ( ** kwargs ) self . n1 = n1 self . n2 = n2 def call ( self , image : jnp . ndarray ) -> jnp . ndarray : image = image . astype ( jnp . float32 ) / 255.0 mlp = elegy . nn . sequential ( elegy . nn . Flatten (), elegy . nn . Linear ( self . n1 ), jax . nn . relu , elegy . nn . Linear ( self . n2 ), jax . nn . relu , elegy . nn . Linear ( 10 ), ) return mlp ( image ) This code should feel familiar to most Keras / PyTorch users. The main difference here is that Jax module systems tend to use hook-based patters so you can actually declare the submodules (e.g. Linear) inline, this tends to produce much shorter and more readable code.","title":"Defining the Architecture"},{"location":"getting-started/high-level-api/#creating-the-model","text":"Now that we have this module we can create an Elegy Model which is Elegy's central API: In [3]: import optax model = elegy . Model ( module = MLP ( n1 = 300 , n2 = 100 ), loss = [ elegy . losses . SparseCategoricalCrossentropy ( from_logits = True ), elegy . regularizers . GlobalL2 ( l = 1e-4 ), ], metrics = elegy . metrics . SparseCategoricalAccuracy (), optimizer = optax . adam ( 1e-3 ), ) An Elegy Model lets you train, evaluate, and perform inference but requires you to define a module, losses, metrics, and an optimizer depending on what you are doing. If you are a Keras user you should feel at home, just be aware that Elegy is a more flexible in the way it handles losses and metrics (for the better) so checkout of the guides on these to see the differences. As in Keras, you can get a rich description of the model by calling Model.summary with a sample input: In [4]: model . summary ( X_train [: 64 ]) \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513 \u2503 Layer \u2503 Outputs Shape \u2503 Trainable \u2503 Non-trainable \u2503 \u2503 \u2503 \u2503 Parameters \u2503 Parameters \u2503 \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529 \u2502 Inputs \u2502 (64, 28, 28) uint8 \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 flatten Flatten \u2502 (64, 784) float32 \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 linear Linear \u2502 (64, 300) float32 \u2502 235,500 942.0 KB \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 relu \u2502 (64, 300) float32 \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 linear_1 Linear \u2502 (64, 100) float32 \u2502 30,100 120.4 KB \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 relu \u2502 (64, 100) float32 \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 linear_2 Linear \u2502 (64, 10) float32 \u2502 1,010 4.0 KB \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 * MLP \u2502 (64, 10) float32 \u2502 \u2502 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 \u2502 Total \u2502 266,610 1.1 MB \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Total Parameters: 266,610 1.1 MB Note: this works on custom Flax and Haiku modules as well but you might see less details since their standard Modules don't support summaries. Check out the Basic API guides on Modules for more information.","title":"Creating the Model"},{"location":"getting-started/high-level-api/#training-the-model","text":"We are now ready to pass our model some data to start training, like in Keras this is done via the fit method which contains more or less the same signature. Elegy support a variety of input data sources like Tensorflow Dataset, Pytorch DataLoader, Elegy DataLoader, and Python Generators, check out the guide on Data Sources for more information. The following code will train our model for 100 epochs while limiting each epoch to 200 steps and using a batch size of 64 : In [ ]: history = model . fit ( x = X_train , y = y_train , epochs = 100 , steps_per_epoch = 200 , batch_size = 64 , validation_data = ( X_test , y_test ), shuffle = True , callbacks = [ elegy . callbacks . ModelCheckpoint ( \"models/high-level\" , save_best_only = True )], ) ... Epoch 99/100 200/200 [==============================] - 1s 4ms/step - l2_regularization_loss: 0.0452 - loss: 0.0662 - sparse_categorical_accuracy: 0.9928 - sparse_categorical_crossentropy_loss: 0.0210 - val_l2_regularization_loss: 0.0451 - val_loss: 0.1259 - val_sparse_categorical_accuracy: 0.9766 - val_sparse_categorical_crossentropy_loss: 0.0808 Epoch 100/100 200/200 [==============================] - 1s 4ms/step - l2_regularization_loss: 0.0450 - loss: 0.0610 - sparse_categorical_accuracy: 0.9953 - sparse_categorical_crossentropy_loss: 0.0161 - val_l2_regularization_loss: 0.0447 - val_loss: 0.1093 - val_sparse_categorical_accuracy: 0.9795 - val_sparse_categorical_crossentropy_loss: 0.0646 The elegy.callbacks.ModelCheckpoint callback will periodicall saves the model a folder called \"models/high-level\" during training which is very useful if we want to load it after the process is finished, we will use it later. fit returns a History object which which contains information the time series for the values of the losses and metrics throughout training, we can use it to generate some nice plots of the evolution of our training: In [7]: import matplotlib.pyplot as plt def plot_history ( history ): n_plots = len ( history . history . keys ()) // 2 plt . figure ( figsize = ( 14 , 24 )) for i , key in enumerate ( list ( history . history . keys ())[: n_plots ]): metric = history . history [ key ] val_metric = history . history [ f \"val_ { key } \" ] plt . subplot ( n_plots , 1 , i + 1 ) plt . plot ( metric , label = f \"Training { key } \" ) plt . plot ( val_metric , label = f \"Validation { key } \" ) plt . legend ( loc = \"lower right\" ) plt . ylabel ( key ) plt . title ( f \"Training and Validation { key } \" ) plt . show () plot_history ( history )","title":"Training the Model"},{"location":"getting-started/high-level-api/#generating-predictions","text":"Having our trained model we can now get some samples from the test set and generate some predictions. First we will just pick some random samples using numpy : In [8]: import numpy as np idxs = np . random . randint ( 0 , 10000 , size = ( 9 ,)) x_sample = X_test [ idxs ] Here we selected 9 random images. Now we can use the predict method to get their labels: In [9]: y_pred = model . predict ( x = x_sample ) Easy right? Finally lets plot the results to see if they are accurate. In [10]: plt . figure ( figsize = ( 12 , 12 )) for i in range ( 3 ): for j in range ( 3 ): k = 3 * i + j plt . subplot ( 3 , 3 , k + 1 ) plt . title ( f \" { np . argmax ( y_pred [ k ]) } \" ) plt . imshow ( x_sample [ k ], cmap = \"gray\" ) Perfect!","title":"Generating Predictions"},{"location":"getting-started/high-level-api/#serialization","text":"To serialize the Model you can use the model.save(...) , this will create a folder with some files that contain the model's code plus all parameters and states, however since we had previously used the ModelCheckpoint callback we can load it using elegy.load . Lets get a new model reference containing the same weights and call its evaluate method to verify it loaded correctly: In [11]: # You can use can use `save` but `ModelCheckpoint already serialized the model # model.save(\"model\") # current model reference print ( \"current model id:\" , id ( model )) # load model from disk model = elegy . load ( \"models/high-level\" ) # new model reference print ( \"new model id: \" , id ( model )) # check that it works! model . evaluate ( x = X_test , y = y_test ) current model id: 140492784455632 new model id: 140490871398304 313/313 [==============================] - 3s 11ms/step - l2_regularization_loss: 0.0454 - loss: 0.1156 - sparse_categorical_accuracy: 0.9785 - sparse_categorical_crossentropy_loss: 0.0702 Out[11]: {'l2_regularization_loss': DeviceArray(0.04541492, dtype=float32), 'loss': DeviceArray(0.10205666, dtype=float32), 'sparse_categorical_accuracy': DeviceArray(0.9816, dtype=float32), 'sparse_categorical_crossentropy_loss': DeviceArray(0.05664176, dtype=float32), 'size': 32} Excellent! We hope you've enjoyed this tutorial.","title":"Serialization"},{"location":"getting-started/high-level-api/#next-steps","text":"Elegy is still in a very early stage, there are probably tons of bugs and missing features but we will get there. If you have some ideas or feedback on the current design we are eager to hear from you, feel free to open an issue.","title":"Next Steps"},{"location":"getting-started/low-level-api/","text":"pre { line-height: 125%; } td.linenos pre { color: #000000; background-color: #f0f0f0; padding-left: 5px; padding-right: 5px; } span.linenos { color: #000000; background-color: #f0f0f0; padding-left: 5px; padding-right: 5px; } td.linenos pre.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; } span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; } .highlight .hll { background-color: var(--jp-cell-editor-active-background) } .highlight { background: var(--jp-cell-editor-background); color: var(--jp-mirror-editor-variable-color) } .highlight .c { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment */ .highlight .err { color: var(--jp-mirror-editor-error-color) } /* Error */ .highlight .k { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword */ .highlight .o { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator */ .highlight .p { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation */ .highlight .ch { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Multiline */ .highlight .cp { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Preproc */ .highlight .cpf { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Single */ .highlight .cs { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Special */ .highlight .kc { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Pseudo */ .highlight .kr { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Type */ .highlight .m { color: var(--jp-mirror-editor-number-color) } /* Literal.Number */ .highlight .s { color: var(--jp-mirror-editor-string-color) } /* Literal.String */ .highlight .ow { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator.Word */ .highlight .w { color: var(--jp-mirror-editor-variable-color) } /* Text.Whitespace */ .highlight .mb { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Bin */ .highlight .mf { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Float */ .highlight .mh { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Hex */ .highlight .mi { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer */ .highlight .mo { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Oct */ .highlight .sa { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Affix */ .highlight .sb { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Backtick */ .highlight .sc { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Char */ .highlight .dl { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Delimiter */ .highlight .sd { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Doc */ .highlight .s2 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Double */ .highlight .se { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Escape */ .highlight .sh { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Heredoc */ .highlight .si { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Interpol */ .highlight .sx { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Other */ .highlight .sr { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Regex */ .highlight .s1 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Single */ .highlight .ss { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Symbol */ .highlight .il { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer.Long */ /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /* * Mozilla scrollbar styling */ /* use standard opaque scrollbars for most nodes */ [data-jp-theme-scrollbars='true'] { scrollbar-color: rgb(var(--jp-scrollbar-thumb-color)) var(--jp-scrollbar-background-color); } /* for code nodes, use a transparent style of scrollbar. These selectors * will match lower in the tree, and so will override the above */ [data-jp-theme-scrollbars='true'] .CodeMirror-hscrollbar, [data-jp-theme-scrollbars='true'] .CodeMirror-vscrollbar { scrollbar-color: rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent; } /* * Webkit scrollbar styling */ /* use standard opaque scrollbars for most nodes */ [data-jp-theme-scrollbars='true'] ::-webkit-scrollbar, [data-jp-theme-scrollbars='true'] ::-webkit-scrollbar-corner { background: var(--jp-scrollbar-background-color); } [data-jp-theme-scrollbars='true'] ::-webkit-scrollbar-thumb { background: rgb(var(--jp-scrollbar-thumb-color)); border: var(--jp-scrollbar-thumb-margin) solid transparent; background-clip: content-box; border-radius: var(--jp-scrollbar-thumb-radius); } [data-jp-theme-scrollbars='true'] ::-webkit-scrollbar-track:horizontal { border-left: var(--jp-scrollbar-endpad) solid var(--jp-scrollbar-background-color); border-right: var(--jp-scrollbar-endpad) solid var(--jp-scrollbar-background-color); } [data-jp-theme-scrollbars='true'] ::-webkit-scrollbar-track:vertical { border-top: var(--jp-scrollbar-endpad) solid var(--jp-scrollbar-background-color); border-bottom: var(--jp-scrollbar-endpad) solid var(--jp-scrollbar-background-color); } /* for code nodes, use a transparent style of scrollbar */ [data-jp-theme-scrollbars='true'] .CodeMirror-hscrollbar::-webkit-scrollbar, [data-jp-theme-scrollbars='true'] .CodeMirror-vscrollbar::-webkit-scrollbar, [data-jp-theme-scrollbars='true'] .CodeMirror-hscrollbar::-webkit-scrollbar-corner, [data-jp-theme-scrollbars='true'] .CodeMirror-vscrollbar::-webkit-scrollbar-corner { background-color: transparent; } [data-jp-theme-scrollbars='true'] .CodeMirror-hscrollbar::-webkit-scrollbar-thumb, [data-jp-theme-scrollbars='true'] .CodeMirror-vscrollbar::-webkit-scrollbar-thumb { background: rgba(var(--jp-scrollbar-thumb-color), 0.5); border: var(--jp-scrollbar-thumb-margin) solid transparent; background-clip: content-box; border-radius: var(--jp-scrollbar-thumb-radius); } [data-jp-theme-scrollbars='true'] .CodeMirror-hscrollbar::-webkit-scrollbar-track:horizontal { border-left: var(--jp-scrollbar-endpad) solid transparent; border-right: var(--jp-scrollbar-endpad) solid transparent; } [data-jp-theme-scrollbars='true'] .CodeMirror-vscrollbar::-webkit-scrollbar-track:vertical { border-top: var(--jp-scrollbar-endpad) solid transparent; border-bottom: var(--jp-scrollbar-endpad) solid transparent; } /* * Phosphor */ .lm-ScrollBar[data-orientation='horizontal'] { min-height: 16px; max-height: 16px; min-width: 45px; border-top: 1px solid #a0a0a0; } .lm-ScrollBar[data-orientation='vertical'] { min-width: 16px; max-width: 16px; min-height: 45px; border-left: 1px solid #a0a0a0; } .lm-ScrollBar-button { background-color: #f0f0f0; background-position: center center; min-height: 15px; max-height: 15px; min-width: 15px; max-width: 15px; } .lm-ScrollBar-button:hover { background-color: #dadada; } .lm-ScrollBar-button.lm-mod-active { background-color: #cdcdcd; } .lm-ScrollBar-track { background: #f0f0f0; } .lm-ScrollBar-thumb { background: #cdcdcd; } .lm-ScrollBar-thumb:hover { background: #bababa; } .lm-ScrollBar-thumb.lm-mod-active { background: #a0a0a0; } .lm-ScrollBar[data-orientation='horizontal'] .lm-ScrollBar-thumb { height: 100%; min-width: 15px; border-left: 1px solid #a0a0a0; border-right: 1px solid #a0a0a0; } .lm-ScrollBar[data-orientation='vertical'] .lm-ScrollBar-thumb { width: 100%; min-height: 15px; border-top: 1px solid #a0a0a0; border-bottom: 1px solid #a0a0a0; } .lm-ScrollBar[data-orientation='horizontal'] .lm-ScrollBar-button[data-action='decrement'] { background-image: var(--jp-icon-caret-left); background-size: 17px; } .lm-ScrollBar[data-orientation='horizontal'] .lm-ScrollBar-button[data-action='increment'] { background-image: var(--jp-icon-caret-right); background-size: 17px; } .lm-ScrollBar[data-orientation='vertical'] .lm-ScrollBar-button[data-action='decrement'] { background-image: var(--jp-icon-caret-up); background-size: 17px; } .lm-ScrollBar[data-orientation='vertical'] .lm-ScrollBar-button[data-action='increment'] { background-image: var(--jp-icon-caret-down); background-size: 17px; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Copyright (c) 2014-2017, PhosphorJS Contributors | | Distributed under the terms of the BSD 3-Clause License. | | The full license is in the file LICENSE, distributed with this software. |----------------------------------------------------------------------------*/ /* <DEPRECATED> */ .p-Widget, /* </DEPRECATED> */ .lm-Widget { box-sizing: border-box; position: relative; overflow: hidden; cursor: default; } /* <DEPRECATED> */ .p-Widget.p-mod-hidden, /* </DEPRECATED> */ .lm-Widget.lm-mod-hidden { display: none !important; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Copyright (c) 2014-2017, PhosphorJS Contributors | | Distributed under the terms of the BSD 3-Clause License. | | The full license is in the file LICENSE, distributed with this software. |----------------------------------------------------------------------------*/ /* <DEPRECATED> */ .p-CommandPalette, /* </DEPRECATED> */ .lm-CommandPalette { display: flex; flex-direction: column; -webkit-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; } /* <DEPRECATED> */ .p-CommandPalette-search, /* </DEPRECATED> */ .lm-CommandPalette-search { flex: 0 0 auto; } /* <DEPRECATED> */ .p-CommandPalette-content, /* </DEPRECATED> */ .lm-CommandPalette-content { flex: 1 1 auto; margin: 0; padding: 0; min-height: 0; overflow: auto; list-style-type: none; } /* <DEPRECATED> */ .p-CommandPalette-header, /* </DEPRECATED> */ .lm-CommandPalette-header { overflow: hidden; white-space: nowrap; text-overflow: ellipsis; } /* <DEPRECATED> */ .p-CommandPalette-item, /* </DEPRECATED> */ .lm-CommandPalette-item { display: flex; flex-direction: row; } /* <DEPRECATED> */ .p-CommandPalette-itemIcon, /* </DEPRECATED> */ .lm-CommandPalette-itemIcon { flex: 0 0 auto; } /* <DEPRECATED> */ .p-CommandPalette-itemContent, /* </DEPRECATED> */ .lm-CommandPalette-itemContent { flex: 1 1 auto; overflow: hidden; } /* <DEPRECATED> */ .p-CommandPalette-itemShortcut, /* </DEPRECATED> */ .lm-CommandPalette-itemShortcut { flex: 0 0 auto; } /* <DEPRECATED> */ .p-CommandPalette-itemLabel, /* </DEPRECATED> */ .lm-CommandPalette-itemLabel { overflow: hidden; white-space: nowrap; text-overflow: ellipsis; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Copyright (c) 2014-2017, PhosphorJS Contributors | | Distributed under the terms of the BSD 3-Clause License. | | The full license is in the file LICENSE, distributed with this software. |----------------------------------------------------------------------------*/ /* <DEPRECATED> */ .p-DockPanel, /* </DEPRECATED> */ .lm-DockPanel { z-index: 0; } /* <DEPRECATED> */ .p-DockPanel-widget, /* </DEPRECATED> */ .lm-DockPanel-widget { z-index: 0; } /* <DEPRECATED> */ .p-DockPanel-tabBar, /* </DEPRECATED> */ .lm-DockPanel-tabBar { z-index: 1; } /* <DEPRECATED> */ .p-DockPanel-handle, /* </DEPRECATED> */ .lm-DockPanel-handle { z-index: 2; } /* <DEPRECATED> */ .p-DockPanel-handle.p-mod-hidden, /* </DEPRECATED> */ .lm-DockPanel-handle.lm-mod-hidden { display: none !important; } /* <DEPRECATED> */ .p-DockPanel-handle:after, /* </DEPRECATED> */ .lm-DockPanel-handle:after { position: absolute; top: 0; left: 0; width: 100%; height: 100%; content: ''; } /* <DEPRECATED> */ .p-DockPanel-handle[data-orientation='horizontal'], /* </DEPRECATED> */ .lm-DockPanel-handle[data-orientation='horizontal'] { cursor: ew-resize; } /* <DEPRECATED> */ .p-DockPanel-handle[data-orientation='vertical'], /* </DEPRECATED> */ .lm-DockPanel-handle[data-orientation='vertical'] { cursor: ns-resize; } /* <DEPRECATED> */ .p-DockPanel-handle[data-orientation='horizontal']:after, /* </DEPRECATED> */ .lm-DockPanel-handle[data-orientation='horizontal']:after { left: 50%; min-width: 8px; transform: translateX(-50%); } /* <DEPRECATED> */ .p-DockPanel-handle[data-orientation='vertical']:after, /* </DEPRECATED> */ .lm-DockPanel-handle[data-orientation='vertical']:after { top: 50%; min-height: 8px; transform: translateY(-50%); } /* <DEPRECATED> */ .p-DockPanel-overlay, /* </DEPRECATED> */ .lm-DockPanel-overlay { z-index: 3; box-sizing: border-box; pointer-events: none; } /* <DEPRECATED> */ .p-DockPanel-overlay.p-mod-hidden, /* </DEPRECATED> */ .lm-DockPanel-overlay.lm-mod-hidden { display: none !important; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Copyright (c) 2014-2017, PhosphorJS Contributors | | Distributed under the terms of the BSD 3-Clause License. | | The full license is in the file LICENSE, distributed with this software. |----------------------------------------------------------------------------*/ /* <DEPRECATED> */ .p-Menu, /* </DEPRECATED> */ .lm-Menu { z-index: 10000; position: absolute; white-space: nowrap; overflow-x: hidden; overflow-y: auto; outline: none; -webkit-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; } /* <DEPRECATED> */ .p-Menu-content, /* </DEPRECATED> */ .lm-Menu-content { margin: 0; padding: 0; display: table; list-style-type: none; } /* <DEPRECATED> */ .p-Menu-item, /* </DEPRECATED> */ .lm-Menu-item { display: table-row; } /* <DEPRECATED> */ .p-Menu-item.p-mod-hidden, .p-Menu-item.p-mod-collapsed, /* </DEPRECATED> */ .lm-Menu-item.lm-mod-hidden, .lm-Menu-item.lm-mod-collapsed { display: none !important; } /* <DEPRECATED> */ .p-Menu-itemIcon, .p-Menu-itemSubmenuIcon, /* </DEPRECATED> */ .lm-Menu-itemIcon, .lm-Menu-itemSubmenuIcon { display: table-cell; text-align: center; } /* <DEPRECATED> */ .p-Menu-itemLabel, /* </DEPRECATED> */ .lm-Menu-itemLabel { display: table-cell; text-align: left; } /* <DEPRECATED> */ .p-Menu-itemShortcut, /* </DEPRECATED> */ .lm-Menu-itemShortcut { display: table-cell; text-align: right; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Copyright (c) 2014-2017, PhosphorJS Contributors | | Distributed under the terms of the BSD 3-Clause License. | | The full license is in the file LICENSE, distributed with this software. |----------------------------------------------------------------------------*/ /* <DEPRECATED> */ .p-MenuBar, /* </DEPRECATED> */ .lm-MenuBar { outline: none; -webkit-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; } /* <DEPRECATED> */ .p-MenuBar-content, /* </DEPRECATED> */ .lm-MenuBar-content { margin: 0; padding: 0; display: flex; flex-direction: row; list-style-type: none; } /* <DEPRECATED> */ .p--MenuBar-item, /* </DEPRECATED> */ .lm-MenuBar-item { box-sizing: border-box; } /* <DEPRECATED> */ .p-MenuBar-itemIcon, .p-MenuBar-itemLabel, /* </DEPRECATED> */ .lm-MenuBar-itemIcon, .lm-MenuBar-itemLabel { display: inline-block; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Copyright (c) 2014-2017, PhosphorJS Contributors | | Distributed under the terms of the BSD 3-Clause License. | | The full license is in the file LICENSE, distributed with this software. |----------------------------------------------------------------------------*/ /* <DEPRECATED> */ .p-ScrollBar, /* </DEPRECATED> */ .lm-ScrollBar { display: flex; -webkit-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; } /* <DEPRECATED> */ .p-ScrollBar[data-orientation='horizontal'], /* </DEPRECATED> */ .lm-ScrollBar[data-orientation='horizontal'] { flex-direction: row; } /* <DEPRECATED> */ .p-ScrollBar[data-orientation='vertical'], /* </DEPRECATED> */ .lm-ScrollBar[data-orientation='vertical'] { flex-direction: column; } /* <DEPRECATED> */ .p-ScrollBar-button, /* </DEPRECATED> */ .lm-ScrollBar-button { box-sizing: border-box; flex: 0 0 auto; } /* <DEPRECATED> */ .p-ScrollBar-track, /* </DEPRECATED> */ .lm-ScrollBar-track { box-sizing: border-box; position: relative; overflow: hidden; flex: 1 1 auto; } /* <DEPRECATED> */ .p-ScrollBar-thumb, /* </DEPRECATED> */ .lm-ScrollBar-thumb { box-sizing: border-box; position: absolute; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Copyright (c) 2014-2017, PhosphorJS Contributors | | Distributed under the terms of the BSD 3-Clause License. | | The full license is in the file LICENSE, distributed with this software. |----------------------------------------------------------------------------*/ /* <DEPRECATED> */ .p-SplitPanel-child, /* </DEPRECATED> */ .lm-SplitPanel-child { z-index: 0; } /* <DEPRECATED> */ .p-SplitPanel-handle, /* </DEPRECATED> */ .lm-SplitPanel-handle { z-index: 1; } /* <DEPRECATED> */ .p-SplitPanel-handle.p-mod-hidden, /* </DEPRECATED> */ .lm-SplitPanel-handle.lm-mod-hidden { display: none !important; } /* <DEPRECATED> */ .p-SplitPanel-handle:after, /* </DEPRECATED> */ .lm-SplitPanel-handle:after { position: absolute; top: 0; left: 0; width: 100%; height: 100%; content: ''; } /* <DEPRECATED> */ .p-SplitPanel[data-orientation='horizontal'] > .p-SplitPanel-handle, /* </DEPRECATED> */ .lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle { cursor: ew-resize; } /* <DEPRECATED> */ .p-SplitPanel[data-orientation='vertical'] > .p-SplitPanel-handle, /* </DEPRECATED> */ .lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle { cursor: ns-resize; } /* <DEPRECATED> */ .p-SplitPanel[data-orientation='horizontal'] > .p-SplitPanel-handle:after, /* </DEPRECATED> */ .lm-SplitPanel[data-orientation='horizontal'] > .lm-SplitPanel-handle:after { left: 50%; min-width: 8px; transform: translateX(-50%); } /* <DEPRECATED> */ .p-SplitPanel[data-orientation='vertical'] > .p-SplitPanel-handle:after, /* </DEPRECATED> */ .lm-SplitPanel[data-orientation='vertical'] > .lm-SplitPanel-handle:after { top: 50%; min-height: 8px; transform: translateY(-50%); } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Copyright (c) 2014-2017, PhosphorJS Contributors | | Distributed under the terms of the BSD 3-Clause License. | | The full license is in the file LICENSE, distributed with this software. |----------------------------------------------------------------------------*/ /* <DEPRECATED> */ .p-TabBar, /* </DEPRECATED> */ .lm-TabBar { display: flex; -webkit-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; } /* <DEPRECATED> */ .p-TabBar[data-orientation='horizontal'], /* </DEPRECATED> */ .lm-TabBar[data-orientation='horizontal'] { flex-direction: row; } /* <DEPRECATED> */ .p-TabBar[data-orientation='vertical'], /* </DEPRECATED> */ .lm-TabBar[data-orientation='vertical'] { flex-direction: column; } /* <DEPRECATED> */ .p-TabBar-content, /* </DEPRECATED> */ .lm-TabBar-content { margin: 0; padding: 0; display: flex; flex: 1 1 auto; list-style-type: none; } /* <DEPRECATED> */ .p-TabBar[data-orientation='horizontal'] > .p-TabBar-content, /* </DEPRECATED> */ .lm-TabBar[data-orientation='horizontal'] > .lm-TabBar-content { flex-direction: row; } /* <DEPRECATED> */ .p-TabBar[data-orientation='vertical'] > .p-TabBar-content, /* </DEPRECATED> */ .lm-TabBar[data-orientation='vertical'] > .lm-TabBar-content { flex-direction: column; } /* <DEPRECATED> */ .p-TabBar-tab, /* </DEPRECATED> */ .lm-TabBar-tab { display: flex; flex-direction: row; box-sizing: border-box; overflow: hidden; } /* <DEPRECATED> */ .p-TabBar-tabIcon, .p-TabBar-tabCloseIcon, /* </DEPRECATED> */ .lm-TabBar-tabIcon, .lm-TabBar-tabCloseIcon { flex: 0 0 auto; } /* <DEPRECATED> */ .p-TabBar-tabLabel, /* </DEPRECATED> */ .lm-TabBar-tabLabel { flex: 1 1 auto; overflow: hidden; white-space: nowrap; } /* <DEPRECATED> */ .p-TabBar-tab.p-mod-hidden, /* </DEPRECATED> */ .lm-TabBar-tab.lm-mod-hidden { display: none !important; } /* <DEPRECATED> */ .p-TabBar.p-mod-dragging .p-TabBar-tab, /* </DEPRECATED> */ .lm-TabBar.lm-mod-dragging .lm-TabBar-tab { position: relative; } /* <DEPRECATED> */ .p-TabBar.p-mod-dragging[data-orientation='horizontal'] .p-TabBar-tab, /* </DEPRECATED> */ .lm-TabBar.lm-mod-dragging[data-orientation='horizontal'] .lm-TabBar-tab { left: 0; transition: left 150ms ease; } /* <DEPRECATED> */ .p-TabBar.p-mod-dragging[data-orientation='vertical'] .p-TabBar-tab, /* </DEPRECATED> */ .lm-TabBar.lm-mod-dragging[data-orientation='vertical'] .lm-TabBar-tab { top: 0; transition: top 150ms ease; } /* <DEPRECATED> */ .p-TabBar.p-mod-dragging .p-TabBar-tab.p-mod-dragging /* </DEPRECATED> */ .lm-TabBar.lm-mod-dragging .lm-TabBar-tab.lm-mod-dragging { transition: none; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Copyright (c) 2014-2017, PhosphorJS Contributors | | Distributed under the terms of the BSD 3-Clause License. | | The full license is in the file LICENSE, distributed with this software. |----------------------------------------------------------------------------*/ /* <DEPRECATED> */ .p-TabPanel-tabBar, /* </DEPRECATED> */ .lm-TabPanel-tabBar { z-index: 1; } /* <DEPRECATED> */ .p-TabPanel-stackedPanel, /* </DEPRECATED> */ .lm-TabPanel-stackedPanel { z-index: 0; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Copyright (c) 2014-2017, PhosphorJS Contributors | | Distributed under the terms of the BSD 3-Clause License. | | The full license is in the file LICENSE, distributed with this software. |----------------------------------------------------------------------------*/ @charset \"UTF-8\"; /*! Copyright 2015-present Palantir Technologies, Inc. All rights reserved. Licensed under the Apache License, Version 2.0. */ html{ -webkit-box-sizing:border-box; box-sizing:border-box; } *, *::before, *::after{ -webkit-box-sizing:inherit; box-sizing:inherit; } body{ text-transform:none; line-height:1.28581; letter-spacing:0; font-size:14px; font-weight:400; color:#182026; font-family:-apple-system, \"BlinkMacSystemFont\", \"Segoe UI\", \"Roboto\", \"Oxygen\", \"Ubuntu\", \"Cantarell\", \"Open Sans\", \"Helvetica Neue\", \"Icons16\", sans-serif; } p{ margin-top:0; margin-bottom:10px; } small{ font-size:12px; } strong{ font-weight:600; } ::-moz-selection{ background:rgba(125, 188, 255, 0.6); } ::selection{ background:rgba(125, 188, 255, 0.6); } .bp3-heading{ color:#182026; font-weight:600; margin:0 0 10px; padding:0; } .bp3-dark .bp3-heading{ color:#f5f8fa; } h1.bp3-heading, .bp3-running-text h1{ line-height:40px; font-size:36px; } h2.bp3-heading, .bp3-running-text h2{ line-height:32px; font-size:28px; } h3.bp3-heading, .bp3-running-text h3{ line-height:25px; font-size:22px; } h4.bp3-heading, .bp3-running-text h4{ line-height:21px; font-size:18px; } h5.bp3-heading, .bp3-running-text h5{ line-height:19px; font-size:16px; } h6.bp3-heading, .bp3-running-text h6{ line-height:16px; font-size:14px; } .bp3-ui-text{ text-transform:none; line-height:1.28581; letter-spacing:0; font-size:14px; font-weight:400; } .bp3-monospace-text{ text-transform:none; font-family:monospace; } .bp3-text-muted{ color:#5c7080; } .bp3-dark .bp3-text-muted{ color:#a7b6c2; } .bp3-text-disabled{ color:rgba(92, 112, 128, 0.6); } .bp3-dark .bp3-text-disabled{ color:rgba(167, 182, 194, 0.6); } .bp3-text-overflow-ellipsis{ overflow:hidden; text-overflow:ellipsis; white-space:nowrap; word-wrap:normal; } .bp3-running-text{ line-height:1.5; font-size:14px; } .bp3-running-text h1{ color:#182026; font-weight:600; margin-top:40px; margin-bottom:20px; } .bp3-dark .bp3-running-text h1{ color:#f5f8fa; } .bp3-running-text h2{ color:#182026; font-weight:600; margin-top:40px; margin-bottom:20px; } .bp3-dark .bp3-running-text h2{ color:#f5f8fa; } .bp3-running-text h3{ color:#182026; font-weight:600; margin-top:40px; margin-bottom:20px; } .bp3-dark .bp3-running-text h3{ color:#f5f8fa; } .bp3-running-text h4{ color:#182026; font-weight:600; margin-top:40px; margin-bottom:20px; } .bp3-dark .bp3-running-text h4{ color:#f5f8fa; } .bp3-running-text h5{ color:#182026; font-weight:600; margin-top:40px; margin-bottom:20px; } .bp3-dark .bp3-running-text h5{ color:#f5f8fa; } .bp3-running-text h6{ color:#182026; font-weight:600; margin-top:40px; margin-bottom:20px; } .bp3-dark .bp3-running-text h6{ color:#f5f8fa; } .bp3-running-text hr{ margin:20px 0; border:none; border-bottom:1px solid rgba(16, 22, 26, 0.15); } .bp3-dark .bp3-running-text hr{ border-color:rgba(255, 255, 255, 0.15); } .bp3-running-text p{ margin:0 0 10px; padding:0; } .bp3-text-large{ font-size:16px; } .bp3-text-small{ font-size:12px; } a{ text-decoration:none; color:#106ba3; } a:hover{ cursor:pointer; text-decoration:underline; color:#106ba3; } a .bp3-icon, a .bp3-icon-standard, a .bp3-icon-large{ color:inherit; } a code, .bp3-dark a code{ color:inherit; } .bp3-dark a, .bp3-dark a:hover{ color:#48aff0; } .bp3-dark a .bp3-icon, .bp3-dark a .bp3-icon-standard, .bp3-dark a .bp3-icon-large, .bp3-dark a:hover .bp3-icon, .bp3-dark a:hover .bp3-icon-standard, .bp3-dark a:hover .bp3-icon-large{ color:inherit; } .bp3-running-text code, .bp3-code{ text-transform:none; font-family:monospace; border-radius:3px; -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2); background:rgba(255, 255, 255, 0.7); padding:2px 5px; color:#5c7080; font-size:smaller; } .bp3-dark .bp3-running-text code, .bp3-running-text .bp3-dark code, .bp3-dark .bp3-code{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4); background:rgba(16, 22, 26, 0.3); color:#a7b6c2; } .bp3-running-text a > code, a > .bp3-code{ color:#137cbd; } .bp3-dark .bp3-running-text a > code, .bp3-running-text .bp3-dark a > code, .bp3-dark a > .bp3-code{ color:inherit; } .bp3-running-text pre, .bp3-code-block{ text-transform:none; font-family:monospace; display:block; margin:10px 0; border-radius:3px; -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.15); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.15); background:rgba(255, 255, 255, 0.7); padding:13px 15px 12px; line-height:1.4; color:#182026; font-size:13px; word-break:break-all; word-wrap:break-word; } .bp3-dark .bp3-running-text pre, .bp3-running-text .bp3-dark pre, .bp3-dark .bp3-code-block{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4); background:rgba(16, 22, 26, 0.3); color:#f5f8fa; } .bp3-running-text pre > code, .bp3-code-block > code{ -webkit-box-shadow:none; box-shadow:none; background:none; padding:0; color:inherit; font-size:inherit; } .bp3-running-text kbd, .bp3-key{ display:-webkit-inline-box; display:-ms-inline-flexbox; display:inline-flex; -webkit-box-align:center; -ms-flex-align:center; align-items:center; -webkit-box-pack:center; -ms-flex-pack:center; justify-content:center; border-radius:3px; -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.2); background:#ffffff; min-width:24px; height:24px; padding:3px 6px; vertical-align:middle; line-height:24px; color:#5c7080; font-family:inherit; font-size:12px; } .bp3-running-text kbd .bp3-icon, .bp3-key .bp3-icon, .bp3-running-text kbd .bp3-icon-standard, .bp3-key .bp3-icon-standard, .bp3-running-text kbd .bp3-icon-large, .bp3-key .bp3-icon-large{ margin-right:5px; } .bp3-dark .bp3-running-text kbd, .bp3-running-text .bp3-dark kbd, .bp3-dark .bp3-key{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4); background:#394b59; color:#a7b6c2; } .bp3-running-text blockquote, .bp3-blockquote{ margin:0 0 10px; border-left:solid 4px rgba(167, 182, 194, 0.5); padding:0 20px; } .bp3-dark .bp3-running-text blockquote, .bp3-running-text .bp3-dark blockquote, .bp3-dark .bp3-blockquote{ border-color:rgba(115, 134, 148, 0.5); } .bp3-running-text ul, .bp3-running-text ol, .bp3-list{ margin:10px 0; padding-left:30px; } .bp3-running-text ul li:not(:last-child), .bp3-running-text ol li:not(:last-child), .bp3-list li:not(:last-child){ margin-bottom:5px; } .bp3-running-text ul ol, .bp3-running-text ol ol, .bp3-list ol, .bp3-running-text ul ul, .bp3-running-text ol ul, .bp3-list ul{ margin-top:5px; } .bp3-list-unstyled{ margin:0; padding:0; list-style:none; } .bp3-list-unstyled li{ padding:0; } .bp3-rtl{ text-align:right; } .bp3-dark{ color:#f5f8fa; } :focus{ outline:rgba(19, 124, 189, 0.6) auto 2px; outline-offset:2px; -moz-outline-radius:6px; } .bp3-focus-disabled :focus{ outline:none !important; } .bp3-focus-disabled :focus ~ .bp3-control-indicator{ outline:none !important; } .bp3-alert{ max-width:400px; padding:20px; } .bp3-alert-body{ display:-webkit-box; display:-ms-flexbox; display:flex; } .bp3-alert-body .bp3-icon{ margin-top:0; margin-right:20px; font-size:40px; } .bp3-alert-footer{ display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-orient:horizontal; -webkit-box-direction:reverse; -ms-flex-direction:row-reverse; flex-direction:row-reverse; margin-top:10px; } .bp3-alert-footer .bp3-button{ margin-left:10px; } .bp3-breadcrumbs{ display:-webkit-box; display:-ms-flexbox; display:flex; -ms-flex-wrap:wrap; flex-wrap:wrap; -webkit-box-align:center; -ms-flex-align:center; align-items:center; margin:0; cursor:default; height:30px; padding:0; list-style:none; } .bp3-breadcrumbs > li{ display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-align:center; -ms-flex-align:center; align-items:center; } .bp3-breadcrumbs > li::after{ display:block; margin:0 5px; background:url(\"data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16'%3e%3cpath fill-rule='evenodd' clip-rule='evenodd' d='M10.71 7.29l-4-4a1.003 1.003 0 0 0-1.42 1.42L8.59 8 5.3 11.29c-.19.18-.3.43-.3.71a1.003 1.003 0 0 0 1.71.71l4-4c.18-.18.29-.43.29-.71 0-.28-.11-.53-.29-.71z' fill='%235C7080'/%3e%3c/svg%3e\"); width:16px; height:16px; content:\"\"; } .bp3-breadcrumbs > li:last-of-type::after{ display:none; } .bp3-breadcrumb, .bp3-breadcrumb-current, .bp3-breadcrumbs-collapsed{ display:-webkit-inline-box; display:-ms-inline-flexbox; display:inline-flex; -webkit-box-align:center; -ms-flex-align:center; align-items:center; font-size:16px; } .bp3-breadcrumb, .bp3-breadcrumbs-collapsed{ color:#5c7080; } .bp3-breadcrumb:hover{ text-decoration:none; } .bp3-breadcrumb.bp3-disabled{ cursor:not-allowed; color:rgba(92, 112, 128, 0.6); } .bp3-breadcrumb .bp3-icon{ margin-right:5px; } .bp3-breadcrumb-current{ color:inherit; font-weight:600; } .bp3-breadcrumb-current .bp3-input{ vertical-align:baseline; font-size:inherit; font-weight:inherit; } .bp3-breadcrumbs-collapsed{ margin-right:2px; border:none; border-radius:3px; background:#ced9e0; cursor:pointer; padding:1px 5px; vertical-align:text-bottom; } .bp3-breadcrumbs-collapsed::before{ display:block; background:url(\"data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16'%3e%3cg fill='%235C7080'%3e%3ccircle cx='2' cy='8.03' r='2'/%3e%3ccircle cx='14' cy='8.03' r='2'/%3e%3ccircle cx='8' cy='8.03' r='2'/%3e%3c/g%3e%3c/svg%3e\") center no-repeat; width:16px; height:16px; content:\"\"; } .bp3-breadcrumbs-collapsed:hover{ background:#bfccd6; text-decoration:none; color:#182026; } .bp3-dark .bp3-breadcrumb, .bp3-dark .bp3-breadcrumbs-collapsed{ color:#a7b6c2; } .bp3-dark .bp3-breadcrumbs > li::after{ color:#a7b6c2; } .bp3-dark .bp3-breadcrumb.bp3-disabled{ color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-breadcrumb-current{ color:#f5f8fa; } .bp3-dark .bp3-breadcrumbs-collapsed{ background:rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-breadcrumbs-collapsed:hover{ background:rgba(16, 22, 26, 0.6); color:#f5f8fa; } .bp3-button{ display:-webkit-inline-box; display:-ms-inline-flexbox; display:inline-flex; -webkit-box-orient:horizontal; -webkit-box-direction:normal; -ms-flex-direction:row; flex-direction:row; -webkit-box-align:center; -ms-flex-align:center; align-items:center; -webkit-box-pack:center; -ms-flex-pack:center; justify-content:center; border:none; border-radius:3px; cursor:pointer; padding:5px 10px; vertical-align:middle; text-align:left; font-size:14px; min-width:30px; min-height:30px; } .bp3-button > *{ -webkit-box-flex:0; -ms-flex-positive:0; flex-grow:0; -ms-flex-negative:0; flex-shrink:0; } .bp3-button > .bp3-fill{ -webkit-box-flex:1; -ms-flex-positive:1; flex-grow:1; -ms-flex-negative:1; flex-shrink:1; } .bp3-button::before, .bp3-button > *{ margin-right:7px; } .bp3-button:empty::before, .bp3-button > :last-child{ margin-right:0; } .bp3-button:empty{ padding:0 !important; } .bp3-button:disabled, .bp3-button.bp3-disabled{ cursor:not-allowed; } .bp3-button.bp3-fill{ display:-webkit-box; display:-ms-flexbox; display:flex; width:100%; } .bp3-button.bp3-align-right, .bp3-align-right .bp3-button{ text-align:right; } .bp3-button.bp3-align-left, .bp3-align-left .bp3-button{ text-align:left; } .bp3-button:not([class*=\"bp3-intent-\"]){ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); background-color:#f5f8fa; background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0))); background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0)); color:#182026; } .bp3-button:not([class*=\"bp3-intent-\"]):hover{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); background-clip:padding-box; background-color:#ebf1f5; } .bp3-button:not([class*=\"bp3-intent-\"]):active, .bp3-button:not([class*=\"bp3-intent-\"]).bp3-active{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2); background-color:#d8e1e8; background-image:none; } .bp3-button:not([class*=\"bp3-intent-\"]):disabled, .bp3-button:not([class*=\"bp3-intent-\"]).bp3-disabled{ outline:none; -webkit-box-shadow:none; box-shadow:none; background-color:rgba(206, 217, 224, 0.5); background-image:none; cursor:not-allowed; color:rgba(92, 112, 128, 0.6); } .bp3-button:not([class*=\"bp3-intent-\"]):disabled.bp3-active, .bp3-button:not([class*=\"bp3-intent-\"]):disabled.bp3-active:hover, .bp3-button:not([class*=\"bp3-intent-\"]).bp3-disabled.bp3-active, .bp3-button:not([class*=\"bp3-intent-\"]).bp3-disabled.bp3-active:hover{ background:rgba(206, 217, 224, 0.7); } .bp3-button.bp3-intent-primary{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); background-color:#137cbd; background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0))); background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0)); color:#ffffff; } .bp3-button.bp3-intent-primary:hover, .bp3-button.bp3-intent-primary:active, .bp3-button.bp3-intent-primary.bp3-active{ color:#ffffff; } .bp3-button.bp3-intent-primary:hover{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); background-color:#106ba3; } .bp3-button.bp3-intent-primary:active, .bp3-button.bp3-intent-primary.bp3-active{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); background-color:#0e5a8a; background-image:none; } .bp3-button.bp3-intent-primary:disabled, .bp3-button.bp3-intent-primary.bp3-disabled{ border-color:transparent; -webkit-box-shadow:none; box-shadow:none; background-color:rgba(19, 124, 189, 0.5); background-image:none; color:rgba(255, 255, 255, 0.6); } .bp3-button.bp3-intent-success{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); background-color:#0f9960; background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0))); background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0)); color:#ffffff; } .bp3-button.bp3-intent-success:hover, .bp3-button.bp3-intent-success:active, .bp3-button.bp3-intent-success.bp3-active{ color:#ffffff; } .bp3-button.bp3-intent-success:hover{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); background-color:#0d8050; } .bp3-button.bp3-intent-success:active, .bp3-button.bp3-intent-success.bp3-active{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); background-color:#0a6640; background-image:none; } .bp3-button.bp3-intent-success:disabled, .bp3-button.bp3-intent-success.bp3-disabled{ border-color:transparent; -webkit-box-shadow:none; box-shadow:none; background-color:rgba(15, 153, 96, 0.5); background-image:none; color:rgba(255, 255, 255, 0.6); } .bp3-button.bp3-intent-warning{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); background-color:#d9822b; background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0))); background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0)); color:#ffffff; } .bp3-button.bp3-intent-warning:hover, .bp3-button.bp3-intent-warning:active, .bp3-button.bp3-intent-warning.bp3-active{ color:#ffffff; } .bp3-button.bp3-intent-warning:hover{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); background-color:#bf7326; } .bp3-button.bp3-intent-warning:active, .bp3-button.bp3-intent-warning.bp3-active{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); background-color:#a66321; background-image:none; } .bp3-button.bp3-intent-warning:disabled, .bp3-button.bp3-intent-warning.bp3-disabled{ border-color:transparent; -webkit-box-shadow:none; box-shadow:none; background-color:rgba(217, 130, 43, 0.5); background-image:none; color:rgba(255, 255, 255, 0.6); } .bp3-button.bp3-intent-danger{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); background-color:#db3737; background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0))); background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0)); color:#ffffff; } .bp3-button.bp3-intent-danger:hover, .bp3-button.bp3-intent-danger:active, .bp3-button.bp3-intent-danger.bp3-active{ color:#ffffff; } .bp3-button.bp3-intent-danger:hover{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); background-color:#c23030; } .bp3-button.bp3-intent-danger:active, .bp3-button.bp3-intent-danger.bp3-active{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); background-color:#a82a2a; background-image:none; } .bp3-button.bp3-intent-danger:disabled, .bp3-button.bp3-intent-danger.bp3-disabled{ border-color:transparent; -webkit-box-shadow:none; box-shadow:none; background-color:rgba(219, 55, 55, 0.5); background-image:none; color:rgba(255, 255, 255, 0.6); } .bp3-button[class*=\"bp3-intent-\"] .bp3-button-spinner .bp3-spinner-head{ stroke:#ffffff; } .bp3-button.bp3-large, .bp3-large .bp3-button{ min-width:40px; min-height:40px; padding:5px 15px; font-size:16px; } .bp3-button.bp3-large::before, .bp3-button.bp3-large > *, .bp3-large .bp3-button::before, .bp3-large .bp3-button > *{ margin-right:10px; } .bp3-button.bp3-large:empty::before, .bp3-button.bp3-large > :last-child, .bp3-large .bp3-button:empty::before, .bp3-large .bp3-button > :last-child{ margin-right:0; } .bp3-button.bp3-small, .bp3-small .bp3-button{ min-width:24px; min-height:24px; padding:0 7px; } .bp3-button.bp3-loading{ position:relative; } .bp3-button.bp3-loading[class*=\"bp3-icon-\"]::before{ visibility:hidden; } .bp3-button.bp3-loading .bp3-button-spinner{ position:absolute; margin:0; } .bp3-button.bp3-loading > :not(.bp3-button-spinner){ visibility:hidden; } .bp3-button[class*=\"bp3-icon-\"]::before{ line-height:1; font-family:\"Icons16\", sans-serif; font-size:16px; font-weight:400; font-style:normal; -moz-osx-font-smoothing:grayscale; -webkit-font-smoothing:antialiased; color:#5c7080; } .bp3-button .bp3-icon, .bp3-button .bp3-icon-standard, .bp3-button .bp3-icon-large{ color:#5c7080; } .bp3-button .bp3-icon.bp3-align-right, .bp3-button .bp3-icon-standard.bp3-align-right, .bp3-button .bp3-icon-large.bp3-align-right{ margin-left:7px; } .bp3-button .bp3-icon:first-child:last-child, .bp3-button .bp3-spinner + .bp3-icon:last-child{ margin:0 -7px; } .bp3-dark .bp3-button:not([class*=\"bp3-intent-\"]){ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); background-color:#394b59; background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0))); background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0)); color:#f5f8fa; } .bp3-dark .bp3-button:not([class*=\"bp3-intent-\"]):hover, .bp3-dark .bp3-button:not([class*=\"bp3-intent-\"]):active, .bp3-dark .bp3-button:not([class*=\"bp3-intent-\"]).bp3-active{ color:#f5f8fa; } .bp3-dark .bp3-button:not([class*=\"bp3-intent-\"]):hover{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); background-color:#30404d; } .bp3-dark .bp3-button:not([class*=\"bp3-intent-\"]):active, .bp3-dark .bp3-button:not([class*=\"bp3-intent-\"]).bp3-active{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2); background-color:#202b33; background-image:none; } .bp3-dark .bp3-button:not([class*=\"bp3-intent-\"]):disabled, .bp3-dark .bp3-button:not([class*=\"bp3-intent-\"]).bp3-disabled{ -webkit-box-shadow:none; box-shadow:none; background-color:rgba(57, 75, 89, 0.5); background-image:none; color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-button:not([class*=\"bp3-intent-\"]):disabled.bp3-active, .bp3-dark .bp3-button:not([class*=\"bp3-intent-\"]).bp3-disabled.bp3-active{ background:rgba(57, 75, 89, 0.7); } .bp3-dark .bp3-button:not([class*=\"bp3-intent-\"]) .bp3-button-spinner .bp3-spinner-head{ background:rgba(16, 22, 26, 0.5); stroke:#8a9ba8; } .bp3-dark .bp3-button:not([class*=\"bp3-intent-\"])[class*=\"bp3-icon-\"]::before{ color:#a7b6c2; } .bp3-dark .bp3-button:not([class*=\"bp3-intent-\"]) .bp3-icon, .bp3-dark .bp3-button:not([class*=\"bp3-intent-\"]) .bp3-icon-standard, .bp3-dark .bp3-button:not([class*=\"bp3-intent-\"]) .bp3-icon-large{ color:#a7b6c2; } .bp3-dark .bp3-button[class*=\"bp3-intent-\"]{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-button[class*=\"bp3-intent-\"]:hover{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-button[class*=\"bp3-intent-\"]:active, .bp3-dark .bp3-button[class*=\"bp3-intent-\"].bp3-active{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); } .bp3-dark .bp3-button[class*=\"bp3-intent-\"]:disabled, .bp3-dark .bp3-button[class*=\"bp3-intent-\"].bp3-disabled{ -webkit-box-shadow:none; box-shadow:none; background-image:none; color:rgba(255, 255, 255, 0.3); } .bp3-dark .bp3-button[class*=\"bp3-intent-\"] .bp3-button-spinner .bp3-spinner-head{ stroke:#8a9ba8; } .bp3-button:disabled::before, .bp3-button:disabled .bp3-icon, .bp3-button:disabled .bp3-icon-standard, .bp3-button:disabled .bp3-icon-large, .bp3-button.bp3-disabled::before, .bp3-button.bp3-disabled .bp3-icon, .bp3-button.bp3-disabled .bp3-icon-standard, .bp3-button.bp3-disabled .bp3-icon-large, .bp3-button[class*=\"bp3-intent-\"]::before, .bp3-button[class*=\"bp3-intent-\"] .bp3-icon, .bp3-button[class*=\"bp3-intent-\"] .bp3-icon-standard, .bp3-button[class*=\"bp3-intent-\"] .bp3-icon-large{ color:inherit !important; } .bp3-button.bp3-minimal{ -webkit-box-shadow:none; box-shadow:none; background:none; } .bp3-button.bp3-minimal:hover{ -webkit-box-shadow:none; box-shadow:none; background:rgba(167, 182, 194, 0.3); text-decoration:none; color:#182026; } .bp3-button.bp3-minimal:active, .bp3-button.bp3-minimal.bp3-active{ -webkit-box-shadow:none; box-shadow:none; background:rgba(115, 134, 148, 0.3); color:#182026; } .bp3-button.bp3-minimal:disabled, .bp3-button.bp3-minimal:disabled:hover, .bp3-button.bp3-minimal.bp3-disabled, .bp3-button.bp3-minimal.bp3-disabled:hover{ background:none; cursor:not-allowed; color:rgba(92, 112, 128, 0.6); } .bp3-button.bp3-minimal:disabled.bp3-active, .bp3-button.bp3-minimal:disabled:hover.bp3-active, .bp3-button.bp3-minimal.bp3-disabled.bp3-active, .bp3-button.bp3-minimal.bp3-disabled:hover.bp3-active{ background:rgba(115, 134, 148, 0.3); } .bp3-dark .bp3-button.bp3-minimal{ -webkit-box-shadow:none; box-shadow:none; background:none; color:inherit; } .bp3-dark .bp3-button.bp3-minimal:hover, .bp3-dark .bp3-button.bp3-minimal:active, .bp3-dark .bp3-button.bp3-minimal.bp3-active{ -webkit-box-shadow:none; box-shadow:none; background:none; } .bp3-dark .bp3-button.bp3-minimal:hover{ background:rgba(138, 155, 168, 0.15); } .bp3-dark .bp3-button.bp3-minimal:active, .bp3-dark .bp3-button.bp3-minimal.bp3-active{ background:rgba(138, 155, 168, 0.3); color:#f5f8fa; } .bp3-dark .bp3-button.bp3-minimal:disabled, .bp3-dark .bp3-button.bp3-minimal:disabled:hover, .bp3-dark .bp3-button.bp3-minimal.bp3-disabled, .bp3-dark .bp3-button.bp3-minimal.bp3-disabled:hover{ background:none; cursor:not-allowed; color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-button.bp3-minimal:disabled.bp3-active, .bp3-dark .bp3-button.bp3-minimal:disabled:hover.bp3-active, .bp3-dark .bp3-button.bp3-minimal.bp3-disabled.bp3-active, .bp3-dark .bp3-button.bp3-minimal.bp3-disabled:hover.bp3-active{ background:rgba(138, 155, 168, 0.3); } .bp3-button.bp3-minimal.bp3-intent-primary{ color:#106ba3; } .bp3-button.bp3-minimal.bp3-intent-primary:hover, .bp3-button.bp3-minimal.bp3-intent-primary:active, .bp3-button.bp3-minimal.bp3-intent-primary.bp3-active{ -webkit-box-shadow:none; box-shadow:none; background:none; color:#106ba3; } .bp3-button.bp3-minimal.bp3-intent-primary:hover{ background:rgba(19, 124, 189, 0.15); color:#106ba3; } .bp3-button.bp3-minimal.bp3-intent-primary:active, .bp3-button.bp3-minimal.bp3-intent-primary.bp3-active{ background:rgba(19, 124, 189, 0.3); color:#106ba3; } .bp3-button.bp3-minimal.bp3-intent-primary:disabled, .bp3-button.bp3-minimal.bp3-intent-primary.bp3-disabled{ background:none; color:rgba(16, 107, 163, 0.5); } .bp3-button.bp3-minimal.bp3-intent-primary:disabled.bp3-active, .bp3-button.bp3-minimal.bp3-intent-primary.bp3-disabled.bp3-active{ background:rgba(19, 124, 189, 0.3); } .bp3-button.bp3-minimal.bp3-intent-primary .bp3-button-spinner .bp3-spinner-head{ stroke:#106ba3; } .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary{ color:#48aff0; } .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary:hover{ background:rgba(19, 124, 189, 0.2); color:#48aff0; } .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary:active, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary.bp3-active{ background:rgba(19, 124, 189, 0.3); color:#48aff0; } .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary:disabled, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary.bp3-disabled{ background:none; color:rgba(72, 175, 240, 0.5); } .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary:disabled.bp3-active, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary.bp3-disabled.bp3-active{ background:rgba(19, 124, 189, 0.3); } .bp3-button.bp3-minimal.bp3-intent-success{ color:#0d8050; } .bp3-button.bp3-minimal.bp3-intent-success:hover, .bp3-button.bp3-minimal.bp3-intent-success:active, .bp3-button.bp3-minimal.bp3-intent-success.bp3-active{ -webkit-box-shadow:none; box-shadow:none; background:none; color:#0d8050; } .bp3-button.bp3-minimal.bp3-intent-success:hover{ background:rgba(15, 153, 96, 0.15); color:#0d8050; } .bp3-button.bp3-minimal.bp3-intent-success:active, .bp3-button.bp3-minimal.bp3-intent-success.bp3-active{ background:rgba(15, 153, 96, 0.3); color:#0d8050; } .bp3-button.bp3-minimal.bp3-intent-success:disabled, .bp3-button.bp3-minimal.bp3-intent-success.bp3-disabled{ background:none; color:rgba(13, 128, 80, 0.5); } .bp3-button.bp3-minimal.bp3-intent-success:disabled.bp3-active, .bp3-button.bp3-minimal.bp3-intent-success.bp3-disabled.bp3-active{ background:rgba(15, 153, 96, 0.3); } .bp3-button.bp3-minimal.bp3-intent-success .bp3-button-spinner .bp3-spinner-head{ stroke:#0d8050; } .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success{ color:#3dcc91; } .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success:hover{ background:rgba(15, 153, 96, 0.2); color:#3dcc91; } .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success:active, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success.bp3-active{ background:rgba(15, 153, 96, 0.3); color:#3dcc91; } .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success:disabled, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success.bp3-disabled{ background:none; color:rgba(61, 204, 145, 0.5); } .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success:disabled.bp3-active, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success.bp3-disabled.bp3-active{ background:rgba(15, 153, 96, 0.3); } .bp3-button.bp3-minimal.bp3-intent-warning{ color:#bf7326; } .bp3-button.bp3-minimal.bp3-intent-warning:hover, .bp3-button.bp3-minimal.bp3-intent-warning:active, .bp3-button.bp3-minimal.bp3-intent-warning.bp3-active{ -webkit-box-shadow:none; box-shadow:none; background:none; color:#bf7326; } .bp3-button.bp3-minimal.bp3-intent-warning:hover{ background:rgba(217, 130, 43, 0.15); color:#bf7326; } .bp3-button.bp3-minimal.bp3-intent-warning:active, .bp3-button.bp3-minimal.bp3-intent-warning.bp3-active{ background:rgba(217, 130, 43, 0.3); color:#bf7326; } .bp3-button.bp3-minimal.bp3-intent-warning:disabled, .bp3-button.bp3-minimal.bp3-intent-warning.bp3-disabled{ background:none; color:rgba(191, 115, 38, 0.5); } .bp3-button.bp3-minimal.bp3-intent-warning:disabled.bp3-active, .bp3-button.bp3-minimal.bp3-intent-warning.bp3-disabled.bp3-active{ background:rgba(217, 130, 43, 0.3); } .bp3-button.bp3-minimal.bp3-intent-warning .bp3-button-spinner .bp3-spinner-head{ stroke:#bf7326; } .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning{ color:#ffb366; } .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning:hover{ background:rgba(217, 130, 43, 0.2); color:#ffb366; } .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning:active, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning.bp3-active{ background:rgba(217, 130, 43, 0.3); color:#ffb366; } .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning:disabled, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning.bp3-disabled{ background:none; color:rgba(255, 179, 102, 0.5); } .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning:disabled.bp3-active, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning.bp3-disabled.bp3-active{ background:rgba(217, 130, 43, 0.3); } .bp3-button.bp3-minimal.bp3-intent-danger{ color:#c23030; } .bp3-button.bp3-minimal.bp3-intent-danger:hover, .bp3-button.bp3-minimal.bp3-intent-danger:active, .bp3-button.bp3-minimal.bp3-intent-danger.bp3-active{ -webkit-box-shadow:none; box-shadow:none; background:none; color:#c23030; } .bp3-button.bp3-minimal.bp3-intent-danger:hover{ background:rgba(219, 55, 55, 0.15); color:#c23030; } .bp3-button.bp3-minimal.bp3-intent-danger:active, .bp3-button.bp3-minimal.bp3-intent-danger.bp3-active{ background:rgba(219, 55, 55, 0.3); color:#c23030; } .bp3-button.bp3-minimal.bp3-intent-danger:disabled, .bp3-button.bp3-minimal.bp3-intent-danger.bp3-disabled{ background:none; color:rgba(194, 48, 48, 0.5); } .bp3-button.bp3-minimal.bp3-intent-danger:disabled.bp3-active, .bp3-button.bp3-minimal.bp3-intent-danger.bp3-disabled.bp3-active{ background:rgba(219, 55, 55, 0.3); } .bp3-button.bp3-minimal.bp3-intent-danger .bp3-button-spinner .bp3-spinner-head{ stroke:#c23030; } .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger{ color:#ff7373; } .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger:hover{ background:rgba(219, 55, 55, 0.2); color:#ff7373; } .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger:active, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger.bp3-active{ background:rgba(219, 55, 55, 0.3); color:#ff7373; } .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger:disabled, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger.bp3-disabled{ background:none; color:rgba(255, 115, 115, 0.5); } .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger:disabled.bp3-active, .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger.bp3-disabled.bp3-active{ background:rgba(219, 55, 55, 0.3); } a.bp3-button{ text-align:center; text-decoration:none; -webkit-transition:none; transition:none; } a.bp3-button, a.bp3-button:hover, a.bp3-button:active{ color:#182026; } a.bp3-button.bp3-disabled{ color:rgba(92, 112, 128, 0.6); } .bp3-button-text{ -webkit-box-flex:0; -ms-flex:0 1 auto; flex:0 1 auto; } .bp3-button.bp3-align-left .bp3-button-text, .bp3-button.bp3-align-right .bp3-button-text, .bp3-button-group.bp3-align-left .bp3-button-text, .bp3-button-group.bp3-align-right .bp3-button-text{ -webkit-box-flex:1; -ms-flex:1 1 auto; flex:1 1 auto; } .bp3-button-group{ display:-webkit-inline-box; display:-ms-inline-flexbox; display:inline-flex; } .bp3-button-group .bp3-button{ -webkit-box-flex:0; -ms-flex:0 0 auto; flex:0 0 auto; position:relative; z-index:4; } .bp3-button-group .bp3-button:focus{ z-index:5; } .bp3-button-group .bp3-button:hover{ z-index:6; } .bp3-button-group .bp3-button:active, .bp3-button-group .bp3-button.bp3-active{ z-index:7; } .bp3-button-group .bp3-button:disabled, .bp3-button-group .bp3-button.bp3-disabled{ z-index:3; } .bp3-button-group .bp3-button[class*=\"bp3-intent-\"]{ z-index:9; } .bp3-button-group .bp3-button[class*=\"bp3-intent-\"]:focus{ z-index:10; } .bp3-button-group .bp3-button[class*=\"bp3-intent-\"]:hover{ z-index:11; } .bp3-button-group .bp3-button[class*=\"bp3-intent-\"]:active, .bp3-button-group .bp3-button[class*=\"bp3-intent-\"].bp3-active{ z-index:12; } .bp3-button-group .bp3-button[class*=\"bp3-intent-\"]:disabled, .bp3-button-group .bp3-button[class*=\"bp3-intent-\"].bp3-disabled{ z-index:8; } .bp3-button-group:not(.bp3-minimal) > .bp3-popover-wrapper:not(:first-child) .bp3-button, .bp3-button-group:not(.bp3-minimal) > .bp3-button:not(:first-child){ border-top-left-radius:0; border-bottom-left-radius:0; } .bp3-button-group:not(.bp3-minimal) > .bp3-popover-wrapper:not(:last-child) .bp3-button, .bp3-button-group:not(.bp3-minimal) > .bp3-button:not(:last-child){ margin-right:-1px; border-top-right-radius:0; border-bottom-right-radius:0; } .bp3-button-group.bp3-minimal .bp3-button{ -webkit-box-shadow:none; box-shadow:none; background:none; } .bp3-button-group.bp3-minimal .bp3-button:hover{ -webkit-box-shadow:none; box-shadow:none; background:rgba(167, 182, 194, 0.3); text-decoration:none; color:#182026; } .bp3-button-group.bp3-minimal .bp3-button:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-active{ -webkit-box-shadow:none; box-shadow:none; background:rgba(115, 134, 148, 0.3); color:#182026; } .bp3-button-group.bp3-minimal .bp3-button:disabled, .bp3-button-group.bp3-minimal .bp3-button:disabled:hover, .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled, .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled:hover{ background:none; cursor:not-allowed; color:rgba(92, 112, 128, 0.6); } .bp3-button-group.bp3-minimal .bp3-button:disabled.bp3-active, .bp3-button-group.bp3-minimal .bp3-button:disabled:hover.bp3-active, .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled.bp3-active, .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled:hover.bp3-active{ background:rgba(115, 134, 148, 0.3); } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button{ -webkit-box-shadow:none; box-shadow:none; background:none; color:inherit; } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:hover, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-active{ -webkit-box-shadow:none; box-shadow:none; background:none; } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:hover{ background:rgba(138, 155, 168, 0.15); } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-active{ background:rgba(138, 155, 168, 0.3); color:#f5f8fa; } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:disabled, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:disabled:hover, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled:hover{ background:none; cursor:not-allowed; color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:disabled.bp3-active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:disabled:hover.bp3-active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled.bp3-active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled:hover.bp3-active{ background:rgba(138, 155, 168, 0.3); } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary{ color:#106ba3; } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:hover, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-active{ -webkit-box-shadow:none; box-shadow:none; background:none; color:#106ba3; } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:hover{ background:rgba(19, 124, 189, 0.15); color:#106ba3; } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-active{ background:rgba(19, 124, 189, 0.3); color:#106ba3; } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:disabled, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-disabled{ background:none; color:rgba(16, 107, 163, 0.5); } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:disabled.bp3-active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-disabled.bp3-active{ background:rgba(19, 124, 189, 0.3); } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary .bp3-button-spinner .bp3-spinner-head{ stroke:#106ba3; } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary{ color:#48aff0; } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:hover{ background:rgba(19, 124, 189, 0.2); color:#48aff0; } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-active{ background:rgba(19, 124, 189, 0.3); color:#48aff0; } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:disabled, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-disabled{ background:none; color:rgba(72, 175, 240, 0.5); } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:disabled.bp3-active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-disabled.bp3-active{ background:rgba(19, 124, 189, 0.3); } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success{ color:#0d8050; } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:hover, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-active{ -webkit-box-shadow:none; box-shadow:none; background:none; color:#0d8050; } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:hover{ background:rgba(15, 153, 96, 0.15); color:#0d8050; } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-active{ background:rgba(15, 153, 96, 0.3); color:#0d8050; } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:disabled, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-disabled{ background:none; color:rgba(13, 128, 80, 0.5); } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:disabled.bp3-active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-disabled.bp3-active{ background:rgba(15, 153, 96, 0.3); } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success .bp3-button-spinner .bp3-spinner-head{ stroke:#0d8050; } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success{ color:#3dcc91; } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:hover{ background:rgba(15, 153, 96, 0.2); color:#3dcc91; } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-active{ background:rgba(15, 153, 96, 0.3); color:#3dcc91; } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:disabled, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-disabled{ background:none; color:rgba(61, 204, 145, 0.5); } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:disabled.bp3-active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-disabled.bp3-active{ background:rgba(15, 153, 96, 0.3); } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning{ color:#bf7326; } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:hover, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-active{ -webkit-box-shadow:none; box-shadow:none; background:none; color:#bf7326; } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:hover{ background:rgba(217, 130, 43, 0.15); color:#bf7326; } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-active{ background:rgba(217, 130, 43, 0.3); color:#bf7326; } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:disabled, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-disabled{ background:none; color:rgba(191, 115, 38, 0.5); } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:disabled.bp3-active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-disabled.bp3-active{ background:rgba(217, 130, 43, 0.3); } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning .bp3-button-spinner .bp3-spinner-head{ stroke:#bf7326; } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning{ color:#ffb366; } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:hover{ background:rgba(217, 130, 43, 0.2); color:#ffb366; } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-active{ background:rgba(217, 130, 43, 0.3); color:#ffb366; } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:disabled, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-disabled{ background:none; color:rgba(255, 179, 102, 0.5); } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:disabled.bp3-active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-disabled.bp3-active{ background:rgba(217, 130, 43, 0.3); } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger{ color:#c23030; } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:hover, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-active{ -webkit-box-shadow:none; box-shadow:none; background:none; color:#c23030; } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:hover{ background:rgba(219, 55, 55, 0.15); color:#c23030; } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-active{ background:rgba(219, 55, 55, 0.3); color:#c23030; } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:disabled, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-disabled{ background:none; color:rgba(194, 48, 48, 0.5); } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:disabled.bp3-active, .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-disabled.bp3-active{ background:rgba(219, 55, 55, 0.3); } .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger .bp3-button-spinner .bp3-spinner-head{ stroke:#c23030; } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger{ color:#ff7373; } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:hover{ background:rgba(219, 55, 55, 0.2); color:#ff7373; } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-active{ background:rgba(219, 55, 55, 0.3); color:#ff7373; } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:disabled, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-disabled{ background:none; color:rgba(255, 115, 115, 0.5); } .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:disabled.bp3-active, .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-disabled.bp3-active{ background:rgba(219, 55, 55, 0.3); } .bp3-button-group .bp3-popover-wrapper, .bp3-button-group .bp3-popover-target{ display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-flex:1; -ms-flex:1 1 auto; flex:1 1 auto; } .bp3-button-group.bp3-fill{ display:-webkit-box; display:-ms-flexbox; display:flex; width:100%; } .bp3-button-group .bp3-button.bp3-fill, .bp3-button-group.bp3-fill .bp3-button:not(.bp3-fixed){ -webkit-box-flex:1; -ms-flex:1 1 auto; flex:1 1 auto; } .bp3-button-group.bp3-vertical{ -webkit-box-orient:vertical; -webkit-box-direction:normal; -ms-flex-direction:column; flex-direction:column; -webkit-box-align:stretch; -ms-flex-align:stretch; align-items:stretch; vertical-align:top; } .bp3-button-group.bp3-vertical.bp3-fill{ width:unset; height:100%; } .bp3-button-group.bp3-vertical .bp3-button{ margin-right:0 !important; width:100%; } .bp3-button-group.bp3-vertical:not(.bp3-minimal) > .bp3-popover-wrapper:first-child .bp3-button, .bp3-button-group.bp3-vertical:not(.bp3-minimal) > .bp3-button:first-child{ border-radius:3px 3px 0 0; } .bp3-button-group.bp3-vertical:not(.bp3-minimal) > .bp3-popover-wrapper:last-child .bp3-button, .bp3-button-group.bp3-vertical:not(.bp3-minimal) > .bp3-button:last-child{ border-radius:0 0 3px 3px; } .bp3-button-group.bp3-vertical:not(.bp3-minimal) > .bp3-popover-wrapper:not(:last-child) .bp3-button, .bp3-button-group.bp3-vertical:not(.bp3-minimal) > .bp3-button:not(:last-child){ margin-bottom:-1px; } .bp3-button-group.bp3-align-left .bp3-button{ text-align:left; } .bp3-dark .bp3-button-group:not(.bp3-minimal) > .bp3-popover-wrapper:not(:last-child) .bp3-button, .bp3-dark .bp3-button-group:not(.bp3-minimal) > .bp3-button:not(:last-child){ margin-right:1px; } .bp3-dark .bp3-button-group.bp3-vertical > .bp3-popover-wrapper:not(:last-child) .bp3-button, .bp3-dark .bp3-button-group.bp3-vertical > .bp3-button:not(:last-child){ margin-bottom:1px; } .bp3-callout{ line-height:1.5; font-size:14px; position:relative; border-radius:3px; background-color:rgba(138, 155, 168, 0.15); width:100%; padding:10px 12px 9px; } .bp3-callout[class*=\"bp3-icon-\"]{ padding-left:40px; } .bp3-callout[class*=\"bp3-icon-\"]::before{ line-height:1; font-family:\"Icons20\", sans-serif; font-size:20px; font-weight:400; font-style:normal; -moz-osx-font-smoothing:grayscale; -webkit-font-smoothing:antialiased; position:absolute; top:10px; left:10px; color:#5c7080; } .bp3-callout.bp3-callout-icon{ padding-left:40px; } .bp3-callout.bp3-callout-icon > .bp3-icon:first-child{ position:absolute; top:10px; left:10px; color:#5c7080; } .bp3-callout .bp3-heading{ margin-top:0; margin-bottom:5px; line-height:20px; } .bp3-callout .bp3-heading:last-child{ margin-bottom:0; } .bp3-dark .bp3-callout{ background-color:rgba(138, 155, 168, 0.2); } .bp3-dark .bp3-callout[class*=\"bp3-icon-\"]::before{ color:#a7b6c2; } .bp3-callout.bp3-intent-primary{ background-color:rgba(19, 124, 189, 0.15); } .bp3-callout.bp3-intent-primary[class*=\"bp3-icon-\"]::before, .bp3-callout.bp3-intent-primary > .bp3-icon:first-child, .bp3-callout.bp3-intent-primary .bp3-heading{ color:#106ba3; } .bp3-dark .bp3-callout.bp3-intent-primary{ background-color:rgba(19, 124, 189, 0.25); } .bp3-dark .bp3-callout.bp3-intent-primary[class*=\"bp3-icon-\"]::before, .bp3-dark .bp3-callout.bp3-intent-primary > .bp3-icon:first-child, .bp3-dark .bp3-callout.bp3-intent-primary .bp3-heading{ color:#48aff0; } .bp3-callout.bp3-intent-success{ background-color:rgba(15, 153, 96, 0.15); } .bp3-callout.bp3-intent-success[class*=\"bp3-icon-\"]::before, .bp3-callout.bp3-intent-success > .bp3-icon:first-child, .bp3-callout.bp3-intent-success .bp3-heading{ color:#0d8050; } .bp3-dark .bp3-callout.bp3-intent-success{ background-color:rgba(15, 153, 96, 0.25); } .bp3-dark .bp3-callout.bp3-intent-success[class*=\"bp3-icon-\"]::before, .bp3-dark .bp3-callout.bp3-intent-success > .bp3-icon:first-child, .bp3-dark .bp3-callout.bp3-intent-success .bp3-heading{ color:#3dcc91; } .bp3-callout.bp3-intent-warning{ background-color:rgba(217, 130, 43, 0.15); } .bp3-callout.bp3-intent-warning[class*=\"bp3-icon-\"]::before, .bp3-callout.bp3-intent-warning > .bp3-icon:first-child, .bp3-callout.bp3-intent-warning .bp3-heading{ color:#bf7326; } .bp3-dark .bp3-callout.bp3-intent-warning{ background-color:rgba(217, 130, 43, 0.25); } .bp3-dark .bp3-callout.bp3-intent-warning[class*=\"bp3-icon-\"]::before, .bp3-dark .bp3-callout.bp3-intent-warning > .bp3-icon:first-child, .bp3-dark .bp3-callout.bp3-intent-warning .bp3-heading{ color:#ffb366; } .bp3-callout.bp3-intent-danger{ background-color:rgba(219, 55, 55, 0.15); } .bp3-callout.bp3-intent-danger[class*=\"bp3-icon-\"]::before, .bp3-callout.bp3-intent-danger > .bp3-icon:first-child, .bp3-callout.bp3-intent-danger .bp3-heading{ color:#c23030; } .bp3-dark .bp3-callout.bp3-intent-danger{ background-color:rgba(219, 55, 55, 0.25); } .bp3-dark .bp3-callout.bp3-intent-danger[class*=\"bp3-icon-\"]::before, .bp3-dark .bp3-callout.bp3-intent-danger > .bp3-icon:first-child, .bp3-dark .bp3-callout.bp3-intent-danger .bp3-heading{ color:#ff7373; } .bp3-running-text .bp3-callout{ margin:20px 0; } .bp3-card{ border-radius:3px; -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.15), 0 0 0 rgba(16, 22, 26, 0), 0 0 0 rgba(16, 22, 26, 0); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.15), 0 0 0 rgba(16, 22, 26, 0), 0 0 0 rgba(16, 22, 26, 0); background-color:#ffffff; padding:20px; -webkit-transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9); transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9); transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9), box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9); transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9), box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9); } .bp3-card.bp3-dark, .bp3-dark .bp3-card{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), 0 0 0 rgba(16, 22, 26, 0), 0 0 0 rgba(16, 22, 26, 0); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), 0 0 0 rgba(16, 22, 26, 0), 0 0 0 rgba(16, 22, 26, 0); background-color:#30404d; } .bp3-elevation-0{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.15), 0 0 0 rgba(16, 22, 26, 0), 0 0 0 rgba(16, 22, 26, 0); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.15), 0 0 0 rgba(16, 22, 26, 0), 0 0 0 rgba(16, 22, 26, 0); } .bp3-elevation-0.bp3-dark, .bp3-dark .bp3-elevation-0{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), 0 0 0 rgba(16, 22, 26, 0), 0 0 0 rgba(16, 22, 26, 0); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), 0 0 0 rgba(16, 22, 26, 0), 0 0 0 rgba(16, 22, 26, 0); } .bp3-elevation-1{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-elevation-1.bp3-dark, .bp3-dark .bp3-elevation-1{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4); } .bp3-elevation-2{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 1px 1px rgba(16, 22, 26, 0.2), 0 2px 6px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 1px 1px rgba(16, 22, 26, 0.2), 0 2px 6px rgba(16, 22, 26, 0.2); } .bp3-elevation-2.bp3-dark, .bp3-dark .bp3-elevation-2{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 1px 1px rgba(16, 22, 26, 0.4), 0 2px 6px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 1px 1px rgba(16, 22, 26, 0.4), 0 2px 6px rgba(16, 22, 26, 0.4); } .bp3-elevation-3{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2); } .bp3-elevation-3.bp3-dark, .bp3-dark .bp3-elevation-3{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); } .bp3-elevation-4{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 4px 8px rgba(16, 22, 26, 0.2), 0 18px 46px 6px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 4px 8px rgba(16, 22, 26, 0.2), 0 18px 46px 6px rgba(16, 22, 26, 0.2); } .bp3-elevation-4.bp3-dark, .bp3-dark .bp3-elevation-4{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 4px 8px rgba(16, 22, 26, 0.4), 0 18px 46px 6px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 4px 8px rgba(16, 22, 26, 0.4), 0 18px 46px 6px rgba(16, 22, 26, 0.4); } .bp3-card.bp3-interactive:hover{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2); cursor:pointer; } .bp3-card.bp3-interactive:hover.bp3-dark, .bp3-dark .bp3-card.bp3-interactive:hover{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); } .bp3-card.bp3-interactive:active{ opacity:0.9; -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.2); -webkit-transition-duration:0; transition-duration:0; } .bp3-card.bp3-interactive:active.bp3-dark, .bp3-dark .bp3-card.bp3-interactive:active{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4); } .bp3-collapse{ height:0; overflow-y:hidden; -webkit-transition:height 200ms cubic-bezier(0.4, 1, 0.75, 0.9); transition:height 200ms cubic-bezier(0.4, 1, 0.75, 0.9); } .bp3-collapse .bp3-collapse-body{ -webkit-transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9); transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9); transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9); transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9); } .bp3-collapse .bp3-collapse-body[aria-hidden=\"true\"]{ display:none; } .bp3-context-menu .bp3-popover-target{ display:block; } .bp3-context-menu-popover-target{ position:fixed; } .bp3-divider{ margin:5px; border-right:1px solid rgba(16, 22, 26, 0.15); border-bottom:1px solid rgba(16, 22, 26, 0.15); } .bp3-dark .bp3-divider{ border-color:rgba(16, 22, 26, 0.4); } .bp3-dialog-container{ opacity:1; -webkit-transform:scale(1); transform:scale(1); display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-align:center; -ms-flex-align:center; align-items:center; -webkit-box-pack:center; -ms-flex-pack:center; justify-content:center; width:100%; min-height:100%; pointer-events:none; -webkit-user-select:none; -moz-user-select:none; -ms-user-select:none; user-select:none; } .bp3-dialog-container.bp3-overlay-enter > .bp3-dialog, .bp3-dialog-container.bp3-overlay-appear > .bp3-dialog{ opacity:0; -webkit-transform:scale(0.5); transform:scale(0.5); } .bp3-dialog-container.bp3-overlay-enter-active > .bp3-dialog, .bp3-dialog-container.bp3-overlay-appear-active > .bp3-dialog{ opacity:1; -webkit-transform:scale(1); transform:scale(1); -webkit-transition-property:opacity, -webkit-transform; transition-property:opacity, -webkit-transform; transition-property:opacity, transform; transition-property:opacity, transform, -webkit-transform; -webkit-transition-duration:300ms; transition-duration:300ms; -webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11); transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11); -webkit-transition-delay:0; transition-delay:0; } .bp3-dialog-container.bp3-overlay-exit > .bp3-dialog{ opacity:1; -webkit-transform:scale(1); transform:scale(1); } .bp3-dialog-container.bp3-overlay-exit-active > .bp3-dialog{ opacity:0; -webkit-transform:scale(0.5); transform:scale(0.5); -webkit-transition-property:opacity, -webkit-transform; transition-property:opacity, -webkit-transform; transition-property:opacity, transform; transition-property:opacity, transform, -webkit-transform; -webkit-transition-duration:300ms; transition-duration:300ms; -webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11); transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11); -webkit-transition-delay:0; transition-delay:0; } .bp3-dialog{ display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-orient:vertical; -webkit-box-direction:normal; -ms-flex-direction:column; flex-direction:column; margin:30px 0; border-radius:6px; -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 4px 8px rgba(16, 22, 26, 0.2), 0 18px 46px 6px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 4px 8px rgba(16, 22, 26, 0.2), 0 18px 46px 6px rgba(16, 22, 26, 0.2); background:#ebf1f5; width:500px; padding-bottom:20px; pointer-events:all; -webkit-user-select:text; -moz-user-select:text; -ms-user-select:text; user-select:text; } .bp3-dialog:focus{ outline:0; } .bp3-dialog.bp3-dark, .bp3-dark .bp3-dialog{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 4px 8px rgba(16, 22, 26, 0.4), 0 18px 46px 6px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 4px 8px rgba(16, 22, 26, 0.4), 0 18px 46px 6px rgba(16, 22, 26, 0.4); background:#293742; color:#f5f8fa; } .bp3-dialog-header{ display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-flex:0; -ms-flex:0 0 auto; flex:0 0 auto; -webkit-box-align:center; -ms-flex-align:center; align-items:center; border-radius:6px 6px 0 0; -webkit-box-shadow:0 1px 0 rgba(16, 22, 26, 0.15); box-shadow:0 1px 0 rgba(16, 22, 26, 0.15); background:#ffffff; min-height:40px; padding-right:5px; padding-left:20px; } .bp3-dialog-header .bp3-icon-large, .bp3-dialog-header .bp3-icon{ -webkit-box-flex:0; -ms-flex:0 0 auto; flex:0 0 auto; margin-right:10px; color:#5c7080; } .bp3-dialog-header .bp3-heading{ overflow:hidden; text-overflow:ellipsis; white-space:nowrap; word-wrap:normal; -webkit-box-flex:1; -ms-flex:1 1 auto; flex:1 1 auto; margin:0; line-height:inherit; } .bp3-dialog-header .bp3-heading:last-child{ margin-right:20px; } .bp3-dark .bp3-dialog-header{ -webkit-box-shadow:0 1px 0 rgba(16, 22, 26, 0.4); box-shadow:0 1px 0 rgba(16, 22, 26, 0.4); background:#30404d; } .bp3-dark .bp3-dialog-header .bp3-icon-large, .bp3-dark .bp3-dialog-header .bp3-icon{ color:#a7b6c2; } .bp3-dialog-body{ -webkit-box-flex:1; -ms-flex:1 1 auto; flex:1 1 auto; margin:20px; line-height:18px; } .bp3-dialog-footer{ -webkit-box-flex:0; -ms-flex:0 0 auto; flex:0 0 auto; margin:0 20px; } .bp3-dialog-footer-actions{ display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-pack:end; -ms-flex-pack:end; justify-content:flex-end; } .bp3-dialog-footer-actions .bp3-button{ margin-left:10px; } .bp3-drawer{ display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-orient:vertical; -webkit-box-direction:normal; -ms-flex-direction:column; flex-direction:column; margin:0; -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 4px 8px rgba(16, 22, 26, 0.2), 0 18px 46px 6px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 4px 8px rgba(16, 22, 26, 0.2), 0 18px 46px 6px rgba(16, 22, 26, 0.2); background:#ffffff; padding:0; } .bp3-drawer:focus{ outline:0; } .bp3-drawer.bp3-position-top{ top:0; right:0; left:0; height:50%; } .bp3-drawer.bp3-position-top.bp3-overlay-enter, .bp3-drawer.bp3-position-top.bp3-overlay-appear{ -webkit-transform:translateY(-100%); transform:translateY(-100%); } .bp3-drawer.bp3-position-top.bp3-overlay-enter-active, .bp3-drawer.bp3-position-top.bp3-overlay-appear-active{ -webkit-transform:translateY(0); transform:translateY(0); -webkit-transition-property:-webkit-transform; transition-property:-webkit-transform; transition-property:transform; transition-property:transform, -webkit-transform; -webkit-transition-duration:200ms; transition-duration:200ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-transition-delay:0; transition-delay:0; } .bp3-drawer.bp3-position-top.bp3-overlay-exit{ -webkit-transform:translateY(0); transform:translateY(0); } .bp3-drawer.bp3-position-top.bp3-overlay-exit-active{ -webkit-transform:translateY(-100%); transform:translateY(-100%); -webkit-transition-property:-webkit-transform; transition-property:-webkit-transform; transition-property:transform; transition-property:transform, -webkit-transform; -webkit-transition-duration:100ms; transition-duration:100ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-transition-delay:0; transition-delay:0; } .bp3-drawer.bp3-position-bottom{ right:0; bottom:0; left:0; height:50%; } .bp3-drawer.bp3-position-bottom.bp3-overlay-enter, .bp3-drawer.bp3-position-bottom.bp3-overlay-appear{ -webkit-transform:translateY(100%); transform:translateY(100%); } .bp3-drawer.bp3-position-bottom.bp3-overlay-enter-active, .bp3-drawer.bp3-position-bottom.bp3-overlay-appear-active{ -webkit-transform:translateY(0); transform:translateY(0); -webkit-transition-property:-webkit-transform; transition-property:-webkit-transform; transition-property:transform; transition-property:transform, -webkit-transform; -webkit-transition-duration:200ms; transition-duration:200ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-transition-delay:0; transition-delay:0; } .bp3-drawer.bp3-position-bottom.bp3-overlay-exit{ -webkit-transform:translateY(0); transform:translateY(0); } .bp3-drawer.bp3-position-bottom.bp3-overlay-exit-active{ -webkit-transform:translateY(100%); transform:translateY(100%); -webkit-transition-property:-webkit-transform; transition-property:-webkit-transform; transition-property:transform; transition-property:transform, -webkit-transform; -webkit-transition-duration:100ms; transition-duration:100ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-transition-delay:0; transition-delay:0; } .bp3-drawer.bp3-position-left{ top:0; bottom:0; left:0; width:50%; } .bp3-drawer.bp3-position-left.bp3-overlay-enter, .bp3-drawer.bp3-position-left.bp3-overlay-appear{ -webkit-transform:translateX(-100%); transform:translateX(-100%); } .bp3-drawer.bp3-position-left.bp3-overlay-enter-active, .bp3-drawer.bp3-position-left.bp3-overlay-appear-active{ -webkit-transform:translateX(0); transform:translateX(0); -webkit-transition-property:-webkit-transform; transition-property:-webkit-transform; transition-property:transform; transition-property:transform, -webkit-transform; -webkit-transition-duration:200ms; transition-duration:200ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-transition-delay:0; transition-delay:0; } .bp3-drawer.bp3-position-left.bp3-overlay-exit{ -webkit-transform:translateX(0); transform:translateX(0); } .bp3-drawer.bp3-position-left.bp3-overlay-exit-active{ -webkit-transform:translateX(-100%); transform:translateX(-100%); -webkit-transition-property:-webkit-transform; transition-property:-webkit-transform; transition-property:transform; transition-property:transform, -webkit-transform; -webkit-transition-duration:100ms; transition-duration:100ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-transition-delay:0; transition-delay:0; } .bp3-drawer.bp3-position-right{ top:0; right:0; bottom:0; width:50%; } .bp3-drawer.bp3-position-right.bp3-overlay-enter, .bp3-drawer.bp3-position-right.bp3-overlay-appear{ -webkit-transform:translateX(100%); transform:translateX(100%); } .bp3-drawer.bp3-position-right.bp3-overlay-enter-active, .bp3-drawer.bp3-position-right.bp3-overlay-appear-active{ -webkit-transform:translateX(0); transform:translateX(0); -webkit-transition-property:-webkit-transform; transition-property:-webkit-transform; transition-property:transform; transition-property:transform, -webkit-transform; -webkit-transition-duration:200ms; transition-duration:200ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-transition-delay:0; transition-delay:0; } .bp3-drawer.bp3-position-right.bp3-overlay-exit{ -webkit-transform:translateX(0); transform:translateX(0); } .bp3-drawer.bp3-position-right.bp3-overlay-exit-active{ -webkit-transform:translateX(100%); transform:translateX(100%); -webkit-transition-property:-webkit-transform; transition-property:-webkit-transform; transition-property:transform; transition-property:transform, -webkit-transform; -webkit-transition-duration:100ms; transition-duration:100ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-transition-delay:0; transition-delay:0; } .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not( .bp3-position-right):not(.bp3-vertical){ top:0; right:0; bottom:0; width:50%; } .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not( .bp3-position-right):not(.bp3-vertical).bp3-overlay-enter, .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not( .bp3-position-right):not(.bp3-vertical).bp3-overlay-appear{ -webkit-transform:translateX(100%); transform:translateX(100%); } .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not( .bp3-position-right):not(.bp3-vertical).bp3-overlay-enter-active, .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not( .bp3-position-right):not(.bp3-vertical).bp3-overlay-appear-active{ -webkit-transform:translateX(0); transform:translateX(0); -webkit-transition-property:-webkit-transform; transition-property:-webkit-transform; transition-property:transform; transition-property:transform, -webkit-transform; -webkit-transition-duration:200ms; transition-duration:200ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-transition-delay:0; transition-delay:0; } .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not( .bp3-position-right):not(.bp3-vertical).bp3-overlay-exit{ -webkit-transform:translateX(0); transform:translateX(0); } .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not( .bp3-position-right):not(.bp3-vertical).bp3-overlay-exit-active{ -webkit-transform:translateX(100%); transform:translateX(100%); -webkit-transition-property:-webkit-transform; transition-property:-webkit-transform; transition-property:transform; transition-property:transform, -webkit-transform; -webkit-transition-duration:100ms; transition-duration:100ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-transition-delay:0; transition-delay:0; } .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not( .bp3-position-right).bp3-vertical{ right:0; bottom:0; left:0; height:50%; } .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not( .bp3-position-right).bp3-vertical.bp3-overlay-enter, .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not( .bp3-position-right).bp3-vertical.bp3-overlay-appear{ -webkit-transform:translateY(100%); transform:translateY(100%); } .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not( .bp3-position-right).bp3-vertical.bp3-overlay-enter-active, .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not( .bp3-position-right).bp3-vertical.bp3-overlay-appear-active{ -webkit-transform:translateY(0); transform:translateY(0); -webkit-transition-property:-webkit-transform; transition-property:-webkit-transform; transition-property:transform; transition-property:transform, -webkit-transform; -webkit-transition-duration:200ms; transition-duration:200ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-transition-delay:0; transition-delay:0; } .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not( .bp3-position-right).bp3-vertical.bp3-overlay-exit{ -webkit-transform:translateY(0); transform:translateY(0); } .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not( .bp3-position-right).bp3-vertical.bp3-overlay-exit-active{ -webkit-transform:translateY(100%); transform:translateY(100%); -webkit-transition-property:-webkit-transform; transition-property:-webkit-transform; transition-property:transform; transition-property:transform, -webkit-transform; -webkit-transition-duration:100ms; transition-duration:100ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-transition-delay:0; transition-delay:0; } .bp3-drawer.bp3-dark, .bp3-dark .bp3-drawer{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 4px 8px rgba(16, 22, 26, 0.4), 0 18px 46px 6px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 4px 8px rgba(16, 22, 26, 0.4), 0 18px 46px 6px rgba(16, 22, 26, 0.4); background:#30404d; color:#f5f8fa; } .bp3-drawer-header{ display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-flex:0; -ms-flex:0 0 auto; flex:0 0 auto; -webkit-box-align:center; -ms-flex-align:center; align-items:center; position:relative; border-radius:0; -webkit-box-shadow:0 1px 0 rgba(16, 22, 26, 0.15); box-shadow:0 1px 0 rgba(16, 22, 26, 0.15); min-height:40px; padding:5px; padding-left:20px; } .bp3-drawer-header .bp3-icon-large, .bp3-drawer-header .bp3-icon{ -webkit-box-flex:0; -ms-flex:0 0 auto; flex:0 0 auto; margin-right:10px; color:#5c7080; } .bp3-drawer-header .bp3-heading{ overflow:hidden; text-overflow:ellipsis; white-space:nowrap; word-wrap:normal; -webkit-box-flex:1; -ms-flex:1 1 auto; flex:1 1 auto; margin:0; line-height:inherit; } .bp3-drawer-header .bp3-heading:last-child{ margin-right:20px; } .bp3-dark .bp3-drawer-header{ -webkit-box-shadow:0 1px 0 rgba(16, 22, 26, 0.4); box-shadow:0 1px 0 rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-drawer-header .bp3-icon-large, .bp3-dark .bp3-drawer-header .bp3-icon{ color:#a7b6c2; } .bp3-drawer-body{ -webkit-box-flex:1; -ms-flex:1 1 auto; flex:1 1 auto; overflow:auto; line-height:18px; } .bp3-drawer-footer{ -webkit-box-flex:0; -ms-flex:0 0 auto; flex:0 0 auto; position:relative; -webkit-box-shadow:inset 0 1px 0 rgba(16, 22, 26, 0.15); box-shadow:inset 0 1px 0 rgba(16, 22, 26, 0.15); padding:10px 20px; } .bp3-dark .bp3-drawer-footer{ -webkit-box-shadow:inset 0 1px 0 rgba(16, 22, 26, 0.4); box-shadow:inset 0 1px 0 rgba(16, 22, 26, 0.4); } .bp3-editable-text{ display:inline-block; position:relative; cursor:text; max-width:100%; vertical-align:top; white-space:nowrap; } .bp3-editable-text::before{ position:absolute; top:-3px; right:-3px; bottom:-3px; left:-3px; border-radius:3px; content:\"\"; -webkit-transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9); transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9); transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9), box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9); transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9), box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9); } .bp3-editable-text:hover::before{ -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.15); box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.15); } .bp3-editable-text.bp3-editable-text-editing::before{ -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); background-color:#ffffff; } .bp3-editable-text.bp3-disabled::before{ -webkit-box-shadow:none; box-shadow:none; } .bp3-editable-text.bp3-intent-primary .bp3-editable-text-input, .bp3-editable-text.bp3-intent-primary .bp3-editable-text-content{ color:#137cbd; } .bp3-editable-text.bp3-intent-primary:hover::before{ -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(19, 124, 189, 0.4); box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(19, 124, 189, 0.4); } .bp3-editable-text.bp3-intent-primary.bp3-editable-text-editing::before{ -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-editable-text.bp3-intent-success .bp3-editable-text-input, .bp3-editable-text.bp3-intent-success .bp3-editable-text-content{ color:#0f9960; } .bp3-editable-text.bp3-intent-success:hover::before{ -webkit-box-shadow:0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), inset 0 0 0 1px rgba(15, 153, 96, 0.4); box-shadow:0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), inset 0 0 0 1px rgba(15, 153, 96, 0.4); } .bp3-editable-text.bp3-intent-success.bp3-editable-text-editing::before{ -webkit-box-shadow:0 0 0 1px #0f9960, 0 0 0 3px rgba(15, 153, 96, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px #0f9960, 0 0 0 3px rgba(15, 153, 96, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-editable-text.bp3-intent-warning .bp3-editable-text-input, .bp3-editable-text.bp3-intent-warning .bp3-editable-text-content{ color:#d9822b; } .bp3-editable-text.bp3-intent-warning:hover::before{ -webkit-box-shadow:0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), inset 0 0 0 1px rgba(217, 130, 43, 0.4); box-shadow:0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), inset 0 0 0 1px rgba(217, 130, 43, 0.4); } .bp3-editable-text.bp3-intent-warning.bp3-editable-text-editing::before{ -webkit-box-shadow:0 0 0 1px #d9822b, 0 0 0 3px rgba(217, 130, 43, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px #d9822b, 0 0 0 3px rgba(217, 130, 43, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-editable-text.bp3-intent-danger .bp3-editable-text-input, .bp3-editable-text.bp3-intent-danger .bp3-editable-text-content{ color:#db3737; } .bp3-editable-text.bp3-intent-danger:hover::before{ -webkit-box-shadow:0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), inset 0 0 0 1px rgba(219, 55, 55, 0.4); box-shadow:0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), inset 0 0 0 1px rgba(219, 55, 55, 0.4); } .bp3-editable-text.bp3-intent-danger.bp3-editable-text-editing::before{ -webkit-box-shadow:0 0 0 1px #db3737, 0 0 0 3px rgba(219, 55, 55, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px #db3737, 0 0 0 3px rgba(219, 55, 55, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-dark .bp3-editable-text:hover::before{ -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(255, 255, 255, 0.15); box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(255, 255, 255, 0.15); } .bp3-dark .bp3-editable-text.bp3-editable-text-editing::before{ -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); background-color:rgba(16, 22, 26, 0.3); } .bp3-dark .bp3-editable-text.bp3-disabled::before{ -webkit-box-shadow:none; box-shadow:none; } .bp3-dark .bp3-editable-text.bp3-intent-primary .bp3-editable-text-content{ color:#48aff0; } .bp3-dark .bp3-editable-text.bp3-intent-primary:hover::before{ -webkit-box-shadow:0 0 0 0 rgba(72, 175, 240, 0), 0 0 0 0 rgba(72, 175, 240, 0), inset 0 0 0 1px rgba(72, 175, 240, 0.4); box-shadow:0 0 0 0 rgba(72, 175, 240, 0), 0 0 0 0 rgba(72, 175, 240, 0), inset 0 0 0 1px rgba(72, 175, 240, 0.4); } .bp3-dark .bp3-editable-text.bp3-intent-primary.bp3-editable-text-editing::before{ -webkit-box-shadow:0 0 0 1px #48aff0, 0 0 0 3px rgba(72, 175, 240, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px #48aff0, 0 0 0 3px rgba(72, 175, 240, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-editable-text.bp3-intent-success .bp3-editable-text-content{ color:#3dcc91; } .bp3-dark .bp3-editable-text.bp3-intent-success:hover::before{ -webkit-box-shadow:0 0 0 0 rgba(61, 204, 145, 0), 0 0 0 0 rgba(61, 204, 145, 0), inset 0 0 0 1px rgba(61, 204, 145, 0.4); box-shadow:0 0 0 0 rgba(61, 204, 145, 0), 0 0 0 0 rgba(61, 204, 145, 0), inset 0 0 0 1px rgba(61, 204, 145, 0.4); } .bp3-dark .bp3-editable-text.bp3-intent-success.bp3-editable-text-editing::before{ -webkit-box-shadow:0 0 0 1px #3dcc91, 0 0 0 3px rgba(61, 204, 145, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px #3dcc91, 0 0 0 3px rgba(61, 204, 145, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-editable-text.bp3-intent-warning .bp3-editable-text-content{ color:#ffb366; } .bp3-dark .bp3-editable-text.bp3-intent-warning:hover::before{ -webkit-box-shadow:0 0 0 0 rgba(255, 179, 102, 0), 0 0 0 0 rgba(255, 179, 102, 0), inset 0 0 0 1px rgba(255, 179, 102, 0.4); box-shadow:0 0 0 0 rgba(255, 179, 102, 0), 0 0 0 0 rgba(255, 179, 102, 0), inset 0 0 0 1px rgba(255, 179, 102, 0.4); } .bp3-dark .bp3-editable-text.bp3-intent-warning.bp3-editable-text-editing::before{ -webkit-box-shadow:0 0 0 1px #ffb366, 0 0 0 3px rgba(255, 179, 102, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px #ffb366, 0 0 0 3px rgba(255, 179, 102, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-editable-text.bp3-intent-danger .bp3-editable-text-content{ color:#ff7373; } .bp3-dark .bp3-editable-text.bp3-intent-danger:hover::before{ -webkit-box-shadow:0 0 0 0 rgba(255, 115, 115, 0), 0 0 0 0 rgba(255, 115, 115, 0), inset 0 0 0 1px rgba(255, 115, 115, 0.4); box-shadow:0 0 0 0 rgba(255, 115, 115, 0), 0 0 0 0 rgba(255, 115, 115, 0), inset 0 0 0 1px rgba(255, 115, 115, 0.4); } .bp3-dark .bp3-editable-text.bp3-intent-danger.bp3-editable-text-editing::before{ -webkit-box-shadow:0 0 0 1px #ff7373, 0 0 0 3px rgba(255, 115, 115, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px #ff7373, 0 0 0 3px rgba(255, 115, 115, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); } .bp3-editable-text-input, .bp3-editable-text-content{ display:inherit; position:relative; min-width:inherit; max-width:inherit; vertical-align:top; text-transform:inherit; letter-spacing:inherit; color:inherit; font:inherit; resize:none; } .bp3-editable-text-input{ border:none; -webkit-box-shadow:none; box-shadow:none; background:none; width:100%; padding:0; white-space:pre-wrap; } .bp3-editable-text-input::-webkit-input-placeholder{ opacity:1; color:rgba(92, 112, 128, 0.6); } .bp3-editable-text-input::-moz-placeholder{ opacity:1; color:rgba(92, 112, 128, 0.6); } .bp3-editable-text-input:-ms-input-placeholder{ opacity:1; color:rgba(92, 112, 128, 0.6); } .bp3-editable-text-input::-ms-input-placeholder{ opacity:1; color:rgba(92, 112, 128, 0.6); } .bp3-editable-text-input::placeholder{ opacity:1; color:rgba(92, 112, 128, 0.6); } .bp3-editable-text-input:focus{ outline:none; } .bp3-editable-text-input::-ms-clear{ display:none; } .bp3-editable-text-content{ overflow:hidden; padding-right:2px; text-overflow:ellipsis; white-space:pre; } .bp3-editable-text-editing > .bp3-editable-text-content{ position:absolute; left:0; visibility:hidden; } .bp3-editable-text-placeholder > .bp3-editable-text-content{ color:rgba(92, 112, 128, 0.6); } .bp3-dark .bp3-editable-text-placeholder > .bp3-editable-text-content{ color:rgba(167, 182, 194, 0.6); } .bp3-editable-text.bp3-multiline{ display:block; } .bp3-editable-text.bp3-multiline .bp3-editable-text-content{ overflow:auto; white-space:pre-wrap; word-wrap:break-word; } .bp3-control-group{ -webkit-transform:translateZ(0); transform:translateZ(0); display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-orient:horizontal; -webkit-box-direction:normal; -ms-flex-direction:row; flex-direction:row; -webkit-box-align:stretch; -ms-flex-align:stretch; align-items:stretch; } .bp3-control-group > *{ -webkit-box-flex:0; -ms-flex-positive:0; flex-grow:0; -ms-flex-negative:0; flex-shrink:0; } .bp3-control-group > .bp3-fill{ -webkit-box-flex:1; -ms-flex-positive:1; flex-grow:1; -ms-flex-negative:1; flex-shrink:1; } .bp3-control-group .bp3-button, .bp3-control-group .bp3-html-select, .bp3-control-group .bp3-input, .bp3-control-group .bp3-select{ position:relative; } .bp3-control-group .bp3-input{ z-index:2; border-radius:inherit; } .bp3-control-group .bp3-input:focus{ z-index:14; border-radius:3px; } .bp3-control-group .bp3-input[class*=\"bp3-intent\"]{ z-index:13; } .bp3-control-group .bp3-input[class*=\"bp3-intent\"]:focus{ z-index:15; } .bp3-control-group .bp3-input[readonly], .bp3-control-group .bp3-input:disabled, .bp3-control-group .bp3-input.bp3-disabled{ z-index:1; } .bp3-control-group .bp3-input-group[class*=\"bp3-intent\"] .bp3-input{ z-index:13; } .bp3-control-group .bp3-input-group[class*=\"bp3-intent\"] .bp3-input:focus{ z-index:15; } .bp3-control-group .bp3-button, .bp3-control-group .bp3-html-select select, .bp3-control-group .bp3-select select{ -webkit-transform:translateZ(0); transform:translateZ(0); z-index:4; border-radius:inherit; } .bp3-control-group .bp3-button:focus, .bp3-control-group .bp3-html-select select:focus, .bp3-control-group .bp3-select select:focus{ z-index:5; } .bp3-control-group .bp3-button:hover, .bp3-control-group .bp3-html-select select:hover, .bp3-control-group .bp3-select select:hover{ z-index:6; } .bp3-control-group .bp3-button:active, .bp3-control-group .bp3-html-select select:active, .bp3-control-group .bp3-select select:active{ z-index:7; } .bp3-control-group .bp3-button[readonly], .bp3-control-group .bp3-button:disabled, .bp3-control-group .bp3-button.bp3-disabled, .bp3-control-group .bp3-html-select select[readonly], .bp3-control-group .bp3-html-select select:disabled, .bp3-control-group .bp3-html-select select.bp3-disabled, .bp3-control-group .bp3-select select[readonly], .bp3-control-group .bp3-select select:disabled, .bp3-control-group .bp3-select select.bp3-disabled{ z-index:3; } .bp3-control-group .bp3-button[class*=\"bp3-intent\"], .bp3-control-group .bp3-html-select select[class*=\"bp3-intent\"], .bp3-control-group .bp3-select select[class*=\"bp3-intent\"]{ z-index:9; } .bp3-control-group .bp3-button[class*=\"bp3-intent\"]:focus, .bp3-control-group .bp3-html-select select[class*=\"bp3-intent\"]:focus, .bp3-control-group .bp3-select select[class*=\"bp3-intent\"]:focus{ z-index:10; } .bp3-control-group .bp3-button[class*=\"bp3-intent\"]:hover, .bp3-control-group .bp3-html-select select[class*=\"bp3-intent\"]:hover, .bp3-control-group .bp3-select select[class*=\"bp3-intent\"]:hover{ z-index:11; } .bp3-control-group .bp3-button[class*=\"bp3-intent\"]:active, .bp3-control-group .bp3-html-select select[class*=\"bp3-intent\"]:active, .bp3-control-group .bp3-select select[class*=\"bp3-intent\"]:active{ z-index:12; } .bp3-control-group .bp3-button[class*=\"bp3-intent\"][readonly], .bp3-control-group .bp3-button[class*=\"bp3-intent\"]:disabled, .bp3-control-group .bp3-button[class*=\"bp3-intent\"].bp3-disabled, .bp3-control-group .bp3-html-select select[class*=\"bp3-intent\"][readonly], .bp3-control-group .bp3-html-select select[class*=\"bp3-intent\"]:disabled, .bp3-control-group .bp3-html-select select[class*=\"bp3-intent\"].bp3-disabled, .bp3-control-group .bp3-select select[class*=\"bp3-intent\"][readonly], .bp3-control-group .bp3-select select[class*=\"bp3-intent\"]:disabled, .bp3-control-group .bp3-select select[class*=\"bp3-intent\"].bp3-disabled{ z-index:8; } .bp3-control-group .bp3-input-group > .bp3-icon, .bp3-control-group .bp3-input-group > .bp3-button, .bp3-control-group .bp3-input-group > .bp3-input-action{ z-index:16; } .bp3-control-group .bp3-select::after, .bp3-control-group .bp3-html-select::after, .bp3-control-group .bp3-select > .bp3-icon, .bp3-control-group .bp3-html-select > .bp3-icon{ z-index:17; } .bp3-control-group:not(.bp3-vertical) > *{ margin-right:-1px; } .bp3-dark .bp3-control-group:not(.bp3-vertical) > *{ margin-right:0; } .bp3-dark .bp3-control-group:not(.bp3-vertical) > .bp3-button + .bp3-button{ margin-left:1px; } .bp3-control-group .bp3-popover-wrapper, .bp3-control-group .bp3-popover-target{ border-radius:inherit; } .bp3-control-group > :first-child{ border-radius:3px 0 0 3px; } .bp3-control-group > :last-child{ margin-right:0; border-radius:0 3px 3px 0; } .bp3-control-group > :only-child{ margin-right:0; border-radius:3px; } .bp3-control-group .bp3-input-group .bp3-button{ border-radius:3px; } .bp3-control-group > .bp3-fill{ -webkit-box-flex:1; -ms-flex:1 1 auto; flex:1 1 auto; } .bp3-control-group.bp3-fill > *:not(.bp3-fixed){ -webkit-box-flex:1; -ms-flex:1 1 auto; flex:1 1 auto; } .bp3-control-group.bp3-vertical{ -webkit-box-orient:vertical; -webkit-box-direction:normal; -ms-flex-direction:column; flex-direction:column; } .bp3-control-group.bp3-vertical > *{ margin-top:-1px; } .bp3-control-group.bp3-vertical > :first-child{ margin-top:0; border-radius:3px 3px 0 0; } .bp3-control-group.bp3-vertical > :last-child{ border-radius:0 0 3px 3px; } .bp3-control{ display:block; position:relative; margin-bottom:10px; cursor:pointer; text-transform:none; } .bp3-control input:checked ~ .bp3-control-indicator{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); background-color:#137cbd; background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0))); background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0)); color:#ffffff; } .bp3-control:hover input:checked ~ .bp3-control-indicator{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); background-color:#106ba3; } .bp3-control input:not(:disabled):active:checked ~ .bp3-control-indicator{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); background:#0e5a8a; } .bp3-control input:disabled:checked ~ .bp3-control-indicator{ -webkit-box-shadow:none; box-shadow:none; background:rgba(19, 124, 189, 0.5); } .bp3-dark .bp3-control input:checked ~ .bp3-control-indicator{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-control:hover input:checked ~ .bp3-control-indicator{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); background-color:#106ba3; } .bp3-dark .bp3-control input:not(:disabled):active:checked ~ .bp3-control-indicator{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); background-color:#0e5a8a; } .bp3-dark .bp3-control input:disabled:checked ~ .bp3-control-indicator{ -webkit-box-shadow:none; box-shadow:none; background:rgba(14, 90, 138, 0.5); } .bp3-control:not(.bp3-align-right){ padding-left:26px; } .bp3-control:not(.bp3-align-right) .bp3-control-indicator{ margin-left:-26px; } .bp3-control.bp3-align-right{ padding-right:26px; } .bp3-control.bp3-align-right .bp3-control-indicator{ margin-right:-26px; } .bp3-control.bp3-disabled{ cursor:not-allowed; color:rgba(92, 112, 128, 0.6); } .bp3-control.bp3-inline{ display:inline-block; margin-right:20px; } .bp3-control input{ position:absolute; top:0; left:0; opacity:0; z-index:-1; } .bp3-control .bp3-control-indicator{ display:inline-block; position:relative; margin-top:-3px; margin-right:10px; border:none; -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); background-clip:padding-box; background-color:#f5f8fa; background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0))); background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0)); cursor:pointer; width:1em; height:1em; vertical-align:middle; font-size:16px; -webkit-user-select:none; -moz-user-select:none; -ms-user-select:none; user-select:none; } .bp3-control .bp3-control-indicator::before{ display:block; width:1em; height:1em; content:\"\"; } .bp3-control:hover .bp3-control-indicator{ background-color:#ebf1f5; } .bp3-control input:not(:disabled):active ~ .bp3-control-indicator{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2); background:#d8e1e8; } .bp3-control input:disabled ~ .bp3-control-indicator{ -webkit-box-shadow:none; box-shadow:none; background:rgba(206, 217, 224, 0.5); cursor:not-allowed; } .bp3-control input:focus ~ .bp3-control-indicator{ outline:rgba(19, 124, 189, 0.6) auto 2px; outline-offset:2px; -moz-outline-radius:6px; } .bp3-control.bp3-align-right .bp3-control-indicator{ float:right; margin-top:1px; margin-left:10px; } .bp3-control.bp3-large{ font-size:16px; } .bp3-control.bp3-large:not(.bp3-align-right){ padding-left:30px; } .bp3-control.bp3-large:not(.bp3-align-right) .bp3-control-indicator{ margin-left:-30px; } .bp3-control.bp3-large.bp3-align-right{ padding-right:30px; } .bp3-control.bp3-large.bp3-align-right .bp3-control-indicator{ margin-right:-30px; } .bp3-control.bp3-large .bp3-control-indicator{ font-size:20px; } .bp3-control.bp3-large.bp3-align-right .bp3-control-indicator{ margin-top:0; } .bp3-control.bp3-checkbox input:indeterminate ~ .bp3-control-indicator{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); background-color:#137cbd; background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0))); background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0)); color:#ffffff; } .bp3-control.bp3-checkbox:hover input:indeterminate ~ .bp3-control-indicator{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 -1px 0 rgba(16, 22, 26, 0.2); background-color:#106ba3; } .bp3-control.bp3-checkbox input:not(:disabled):active:indeterminate ~ .bp3-control-indicator{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); background:#0e5a8a; } .bp3-control.bp3-checkbox input:disabled:indeterminate ~ .bp3-control-indicator{ -webkit-box-shadow:none; box-shadow:none; background:rgba(19, 124, 189, 0.5); } .bp3-dark .bp3-control.bp3-checkbox input:indeterminate ~ .bp3-control-indicator{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-control.bp3-checkbox:hover input:indeterminate ~ .bp3-control-indicator{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); background-color:#106ba3; } .bp3-dark .bp3-control.bp3-checkbox input:not(:disabled):active:indeterminate ~ .bp3-control-indicator{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4), inset 0 1px 2px rgba(16, 22, 26, 0.2); background-color:#0e5a8a; } .bp3-dark .bp3-control.bp3-checkbox input:disabled:indeterminate ~ .bp3-control-indicator{ -webkit-box-shadow:none; box-shadow:none; background:rgba(14, 90, 138, 0.5); } .bp3-control.bp3-checkbox .bp3-control-indicator{ border-radius:3px; } .bp3-control.bp3-checkbox input:checked ~ .bp3-control-indicator::before{ background-image:url(\"data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16'%3e%3cpath fill-rule='evenodd' clip-rule='evenodd' d='M12 5c-.28 0-.53.11-.71.29L7 9.59l-2.29-2.3a1.003 1.003 0 0 0-1.42 1.42l3 3c.18.18.43.29.71.29s.53-.11.71-.29l5-5A1.003 1.003 0 0 0 12 5z' fill='white'/%3e%3c/svg%3e\"); } .bp3-control.bp3-checkbox input:indeterminate ~ .bp3-control-indicator::before{ background-image:url(\"data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16'%3e%3cpath fill-rule='evenodd' clip-rule='evenodd' d='M11 7H5c-.55 0-1 .45-1 1s.45 1 1 1h6c.55 0 1-.45 1-1s-.45-1-1-1z' fill='white'/%3e%3c/svg%3e\"); } .bp3-control.bp3-radio .bp3-control-indicator{ border-radius:50%; } .bp3-control.bp3-radio input:checked ~ .bp3-control-indicator::before{ background-image:radial-gradient(#ffffff, #ffffff 28%, transparent 32%); } .bp3-control.bp3-radio input:checked:disabled ~ .bp3-control-indicator::before{ opacity:0.5; } .bp3-control.bp3-radio input:focus ~ .bp3-control-indicator{ -moz-outline-radius:16px; } .bp3-control.bp3-switch input ~ .bp3-control-indicator{ background:rgba(167, 182, 194, 0.5); } .bp3-control.bp3-switch:hover input ~ .bp3-control-indicator{ background:rgba(115, 134, 148, 0.5); } .bp3-control.bp3-switch input:not(:disabled):active ~ .bp3-control-indicator{ background:rgba(92, 112, 128, 0.5); } .bp3-control.bp3-switch input:disabled ~ .bp3-control-indicator{ background:rgba(206, 217, 224, 0.5); } .bp3-control.bp3-switch input:disabled ~ .bp3-control-indicator::before{ background:rgba(255, 255, 255, 0.8); } .bp3-control.bp3-switch input:checked ~ .bp3-control-indicator{ background:#137cbd; } .bp3-control.bp3-switch:hover input:checked ~ .bp3-control-indicator{ background:#106ba3; } .bp3-control.bp3-switch input:checked:not(:disabled):active ~ .bp3-control-indicator{ background:#0e5a8a; } .bp3-control.bp3-switch input:checked:disabled ~ .bp3-control-indicator{ background:rgba(19, 124, 189, 0.5); } .bp3-control.bp3-switch input:checked:disabled ~ .bp3-control-indicator::before{ background:rgba(255, 255, 255, 0.8); } .bp3-control.bp3-switch:not(.bp3-align-right){ padding-left:38px; } .bp3-control.bp3-switch:not(.bp3-align-right) .bp3-control-indicator{ margin-left:-38px; } .bp3-control.bp3-switch.bp3-align-right{ padding-right:38px; } .bp3-control.bp3-switch.bp3-align-right .bp3-control-indicator{ margin-right:-38px; } .bp3-control.bp3-switch .bp3-control-indicator{ border:none; border-radius:1.75em; -webkit-box-shadow:none !important; box-shadow:none !important; width:auto; min-width:1.75em; -webkit-transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9); transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9); } .bp3-control.bp3-switch .bp3-control-indicator::before{ position:absolute; left:0; margin:2px; border-radius:50%; -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 1px 1px rgba(16, 22, 26, 0.2); background:#ffffff; width:calc(1em - 4px); height:calc(1em - 4px); -webkit-transition:left 100ms cubic-bezier(0.4, 1, 0.75, 0.9); transition:left 100ms cubic-bezier(0.4, 1, 0.75, 0.9); } .bp3-control.bp3-switch input:checked ~ .bp3-control-indicator::before{ left:calc(100% - 1em); } .bp3-control.bp3-switch.bp3-large:not(.bp3-align-right){ padding-left:45px; } .bp3-control.bp3-switch.bp3-large:not(.bp3-align-right) .bp3-control-indicator{ margin-left:-45px; } .bp3-control.bp3-switch.bp3-large.bp3-align-right{ padding-right:45px; } .bp3-control.bp3-switch.bp3-large.bp3-align-right .bp3-control-indicator{ margin-right:-45px; } .bp3-dark .bp3-control.bp3-switch input ~ .bp3-control-indicator{ background:rgba(16, 22, 26, 0.5); } .bp3-dark .bp3-control.bp3-switch:hover input ~ .bp3-control-indicator{ background:rgba(16, 22, 26, 0.7); } .bp3-dark .bp3-control.bp3-switch input:not(:disabled):active ~ .bp3-control-indicator{ background:rgba(16, 22, 26, 0.9); } .bp3-dark .bp3-control.bp3-switch input:disabled ~ .bp3-control-indicator{ background:rgba(57, 75, 89, 0.5); } .bp3-dark .bp3-control.bp3-switch input:disabled ~ .bp3-control-indicator::before{ background:rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-control.bp3-switch input:checked ~ .bp3-control-indicator{ background:#137cbd; } .bp3-dark .bp3-control.bp3-switch:hover input:checked ~ .bp3-control-indicator{ background:#106ba3; } .bp3-dark .bp3-control.bp3-switch input:checked:not(:disabled):active ~ .bp3-control-indicator{ background:#0e5a8a; } .bp3-dark .bp3-control.bp3-switch input:checked:disabled ~ .bp3-control-indicator{ background:rgba(14, 90, 138, 0.5); } .bp3-dark .bp3-control.bp3-switch input:checked:disabled ~ .bp3-control-indicator::before{ background:rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-control.bp3-switch .bp3-control-indicator::before{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); background:#394b59; } .bp3-dark .bp3-control.bp3-switch input:checked ~ .bp3-control-indicator::before{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4); } .bp3-control.bp3-switch .bp3-switch-inner-text{ text-align:center; font-size:0.7em; } .bp3-control.bp3-switch .bp3-control-indicator-child:first-child{ visibility:hidden; margin-right:1.2em; margin-left:0.5em; line-height:0; } .bp3-control.bp3-switch .bp3-control-indicator-child:last-child{ visibility:visible; margin-right:0.5em; margin-left:1.2em; line-height:1em; } .bp3-control.bp3-switch input:checked ~ .bp3-control-indicator .bp3-control-indicator-child:first-child{ visibility:visible; line-height:1em; } .bp3-control.bp3-switch input:checked ~ .bp3-control-indicator .bp3-control-indicator-child:last-child{ visibility:hidden; line-height:0; } .bp3-dark .bp3-control{ color:#f5f8fa; } .bp3-dark .bp3-control.bp3-disabled{ color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-control .bp3-control-indicator{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); background-color:#394b59; background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0))); background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0)); } .bp3-dark .bp3-control:hover .bp3-control-indicator{ background-color:#30404d; } .bp3-dark .bp3-control input:not(:disabled):active ~ .bp3-control-indicator{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2); background:#202b33; } .bp3-dark .bp3-control input:disabled ~ .bp3-control-indicator{ -webkit-box-shadow:none; box-shadow:none; background:rgba(57, 75, 89, 0.5); cursor:not-allowed; } .bp3-dark .bp3-control.bp3-checkbox input:disabled:checked ~ .bp3-control-indicator, .bp3-dark .bp3-control.bp3-checkbox input:disabled:indeterminate ~ .bp3-control-indicator{ color:rgba(167, 182, 194, 0.6); } .bp3-file-input{ display:inline-block; position:relative; cursor:pointer; height:30px; } .bp3-file-input input{ opacity:0; margin:0; min-width:200px; } .bp3-file-input input:disabled + .bp3-file-upload-input, .bp3-file-input input.bp3-disabled + .bp3-file-upload-input{ -webkit-box-shadow:none; box-shadow:none; background:rgba(206, 217, 224, 0.5); cursor:not-allowed; color:rgba(92, 112, 128, 0.6); resize:none; } .bp3-file-input input:disabled + .bp3-file-upload-input::after, .bp3-file-input input.bp3-disabled + .bp3-file-upload-input::after{ outline:none; -webkit-box-shadow:none; box-shadow:none; background-color:rgba(206, 217, 224, 0.5); background-image:none; cursor:not-allowed; color:rgba(92, 112, 128, 0.6); } .bp3-file-input input:disabled + .bp3-file-upload-input::after.bp3-active, .bp3-file-input input:disabled + .bp3-file-upload-input::after.bp3-active:hover, .bp3-file-input input.bp3-disabled + .bp3-file-upload-input::after.bp3-active, .bp3-file-input input.bp3-disabled + .bp3-file-upload-input::after.bp3-active:hover{ background:rgba(206, 217, 224, 0.7); } .bp3-dark .bp3-file-input input:disabled + .bp3-file-upload-input, .bp3-dark .bp3-file-input input.bp3-disabled + .bp3-file-upload-input{ -webkit-box-shadow:none; box-shadow:none; background:rgba(57, 75, 89, 0.5); color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-file-input input:disabled + .bp3-file-upload-input::after, .bp3-dark .bp3-file-input input.bp3-disabled + .bp3-file-upload-input::after{ -webkit-box-shadow:none; box-shadow:none; background-color:rgba(57, 75, 89, 0.5); background-image:none; color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-file-input input:disabled + .bp3-file-upload-input::after.bp3-active, .bp3-dark .bp3-file-input input.bp3-disabled + .bp3-file-upload-input::after.bp3-active{ background:rgba(57, 75, 89, 0.7); } .bp3-file-input.bp3-file-input-has-selection .bp3-file-upload-input{ color:#182026; } .bp3-dark .bp3-file-input.bp3-file-input-has-selection .bp3-file-upload-input{ color:#f5f8fa; } .bp3-file-input.bp3-fill{ width:100%; } .bp3-file-input.bp3-large, .bp3-large .bp3-file-input{ height:40px; } .bp3-file-input .bp3-file-upload-input-custom-text::after{ content:attr(bp3-button-text); } .bp3-file-upload-input{ outline:none; border:none; border-radius:3px; -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); background:#ffffff; height:30px; padding:0 10px; vertical-align:middle; line-height:30px; color:#182026; font-size:14px; font-weight:400; -webkit-transition:-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9); transition:-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9); transition:box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9); transition:box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-appearance:none; -moz-appearance:none; appearance:none; overflow:hidden; text-overflow:ellipsis; white-space:nowrap; word-wrap:normal; position:absolute; top:0; right:0; left:0; padding-right:80px; color:rgba(92, 112, 128, 0.6); -webkit-user-select:none; -moz-user-select:none; -ms-user-select:none; user-select:none; } .bp3-file-upload-input::-webkit-input-placeholder{ opacity:1; color:rgba(92, 112, 128, 0.6); } .bp3-file-upload-input::-moz-placeholder{ opacity:1; color:rgba(92, 112, 128, 0.6); } .bp3-file-upload-input:-ms-input-placeholder{ opacity:1; color:rgba(92, 112, 128, 0.6); } .bp3-file-upload-input::-ms-input-placeholder{ opacity:1; color:rgba(92, 112, 128, 0.6); } .bp3-file-upload-input::placeholder{ opacity:1; color:rgba(92, 112, 128, 0.6); } .bp3-file-upload-input:focus, .bp3-file-upload-input.bp3-active{ -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-file-upload-input[type=\"search\"], .bp3-file-upload-input.bp3-round{ border-radius:30px; -webkit-box-sizing:border-box; box-sizing:border-box; padding-left:10px; } .bp3-file-upload-input[readonly]{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.15); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.15); } .bp3-file-upload-input:disabled, .bp3-file-upload-input.bp3-disabled{ -webkit-box-shadow:none; box-shadow:none; background:rgba(206, 217, 224, 0.5); cursor:not-allowed; color:rgba(92, 112, 128, 0.6); resize:none; } .bp3-file-upload-input::after{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); background-color:#f5f8fa; background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0))); background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0)); color:#182026; min-width:24px; min-height:24px; overflow:hidden; text-overflow:ellipsis; white-space:nowrap; word-wrap:normal; position:absolute; top:0; right:0; margin:3px; border-radius:3px; width:70px; text-align:center; line-height:24px; content:\"Browse\"; } .bp3-file-upload-input::after:hover{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); background-clip:padding-box; background-color:#ebf1f5; } .bp3-file-upload-input::after:active, .bp3-file-upload-input::after.bp3-active{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2); background-color:#d8e1e8; background-image:none; } .bp3-file-upload-input::after:disabled, .bp3-file-upload-input::after.bp3-disabled{ outline:none; -webkit-box-shadow:none; box-shadow:none; background-color:rgba(206, 217, 224, 0.5); background-image:none; cursor:not-allowed; color:rgba(92, 112, 128, 0.6); } .bp3-file-upload-input::after:disabled.bp3-active, .bp3-file-upload-input::after:disabled.bp3-active:hover, .bp3-file-upload-input::after.bp3-disabled.bp3-active, .bp3-file-upload-input::after.bp3-disabled.bp3-active:hover{ background:rgba(206, 217, 224, 0.7); } .bp3-file-upload-input:hover::after{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); background-clip:padding-box; background-color:#ebf1f5; } .bp3-file-upload-input:active::after{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2); background-color:#d8e1e8; background-image:none; } .bp3-large .bp3-file-upload-input{ height:40px; line-height:40px; font-size:16px; padding-right:95px; } .bp3-large .bp3-file-upload-input[type=\"search\"], .bp3-large .bp3-file-upload-input.bp3-round{ padding:0 15px; } .bp3-large .bp3-file-upload-input::after{ min-width:30px; min-height:30px; margin:5px; width:85px; line-height:30px; } .bp3-dark .bp3-file-upload-input{ -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); background:rgba(16, 22, 26, 0.3); color:#f5f8fa; color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-file-upload-input::-webkit-input-placeholder{ color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-file-upload-input::-moz-placeholder{ color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-file-upload-input:-ms-input-placeholder{ color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-file-upload-input::-ms-input-placeholder{ color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-file-upload-input::placeholder{ color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-file-upload-input:focus{ -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-file-upload-input[readonly]{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-file-upload-input:disabled, .bp3-dark .bp3-file-upload-input.bp3-disabled{ -webkit-box-shadow:none; box-shadow:none; background:rgba(57, 75, 89, 0.5); color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-file-upload-input::after{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); background-color:#394b59; background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0))); background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0)); color:#f5f8fa; } .bp3-dark .bp3-file-upload-input::after:hover, .bp3-dark .bp3-file-upload-input::after:active, .bp3-dark .bp3-file-upload-input::after.bp3-active{ color:#f5f8fa; } .bp3-dark .bp3-file-upload-input::after:hover{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); background-color:#30404d; } .bp3-dark .bp3-file-upload-input::after:active, .bp3-dark .bp3-file-upload-input::after.bp3-active{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2); background-color:#202b33; background-image:none; } .bp3-dark .bp3-file-upload-input::after:disabled, .bp3-dark .bp3-file-upload-input::after.bp3-disabled{ -webkit-box-shadow:none; box-shadow:none; background-color:rgba(57, 75, 89, 0.5); background-image:none; color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-file-upload-input::after:disabled.bp3-active, .bp3-dark .bp3-file-upload-input::after.bp3-disabled.bp3-active{ background:rgba(57, 75, 89, 0.7); } .bp3-dark .bp3-file-upload-input::after .bp3-button-spinner .bp3-spinner-head{ background:rgba(16, 22, 26, 0.5); stroke:#8a9ba8; } .bp3-dark .bp3-file-upload-input:hover::after{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); background-color:#30404d; } .bp3-dark .bp3-file-upload-input:active::after{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2); background-color:#202b33; background-image:none; } .bp3-file-upload-input::after{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); } .bp3-form-group{ display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-orient:vertical; -webkit-box-direction:normal; -ms-flex-direction:column; flex-direction:column; margin:0 0 15px; } .bp3-form-group label.bp3-label{ margin-bottom:5px; } .bp3-form-group .bp3-control{ margin-top:7px; } .bp3-form-group .bp3-form-helper-text{ margin-top:5px; color:#5c7080; font-size:12px; } .bp3-form-group.bp3-intent-primary .bp3-form-helper-text{ color:#106ba3; } .bp3-form-group.bp3-intent-success .bp3-form-helper-text{ color:#0d8050; } .bp3-form-group.bp3-intent-warning .bp3-form-helper-text{ color:#bf7326; } .bp3-form-group.bp3-intent-danger .bp3-form-helper-text{ color:#c23030; } .bp3-form-group.bp3-inline{ -webkit-box-orient:horizontal; -webkit-box-direction:normal; -ms-flex-direction:row; flex-direction:row; -webkit-box-align:start; -ms-flex-align:start; align-items:flex-start; } .bp3-form-group.bp3-inline.bp3-large label.bp3-label{ margin:0 10px 0 0; line-height:40px; } .bp3-form-group.bp3-inline label.bp3-label{ margin:0 10px 0 0; line-height:30px; } .bp3-form-group.bp3-disabled .bp3-label, .bp3-form-group.bp3-disabled .bp3-text-muted, .bp3-form-group.bp3-disabled .bp3-form-helper-text{ color:rgba(92, 112, 128, 0.6) !important; } .bp3-dark .bp3-form-group.bp3-intent-primary .bp3-form-helper-text{ color:#48aff0; } .bp3-dark .bp3-form-group.bp3-intent-success .bp3-form-helper-text{ color:#3dcc91; } .bp3-dark .bp3-form-group.bp3-intent-warning .bp3-form-helper-text{ color:#ffb366; } .bp3-dark .bp3-form-group.bp3-intent-danger .bp3-form-helper-text{ color:#ff7373; } .bp3-dark .bp3-form-group .bp3-form-helper-text{ color:#a7b6c2; } .bp3-dark .bp3-form-group.bp3-disabled .bp3-label, .bp3-dark .bp3-form-group.bp3-disabled .bp3-text-muted, .bp3-dark .bp3-form-group.bp3-disabled .bp3-form-helper-text{ color:rgba(167, 182, 194, 0.6) !important; } .bp3-input-group{ display:block; position:relative; } .bp3-input-group .bp3-input{ position:relative; width:100%; } .bp3-input-group .bp3-input:not(:first-child){ padding-left:30px; } .bp3-input-group .bp3-input:not(:last-child){ padding-right:30px; } .bp3-input-group .bp3-input-action, .bp3-input-group > .bp3-button, .bp3-input-group > .bp3-icon{ position:absolute; top:0; } .bp3-input-group .bp3-input-action:first-child, .bp3-input-group > .bp3-button:first-child, .bp3-input-group > .bp3-icon:first-child{ left:0; } .bp3-input-group .bp3-input-action:last-child, .bp3-input-group > .bp3-button:last-child, .bp3-input-group > .bp3-icon:last-child{ right:0; } .bp3-input-group .bp3-button{ min-width:24px; min-height:24px; margin:3px; padding:0 7px; } .bp3-input-group .bp3-button:empty{ padding:0; } .bp3-input-group > .bp3-icon{ z-index:1; color:#5c7080; } .bp3-input-group > .bp3-icon:empty{ line-height:1; font-family:\"Icons16\", sans-serif; font-size:16px; font-weight:400; font-style:normal; -moz-osx-font-smoothing:grayscale; -webkit-font-smoothing:antialiased; } .bp3-input-group > .bp3-icon, .bp3-input-group .bp3-input-action > .bp3-spinner{ margin:7px; } .bp3-input-group .bp3-tag{ margin:5px; } .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:not(:hover):not(:focus), .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus){ color:#5c7080; } .bp3-dark .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:not(:hover):not(:focus), .bp3-dark .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus){ color:#a7b6c2; } .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon, .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon-standard, .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon-large, .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon, .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon-standard, .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon-large{ color:#5c7080; } .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:disabled, .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:disabled{ color:rgba(92, 112, 128, 0.6) !important; } .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:disabled .bp3-icon, .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:disabled .bp3-icon-standard, .bp3-input-group .bp3-input:not(:focus) + .bp3-button.bp3-minimal:disabled .bp3-icon-large, .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:disabled .bp3-icon, .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:disabled .bp3-icon-standard, .bp3-input-group .bp3-input:not(:focus) + .bp3-input-action .bp3-button.bp3-minimal:disabled .bp3-icon-large{ color:rgba(92, 112, 128, 0.6) !important; } .bp3-input-group.bp3-disabled{ cursor:not-allowed; } .bp3-input-group.bp3-disabled .bp3-icon{ color:rgba(92, 112, 128, 0.6); } .bp3-input-group.bp3-large .bp3-button{ min-width:30px; min-height:30px; margin:5px; } .bp3-input-group.bp3-large > .bp3-icon, .bp3-input-group.bp3-large .bp3-input-action > .bp3-spinner{ margin:12px; } .bp3-input-group.bp3-large .bp3-input{ height:40px; line-height:40px; font-size:16px; } .bp3-input-group.bp3-large .bp3-input[type=\"search\"], .bp3-input-group.bp3-large .bp3-input.bp3-round{ padding:0 15px; } .bp3-input-group.bp3-large .bp3-input:not(:first-child){ padding-left:40px; } .bp3-input-group.bp3-large .bp3-input:not(:last-child){ padding-right:40px; } .bp3-input-group.bp3-small .bp3-button{ min-width:20px; min-height:20px; margin:2px; } .bp3-input-group.bp3-small .bp3-tag{ min-width:20px; min-height:20px; margin:2px; } .bp3-input-group.bp3-small > .bp3-icon, .bp3-input-group.bp3-small .bp3-input-action > .bp3-spinner{ margin:4px; } .bp3-input-group.bp3-small .bp3-input{ height:24px; padding-right:8px; padding-left:8px; line-height:24px; font-size:12px; } .bp3-input-group.bp3-small .bp3-input[type=\"search\"], .bp3-input-group.bp3-small .bp3-input.bp3-round{ padding:0 12px; } .bp3-input-group.bp3-small .bp3-input:not(:first-child){ padding-left:24px; } .bp3-input-group.bp3-small .bp3-input:not(:last-child){ padding-right:24px; } .bp3-input-group.bp3-fill{ -webkit-box-flex:1; -ms-flex:1 1 auto; flex:1 1 auto; width:100%; } .bp3-input-group.bp3-round .bp3-button, .bp3-input-group.bp3-round .bp3-input, .bp3-input-group.bp3-round .bp3-tag{ border-radius:30px; } .bp3-dark .bp3-input-group .bp3-icon{ color:#a7b6c2; } .bp3-dark .bp3-input-group.bp3-disabled .bp3-icon{ color:rgba(167, 182, 194, 0.6); } .bp3-input-group.bp3-intent-primary .bp3-input{ -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px #137cbd, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px #137cbd, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-input-group.bp3-intent-primary .bp3-input:focus{ -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-input-group.bp3-intent-primary .bp3-input[readonly]{ -webkit-box-shadow:inset 0 0 0 1px #137cbd; box-shadow:inset 0 0 0 1px #137cbd; } .bp3-input-group.bp3-intent-primary .bp3-input:disabled, .bp3-input-group.bp3-intent-primary .bp3-input.bp3-disabled{ -webkit-box-shadow:none; box-shadow:none; } .bp3-input-group.bp3-intent-primary > .bp3-icon{ color:#106ba3; } .bp3-dark .bp3-input-group.bp3-intent-primary > .bp3-icon{ color:#48aff0; } .bp3-input-group.bp3-intent-success .bp3-input{ -webkit-box-shadow:0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), inset 0 0 0 1px #0f9960, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), inset 0 0 0 1px #0f9960, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-input-group.bp3-intent-success .bp3-input:focus{ -webkit-box-shadow:0 0 0 1px #0f9960, 0 0 0 3px rgba(15, 153, 96, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px #0f9960, 0 0 0 3px rgba(15, 153, 96, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-input-group.bp3-intent-success .bp3-input[readonly]{ -webkit-box-shadow:inset 0 0 0 1px #0f9960; box-shadow:inset 0 0 0 1px #0f9960; } .bp3-input-group.bp3-intent-success .bp3-input:disabled, .bp3-input-group.bp3-intent-success .bp3-input.bp3-disabled{ -webkit-box-shadow:none; box-shadow:none; } .bp3-input-group.bp3-intent-success > .bp3-icon{ color:#0d8050; } .bp3-dark .bp3-input-group.bp3-intent-success > .bp3-icon{ color:#3dcc91; } .bp3-input-group.bp3-intent-warning .bp3-input{ -webkit-box-shadow:0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), inset 0 0 0 1px #d9822b, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), inset 0 0 0 1px #d9822b, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-input-group.bp3-intent-warning .bp3-input:focus{ -webkit-box-shadow:0 0 0 1px #d9822b, 0 0 0 3px rgba(217, 130, 43, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px #d9822b, 0 0 0 3px rgba(217, 130, 43, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-input-group.bp3-intent-warning .bp3-input[readonly]{ -webkit-box-shadow:inset 0 0 0 1px #d9822b; box-shadow:inset 0 0 0 1px #d9822b; } .bp3-input-group.bp3-intent-warning .bp3-input:disabled, .bp3-input-group.bp3-intent-warning .bp3-input.bp3-disabled{ -webkit-box-shadow:none; box-shadow:none; } .bp3-input-group.bp3-intent-warning > .bp3-icon{ color:#bf7326; } .bp3-dark .bp3-input-group.bp3-intent-warning > .bp3-icon{ color:#ffb366; } .bp3-input-group.bp3-intent-danger .bp3-input{ -webkit-box-shadow:0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), inset 0 0 0 1px #db3737, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), inset 0 0 0 1px #db3737, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-input-group.bp3-intent-danger .bp3-input:focus{ -webkit-box-shadow:0 0 0 1px #db3737, 0 0 0 3px rgba(219, 55, 55, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px #db3737, 0 0 0 3px rgba(219, 55, 55, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-input-group.bp3-intent-danger .bp3-input[readonly]{ -webkit-box-shadow:inset 0 0 0 1px #db3737; box-shadow:inset 0 0 0 1px #db3737; } .bp3-input-group.bp3-intent-danger .bp3-input:disabled, .bp3-input-group.bp3-intent-danger .bp3-input.bp3-disabled{ -webkit-box-shadow:none; box-shadow:none; } .bp3-input-group.bp3-intent-danger > .bp3-icon{ color:#c23030; } .bp3-dark .bp3-input-group.bp3-intent-danger > .bp3-icon{ color:#ff7373; } .bp3-input{ outline:none; border:none; border-radius:3px; -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); background:#ffffff; height:30px; padding:0 10px; vertical-align:middle; line-height:30px; color:#182026; font-size:14px; font-weight:400; -webkit-transition:-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9); transition:-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9); transition:box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9); transition:box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-appearance:none; -moz-appearance:none; appearance:none; } .bp3-input::-webkit-input-placeholder{ opacity:1; color:rgba(92, 112, 128, 0.6); } .bp3-input::-moz-placeholder{ opacity:1; color:rgba(92, 112, 128, 0.6); } .bp3-input:-ms-input-placeholder{ opacity:1; color:rgba(92, 112, 128, 0.6); } .bp3-input::-ms-input-placeholder{ opacity:1; color:rgba(92, 112, 128, 0.6); } .bp3-input::placeholder{ opacity:1; color:rgba(92, 112, 128, 0.6); } .bp3-input:focus, .bp3-input.bp3-active{ -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-input[type=\"search\"], .bp3-input.bp3-round{ border-radius:30px; -webkit-box-sizing:border-box; box-sizing:border-box; padding-left:10px; } .bp3-input[readonly]{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.15); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.15); } .bp3-input:disabled, .bp3-input.bp3-disabled{ -webkit-box-shadow:none; box-shadow:none; background:rgba(206, 217, 224, 0.5); cursor:not-allowed; color:rgba(92, 112, 128, 0.6); resize:none; } .bp3-input.bp3-large{ height:40px; line-height:40px; font-size:16px; } .bp3-input.bp3-large[type=\"search\"], .bp3-input.bp3-large.bp3-round{ padding:0 15px; } .bp3-input.bp3-small{ height:24px; padding-right:8px; padding-left:8px; line-height:24px; font-size:12px; } .bp3-input.bp3-small[type=\"search\"], .bp3-input.bp3-small.bp3-round{ padding:0 12px; } .bp3-input.bp3-fill{ -webkit-box-flex:1; -ms-flex:1 1 auto; flex:1 1 auto; width:100%; } .bp3-dark .bp3-input{ -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); background:rgba(16, 22, 26, 0.3); color:#f5f8fa; } .bp3-dark .bp3-input::-webkit-input-placeholder{ color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-input::-moz-placeholder{ color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-input:-ms-input-placeholder{ color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-input::-ms-input-placeholder{ color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-input::placeholder{ color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-input:focus{ -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-input[readonly]{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-input:disabled, .bp3-dark .bp3-input.bp3-disabled{ -webkit-box-shadow:none; box-shadow:none; background:rgba(57, 75, 89, 0.5); color:rgba(167, 182, 194, 0.6); } .bp3-input.bp3-intent-primary{ -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px #137cbd, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px #137cbd, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-input.bp3-intent-primary:focus{ -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-input.bp3-intent-primary[readonly]{ -webkit-box-shadow:inset 0 0 0 1px #137cbd; box-shadow:inset 0 0 0 1px #137cbd; } .bp3-input.bp3-intent-primary:disabled, .bp3-input.bp3-intent-primary.bp3-disabled{ -webkit-box-shadow:none; box-shadow:none; } .bp3-dark .bp3-input.bp3-intent-primary{ -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px #137cbd, inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px #137cbd, inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-input.bp3-intent-primary:focus{ -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-input.bp3-intent-primary[readonly]{ -webkit-box-shadow:inset 0 0 0 1px #137cbd; box-shadow:inset 0 0 0 1px #137cbd; } .bp3-dark .bp3-input.bp3-intent-primary:disabled, .bp3-dark .bp3-input.bp3-intent-primary.bp3-disabled{ -webkit-box-shadow:none; box-shadow:none; } .bp3-input.bp3-intent-success{ -webkit-box-shadow:0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), inset 0 0 0 1px #0f9960, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), inset 0 0 0 1px #0f9960, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-input.bp3-intent-success:focus{ -webkit-box-shadow:0 0 0 1px #0f9960, 0 0 0 3px rgba(15, 153, 96, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px #0f9960, 0 0 0 3px rgba(15, 153, 96, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-input.bp3-intent-success[readonly]{ -webkit-box-shadow:inset 0 0 0 1px #0f9960; box-shadow:inset 0 0 0 1px #0f9960; } .bp3-input.bp3-intent-success:disabled, .bp3-input.bp3-intent-success.bp3-disabled{ -webkit-box-shadow:none; box-shadow:none; } .bp3-dark .bp3-input.bp3-intent-success{ -webkit-box-shadow:0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), inset 0 0 0 1px #0f9960, inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), 0 0 0 0 rgba(15, 153, 96, 0), inset 0 0 0 1px #0f9960, inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-input.bp3-intent-success:focus{ -webkit-box-shadow:0 0 0 1px #0f9960, 0 0 0 1px #0f9960, 0 0 0 3px rgba(15, 153, 96, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px #0f9960, 0 0 0 1px #0f9960, 0 0 0 3px rgba(15, 153, 96, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-input.bp3-intent-success[readonly]{ -webkit-box-shadow:inset 0 0 0 1px #0f9960; box-shadow:inset 0 0 0 1px #0f9960; } .bp3-dark .bp3-input.bp3-intent-success:disabled, .bp3-dark .bp3-input.bp3-intent-success.bp3-disabled{ -webkit-box-shadow:none; box-shadow:none; } .bp3-input.bp3-intent-warning{ -webkit-box-shadow:0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), inset 0 0 0 1px #d9822b, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), inset 0 0 0 1px #d9822b, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-input.bp3-intent-warning:focus{ -webkit-box-shadow:0 0 0 1px #d9822b, 0 0 0 3px rgba(217, 130, 43, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px #d9822b, 0 0 0 3px rgba(217, 130, 43, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-input.bp3-intent-warning[readonly]{ -webkit-box-shadow:inset 0 0 0 1px #d9822b; box-shadow:inset 0 0 0 1px #d9822b; } .bp3-input.bp3-intent-warning:disabled, .bp3-input.bp3-intent-warning.bp3-disabled{ -webkit-box-shadow:none; box-shadow:none; } .bp3-dark .bp3-input.bp3-intent-warning{ -webkit-box-shadow:0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), inset 0 0 0 1px #d9822b, inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), 0 0 0 0 rgba(217, 130, 43, 0), inset 0 0 0 1px #d9822b, inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-input.bp3-intent-warning:focus{ -webkit-box-shadow:0 0 0 1px #d9822b, 0 0 0 1px #d9822b, 0 0 0 3px rgba(217, 130, 43, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px #d9822b, 0 0 0 1px #d9822b, 0 0 0 3px rgba(217, 130, 43, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-input.bp3-intent-warning[readonly]{ -webkit-box-shadow:inset 0 0 0 1px #d9822b; box-shadow:inset 0 0 0 1px #d9822b; } .bp3-dark .bp3-input.bp3-intent-warning:disabled, .bp3-dark .bp3-input.bp3-intent-warning.bp3-disabled{ -webkit-box-shadow:none; box-shadow:none; } .bp3-input.bp3-intent-danger{ -webkit-box-shadow:0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), inset 0 0 0 1px #db3737, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), inset 0 0 0 1px #db3737, inset 0 0 0 1px rgba(16, 22, 26, 0.15), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-input.bp3-intent-danger:focus{ -webkit-box-shadow:0 0 0 1px #db3737, 0 0 0 3px rgba(219, 55, 55, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px #db3737, 0 0 0 3px rgba(219, 55, 55, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-input.bp3-intent-danger[readonly]{ -webkit-box-shadow:inset 0 0 0 1px #db3737; box-shadow:inset 0 0 0 1px #db3737; } .bp3-input.bp3-intent-danger:disabled, .bp3-input.bp3-intent-danger.bp3-disabled{ -webkit-box-shadow:none; box-shadow:none; } .bp3-dark .bp3-input.bp3-intent-danger{ -webkit-box-shadow:0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), inset 0 0 0 1px #db3737, inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), 0 0 0 0 rgba(219, 55, 55, 0), inset 0 0 0 1px #db3737, inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-input.bp3-intent-danger:focus{ -webkit-box-shadow:0 0 0 1px #db3737, 0 0 0 1px #db3737, 0 0 0 3px rgba(219, 55, 55, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px #db3737, 0 0 0 1px #db3737, 0 0 0 3px rgba(219, 55, 55, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-input.bp3-intent-danger[readonly]{ -webkit-box-shadow:inset 0 0 0 1px #db3737; box-shadow:inset 0 0 0 1px #db3737; } .bp3-dark .bp3-input.bp3-intent-danger:disabled, .bp3-dark .bp3-input.bp3-intent-danger.bp3-disabled{ -webkit-box-shadow:none; box-shadow:none; } .bp3-input::-ms-clear{ display:none; } textarea.bp3-input{ max-width:100%; padding:10px; } textarea.bp3-input, textarea.bp3-input.bp3-large, textarea.bp3-input.bp3-small{ height:auto; line-height:inherit; } textarea.bp3-input.bp3-small{ padding:8px; } .bp3-dark textarea.bp3-input{ -webkit-box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), 0 0 0 0 rgba(19, 124, 189, 0), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); background:rgba(16, 22, 26, 0.3); color:#f5f8fa; } .bp3-dark textarea.bp3-input::-webkit-input-placeholder{ color:rgba(167, 182, 194, 0.6); } .bp3-dark textarea.bp3-input::-moz-placeholder{ color:rgba(167, 182, 194, 0.6); } .bp3-dark textarea.bp3-input:-ms-input-placeholder{ color:rgba(167, 182, 194, 0.6); } .bp3-dark textarea.bp3-input::-ms-input-placeholder{ color:rgba(167, 182, 194, 0.6); } .bp3-dark textarea.bp3-input::placeholder{ color:rgba(167, 182, 194, 0.6); } .bp3-dark textarea.bp3-input:focus{ -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); } .bp3-dark textarea.bp3-input[readonly]{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.4); } .bp3-dark textarea.bp3-input:disabled, .bp3-dark textarea.bp3-input.bp3-disabled{ -webkit-box-shadow:none; box-shadow:none; background:rgba(57, 75, 89, 0.5); color:rgba(167, 182, 194, 0.6); } label.bp3-label{ display:block; margin-top:0; margin-bottom:15px; } label.bp3-label .bp3-html-select, label.bp3-label .bp3-input, label.bp3-label .bp3-select, label.bp3-label .bp3-slider, label.bp3-label .bp3-popover-wrapper{ display:block; margin-top:5px; text-transform:none; } label.bp3-label .bp3-button-group{ margin-top:5px; } label.bp3-label .bp3-select select, label.bp3-label .bp3-html-select select{ width:100%; vertical-align:top; font-weight:400; } label.bp3-label.bp3-disabled, label.bp3-label.bp3-disabled .bp3-text-muted{ color:rgba(92, 112, 128, 0.6); } label.bp3-label.bp3-inline{ line-height:30px; } label.bp3-label.bp3-inline .bp3-html-select, label.bp3-label.bp3-inline .bp3-input, label.bp3-label.bp3-inline .bp3-input-group, label.bp3-label.bp3-inline .bp3-select, label.bp3-label.bp3-inline .bp3-popover-wrapper{ display:inline-block; margin:0 0 0 5px; vertical-align:top; } label.bp3-label.bp3-inline .bp3-button-group{ margin:0 0 0 5px; } label.bp3-label.bp3-inline .bp3-input-group .bp3-input{ margin-left:0; } label.bp3-label.bp3-inline.bp3-large{ line-height:40px; } label.bp3-label:not(.bp3-inline) .bp3-popover-target{ display:block; } .bp3-dark label.bp3-label{ color:#f5f8fa; } .bp3-dark label.bp3-label.bp3-disabled, .bp3-dark label.bp3-label.bp3-disabled .bp3-text-muted{ color:rgba(167, 182, 194, 0.6); } .bp3-numeric-input .bp3-button-group.bp3-vertical > .bp3-button{ -webkit-box-flex:1; -ms-flex:1 1 14px; flex:1 1 14px; width:30px; min-height:0; padding:0; } .bp3-numeric-input .bp3-button-group.bp3-vertical > .bp3-button:first-child{ border-radius:0 3px 0 0; } .bp3-numeric-input .bp3-button-group.bp3-vertical > .bp3-button:last-child{ border-radius:0 0 3px 0; } .bp3-numeric-input .bp3-button-group.bp3-vertical:first-child > .bp3-button:first-child{ border-radius:3px 0 0 0; } .bp3-numeric-input .bp3-button-group.bp3-vertical:first-child > .bp3-button:last-child{ border-radius:0 0 0 3px; } .bp3-numeric-input.bp3-large .bp3-button-group.bp3-vertical > .bp3-button{ width:40px; } form{ display:block; } .bp3-html-select select, .bp3-select select{ display:-webkit-inline-box; display:-ms-inline-flexbox; display:inline-flex; -webkit-box-orient:horizontal; -webkit-box-direction:normal; -ms-flex-direction:row; flex-direction:row; -webkit-box-align:center; -ms-flex-align:center; align-items:center; -webkit-box-pack:center; -ms-flex-pack:center; justify-content:center; border:none; border-radius:3px; cursor:pointer; padding:5px 10px; vertical-align:middle; text-align:left; font-size:14px; -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); background-color:#f5f8fa; background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0))); background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0)); color:#182026; border-radius:3px; width:100%; height:30px; padding:0 25px 0 10px; -moz-appearance:none; -webkit-appearance:none; } .bp3-html-select select > *, .bp3-select select > *{ -webkit-box-flex:0; -ms-flex-positive:0; flex-grow:0; -ms-flex-negative:0; flex-shrink:0; } .bp3-html-select select > .bp3-fill, .bp3-select select > .bp3-fill{ -webkit-box-flex:1; -ms-flex-positive:1; flex-grow:1; -ms-flex-negative:1; flex-shrink:1; } .bp3-html-select select::before, .bp3-select select::before, .bp3-html-select select > *, .bp3-select select > *{ margin-right:7px; } .bp3-html-select select:empty::before, .bp3-select select:empty::before, .bp3-html-select select > :last-child, .bp3-select select > :last-child{ margin-right:0; } .bp3-html-select select:hover, .bp3-select select:hover{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); background-clip:padding-box; background-color:#ebf1f5; } .bp3-html-select select:active, .bp3-select select:active, .bp3-html-select select.bp3-active, .bp3-select select.bp3-active{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2); background-color:#d8e1e8; background-image:none; } .bp3-html-select select:disabled, .bp3-select select:disabled, .bp3-html-select select.bp3-disabled, .bp3-select select.bp3-disabled{ outline:none; -webkit-box-shadow:none; box-shadow:none; background-color:rgba(206, 217, 224, 0.5); background-image:none; cursor:not-allowed; color:rgba(92, 112, 128, 0.6); } .bp3-html-select select:disabled.bp3-active, .bp3-select select:disabled.bp3-active, .bp3-html-select select:disabled.bp3-active:hover, .bp3-select select:disabled.bp3-active:hover, .bp3-html-select select.bp3-disabled.bp3-active, .bp3-select select.bp3-disabled.bp3-active, .bp3-html-select select.bp3-disabled.bp3-active:hover, .bp3-select select.bp3-disabled.bp3-active:hover{ background:rgba(206, 217, 224, 0.7); } .bp3-html-select.bp3-minimal select, .bp3-select.bp3-minimal select{ -webkit-box-shadow:none; box-shadow:none; background:none; } .bp3-html-select.bp3-minimal select:hover, .bp3-select.bp3-minimal select:hover{ -webkit-box-shadow:none; box-shadow:none; background:rgba(167, 182, 194, 0.3); text-decoration:none; color:#182026; } .bp3-html-select.bp3-minimal select:active, .bp3-select.bp3-minimal select:active, .bp3-html-select.bp3-minimal select.bp3-active, .bp3-select.bp3-minimal select.bp3-active{ -webkit-box-shadow:none; box-shadow:none; background:rgba(115, 134, 148, 0.3); color:#182026; } .bp3-html-select.bp3-minimal select:disabled, .bp3-select.bp3-minimal select:disabled, .bp3-html-select.bp3-minimal select:disabled:hover, .bp3-select.bp3-minimal select:disabled:hover, .bp3-html-select.bp3-minimal select.bp3-disabled, .bp3-select.bp3-minimal select.bp3-disabled, .bp3-html-select.bp3-minimal select.bp3-disabled:hover, .bp3-select.bp3-minimal select.bp3-disabled:hover{ background:none; cursor:not-allowed; color:rgba(92, 112, 128, 0.6); } .bp3-html-select.bp3-minimal select:disabled.bp3-active, .bp3-select.bp3-minimal select:disabled.bp3-active, .bp3-html-select.bp3-minimal select:disabled:hover.bp3-active, .bp3-select.bp3-minimal select:disabled:hover.bp3-active, .bp3-html-select.bp3-minimal select.bp3-disabled.bp3-active, .bp3-select.bp3-minimal select.bp3-disabled.bp3-active, .bp3-html-select.bp3-minimal select.bp3-disabled:hover.bp3-active, .bp3-select.bp3-minimal select.bp3-disabled:hover.bp3-active{ background:rgba(115, 134, 148, 0.3); } .bp3-dark .bp3-html-select.bp3-minimal select, .bp3-html-select.bp3-minimal .bp3-dark select, .bp3-dark .bp3-select.bp3-minimal select, .bp3-select.bp3-minimal .bp3-dark select{ -webkit-box-shadow:none; box-shadow:none; background:none; color:inherit; } .bp3-dark .bp3-html-select.bp3-minimal select:hover, .bp3-html-select.bp3-minimal .bp3-dark select:hover, .bp3-dark .bp3-select.bp3-minimal select:hover, .bp3-select.bp3-minimal .bp3-dark select:hover, .bp3-dark .bp3-html-select.bp3-minimal select:active, .bp3-html-select.bp3-minimal .bp3-dark select:active, .bp3-dark .bp3-select.bp3-minimal select:active, .bp3-select.bp3-minimal .bp3-dark select:active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-active, .bp3-dark .bp3-select.bp3-minimal select.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-active{ -webkit-box-shadow:none; box-shadow:none; background:none; } .bp3-dark .bp3-html-select.bp3-minimal select:hover, .bp3-html-select.bp3-minimal .bp3-dark select:hover, .bp3-dark .bp3-select.bp3-minimal select:hover, .bp3-select.bp3-minimal .bp3-dark select:hover{ background:rgba(138, 155, 168, 0.15); } .bp3-dark .bp3-html-select.bp3-minimal select:active, .bp3-html-select.bp3-minimal .bp3-dark select:active, .bp3-dark .bp3-select.bp3-minimal select:active, .bp3-select.bp3-minimal .bp3-dark select:active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-active, .bp3-dark .bp3-select.bp3-minimal select.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-active{ background:rgba(138, 155, 168, 0.3); color:#f5f8fa; } .bp3-dark .bp3-html-select.bp3-minimal select:disabled, .bp3-html-select.bp3-minimal .bp3-dark select:disabled, .bp3-dark .bp3-select.bp3-minimal select:disabled, .bp3-select.bp3-minimal .bp3-dark select:disabled, .bp3-dark .bp3-html-select.bp3-minimal select:disabled:hover, .bp3-html-select.bp3-minimal .bp3-dark select:disabled:hover, .bp3-dark .bp3-select.bp3-minimal select:disabled:hover, .bp3-select.bp3-minimal .bp3-dark select:disabled:hover, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-disabled, .bp3-dark .bp3-select.bp3-minimal select.bp3-disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-disabled, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-disabled:hover, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-disabled:hover, .bp3-dark .bp3-select.bp3-minimal select.bp3-disabled:hover, .bp3-select.bp3-minimal .bp3-dark select.bp3-disabled:hover{ background:none; cursor:not-allowed; color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-html-select.bp3-minimal select:disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select:disabled.bp3-active, .bp3-dark .bp3-select.bp3-minimal select:disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select:disabled.bp3-active, .bp3-dark .bp3-html-select.bp3-minimal select:disabled:hover.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select:disabled:hover.bp3-active, .bp3-dark .bp3-select.bp3-minimal select:disabled:hover.bp3-active, .bp3-select.bp3-minimal .bp3-dark select:disabled:hover.bp3-active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-disabled.bp3-active, .bp3-dark .bp3-select.bp3-minimal select.bp3-disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-disabled.bp3-active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-disabled:hover.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-disabled:hover.bp3-active, .bp3-dark .bp3-select.bp3-minimal select.bp3-disabled:hover.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-disabled:hover.bp3-active{ background:rgba(138, 155, 168, 0.3); } .bp3-html-select.bp3-minimal select.bp3-intent-primary, .bp3-select.bp3-minimal select.bp3-intent-primary{ color:#106ba3; } .bp3-html-select.bp3-minimal select.bp3-intent-primary:hover, .bp3-select.bp3-minimal select.bp3-intent-primary:hover, .bp3-html-select.bp3-minimal select.bp3-intent-primary:active, .bp3-select.bp3-minimal select.bp3-intent-primary:active, .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-active, .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-active{ -webkit-box-shadow:none; box-shadow:none; background:none; color:#106ba3; } .bp3-html-select.bp3-minimal select.bp3-intent-primary:hover, .bp3-select.bp3-minimal select.bp3-intent-primary:hover{ background:rgba(19, 124, 189, 0.15); color:#106ba3; } .bp3-html-select.bp3-minimal select.bp3-intent-primary:active, .bp3-select.bp3-minimal select.bp3-intent-primary:active, .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-active, .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-active{ background:rgba(19, 124, 189, 0.3); color:#106ba3; } .bp3-html-select.bp3-minimal select.bp3-intent-primary:disabled, .bp3-select.bp3-minimal select.bp3-intent-primary:disabled, .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-disabled, .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-disabled{ background:none; color:rgba(16, 107, 163, 0.5); } .bp3-html-select.bp3-minimal select.bp3-intent-primary:disabled.bp3-active, .bp3-select.bp3-minimal select.bp3-intent-primary:disabled.bp3-active, .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-disabled.bp3-active, .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-disabled.bp3-active{ background:rgba(19, 124, 189, 0.3); } .bp3-html-select.bp3-minimal select.bp3-intent-primary .bp3-button-spinner .bp3-spinner-head, .bp3-select.bp3-minimal select.bp3-intent-primary .bp3-button-spinner .bp3-spinner-head{ stroke:#106ba3; } .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary{ color:#48aff0; } .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary:hover, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary:hover, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary:hover, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary:hover{ background:rgba(19, 124, 189, 0.2); color:#48aff0; } .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary:active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary:active, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary:active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary:active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-active, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-active{ background:rgba(19, 124, 189, 0.3); color:#48aff0; } .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary:disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary:disabled, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary:disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary:disabled, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-disabled, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-disabled{ background:none; color:rgba(72, 175, 240, 0.5); } .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary:disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary:disabled.bp3-active, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary:disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary:disabled.bp3-active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-disabled.bp3-active, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-disabled.bp3-active{ background:rgba(19, 124, 189, 0.3); } .bp3-html-select.bp3-minimal select.bp3-intent-success, .bp3-select.bp3-minimal select.bp3-intent-success{ color:#0d8050; } .bp3-html-select.bp3-minimal select.bp3-intent-success:hover, .bp3-select.bp3-minimal select.bp3-intent-success:hover, .bp3-html-select.bp3-minimal select.bp3-intent-success:active, .bp3-select.bp3-minimal select.bp3-intent-success:active, .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-active, .bp3-select.bp3-minimal select.bp3-intent-success.bp3-active{ -webkit-box-shadow:none; box-shadow:none; background:none; color:#0d8050; } .bp3-html-select.bp3-minimal select.bp3-intent-success:hover, .bp3-select.bp3-minimal select.bp3-intent-success:hover{ background:rgba(15, 153, 96, 0.15); color:#0d8050; } .bp3-html-select.bp3-minimal select.bp3-intent-success:active, .bp3-select.bp3-minimal select.bp3-intent-success:active, .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-active, .bp3-select.bp3-minimal select.bp3-intent-success.bp3-active{ background:rgba(15, 153, 96, 0.3); color:#0d8050; } .bp3-html-select.bp3-minimal select.bp3-intent-success:disabled, .bp3-select.bp3-minimal select.bp3-intent-success:disabled, .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-disabled, .bp3-select.bp3-minimal select.bp3-intent-success.bp3-disabled{ background:none; color:rgba(13, 128, 80, 0.5); } .bp3-html-select.bp3-minimal select.bp3-intent-success:disabled.bp3-active, .bp3-select.bp3-minimal select.bp3-intent-success:disabled.bp3-active, .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-disabled.bp3-active, .bp3-select.bp3-minimal select.bp3-intent-success.bp3-disabled.bp3-active{ background:rgba(15, 153, 96, 0.3); } .bp3-html-select.bp3-minimal select.bp3-intent-success .bp3-button-spinner .bp3-spinner-head, .bp3-select.bp3-minimal select.bp3-intent-success .bp3-button-spinner .bp3-spinner-head{ stroke:#0d8050; } .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success{ color:#3dcc91; } .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success:hover, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success:hover, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success:hover, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success:hover{ background:rgba(15, 153, 96, 0.2); color:#3dcc91; } .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success:active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success:active, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success:active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success:active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-active, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-active{ background:rgba(15, 153, 96, 0.3); color:#3dcc91; } .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success:disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success:disabled, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success:disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success:disabled, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-disabled, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success.bp3-disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-disabled{ background:none; color:rgba(61, 204, 145, 0.5); } .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success:disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success:disabled.bp3-active, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success:disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success:disabled.bp3-active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-disabled.bp3-active, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success.bp3-disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-disabled.bp3-active{ background:rgba(15, 153, 96, 0.3); } .bp3-html-select.bp3-minimal select.bp3-intent-warning, .bp3-select.bp3-minimal select.bp3-intent-warning{ color:#bf7326; } .bp3-html-select.bp3-minimal select.bp3-intent-warning:hover, .bp3-select.bp3-minimal select.bp3-intent-warning:hover, .bp3-html-select.bp3-minimal select.bp3-intent-warning:active, .bp3-select.bp3-minimal select.bp3-intent-warning:active, .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-active, .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-active{ -webkit-box-shadow:none; box-shadow:none; background:none; color:#bf7326; } .bp3-html-select.bp3-minimal select.bp3-intent-warning:hover, .bp3-select.bp3-minimal select.bp3-intent-warning:hover{ background:rgba(217, 130, 43, 0.15); color:#bf7326; } .bp3-html-select.bp3-minimal select.bp3-intent-warning:active, .bp3-select.bp3-minimal select.bp3-intent-warning:active, .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-active, .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-active{ background:rgba(217, 130, 43, 0.3); color:#bf7326; } .bp3-html-select.bp3-minimal select.bp3-intent-warning:disabled, .bp3-select.bp3-minimal select.bp3-intent-warning:disabled, .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-disabled, .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-disabled{ background:none; color:rgba(191, 115, 38, 0.5); } .bp3-html-select.bp3-minimal select.bp3-intent-warning:disabled.bp3-active, .bp3-select.bp3-minimal select.bp3-intent-warning:disabled.bp3-active, .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-disabled.bp3-active, .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-disabled.bp3-active{ background:rgba(217, 130, 43, 0.3); } .bp3-html-select.bp3-minimal select.bp3-intent-warning .bp3-button-spinner .bp3-spinner-head, .bp3-select.bp3-minimal select.bp3-intent-warning .bp3-button-spinner .bp3-spinner-head{ stroke:#bf7326; } .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning{ color:#ffb366; } .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning:hover, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning:hover, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning:hover, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning:hover{ background:rgba(217, 130, 43, 0.2); color:#ffb366; } .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning:active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning:active, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning:active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning:active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-active, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-active{ background:rgba(217, 130, 43, 0.3); color:#ffb366; } .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning:disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning:disabled, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning:disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning:disabled, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-disabled, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-disabled{ background:none; color:rgba(255, 179, 102, 0.5); } .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning:disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning:disabled.bp3-active, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning:disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning:disabled.bp3-active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-disabled.bp3-active, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-disabled.bp3-active{ background:rgba(217, 130, 43, 0.3); } .bp3-html-select.bp3-minimal select.bp3-intent-danger, .bp3-select.bp3-minimal select.bp3-intent-danger{ color:#c23030; } .bp3-html-select.bp3-minimal select.bp3-intent-danger:hover, .bp3-select.bp3-minimal select.bp3-intent-danger:hover, .bp3-html-select.bp3-minimal select.bp3-intent-danger:active, .bp3-select.bp3-minimal select.bp3-intent-danger:active, .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-active, .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-active{ -webkit-box-shadow:none; box-shadow:none; background:none; color:#c23030; } .bp3-html-select.bp3-minimal select.bp3-intent-danger:hover, .bp3-select.bp3-minimal select.bp3-intent-danger:hover{ background:rgba(219, 55, 55, 0.15); color:#c23030; } .bp3-html-select.bp3-minimal select.bp3-intent-danger:active, .bp3-select.bp3-minimal select.bp3-intent-danger:active, .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-active, .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-active{ background:rgba(219, 55, 55, 0.3); color:#c23030; } .bp3-html-select.bp3-minimal select.bp3-intent-danger:disabled, .bp3-select.bp3-minimal select.bp3-intent-danger:disabled, .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-disabled, .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-disabled{ background:none; color:rgba(194, 48, 48, 0.5); } .bp3-html-select.bp3-minimal select.bp3-intent-danger:disabled.bp3-active, .bp3-select.bp3-minimal select.bp3-intent-danger:disabled.bp3-active, .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-disabled.bp3-active, .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-disabled.bp3-active{ background:rgba(219, 55, 55, 0.3); } .bp3-html-select.bp3-minimal select.bp3-intent-danger .bp3-button-spinner .bp3-spinner-head, .bp3-select.bp3-minimal select.bp3-intent-danger .bp3-button-spinner .bp3-spinner-head{ stroke:#c23030; } .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger{ color:#ff7373; } .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger:hover, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger:hover, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger:hover, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger:hover{ background:rgba(219, 55, 55, 0.2); color:#ff7373; } .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger:active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger:active, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger:active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger:active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-active, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-active{ background:rgba(219, 55, 55, 0.3); color:#ff7373; } .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger:disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger:disabled, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger:disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger:disabled, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-disabled, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-disabled, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-disabled, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-disabled{ background:none; color:rgba(255, 115, 115, 0.5); } .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger:disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger:disabled.bp3-active, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger:disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger:disabled.bp3-active, .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-disabled.bp3-active, .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-disabled.bp3-active, .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-disabled.bp3-active, .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-disabled.bp3-active{ background:rgba(219, 55, 55, 0.3); } .bp3-html-select.bp3-large select, .bp3-select.bp3-large select{ height:40px; padding-right:35px; font-size:16px; } .bp3-dark .bp3-html-select select, .bp3-dark .bp3-select select{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); background-color:#394b59; background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0))); background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0)); color:#f5f8fa; } .bp3-dark .bp3-html-select select:hover, .bp3-dark .bp3-select select:hover, .bp3-dark .bp3-html-select select:active, .bp3-dark .bp3-select select:active, .bp3-dark .bp3-html-select select.bp3-active, .bp3-dark .bp3-select select.bp3-active{ color:#f5f8fa; } .bp3-dark .bp3-html-select select:hover, .bp3-dark .bp3-select select:hover{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); background-color:#30404d; } .bp3-dark .bp3-html-select select:active, .bp3-dark .bp3-select select:active, .bp3-dark .bp3-html-select select.bp3-active, .bp3-dark .bp3-select select.bp3-active{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2); background-color:#202b33; background-image:none; } .bp3-dark .bp3-html-select select:disabled, .bp3-dark .bp3-select select:disabled, .bp3-dark .bp3-html-select select.bp3-disabled, .bp3-dark .bp3-select select.bp3-disabled{ -webkit-box-shadow:none; box-shadow:none; background-color:rgba(57, 75, 89, 0.5); background-image:none; color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-html-select select:disabled.bp3-active, .bp3-dark .bp3-select select:disabled.bp3-active, .bp3-dark .bp3-html-select select.bp3-disabled.bp3-active, .bp3-dark .bp3-select select.bp3-disabled.bp3-active{ background:rgba(57, 75, 89, 0.7); } .bp3-dark .bp3-html-select select .bp3-button-spinner .bp3-spinner-head, .bp3-dark .bp3-select select .bp3-button-spinner .bp3-spinner-head{ background:rgba(16, 22, 26, 0.5); stroke:#8a9ba8; } .bp3-html-select select:disabled, .bp3-select select:disabled{ -webkit-box-shadow:none; box-shadow:none; background-color:rgba(206, 217, 224, 0.5); cursor:not-allowed; color:rgba(92, 112, 128, 0.6); } .bp3-html-select .bp3-icon, .bp3-select .bp3-icon, .bp3-select::after{ position:absolute; top:7px; right:7px; color:#5c7080; pointer-events:none; } .bp3-html-select .bp3-disabled.bp3-icon, .bp3-select .bp3-disabled.bp3-icon, .bp3-disabled.bp3-select::after{ color:rgba(92, 112, 128, 0.6); } .bp3-html-select, .bp3-select{ display:inline-block; position:relative; vertical-align:middle; letter-spacing:normal; } .bp3-html-select select::-ms-expand, .bp3-select select::-ms-expand{ display:none; } .bp3-html-select .bp3-icon, .bp3-select .bp3-icon{ color:#5c7080; } .bp3-html-select .bp3-icon:hover, .bp3-select .bp3-icon:hover{ color:#182026; } .bp3-dark .bp3-html-select .bp3-icon, .bp3-dark .bp3-select .bp3-icon{ color:#a7b6c2; } .bp3-dark .bp3-html-select .bp3-icon:hover, .bp3-dark .bp3-select .bp3-icon:hover{ color:#f5f8fa; } .bp3-html-select.bp3-large::after, .bp3-html-select.bp3-large .bp3-icon, .bp3-select.bp3-large::after, .bp3-select.bp3-large .bp3-icon{ top:12px; right:12px; } .bp3-html-select.bp3-fill, .bp3-html-select.bp3-fill select, .bp3-select.bp3-fill, .bp3-select.bp3-fill select{ width:100%; } .bp3-dark .bp3-html-select option, .bp3-dark .bp3-select option{ background-color:#30404d; color:#f5f8fa; } .bp3-dark .bp3-html-select::after, .bp3-dark .bp3-select::after{ color:#a7b6c2; } .bp3-select::after{ line-height:1; font-family:\"Icons16\", sans-serif; font-size:16px; font-weight:400; font-style:normal; -moz-osx-font-smoothing:grayscale; -webkit-font-smoothing:antialiased; content:\"\ue6c6\"; } .bp3-running-text table, table.bp3-html-table{ border-spacing:0; font-size:14px; } .bp3-running-text table th, table.bp3-html-table th, .bp3-running-text table td, table.bp3-html-table td{ padding:11px; vertical-align:top; text-align:left; } .bp3-running-text table th, table.bp3-html-table th{ color:#182026; font-weight:600; } .bp3-running-text table td, table.bp3-html-table td{ color:#182026; } .bp3-running-text table tbody tr:first-child th, table.bp3-html-table tbody tr:first-child th, .bp3-running-text table tbody tr:first-child td, table.bp3-html-table tbody tr:first-child td{ -webkit-box-shadow:inset 0 1px 0 0 rgba(16, 22, 26, 0.15); box-shadow:inset 0 1px 0 0 rgba(16, 22, 26, 0.15); } .bp3-dark .bp3-running-text table th, .bp3-running-text .bp3-dark table th, .bp3-dark table.bp3-html-table th{ color:#f5f8fa; } .bp3-dark .bp3-running-text table td, .bp3-running-text .bp3-dark table td, .bp3-dark table.bp3-html-table td{ color:#f5f8fa; } .bp3-dark .bp3-running-text table tbody tr:first-child th, .bp3-running-text .bp3-dark table tbody tr:first-child th, .bp3-dark table.bp3-html-table tbody tr:first-child th, .bp3-dark .bp3-running-text table tbody tr:first-child td, .bp3-running-text .bp3-dark table tbody tr:first-child td, .bp3-dark table.bp3-html-table tbody tr:first-child td{ -webkit-box-shadow:inset 0 1px 0 0 rgba(255, 255, 255, 0.15); box-shadow:inset 0 1px 0 0 rgba(255, 255, 255, 0.15); } table.bp3-html-table.bp3-html-table-condensed th, table.bp3-html-table.bp3-html-table-condensed td, table.bp3-html-table.bp3-small th, table.bp3-html-table.bp3-small td{ padding-top:6px; padding-bottom:6px; } table.bp3-html-table.bp3-html-table-striped tbody tr:nth-child(odd) td{ background:rgba(191, 204, 214, 0.15); } table.bp3-html-table.bp3-html-table-bordered th:not(:first-child){ -webkit-box-shadow:inset 1px 0 0 0 rgba(16, 22, 26, 0.15); box-shadow:inset 1px 0 0 0 rgba(16, 22, 26, 0.15); } table.bp3-html-table.bp3-html-table-bordered tbody tr td{ -webkit-box-shadow:inset 0 1px 0 0 rgba(16, 22, 26, 0.15); box-shadow:inset 0 1px 0 0 rgba(16, 22, 26, 0.15); } table.bp3-html-table.bp3-html-table-bordered tbody tr td:not(:first-child){ -webkit-box-shadow:inset 1px 1px 0 0 rgba(16, 22, 26, 0.15); box-shadow:inset 1px 1px 0 0 rgba(16, 22, 26, 0.15); } table.bp3-html-table.bp3-html-table-bordered.bp3-html-table-striped tbody tr:not(:first-child) td{ -webkit-box-shadow:none; box-shadow:none; } table.bp3-html-table.bp3-html-table-bordered.bp3-html-table-striped tbody tr:not(:first-child) td:not(:first-child){ -webkit-box-shadow:inset 1px 0 0 0 rgba(16, 22, 26, 0.15); box-shadow:inset 1px 0 0 0 rgba(16, 22, 26, 0.15); } table.bp3-html-table.bp3-interactive tbody tr:hover td{ background-color:rgba(191, 204, 214, 0.3); cursor:pointer; } table.bp3-html-table.bp3-interactive tbody tr:active td{ background-color:rgba(191, 204, 214, 0.4); } .bp3-dark table.bp3-html-table.bp3-html-table-striped tbody tr:nth-child(odd) td{ background:rgba(92, 112, 128, 0.15); } .bp3-dark table.bp3-html-table.bp3-html-table-bordered th:not(:first-child){ -webkit-box-shadow:inset 1px 0 0 0 rgba(255, 255, 255, 0.15); box-shadow:inset 1px 0 0 0 rgba(255, 255, 255, 0.15); } .bp3-dark table.bp3-html-table.bp3-html-table-bordered tbody tr td{ -webkit-box-shadow:inset 0 1px 0 0 rgba(255, 255, 255, 0.15); box-shadow:inset 0 1px 0 0 rgba(255, 255, 255, 0.15); } .bp3-dark table.bp3-html-table.bp3-html-table-bordered tbody tr td:not(:first-child){ -webkit-box-shadow:inset 1px 1px 0 0 rgba(255, 255, 255, 0.15); box-shadow:inset 1px 1px 0 0 rgba(255, 255, 255, 0.15); } .bp3-dark table.bp3-html-table.bp3-html-table-bordered.bp3-html-table-striped tbody tr:not(:first-child) td{ -webkit-box-shadow:inset 1px 0 0 0 rgba(255, 255, 255, 0.15); box-shadow:inset 1px 0 0 0 rgba(255, 255, 255, 0.15); } .bp3-dark table.bp3-html-table.bp3-html-table-bordered.bp3-html-table-striped tbody tr:not(:first-child) td:first-child{ -webkit-box-shadow:none; box-shadow:none; } .bp3-dark table.bp3-html-table.bp3-interactive tbody tr:hover td{ background-color:rgba(92, 112, 128, 0.3); cursor:pointer; } .bp3-dark table.bp3-html-table.bp3-interactive tbody tr:active td{ background-color:rgba(92, 112, 128, 0.4); } .bp3-key-combo{ display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-orient:horizontal; -webkit-box-direction:normal; -ms-flex-direction:row; flex-direction:row; -webkit-box-align:center; -ms-flex-align:center; align-items:center; } .bp3-key-combo > *{ -webkit-box-flex:0; -ms-flex-positive:0; flex-grow:0; -ms-flex-negative:0; flex-shrink:0; } .bp3-key-combo > .bp3-fill{ -webkit-box-flex:1; -ms-flex-positive:1; flex-grow:1; -ms-flex-negative:1; flex-shrink:1; } .bp3-key-combo::before, .bp3-key-combo > *{ margin-right:5px; } .bp3-key-combo:empty::before, .bp3-key-combo > :last-child{ margin-right:0; } .bp3-hotkey-dialog{ top:40px; padding-bottom:0; } .bp3-hotkey-dialog .bp3-dialog-body{ margin:0; padding:0; } .bp3-hotkey-dialog .bp3-hotkey-label{ -webkit-box-flex:1; -ms-flex-positive:1; flex-grow:1; } .bp3-hotkey-column{ margin:auto; max-height:80vh; overflow-y:auto; padding:30px; } .bp3-hotkey-column .bp3-heading{ margin-bottom:20px; } .bp3-hotkey-column .bp3-heading:not(:first-child){ margin-top:40px; } .bp3-hotkey{ display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-align:center; -ms-flex-align:center; align-items:center; -webkit-box-pack:justify; -ms-flex-pack:justify; justify-content:space-between; margin-right:0; margin-left:0; } .bp3-hotkey:not(:last-child){ margin-bottom:10px; } .bp3-icon{ display:inline-block; -webkit-box-flex:0; -ms-flex:0 0 auto; flex:0 0 auto; vertical-align:text-bottom; } .bp3-icon:not(:empty)::before{ content:\"\" !important; content:unset !important; } .bp3-icon > svg{ display:block; } .bp3-icon > svg:not([fill]){ fill:currentColor; } .bp3-icon.bp3-intent-primary, .bp3-icon-standard.bp3-intent-primary, .bp3-icon-large.bp3-intent-primary{ color:#106ba3; } .bp3-dark .bp3-icon.bp3-intent-primary, .bp3-dark .bp3-icon-standard.bp3-intent-primary, .bp3-dark .bp3-icon-large.bp3-intent-primary{ color:#48aff0; } .bp3-icon.bp3-intent-success, .bp3-icon-standard.bp3-intent-success, .bp3-icon-large.bp3-intent-success{ color:#0d8050; } .bp3-dark .bp3-icon.bp3-intent-success, .bp3-dark .bp3-icon-standard.bp3-intent-success, .bp3-dark .bp3-icon-large.bp3-intent-success{ color:#3dcc91; } .bp3-icon.bp3-intent-warning, .bp3-icon-standard.bp3-intent-warning, .bp3-icon-large.bp3-intent-warning{ color:#bf7326; } .bp3-dark .bp3-icon.bp3-intent-warning, .bp3-dark .bp3-icon-standard.bp3-intent-warning, .bp3-dark .bp3-icon-large.bp3-intent-warning{ color:#ffb366; } .bp3-icon.bp3-intent-danger, .bp3-icon-standard.bp3-intent-danger, .bp3-icon-large.bp3-intent-danger{ color:#c23030; } .bp3-dark .bp3-icon.bp3-intent-danger, .bp3-dark .bp3-icon-standard.bp3-intent-danger, .bp3-dark .bp3-icon-large.bp3-intent-danger{ color:#ff7373; } span.bp3-icon-standard{ line-height:1; font-family:\"Icons16\", sans-serif; font-size:16px; font-weight:400; font-style:normal; -moz-osx-font-smoothing:grayscale; -webkit-font-smoothing:antialiased; display:inline-block; } span.bp3-icon-large{ line-height:1; font-family:\"Icons20\", sans-serif; font-size:20px; font-weight:400; font-style:normal; -moz-osx-font-smoothing:grayscale; -webkit-font-smoothing:antialiased; display:inline-block; } span.bp3-icon:empty{ line-height:1; font-family:\"Icons20\"; font-size:inherit; font-weight:400; font-style:normal; } span.bp3-icon:empty::before{ -moz-osx-font-smoothing:grayscale; -webkit-font-smoothing:antialiased; } .bp3-icon-add::before{ content:\"\ue63e\"; } .bp3-icon-add-column-left::before{ content:\"\ue6f9\"; } .bp3-icon-add-column-right::before{ content:\"\ue6fa\"; } .bp3-icon-add-row-bottom::before{ content:\"\ue6f8\"; } .bp3-icon-add-row-top::before{ content:\"\ue6f7\"; } .bp3-icon-add-to-artifact::before{ content:\"\ue67c\"; } .bp3-icon-add-to-folder::before{ content:\"\ue6d2\"; } .bp3-icon-airplane::before{ content:\"\ue74b\"; } .bp3-icon-align-center::before{ content:\"\ue603\"; } .bp3-icon-align-justify::before{ content:\"\ue605\"; } .bp3-icon-align-left::before{ content:\"\ue602\"; } .bp3-icon-align-right::before{ content:\"\ue604\"; } .bp3-icon-alignment-bottom::before{ content:\"\ue727\"; } .bp3-icon-alignment-horizontal-center::before{ content:\"\ue726\"; } .bp3-icon-alignment-left::before{ content:\"\ue722\"; } .bp3-icon-alignment-right::before{ content:\"\ue724\"; } .bp3-icon-alignment-top::before{ content:\"\ue725\"; } .bp3-icon-alignment-vertical-center::before{ content:\"\ue723\"; } .bp3-icon-annotation::before{ content:\"\ue6f0\"; } .bp3-icon-application::before{ content:\"\ue735\"; } .bp3-icon-applications::before{ content:\"\ue621\"; } .bp3-icon-archive::before{ content:\"\ue907\"; } .bp3-icon-arrow-bottom-left::before{ content:\"\u2199\"; } .bp3-icon-arrow-bottom-right::before{ content:\"\u2198\"; } .bp3-icon-arrow-down::before{ content:\"\u2193\"; } .bp3-icon-arrow-left::before{ content:\"\u2190\"; } .bp3-icon-arrow-right::before{ content:\"\u2192\"; } .bp3-icon-arrow-top-left::before{ content:\"\u2196\"; } .bp3-icon-arrow-top-right::before{ content:\"\u2197\"; } .bp3-icon-arrow-up::before{ content:\"\u2191\"; } .bp3-icon-arrows-horizontal::before{ content:\"\u2194\"; } .bp3-icon-arrows-vertical::before{ content:\"\u2195\"; } .bp3-icon-asterisk::before{ content:\"*\"; } .bp3-icon-automatic-updates::before{ content:\"\ue65f\"; } .bp3-icon-badge::before{ content:\"\ue6e3\"; } .bp3-icon-ban-circle::before{ content:\"\ue69d\"; } .bp3-icon-bank-account::before{ content:\"\ue76f\"; } .bp3-icon-barcode::before{ content:\"\ue676\"; } .bp3-icon-blank::before{ content:\"\ue900\"; } .bp3-icon-blocked-person::before{ content:\"\ue768\"; } .bp3-icon-bold::before{ content:\"\ue606\"; } .bp3-icon-book::before{ content:\"\ue6b8\"; } .bp3-icon-bookmark::before{ content:\"\ue61a\"; } .bp3-icon-box::before{ content:\"\ue6bf\"; } .bp3-icon-briefcase::before{ content:\"\ue674\"; } .bp3-icon-bring-data::before{ content:\"\ue90a\"; } .bp3-icon-build::before{ content:\"\ue72d\"; } .bp3-icon-calculator::before{ content:\"\ue70b\"; } .bp3-icon-calendar::before{ content:\"\ue62b\"; } .bp3-icon-camera::before{ content:\"\ue69e\"; } .bp3-icon-caret-down::before{ content:\"\u2304\"; } .bp3-icon-caret-left::before{ content:\"\u2329\"; } .bp3-icon-caret-right::before{ content:\"\u232a\"; } .bp3-icon-caret-up::before{ content:\"\u2303\"; } .bp3-icon-cell-tower::before{ content:\"\ue770\"; } .bp3-icon-changes::before{ content:\"\ue623\"; } .bp3-icon-chart::before{ content:\"\ue67e\"; } .bp3-icon-chat::before{ content:\"\ue689\"; } .bp3-icon-chevron-backward::before{ content:\"\ue6df\"; } .bp3-icon-chevron-down::before{ content:\"\ue697\"; } .bp3-icon-chevron-forward::before{ content:\"\ue6e0\"; } .bp3-icon-chevron-left::before{ content:\"\ue694\"; } .bp3-icon-chevron-right::before{ content:\"\ue695\"; } .bp3-icon-chevron-up::before{ content:\"\ue696\"; } .bp3-icon-circle::before{ content:\"\ue66a\"; } .bp3-icon-circle-arrow-down::before{ content:\"\ue68e\"; } .bp3-icon-circle-arrow-left::before{ content:\"\ue68c\"; } .bp3-icon-circle-arrow-right::before{ content:\"\ue68b\"; } .bp3-icon-circle-arrow-up::before{ content:\"\ue68d\"; } .bp3-icon-citation::before{ content:\"\ue61b\"; } .bp3-icon-clean::before{ content:\"\ue7c5\"; } .bp3-icon-clipboard::before{ content:\"\ue61d\"; } .bp3-icon-cloud::before{ content:\"\u2601\"; } .bp3-icon-cloud-download::before{ content:\"\ue690\"; } .bp3-icon-cloud-upload::before{ content:\"\ue691\"; } .bp3-icon-code::before{ content:\"\ue661\"; } .bp3-icon-code-block::before{ content:\"\ue6c5\"; } .bp3-icon-cog::before{ content:\"\ue645\"; } .bp3-icon-collapse-all::before{ content:\"\ue763\"; } .bp3-icon-column-layout::before{ content:\"\ue6da\"; } .bp3-icon-comment::before{ content:\"\ue68a\"; } .bp3-icon-comparison::before{ content:\"\ue637\"; } .bp3-icon-compass::before{ content:\"\ue79c\"; } .bp3-icon-compressed::before{ content:\"\ue6c0\"; } .bp3-icon-confirm::before{ content:\"\ue639\"; } .bp3-icon-console::before{ content:\"\ue79b\"; } .bp3-icon-contrast::before{ content:\"\ue6cb\"; } .bp3-icon-control::before{ content:\"\ue67f\"; } .bp3-icon-credit-card::before{ content:\"\ue649\"; } .bp3-icon-cross::before{ content:\"\u2717\"; } .bp3-icon-crown::before{ content:\"\ue7b4\"; } .bp3-icon-cube::before{ content:\"\ue7c8\"; } .bp3-icon-cube-add::before{ content:\"\ue7c9\"; } .bp3-icon-cube-remove::before{ content:\"\ue7d0\"; } .bp3-icon-curved-range-chart::before{ content:\"\ue71b\"; } .bp3-icon-cut::before{ content:\"\ue6ef\"; } .bp3-icon-dashboard::before{ content:\"\ue751\"; } .bp3-icon-data-lineage::before{ content:\"\ue908\"; } .bp3-icon-database::before{ content:\"\ue683\"; } .bp3-icon-delete::before{ content:\"\ue644\"; } .bp3-icon-delta::before{ content:\"\u0394\"; } .bp3-icon-derive-column::before{ content:\"\ue739\"; } .bp3-icon-desktop::before{ content:\"\ue6af\"; } .bp3-icon-diagram-tree::before{ content:\"\ue7b3\"; } .bp3-icon-direction-left::before{ content:\"\ue681\"; } .bp3-icon-direction-right::before{ content:\"\ue682\"; } .bp3-icon-disable::before{ content:\"\ue600\"; } .bp3-icon-document::before{ content:\"\ue630\"; } .bp3-icon-document-open::before{ content:\"\ue71e\"; } .bp3-icon-document-share::before{ content:\"\ue71f\"; } .bp3-icon-dollar::before{ content:\"$\"; } .bp3-icon-dot::before{ content:\"\u2022\"; } .bp3-icon-double-caret-horizontal::before{ content:\"\ue6c7\"; } .bp3-icon-double-caret-vertical::before{ content:\"\ue6c6\"; } .bp3-icon-double-chevron-down::before{ content:\"\ue703\"; } .bp3-icon-double-chevron-left::before{ content:\"\ue6ff\"; } .bp3-icon-double-chevron-right::before{ content:\"\ue701\"; } .bp3-icon-double-chevron-up::before{ content:\"\ue702\"; } .bp3-icon-doughnut-chart::before{ content:\"\ue6ce\"; } .bp3-icon-download::before{ content:\"\ue62f\"; } .bp3-icon-drag-handle-horizontal::before{ content:\"\ue716\"; } .bp3-icon-drag-handle-vertical::before{ content:\"\ue715\"; } .bp3-icon-draw::before{ content:\"\ue66b\"; } .bp3-icon-drive-time::before{ content:\"\ue615\"; } .bp3-icon-duplicate::before{ content:\"\ue69c\"; } .bp3-icon-edit::before{ content:\"\u270e\"; } .bp3-icon-eject::before{ content:\"\u23cf\"; } .bp3-icon-endorsed::before{ content:\"\ue75f\"; } .bp3-icon-envelope::before{ content:\"\u2709\"; } .bp3-icon-equals::before{ content:\"\ue7d9\"; } .bp3-icon-eraser::before{ content:\"\ue773\"; } .bp3-icon-error::before{ content:\"\ue648\"; } .bp3-icon-euro::before{ content:\"\u20ac\"; } .bp3-icon-exchange::before{ content:\"\ue636\"; } .bp3-icon-exclude-row::before{ content:\"\ue6ea\"; } .bp3-icon-expand-all::before{ content:\"\ue764\"; } .bp3-icon-export::before{ content:\"\ue633\"; } .bp3-icon-eye-off::before{ content:\"\ue6cc\"; } .bp3-icon-eye-on::before{ content:\"\ue75a\"; } .bp3-icon-eye-open::before{ content:\"\ue66f\"; } .bp3-icon-fast-backward::before{ content:\"\ue6a8\"; } .bp3-icon-fast-forward::before{ content:\"\ue6ac\"; } .bp3-icon-feed::before{ content:\"\ue656\"; } .bp3-icon-feed-subscribed::before{ content:\"\ue78f\"; } .bp3-icon-film::before{ content:\"\ue6a1\"; } .bp3-icon-filter::before{ content:\"\ue638\"; } .bp3-icon-filter-keep::before{ content:\"\ue78c\"; } .bp3-icon-filter-list::before{ content:\"\ue6ee\"; } .bp3-icon-filter-open::before{ content:\"\ue7d7\"; } .bp3-icon-filter-remove::before{ content:\"\ue78d\"; } .bp3-icon-flag::before{ content:\"\u2691\"; } .bp3-icon-flame::before{ content:\"\ue7a9\"; } .bp3-icon-flash::before{ content:\"\ue6b3\"; } .bp3-icon-floppy-disk::before{ content:\"\ue6b7\"; } .bp3-icon-flow-branch::before{ content:\"\ue7c1\"; } .bp3-icon-flow-end::before{ content:\"\ue7c4\"; } .bp3-icon-flow-linear::before{ content:\"\ue7c0\"; } .bp3-icon-flow-review::before{ content:\"\ue7c2\"; } .bp3-icon-flow-review-branch::before{ content:\"\ue7c3\"; } .bp3-icon-flows::before{ content:\"\ue659\"; } .bp3-icon-folder-close::before{ content:\"\ue652\"; } .bp3-icon-folder-new::before{ content:\"\ue7b0\"; } .bp3-icon-folder-open::before{ content:\"\ue651\"; } .bp3-icon-folder-shared::before{ content:\"\ue653\"; } .bp3-icon-folder-shared-open::before{ content:\"\ue670\"; } .bp3-icon-follower::before{ content:\"\ue760\"; } .bp3-icon-following::before{ content:\"\ue761\"; } .bp3-icon-font::before{ content:\"\ue6b4\"; } .bp3-icon-fork::before{ content:\"\ue63a\"; } .bp3-icon-form::before{ content:\"\ue795\"; } .bp3-icon-full-circle::before{ content:\"\ue685\"; } .bp3-icon-full-stacked-chart::before{ content:\"\ue75e\"; } .bp3-icon-fullscreen::before{ content:\"\ue699\"; } .bp3-icon-function::before{ content:\"\ue6e5\"; } .bp3-icon-gantt-chart::before{ content:\"\ue6f4\"; } .bp3-icon-geolocation::before{ content:\"\ue640\"; } .bp3-icon-geosearch::before{ content:\"\ue613\"; } .bp3-icon-git-branch::before{ content:\"\ue72a\"; } .bp3-icon-git-commit::before{ content:\"\ue72b\"; } .bp3-icon-git-merge::before{ content:\"\ue729\"; } .bp3-icon-git-new-branch::before{ content:\"\ue749\"; } .bp3-icon-git-pull::before{ content:\"\ue728\"; } .bp3-icon-git-push::before{ content:\"\ue72c\"; } .bp3-icon-git-repo::before{ content:\"\ue748\"; } .bp3-icon-glass::before{ content:\"\ue6b1\"; } .bp3-icon-globe::before{ content:\"\ue666\"; } .bp3-icon-globe-network::before{ content:\"\ue7b5\"; } .bp3-icon-graph::before{ content:\"\ue673\"; } .bp3-icon-graph-remove::before{ content:\"\ue609\"; } .bp3-icon-greater-than::before{ content:\"\ue7e1\"; } .bp3-icon-greater-than-or-equal-to::before{ content:\"\ue7e2\"; } .bp3-icon-grid::before{ content:\"\ue6d0\"; } .bp3-icon-grid-view::before{ content:\"\ue6e4\"; } .bp3-icon-group-objects::before{ content:\"\ue60a\"; } .bp3-icon-grouped-bar-chart::before{ content:\"\ue75d\"; } .bp3-icon-hand::before{ content:\"\ue6de\"; } .bp3-icon-hand-down::before{ content:\"\ue6bb\"; } .bp3-icon-hand-left::before{ content:\"\ue6bc\"; } .bp3-icon-hand-right::before{ content:\"\ue6b9\"; } .bp3-icon-hand-up::before{ content:\"\ue6ba\"; } .bp3-icon-header::before{ content:\"\ue6b5\"; } .bp3-icon-header-one::before{ content:\"\ue793\"; } .bp3-icon-header-two::before{ content:\"\ue794\"; } .bp3-icon-headset::before{ content:\"\ue6dc\"; } .bp3-icon-heart::before{ content:\"\u2665\"; } .bp3-icon-heart-broken::before{ content:\"\ue7a2\"; } .bp3-icon-heat-grid::before{ content:\"\ue6f3\"; } .bp3-icon-heatmap::before{ content:\"\ue614\"; } .bp3-icon-help::before{ content:\"?\"; } .bp3-icon-helper-management::before{ content:\"\ue66d\"; } .bp3-icon-highlight::before{ content:\"\ue6ed\"; } .bp3-icon-history::before{ content:\"\ue64a\"; } .bp3-icon-home::before{ content:\"\u2302\"; } .bp3-icon-horizontal-bar-chart::before{ content:\"\ue70c\"; } .bp3-icon-horizontal-bar-chart-asc::before{ content:\"\ue75c\"; } .bp3-icon-horizontal-bar-chart-desc::before{ content:\"\ue71d\"; } .bp3-icon-horizontal-distribution::before{ content:\"\ue720\"; } .bp3-icon-id-number::before{ content:\"\ue771\"; } .bp3-icon-image-rotate-left::before{ content:\"\ue73a\"; } .bp3-icon-image-rotate-right::before{ content:\"\ue73b\"; } .bp3-icon-import::before{ content:\"\ue632\"; } .bp3-icon-inbox::before{ content:\"\ue629\"; } .bp3-icon-inbox-filtered::before{ content:\"\ue7d1\"; } .bp3-icon-inbox-geo::before{ content:\"\ue7d2\"; } .bp3-icon-inbox-search::before{ content:\"\ue7d3\"; } .bp3-icon-inbox-update::before{ content:\"\ue7d4\"; } .bp3-icon-info-sign::before{ content:\"\u2139\"; } .bp3-icon-inheritance::before{ content:\"\ue7d5\"; } .bp3-icon-inner-join::before{ content:\"\ue7a3\"; } .bp3-icon-insert::before{ content:\"\ue66c\"; } .bp3-icon-intersection::before{ content:\"\ue765\"; } .bp3-icon-ip-address::before{ content:\"\ue772\"; } .bp3-icon-issue::before{ content:\"\ue774\"; } .bp3-icon-issue-closed::before{ content:\"\ue776\"; } .bp3-icon-issue-new::before{ content:\"\ue775\"; } .bp3-icon-italic::before{ content:\"\ue607\"; } .bp3-icon-join-table::before{ content:\"\ue738\"; } .bp3-icon-key::before{ content:\"\ue78e\"; } .bp3-icon-key-backspace::before{ content:\"\ue707\"; } .bp3-icon-key-command::before{ content:\"\ue705\"; } .bp3-icon-key-control::before{ content:\"\ue704\"; } .bp3-icon-key-delete::before{ content:\"\ue708\"; } .bp3-icon-key-enter::before{ content:\"\ue70a\"; } .bp3-icon-key-escape::before{ content:\"\ue709\"; } .bp3-icon-key-option::before{ content:\"\ue742\"; } .bp3-icon-key-shift::before{ content:\"\ue706\"; } .bp3-icon-key-tab::before{ content:\"\ue757\"; } .bp3-icon-known-vehicle::before{ content:\"\ue73c\"; } .bp3-icon-label::before{ content:\"\ue665\"; } .bp3-icon-layer::before{ content:\"\ue6cf\"; } .bp3-icon-layers::before{ content:\"\ue618\"; } .bp3-icon-layout::before{ content:\"\ue60c\"; } .bp3-icon-layout-auto::before{ content:\"\ue60d\"; } .bp3-icon-layout-balloon::before{ content:\"\ue6d3\"; } .bp3-icon-layout-circle::before{ content:\"\ue60e\"; } .bp3-icon-layout-grid::before{ content:\"\ue610\"; } .bp3-icon-layout-group-by::before{ content:\"\ue611\"; } .bp3-icon-layout-hierarchy::before{ content:\"\ue60f\"; } .bp3-icon-layout-linear::before{ content:\"\ue6c3\"; } .bp3-icon-layout-skew-grid::before{ content:\"\ue612\"; } .bp3-icon-layout-sorted-clusters::before{ content:\"\ue6d4\"; } .bp3-icon-learning::before{ content:\"\ue904\"; } .bp3-icon-left-join::before{ content:\"\ue7a4\"; } .bp3-icon-less-than::before{ content:\"\ue7e3\"; } .bp3-icon-less-than-or-equal-to::before{ content:\"\ue7e4\"; } .bp3-icon-lifesaver::before{ content:\"\ue7c7\"; } .bp3-icon-lightbulb::before{ content:\"\ue6b0\"; } .bp3-icon-link::before{ content:\"\ue62d\"; } .bp3-icon-list::before{ content:\"\u2630\"; } .bp3-icon-list-columns::before{ content:\"\ue7b9\"; } .bp3-icon-list-detail-view::before{ content:\"\ue743\"; } .bp3-icon-locate::before{ content:\"\ue619\"; } .bp3-icon-lock::before{ content:\"\ue625\"; } .bp3-icon-log-in::before{ content:\"\ue69a\"; } .bp3-icon-log-out::before{ content:\"\ue64c\"; } .bp3-icon-manual::before{ content:\"\ue6f6\"; } .bp3-icon-manually-entered-data::before{ content:\"\ue74a\"; } .bp3-icon-map::before{ content:\"\ue662\"; } .bp3-icon-map-create::before{ content:\"\ue741\"; } .bp3-icon-map-marker::before{ content:\"\ue67d\"; } .bp3-icon-maximize::before{ content:\"\ue635\"; } .bp3-icon-media::before{ content:\"\ue62c\"; } .bp3-icon-menu::before{ content:\"\ue762\"; } .bp3-icon-menu-closed::before{ content:\"\ue655\"; } .bp3-icon-menu-open::before{ content:\"\ue654\"; } .bp3-icon-merge-columns::before{ content:\"\ue74f\"; } .bp3-icon-merge-links::before{ content:\"\ue60b\"; } .bp3-icon-minimize::before{ content:\"\ue634\"; } .bp3-icon-minus::before{ content:\"\u2212\"; } .bp3-icon-mobile-phone::before{ content:\"\ue717\"; } .bp3-icon-mobile-video::before{ content:\"\ue69f\"; } .bp3-icon-moon::before{ content:\"\ue754\"; } .bp3-icon-more::before{ content:\"\ue62a\"; } .bp3-icon-mountain::before{ content:\"\ue7b1\"; } .bp3-icon-move::before{ content:\"\ue693\"; } .bp3-icon-mugshot::before{ content:\"\ue6db\"; } .bp3-icon-multi-select::before{ content:\"\ue680\"; } .bp3-icon-music::before{ content:\"\ue6a6\"; } .bp3-icon-new-drawing::before{ content:\"\ue905\"; } .bp3-icon-new-grid-item::before{ content:\"\ue747\"; } .bp3-icon-new-layer::before{ content:\"\ue902\"; } .bp3-icon-new-layers::before{ content:\"\ue903\"; } .bp3-icon-new-link::before{ content:\"\ue65c\"; } .bp3-icon-new-object::before{ content:\"\ue65d\"; } .bp3-icon-new-person::before{ content:\"\ue6e9\"; } .bp3-icon-new-prescription::before{ content:\"\ue78b\"; } .bp3-icon-new-text-box::before{ content:\"\ue65b\"; } .bp3-icon-ninja::before{ content:\"\ue675\"; } .bp3-icon-not-equal-to::before{ content:\"\ue7e0\"; } .bp3-icon-notifications::before{ content:\"\ue624\"; } .bp3-icon-notifications-updated::before{ content:\"\ue7b8\"; } .bp3-icon-numbered-list::before{ content:\"\ue746\"; } .bp3-icon-numerical::before{ content:\"\ue756\"; } .bp3-icon-office::before{ content:\"\ue69b\"; } .bp3-icon-offline::before{ content:\"\ue67a\"; } .bp3-icon-oil-field::before{ content:\"\ue73f\"; } .bp3-icon-one-column::before{ content:\"\ue658\"; } .bp3-icon-outdated::before{ content:\"\ue7a8\"; } .bp3-icon-page-layout::before{ content:\"\ue660\"; } .bp3-icon-panel-stats::before{ content:\"\ue777\"; } .bp3-icon-panel-table::before{ content:\"\ue778\"; } .bp3-icon-paperclip::before{ content:\"\ue664\"; } .bp3-icon-paragraph::before{ content:\"\ue76c\"; } .bp3-icon-path::before{ content:\"\ue753\"; } .bp3-icon-path-search::before{ content:\"\ue65e\"; } .bp3-icon-pause::before{ content:\"\ue6a9\"; } .bp3-icon-people::before{ content:\"\ue63d\"; } .bp3-icon-percentage::before{ content:\"\ue76a\"; } .bp3-icon-person::before{ content:\"\ue63c\"; } .bp3-icon-phone::before{ content:\"\u260e\"; } .bp3-icon-pie-chart::before{ content:\"\ue684\"; } .bp3-icon-pin::before{ content:\"\ue646\"; } .bp3-icon-pivot::before{ content:\"\ue6f1\"; } .bp3-icon-pivot-table::before{ content:\"\ue6eb\"; } .bp3-icon-play::before{ content:\"\ue6ab\"; } .bp3-icon-plus::before{ content:\"+\"; } .bp3-icon-polygon-filter::before{ content:\"\ue6d1\"; } .bp3-icon-power::before{ content:\"\ue6d9\"; } .bp3-icon-predictive-analysis::before{ content:\"\ue617\"; } .bp3-icon-prescription::before{ content:\"\ue78a\"; } .bp3-icon-presentation::before{ content:\"\ue687\"; } .bp3-icon-print::before{ content:\"\u2399\"; } .bp3-icon-projects::before{ content:\"\ue622\"; } .bp3-icon-properties::before{ content:\"\ue631\"; } .bp3-icon-property::before{ content:\"\ue65a\"; } .bp3-icon-publish-function::before{ content:\"\ue752\"; } .bp3-icon-pulse::before{ content:\"\ue6e8\"; } .bp3-icon-random::before{ content:\"\ue698\"; } .bp3-icon-record::before{ content:\"\ue6ae\"; } .bp3-icon-redo::before{ content:\"\ue6c4\"; } .bp3-icon-refresh::before{ content:\"\ue643\"; } .bp3-icon-regression-chart::before{ content:\"\ue758\"; } .bp3-icon-remove::before{ content:\"\ue63f\"; } .bp3-icon-remove-column::before{ content:\"\ue755\"; } .bp3-icon-remove-column-left::before{ content:\"\ue6fd\"; } .bp3-icon-remove-column-right::before{ content:\"\ue6fe\"; } .bp3-icon-remove-row-bottom::before{ content:\"\ue6fc\"; } .bp3-icon-remove-row-top::before{ content:\"\ue6fb\"; } .bp3-icon-repeat::before{ content:\"\ue692\"; } .bp3-icon-reset::before{ content:\"\ue7d6\"; } .bp3-icon-resolve::before{ content:\"\ue672\"; } .bp3-icon-rig::before{ content:\"\ue740\"; } .bp3-icon-right-join::before{ content:\"\ue7a5\"; } .bp3-icon-ring::before{ content:\"\ue6f2\"; } .bp3-icon-rotate-document::before{ content:\"\ue6e1\"; } .bp3-icon-rotate-page::before{ content:\"\ue6e2\"; } .bp3-icon-satellite::before{ content:\"\ue76b\"; } .bp3-icon-saved::before{ content:\"\ue6b6\"; } .bp3-icon-scatter-plot::before{ content:\"\ue73e\"; } .bp3-icon-search::before{ content:\"\ue64b\"; } .bp3-icon-search-around::before{ content:\"\ue608\"; } .bp3-icon-search-template::before{ content:\"\ue628\"; } .bp3-icon-search-text::before{ content:\"\ue663\"; } .bp3-icon-segmented-control::before{ content:\"\ue6ec\"; } .bp3-icon-select::before{ content:\"\ue616\"; } .bp3-icon-selection::before{ content:\"\u29bf\"; } .bp3-icon-send-to::before{ content:\"\ue66e\"; } .bp3-icon-send-to-graph::before{ content:\"\ue736\"; } .bp3-icon-send-to-map::before{ content:\"\ue737\"; } .bp3-icon-series-add::before{ content:\"\ue796\"; } .bp3-icon-series-configuration::before{ content:\"\ue79a\"; } .bp3-icon-series-derived::before{ content:\"\ue799\"; } .bp3-icon-series-filtered::before{ content:\"\ue798\"; } .bp3-icon-series-search::before{ content:\"\ue797\"; } .bp3-icon-settings::before{ content:\"\ue6a2\"; } .bp3-icon-share::before{ content:\"\ue62e\"; } .bp3-icon-shield::before{ content:\"\ue7b2\"; } .bp3-icon-shop::before{ content:\"\ue6c2\"; } .bp3-icon-shopping-cart::before{ content:\"\ue6c1\"; } .bp3-icon-signal-search::before{ content:\"\ue909\"; } .bp3-icon-sim-card::before{ content:\"\ue718\"; } .bp3-icon-slash::before{ content:\"\ue769\"; } .bp3-icon-small-cross::before{ content:\"\ue6d7\"; } .bp3-icon-small-minus::before{ content:\"\ue70e\"; } .bp3-icon-small-plus::before{ content:\"\ue70d\"; } .bp3-icon-small-tick::before{ content:\"\ue6d8\"; } .bp3-icon-snowflake::before{ content:\"\ue7b6\"; } .bp3-icon-social-media::before{ content:\"\ue671\"; } .bp3-icon-sort::before{ content:\"\ue64f\"; } .bp3-icon-sort-alphabetical::before{ content:\"\ue64d\"; } .bp3-icon-sort-alphabetical-desc::before{ content:\"\ue6c8\"; } .bp3-icon-sort-asc::before{ content:\"\ue6d5\"; } .bp3-icon-sort-desc::before{ content:\"\ue6d6\"; } .bp3-icon-sort-numerical::before{ content:\"\ue64e\"; } .bp3-icon-sort-numerical-desc::before{ content:\"\ue6c9\"; } .bp3-icon-split-columns::before{ content:\"\ue750\"; } .bp3-icon-square::before{ content:\"\ue686\"; } .bp3-icon-stacked-chart::before{ content:\"\ue6e7\"; } .bp3-icon-star::before{ content:\"\u2605\"; } .bp3-icon-star-empty::before{ content:\"\u2606\"; } .bp3-icon-step-backward::before{ content:\"\ue6a7\"; } .bp3-icon-step-chart::before{ content:\"\ue70f\"; } .bp3-icon-step-forward::before{ content:\"\ue6ad\"; } .bp3-icon-stop::before{ content:\"\ue6aa\"; } .bp3-icon-stopwatch::before{ content:\"\ue901\"; } .bp3-icon-strikethrough::before{ content:\"\ue7a6\"; } .bp3-icon-style::before{ content:\"\ue601\"; } .bp3-icon-swap-horizontal::before{ content:\"\ue745\"; } .bp3-icon-swap-vertical::before{ content:\"\ue744\"; } .bp3-icon-symbol-circle::before{ content:\"\ue72e\"; } .bp3-icon-symbol-cross::before{ content:\"\ue731\"; } .bp3-icon-symbol-diamond::before{ content:\"\ue730\"; } .bp3-icon-symbol-square::before{ content:\"\ue72f\"; } .bp3-icon-symbol-triangle-down::before{ content:\"\ue733\"; } .bp3-icon-symbol-triangle-up::before{ content:\"\ue732\"; } .bp3-icon-tag::before{ content:\"\ue61c\"; } .bp3-icon-take-action::before{ content:\"\ue6ca\"; } .bp3-icon-taxi::before{ content:\"\ue79e\"; } .bp3-icon-text-highlight::before{ content:\"\ue6dd\"; } .bp3-icon-th::before{ content:\"\ue667\"; } .bp3-icon-th-derived::before{ content:\"\ue669\"; } .bp3-icon-th-disconnect::before{ content:\"\ue7d8\"; } .bp3-icon-th-filtered::before{ content:\"\ue7c6\"; } .bp3-icon-th-list::before{ content:\"\ue668\"; } .bp3-icon-thumbs-down::before{ content:\"\ue6be\"; } .bp3-icon-thumbs-up::before{ content:\"\ue6bd\"; } .bp3-icon-tick::before{ content:\"\u2713\"; } .bp3-icon-tick-circle::before{ content:\"\ue779\"; } .bp3-icon-time::before{ content:\"\u23f2\"; } .bp3-icon-timeline-area-chart::before{ content:\"\ue6cd\"; } .bp3-icon-timeline-bar-chart::before{ content:\"\ue620\"; } .bp3-icon-timeline-events::before{ content:\"\ue61e\"; } .bp3-icon-timeline-line-chart::before{ content:\"\ue61f\"; } .bp3-icon-tint::before{ content:\"\ue6b2\"; } .bp3-icon-torch::before{ content:\"\ue677\"; } .bp3-icon-tractor::before{ content:\"\ue90c\"; } .bp3-icon-train::before{ content:\"\ue79f\"; } .bp3-icon-translate::before{ content:\"\ue759\"; } .bp3-icon-trash::before{ content:\"\ue63b\"; } .bp3-icon-tree::before{ content:\"\ue7b7\"; } .bp3-icon-trending-down::before{ content:\"\ue71a\"; } .bp3-icon-trending-up::before{ content:\"\ue719\"; } .bp3-icon-truck::before{ content:\"\ue90b\"; } .bp3-icon-two-columns::before{ content:\"\ue657\"; } .bp3-icon-unarchive::before{ content:\"\ue906\"; } .bp3-icon-underline::before{ content:\"\u2381\"; } .bp3-icon-undo::before{ content:\"\u238c\"; } .bp3-icon-ungroup-objects::before{ content:\"\ue688\"; } .bp3-icon-unknown-vehicle::before{ content:\"\ue73d\"; } .bp3-icon-unlock::before{ content:\"\ue626\"; } .bp3-icon-unpin::before{ content:\"\ue650\"; } .bp3-icon-unresolve::before{ content:\"\ue679\"; } .bp3-icon-updated::before{ content:\"\ue7a7\"; } .bp3-icon-upload::before{ content:\"\ue68f\"; } .bp3-icon-user::before{ content:\"\ue627\"; } .bp3-icon-variable::before{ content:\"\ue6f5\"; } .bp3-icon-vertical-bar-chart-asc::before{ content:\"\ue75b\"; } .bp3-icon-vertical-bar-chart-desc::before{ content:\"\ue71c\"; } .bp3-icon-vertical-distribution::before{ content:\"\ue721\"; } .bp3-icon-video::before{ content:\"\ue6a0\"; } .bp3-icon-volume-down::before{ content:\"\ue6a4\"; } .bp3-icon-volume-off::before{ content:\"\ue6a3\"; } .bp3-icon-volume-up::before{ content:\"\ue6a5\"; } .bp3-icon-walk::before{ content:\"\ue79d\"; } .bp3-icon-warning-sign::before{ content:\"\ue647\"; } .bp3-icon-waterfall-chart::before{ content:\"\ue6e6\"; } .bp3-icon-widget::before{ content:\"\ue678\"; } .bp3-icon-widget-button::before{ content:\"\ue790\"; } .bp3-icon-widget-footer::before{ content:\"\ue792\"; } .bp3-icon-widget-header::before{ content:\"\ue791\"; } .bp3-icon-wrench::before{ content:\"\ue734\"; } .bp3-icon-zoom-in::before{ content:\"\ue641\"; } .bp3-icon-zoom-out::before{ content:\"\ue642\"; } .bp3-icon-zoom-to-fit::before{ content:\"\ue67b\"; } .bp3-submenu > .bp3-popover-wrapper{ display:block; } .bp3-submenu .bp3-popover-target{ display:block; } .bp3-submenu.bp3-popover{ -webkit-box-shadow:none; box-shadow:none; padding:0 5px; } .bp3-submenu.bp3-popover > .bp3-popover-content{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2); } .bp3-dark .bp3-submenu.bp3-popover, .bp3-submenu.bp3-popover.bp3-dark{ -webkit-box-shadow:none; box-shadow:none; } .bp3-dark .bp3-submenu.bp3-popover > .bp3-popover-content, .bp3-submenu.bp3-popover.bp3-dark > .bp3-popover-content{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); } .bp3-menu{ margin:0; border-radius:3px; background:#ffffff; min-width:180px; padding:5px; list-style:none; text-align:left; color:#182026; } .bp3-menu-divider{ display:block; margin:5px; border-top:1px solid rgba(16, 22, 26, 0.15); } .bp3-dark .bp3-menu-divider{ border-top-color:rgba(255, 255, 255, 0.15); } .bp3-menu-item{ display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-orient:horizontal; -webkit-box-direction:normal; -ms-flex-direction:row; flex-direction:row; -webkit-box-align:start; -ms-flex-align:start; align-items:flex-start; border-radius:2px; padding:5px 7px; text-decoration:none; line-height:20px; color:inherit; -webkit-user-select:none; -moz-user-select:none; -ms-user-select:none; user-select:none; } .bp3-menu-item > *{ -webkit-box-flex:0; -ms-flex-positive:0; flex-grow:0; -ms-flex-negative:0; flex-shrink:0; } .bp3-menu-item > .bp3-fill{ -webkit-box-flex:1; -ms-flex-positive:1; flex-grow:1; -ms-flex-negative:1; flex-shrink:1; } .bp3-menu-item::before, .bp3-menu-item > *{ margin-right:7px; } .bp3-menu-item:empty::before, .bp3-menu-item > :last-child{ margin-right:0; } .bp3-menu-item > .bp3-fill{ word-break:break-word; } .bp3-menu-item:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-menu-item{ background-color:rgba(167, 182, 194, 0.3); cursor:pointer; text-decoration:none; } .bp3-menu-item.bp3-disabled{ background-color:inherit; cursor:not-allowed; color:rgba(92, 112, 128, 0.6); } .bp3-dark .bp3-menu-item{ color:inherit; } .bp3-dark .bp3-menu-item:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-menu-item{ background-color:rgba(138, 155, 168, 0.15); color:inherit; } .bp3-dark .bp3-menu-item.bp3-disabled{ background-color:inherit; color:rgba(167, 182, 194, 0.6); } .bp3-menu-item.bp3-intent-primary{ color:#106ba3; } .bp3-menu-item.bp3-intent-primary .bp3-icon{ color:inherit; } .bp3-menu-item.bp3-intent-primary::before, .bp3-menu-item.bp3-intent-primary::after, .bp3-menu-item.bp3-intent-primary .bp3-menu-item-label{ color:#106ba3; } .bp3-menu-item.bp3-intent-primary:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item, .bp3-menu-item.bp3-intent-primary.bp3-active{ background-color:#137cbd; } .bp3-menu-item.bp3-intent-primary:active{ background-color:#106ba3; } .bp3-menu-item.bp3-intent-primary:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item, .bp3-menu-item.bp3-intent-primary:hover::before, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item::before, .bp3-menu-item.bp3-intent-primary:hover::after, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item::after, .bp3-menu-item.bp3-intent-primary:hover .bp3-menu-item-label, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item .bp3-menu-item-label, .bp3-menu-item.bp3-intent-primary:active, .bp3-menu-item.bp3-intent-primary:active::before, .bp3-menu-item.bp3-intent-primary:active::after, .bp3-menu-item.bp3-intent-primary:active .bp3-menu-item-label, .bp3-menu-item.bp3-intent-primary.bp3-active, .bp3-menu-item.bp3-intent-primary.bp3-active::before, .bp3-menu-item.bp3-intent-primary.bp3-active::after, .bp3-menu-item.bp3-intent-primary.bp3-active .bp3-menu-item-label{ color:#ffffff; } .bp3-menu-item.bp3-intent-success{ color:#0d8050; } .bp3-menu-item.bp3-intent-success .bp3-icon{ color:inherit; } .bp3-menu-item.bp3-intent-success::before, .bp3-menu-item.bp3-intent-success::after, .bp3-menu-item.bp3-intent-success .bp3-menu-item-label{ color:#0d8050; } .bp3-menu-item.bp3-intent-success:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item, .bp3-menu-item.bp3-intent-success.bp3-active{ background-color:#0f9960; } .bp3-menu-item.bp3-intent-success:active{ background-color:#0d8050; } .bp3-menu-item.bp3-intent-success:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item, .bp3-menu-item.bp3-intent-success:hover::before, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item::before, .bp3-menu-item.bp3-intent-success:hover::after, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item::after, .bp3-menu-item.bp3-intent-success:hover .bp3-menu-item-label, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item .bp3-menu-item-label, .bp3-menu-item.bp3-intent-success:active, .bp3-menu-item.bp3-intent-success:active::before, .bp3-menu-item.bp3-intent-success:active::after, .bp3-menu-item.bp3-intent-success:active .bp3-menu-item-label, .bp3-menu-item.bp3-intent-success.bp3-active, .bp3-menu-item.bp3-intent-success.bp3-active::before, .bp3-menu-item.bp3-intent-success.bp3-active::after, .bp3-menu-item.bp3-intent-success.bp3-active .bp3-menu-item-label{ color:#ffffff; } .bp3-menu-item.bp3-intent-warning{ color:#bf7326; } .bp3-menu-item.bp3-intent-warning .bp3-icon{ color:inherit; } .bp3-menu-item.bp3-intent-warning::before, .bp3-menu-item.bp3-intent-warning::after, .bp3-menu-item.bp3-intent-warning .bp3-menu-item-label{ color:#bf7326; } .bp3-menu-item.bp3-intent-warning:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item, .bp3-menu-item.bp3-intent-warning.bp3-active{ background-color:#d9822b; } .bp3-menu-item.bp3-intent-warning:active{ background-color:#bf7326; } .bp3-menu-item.bp3-intent-warning:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item, .bp3-menu-item.bp3-intent-warning:hover::before, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item::before, .bp3-menu-item.bp3-intent-warning:hover::after, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item::after, .bp3-menu-item.bp3-intent-warning:hover .bp3-menu-item-label, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item .bp3-menu-item-label, .bp3-menu-item.bp3-intent-warning:active, .bp3-menu-item.bp3-intent-warning:active::before, .bp3-menu-item.bp3-intent-warning:active::after, .bp3-menu-item.bp3-intent-warning:active .bp3-menu-item-label, .bp3-menu-item.bp3-intent-warning.bp3-active, .bp3-menu-item.bp3-intent-warning.bp3-active::before, .bp3-menu-item.bp3-intent-warning.bp3-active::after, .bp3-menu-item.bp3-intent-warning.bp3-active .bp3-menu-item-label{ color:#ffffff; } .bp3-menu-item.bp3-intent-danger{ color:#c23030; } .bp3-menu-item.bp3-intent-danger .bp3-icon{ color:inherit; } .bp3-menu-item.bp3-intent-danger::before, .bp3-menu-item.bp3-intent-danger::after, .bp3-menu-item.bp3-intent-danger .bp3-menu-item-label{ color:#c23030; } .bp3-menu-item.bp3-intent-danger:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item, .bp3-menu-item.bp3-intent-danger.bp3-active{ background-color:#db3737; } .bp3-menu-item.bp3-intent-danger:active{ background-color:#c23030; } .bp3-menu-item.bp3-intent-danger:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item, .bp3-menu-item.bp3-intent-danger:hover::before, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item::before, .bp3-menu-item.bp3-intent-danger:hover::after, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item::after, .bp3-menu-item.bp3-intent-danger:hover .bp3-menu-item-label, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item .bp3-menu-item-label, .bp3-menu-item.bp3-intent-danger:active, .bp3-menu-item.bp3-intent-danger:active::before, .bp3-menu-item.bp3-intent-danger:active::after, .bp3-menu-item.bp3-intent-danger:active .bp3-menu-item-label, .bp3-menu-item.bp3-intent-danger.bp3-active, .bp3-menu-item.bp3-intent-danger.bp3-active::before, .bp3-menu-item.bp3-intent-danger.bp3-active::after, .bp3-menu-item.bp3-intent-danger.bp3-active .bp3-menu-item-label{ color:#ffffff; } .bp3-menu-item::before{ line-height:1; font-family:\"Icons16\", sans-serif; font-size:16px; font-weight:400; font-style:normal; -moz-osx-font-smoothing:grayscale; -webkit-font-smoothing:antialiased; margin-right:7px; } .bp3-menu-item::before, .bp3-menu-item > .bp3-icon{ margin-top:2px; color:#5c7080; } .bp3-menu-item .bp3-menu-item-label{ color:#5c7080; } .bp3-menu-item:hover, .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-menu-item{ color:inherit; } .bp3-menu-item.bp3-active, .bp3-menu-item:active{ background-color:rgba(115, 134, 148, 0.3); } .bp3-menu-item.bp3-disabled{ outline:none !important; background-color:inherit !important; cursor:not-allowed !important; color:rgba(92, 112, 128, 0.6) !important; } .bp3-menu-item.bp3-disabled::before, .bp3-menu-item.bp3-disabled > .bp3-icon, .bp3-menu-item.bp3-disabled .bp3-menu-item-label{ color:rgba(92, 112, 128, 0.6) !important; } .bp3-large .bp3-menu-item{ padding:9px 7px; line-height:22px; font-size:16px; } .bp3-large .bp3-menu-item .bp3-icon{ margin-top:3px; } .bp3-large .bp3-menu-item::before{ line-height:1; font-family:\"Icons20\", sans-serif; font-size:20px; font-weight:400; font-style:normal; -moz-osx-font-smoothing:grayscale; -webkit-font-smoothing:antialiased; margin-top:1px; margin-right:10px; } button.bp3-menu-item{ border:none; background:none; width:100%; text-align:left; } .bp3-menu-header{ display:block; margin:5px; border-top:1px solid rgba(16, 22, 26, 0.15); cursor:default; padding-left:2px; } .bp3-dark .bp3-menu-header{ border-top-color:rgba(255, 255, 255, 0.15); } .bp3-menu-header:first-of-type{ border-top:none; } .bp3-menu-header > h6{ color:#182026; font-weight:600; overflow:hidden; text-overflow:ellipsis; white-space:nowrap; word-wrap:normal; margin:0; padding:10px 7px 0 1px; line-height:17px; } .bp3-dark .bp3-menu-header > h6{ color:#f5f8fa; } .bp3-menu-header:first-of-type > h6{ padding-top:0; } .bp3-large .bp3-menu-header > h6{ padding-top:15px; padding-bottom:5px; font-size:18px; } .bp3-large .bp3-menu-header:first-of-type > h6{ padding-top:0; } .bp3-dark .bp3-menu{ background:#30404d; color:#f5f8fa; } .bp3-dark .bp3-menu-item.bp3-intent-primary{ color:#48aff0; } .bp3-dark .bp3-menu-item.bp3-intent-primary .bp3-icon{ color:inherit; } .bp3-dark .bp3-menu-item.bp3-intent-primary::before, .bp3-dark .bp3-menu-item.bp3-intent-primary::after, .bp3-dark .bp3-menu-item.bp3-intent-primary .bp3-menu-item-label{ color:#48aff0; } .bp3-dark .bp3-menu-item.bp3-intent-primary:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item, .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active{ background-color:#137cbd; } .bp3-dark .bp3-menu-item.bp3-intent-primary:active{ background-color:#106ba3; } .bp3-dark .bp3-menu-item.bp3-intent-primary:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item, .bp3-dark .bp3-menu-item.bp3-intent-primary:hover::before, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item::before, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item::before, .bp3-dark .bp3-menu-item.bp3-intent-primary:hover::after, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item::after, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item::after, .bp3-dark .bp3-menu-item.bp3-intent-primary:hover .bp3-menu-item-label, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item .bp3-menu-item-label, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-primary.bp3-menu-item .bp3-menu-item-label, .bp3-dark .bp3-menu-item.bp3-intent-primary:active, .bp3-dark .bp3-menu-item.bp3-intent-primary:active::before, .bp3-dark .bp3-menu-item.bp3-intent-primary:active::after, .bp3-dark .bp3-menu-item.bp3-intent-primary:active .bp3-menu-item-label, .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active, .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active::before, .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active::after, .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active .bp3-menu-item-label{ color:#ffffff; } .bp3-dark .bp3-menu-item.bp3-intent-success{ color:#3dcc91; } .bp3-dark .bp3-menu-item.bp3-intent-success .bp3-icon{ color:inherit; } .bp3-dark .bp3-menu-item.bp3-intent-success::before, .bp3-dark .bp3-menu-item.bp3-intent-success::after, .bp3-dark .bp3-menu-item.bp3-intent-success .bp3-menu-item-label{ color:#3dcc91; } .bp3-dark .bp3-menu-item.bp3-intent-success:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item, .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active{ background-color:#0f9960; } .bp3-dark .bp3-menu-item.bp3-intent-success:active{ background-color:#0d8050; } .bp3-dark .bp3-menu-item.bp3-intent-success:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item, .bp3-dark .bp3-menu-item.bp3-intent-success:hover::before, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item::before, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item::before, .bp3-dark .bp3-menu-item.bp3-intent-success:hover::after, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item::after, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item::after, .bp3-dark .bp3-menu-item.bp3-intent-success:hover .bp3-menu-item-label, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item .bp3-menu-item-label, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-success.bp3-menu-item .bp3-menu-item-label, .bp3-dark .bp3-menu-item.bp3-intent-success:active, .bp3-dark .bp3-menu-item.bp3-intent-success:active::before, .bp3-dark .bp3-menu-item.bp3-intent-success:active::after, .bp3-dark .bp3-menu-item.bp3-intent-success:active .bp3-menu-item-label, .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active, .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active::before, .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active::after, .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active .bp3-menu-item-label{ color:#ffffff; } .bp3-dark .bp3-menu-item.bp3-intent-warning{ color:#ffb366; } .bp3-dark .bp3-menu-item.bp3-intent-warning .bp3-icon{ color:inherit; } .bp3-dark .bp3-menu-item.bp3-intent-warning::before, .bp3-dark .bp3-menu-item.bp3-intent-warning::after, .bp3-dark .bp3-menu-item.bp3-intent-warning .bp3-menu-item-label{ color:#ffb366; } .bp3-dark .bp3-menu-item.bp3-intent-warning:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item, .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active{ background-color:#d9822b; } .bp3-dark .bp3-menu-item.bp3-intent-warning:active{ background-color:#bf7326; } .bp3-dark .bp3-menu-item.bp3-intent-warning:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item, .bp3-dark .bp3-menu-item.bp3-intent-warning:hover::before, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item::before, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item::before, .bp3-dark .bp3-menu-item.bp3-intent-warning:hover::after, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item::after, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item::after, .bp3-dark .bp3-menu-item.bp3-intent-warning:hover .bp3-menu-item-label, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item .bp3-menu-item-label, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-warning.bp3-menu-item .bp3-menu-item-label, .bp3-dark .bp3-menu-item.bp3-intent-warning:active, .bp3-dark .bp3-menu-item.bp3-intent-warning:active::before, .bp3-dark .bp3-menu-item.bp3-intent-warning:active::after, .bp3-dark .bp3-menu-item.bp3-intent-warning:active .bp3-menu-item-label, .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active, .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active::before, .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active::after, .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active .bp3-menu-item-label{ color:#ffffff; } .bp3-dark .bp3-menu-item.bp3-intent-danger{ color:#ff7373; } .bp3-dark .bp3-menu-item.bp3-intent-danger .bp3-icon{ color:inherit; } .bp3-dark .bp3-menu-item.bp3-intent-danger::before, .bp3-dark .bp3-menu-item.bp3-intent-danger::after, .bp3-dark .bp3-menu-item.bp3-intent-danger .bp3-menu-item-label{ color:#ff7373; } .bp3-dark .bp3-menu-item.bp3-intent-danger:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item, .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active{ background-color:#db3737; } .bp3-dark .bp3-menu-item.bp3-intent-danger:active{ background-color:#c23030; } .bp3-dark .bp3-menu-item.bp3-intent-danger:hover, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item, .bp3-dark .bp3-menu-item.bp3-intent-danger:hover::before, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item::before, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item::before, .bp3-dark .bp3-menu-item.bp3-intent-danger:hover::after, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item::after, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item::after, .bp3-dark .bp3-menu-item.bp3-intent-danger:hover .bp3-menu-item-label, .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item .bp3-menu-item-label, .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open > .bp3-intent-danger.bp3-menu-item .bp3-menu-item-label, .bp3-dark .bp3-menu-item.bp3-intent-danger:active, .bp3-dark .bp3-menu-item.bp3-intent-danger:active::before, .bp3-dark .bp3-menu-item.bp3-intent-danger:active::after, .bp3-dark .bp3-menu-item.bp3-intent-danger:active .bp3-menu-item-label, .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active, .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active::before, .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active::after, .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active .bp3-menu-item-label{ color:#ffffff; } .bp3-dark .bp3-menu-item::before, .bp3-dark .bp3-menu-item > .bp3-icon{ color:#a7b6c2; } .bp3-dark .bp3-menu-item .bp3-menu-item-label{ color:#a7b6c2; } .bp3-dark .bp3-menu-item.bp3-active, .bp3-dark .bp3-menu-item:active{ background-color:rgba(138, 155, 168, 0.3); } .bp3-dark .bp3-menu-item.bp3-disabled{ color:rgba(167, 182, 194, 0.6) !important; } .bp3-dark .bp3-menu-item.bp3-disabled::before, .bp3-dark .bp3-menu-item.bp3-disabled > .bp3-icon, .bp3-dark .bp3-menu-item.bp3-disabled .bp3-menu-item-label{ color:rgba(167, 182, 194, 0.6) !important; } .bp3-dark .bp3-menu-divider, .bp3-dark .bp3-menu-header{ border-color:rgba(255, 255, 255, 0.15); } .bp3-dark .bp3-menu-header > h6{ color:#f5f8fa; } .bp3-label .bp3-menu{ margin-top:5px; } .bp3-navbar{ position:relative; z-index:10; -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.2); background-color:#ffffff; width:100%; height:50px; padding:0 15px; } .bp3-navbar.bp3-dark, .bp3-dark .bp3-navbar{ background-color:#394b59; } .bp3-navbar.bp3-dark{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-navbar{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 0 0 rgba(16, 22, 26, 0), 0 1px 1px rgba(16, 22, 26, 0.4); } .bp3-navbar.bp3-fixed-top{ position:fixed; top:0; right:0; left:0; } .bp3-navbar-heading{ margin-right:15px; font-size:16px; } .bp3-navbar-group{ display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-align:center; -ms-flex-align:center; align-items:center; height:50px; } .bp3-navbar-group.bp3-align-left{ float:left; } .bp3-navbar-group.bp3-align-right{ float:right; } .bp3-navbar-divider{ margin:0 10px; border-left:1px solid rgba(16, 22, 26, 0.15); height:20px; } .bp3-dark .bp3-navbar-divider{ border-left-color:rgba(255, 255, 255, 0.15); } .bp3-non-ideal-state{ display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-orient:vertical; -webkit-box-direction:normal; -ms-flex-direction:column; flex-direction:column; -webkit-box-align:center; -ms-flex-align:center; align-items:center; -webkit-box-pack:center; -ms-flex-pack:center; justify-content:center; width:100%; height:100%; text-align:center; } .bp3-non-ideal-state > *{ -webkit-box-flex:0; -ms-flex-positive:0; flex-grow:0; -ms-flex-negative:0; flex-shrink:0; } .bp3-non-ideal-state > .bp3-fill{ -webkit-box-flex:1; -ms-flex-positive:1; flex-grow:1; -ms-flex-negative:1; flex-shrink:1; } .bp3-non-ideal-state::before, .bp3-non-ideal-state > *{ margin-bottom:20px; } .bp3-non-ideal-state:empty::before, .bp3-non-ideal-state > :last-child{ margin-bottom:0; } .bp3-non-ideal-state > *{ max-width:400px; } .bp3-non-ideal-state-visual{ color:rgba(92, 112, 128, 0.6); font-size:60px; } .bp3-dark .bp3-non-ideal-state-visual{ color:rgba(167, 182, 194, 0.6); } .bp3-overflow-list{ display:-webkit-box; display:-ms-flexbox; display:flex; -ms-flex-wrap:nowrap; flex-wrap:nowrap; min-width:0; } .bp3-overflow-list-spacer{ -ms-flex-negative:1; flex-shrink:1; width:1px; } body.bp3-overlay-open{ overflow:hidden; } .bp3-overlay{ position:static; top:0; right:0; bottom:0; left:0; z-index:20; } .bp3-overlay:not(.bp3-overlay-open){ pointer-events:none; } .bp3-overlay.bp3-overlay-container{ position:fixed; overflow:hidden; } .bp3-overlay.bp3-overlay-container.bp3-overlay-inline{ position:absolute; } .bp3-overlay.bp3-overlay-scroll-container{ position:fixed; overflow:auto; } .bp3-overlay.bp3-overlay-scroll-container.bp3-overlay-inline{ position:absolute; } .bp3-overlay.bp3-overlay-inline{ display:inline; overflow:visible; } .bp3-overlay-content{ position:fixed; z-index:20; } .bp3-overlay-inline .bp3-overlay-content, .bp3-overlay-scroll-container .bp3-overlay-content{ position:absolute; } .bp3-overlay-backdrop{ position:fixed; top:0; right:0; bottom:0; left:0; opacity:1; z-index:20; background-color:rgba(16, 22, 26, 0.7); overflow:auto; -webkit-user-select:none; -moz-user-select:none; -ms-user-select:none; user-select:none; } .bp3-overlay-backdrop.bp3-overlay-enter, .bp3-overlay-backdrop.bp3-overlay-appear{ opacity:0; } .bp3-overlay-backdrop.bp3-overlay-enter-active, .bp3-overlay-backdrop.bp3-overlay-appear-active{ opacity:1; -webkit-transition-property:opacity; transition-property:opacity; -webkit-transition-duration:200ms; transition-duration:200ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-transition-delay:0; transition-delay:0; } .bp3-overlay-backdrop.bp3-overlay-exit{ opacity:1; } .bp3-overlay-backdrop.bp3-overlay-exit-active{ opacity:0; -webkit-transition-property:opacity; transition-property:opacity; -webkit-transition-duration:200ms; transition-duration:200ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-transition-delay:0; transition-delay:0; } .bp3-overlay-backdrop:focus{ outline:none; } .bp3-overlay-inline .bp3-overlay-backdrop{ position:absolute; } .bp3-panel-stack{ position:relative; overflow:hidden; } .bp3-panel-stack-header{ display:-webkit-box; display:-ms-flexbox; display:flex; -ms-flex-negative:0; flex-shrink:0; -webkit-box-align:center; -ms-flex-align:center; align-items:center; z-index:1; -webkit-box-shadow:0 1px rgba(16, 22, 26, 0.15); box-shadow:0 1px rgba(16, 22, 26, 0.15); height:30px; } .bp3-dark .bp3-panel-stack-header{ -webkit-box-shadow:0 1px rgba(255, 255, 255, 0.15); box-shadow:0 1px rgba(255, 255, 255, 0.15); } .bp3-panel-stack-header > span{ display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-flex:1; -ms-flex:1; flex:1; -webkit-box-align:stretch; -ms-flex-align:stretch; align-items:stretch; } .bp3-panel-stack-header .bp3-heading{ margin:0 5px; } .bp3-button.bp3-panel-stack-header-back{ margin-left:5px; padding-left:0; white-space:nowrap; } .bp3-button.bp3-panel-stack-header-back .bp3-icon{ margin:0 2px; } .bp3-panel-stack-view{ position:absolute; top:0; right:0; bottom:0; left:0; display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-orient:vertical; -webkit-box-direction:normal; -ms-flex-direction:column; flex-direction:column; margin-right:-1px; border-right:1px solid rgba(16, 22, 26, 0.15); background-color:#ffffff; overflow-y:auto; } .bp3-dark .bp3-panel-stack-view{ background-color:#30404d; } .bp3-panel-stack-push .bp3-panel-stack-enter, .bp3-panel-stack-push .bp3-panel-stack-appear{ -webkit-transform:translateX(100%); transform:translateX(100%); opacity:0; } .bp3-panel-stack-push .bp3-panel-stack-enter-active, .bp3-panel-stack-push .bp3-panel-stack-appear-active{ -webkit-transform:translate(0%); transform:translate(0%); opacity:1; -webkit-transition-property:opacity, -webkit-transform; transition-property:opacity, -webkit-transform; transition-property:transform, opacity; transition-property:transform, opacity, -webkit-transform; -webkit-transition-duration:400ms; transition-duration:400ms; -webkit-transition-timing-function:ease; transition-timing-function:ease; -webkit-transition-delay:0; transition-delay:0; } .bp3-panel-stack-push .bp3-panel-stack-exit{ -webkit-transform:translate(0%); transform:translate(0%); opacity:1; } .bp3-panel-stack-push .bp3-panel-stack-exit-active{ -webkit-transform:translateX(-50%); transform:translateX(-50%); opacity:0; -webkit-transition-property:opacity, -webkit-transform; transition-property:opacity, -webkit-transform; transition-property:transform, opacity; transition-property:transform, opacity, -webkit-transform; -webkit-transition-duration:400ms; transition-duration:400ms; -webkit-transition-timing-function:ease; transition-timing-function:ease; -webkit-transition-delay:0; transition-delay:0; } .bp3-panel-stack-pop .bp3-panel-stack-enter, .bp3-panel-stack-pop .bp3-panel-stack-appear{ -webkit-transform:translateX(-50%); transform:translateX(-50%); opacity:0; } .bp3-panel-stack-pop .bp3-panel-stack-enter-active, .bp3-panel-stack-pop .bp3-panel-stack-appear-active{ -webkit-transform:translate(0%); transform:translate(0%); opacity:1; -webkit-transition-property:opacity, -webkit-transform; transition-property:opacity, -webkit-transform; transition-property:transform, opacity; transition-property:transform, opacity, -webkit-transform; -webkit-transition-duration:400ms; transition-duration:400ms; -webkit-transition-timing-function:ease; transition-timing-function:ease; -webkit-transition-delay:0; transition-delay:0; } .bp3-panel-stack-pop .bp3-panel-stack-exit{ -webkit-transform:translate(0%); transform:translate(0%); opacity:1; } .bp3-panel-stack-pop .bp3-panel-stack-exit-active{ -webkit-transform:translateX(100%); transform:translateX(100%); opacity:0; -webkit-transition-property:opacity, -webkit-transform; transition-property:opacity, -webkit-transform; transition-property:transform, opacity; transition-property:transform, opacity, -webkit-transform; -webkit-transition-duration:400ms; transition-duration:400ms; -webkit-transition-timing-function:ease; transition-timing-function:ease; -webkit-transition-delay:0; transition-delay:0; } .bp3-popover{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2); -webkit-transform:scale(1); transform:scale(1); display:inline-block; z-index:20; border-radius:3px; } .bp3-popover .bp3-popover-arrow{ position:absolute; width:30px; height:30px; } .bp3-popover .bp3-popover-arrow::before{ margin:5px; width:20px; height:20px; } .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top > .bp3-popover{ margin-top:-17px; margin-bottom:17px; } .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top > .bp3-popover > .bp3-popover-arrow{ bottom:-11px; } .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top > .bp3-popover > .bp3-popover-arrow svg{ -webkit-transform:rotate(-90deg); transform:rotate(-90deg); } .bp3-tether-element-attached-left.bp3-tether-target-attached-right > .bp3-popover{ margin-left:17px; } .bp3-tether-element-attached-left.bp3-tether-target-attached-right > .bp3-popover > .bp3-popover-arrow{ left:-11px; } .bp3-tether-element-attached-left.bp3-tether-target-attached-right > .bp3-popover > .bp3-popover-arrow svg{ -webkit-transform:rotate(0); transform:rotate(0); } .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom > .bp3-popover{ margin-top:17px; } .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom > .bp3-popover > .bp3-popover-arrow{ top:-11px; } .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom > .bp3-popover > .bp3-popover-arrow svg{ -webkit-transform:rotate(90deg); transform:rotate(90deg); } .bp3-tether-element-attached-right.bp3-tether-target-attached-left > .bp3-popover{ margin-right:17px; margin-left:-17px; } .bp3-tether-element-attached-right.bp3-tether-target-attached-left > .bp3-popover > .bp3-popover-arrow{ right:-11px; } .bp3-tether-element-attached-right.bp3-tether-target-attached-left > .bp3-popover > .bp3-popover-arrow svg{ -webkit-transform:rotate(180deg); transform:rotate(180deg); } .bp3-tether-element-attached-middle > .bp3-popover > .bp3-popover-arrow{ top:50%; -webkit-transform:translateY(-50%); transform:translateY(-50%); } .bp3-tether-element-attached-center > .bp3-popover > .bp3-popover-arrow{ right:50%; -webkit-transform:translateX(50%); transform:translateX(50%); } .bp3-tether-element-attached-top.bp3-tether-target-attached-top > .bp3-popover > .bp3-popover-arrow{ top:-0.3934px; } .bp3-tether-element-attached-right.bp3-tether-target-attached-right > .bp3-popover > .bp3-popover-arrow{ right:-0.3934px; } .bp3-tether-element-attached-left.bp3-tether-target-attached-left > .bp3-popover > .bp3-popover-arrow{ left:-0.3934px; } .bp3-tether-element-attached-bottom.bp3-tether-target-attached-bottom > .bp3-popover > .bp3-popover-arrow{ bottom:-0.3934px; } .bp3-tether-element-attached-top.bp3-tether-element-attached-left > .bp3-popover{ -webkit-transform-origin:top left; transform-origin:top left; } .bp3-tether-element-attached-top.bp3-tether-element-attached-center > .bp3-popover{ -webkit-transform-origin:top center; transform-origin:top center; } .bp3-tether-element-attached-top.bp3-tether-element-attached-right > .bp3-popover{ -webkit-transform-origin:top right; transform-origin:top right; } .bp3-tether-element-attached-middle.bp3-tether-element-attached-left > .bp3-popover{ -webkit-transform-origin:center left; transform-origin:center left; } .bp3-tether-element-attached-middle.bp3-tether-element-attached-center > .bp3-popover{ -webkit-transform-origin:center center; transform-origin:center center; } .bp3-tether-element-attached-middle.bp3-tether-element-attached-right > .bp3-popover{ -webkit-transform-origin:center right; transform-origin:center right; } .bp3-tether-element-attached-bottom.bp3-tether-element-attached-left > .bp3-popover{ -webkit-transform-origin:bottom left; transform-origin:bottom left; } .bp3-tether-element-attached-bottom.bp3-tether-element-attached-center > .bp3-popover{ -webkit-transform-origin:bottom center; transform-origin:bottom center; } .bp3-tether-element-attached-bottom.bp3-tether-element-attached-right > .bp3-popover{ -webkit-transform-origin:bottom right; transform-origin:bottom right; } .bp3-popover .bp3-popover-content{ background:#ffffff; color:inherit; } .bp3-popover .bp3-popover-arrow::before{ -webkit-box-shadow:1px 1px 6px rgba(16, 22, 26, 0.2); box-shadow:1px 1px 6px rgba(16, 22, 26, 0.2); } .bp3-popover .bp3-popover-arrow-border{ fill:#10161a; fill-opacity:0.1; } .bp3-popover .bp3-popover-arrow-fill{ fill:#ffffff; } .bp3-popover-enter > .bp3-popover, .bp3-popover-appear > .bp3-popover{ -webkit-transform:scale(0.3); transform:scale(0.3); } .bp3-popover-enter-active > .bp3-popover, .bp3-popover-appear-active > .bp3-popover{ -webkit-transform:scale(1); transform:scale(1); -webkit-transition-property:-webkit-transform; transition-property:-webkit-transform; transition-property:transform; transition-property:transform, -webkit-transform; -webkit-transition-duration:300ms; transition-duration:300ms; -webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11); transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11); -webkit-transition-delay:0; transition-delay:0; } .bp3-popover-exit > .bp3-popover{ -webkit-transform:scale(1); transform:scale(1); } .bp3-popover-exit-active > .bp3-popover{ -webkit-transform:scale(0.3); transform:scale(0.3); -webkit-transition-property:-webkit-transform; transition-property:-webkit-transform; transition-property:transform; transition-property:transform, -webkit-transform; -webkit-transition-duration:300ms; transition-duration:300ms; -webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11); transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11); -webkit-transition-delay:0; transition-delay:0; } .bp3-popover .bp3-popover-content{ position:relative; border-radius:3px; } .bp3-popover.bp3-popover-content-sizing .bp3-popover-content{ max-width:350px; padding:20px; } .bp3-popover-target + .bp3-overlay .bp3-popover.bp3-popover-content-sizing{ width:350px; } .bp3-popover.bp3-minimal{ margin:0 !important; } .bp3-popover.bp3-minimal .bp3-popover-arrow{ display:none; } .bp3-popover.bp3-minimal.bp3-popover{ -webkit-transform:scale(1); transform:scale(1); } .bp3-popover-enter > .bp3-popover.bp3-minimal.bp3-popover, .bp3-popover-appear > .bp3-popover.bp3-minimal.bp3-popover{ -webkit-transform:scale(1); transform:scale(1); } .bp3-popover-enter-active > .bp3-popover.bp3-minimal.bp3-popover, .bp3-popover-appear-active > .bp3-popover.bp3-minimal.bp3-popover{ -webkit-transform:scale(1); transform:scale(1); -webkit-transition-property:-webkit-transform; transition-property:-webkit-transform; transition-property:transform; transition-property:transform, -webkit-transform; -webkit-transition-duration:100ms; transition-duration:100ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-transition-delay:0; transition-delay:0; } .bp3-popover-exit > .bp3-popover.bp3-minimal.bp3-popover{ -webkit-transform:scale(1); transform:scale(1); } .bp3-popover-exit-active > .bp3-popover.bp3-minimal.bp3-popover{ -webkit-transform:scale(1); transform:scale(1); -webkit-transition-property:-webkit-transform; transition-property:-webkit-transform; transition-property:transform; transition-property:transform, -webkit-transform; -webkit-transition-duration:100ms; transition-duration:100ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-transition-delay:0; transition-delay:0; } .bp3-popover.bp3-dark, .bp3-dark .bp3-popover{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); } .bp3-popover.bp3-dark .bp3-popover-content, .bp3-dark .bp3-popover .bp3-popover-content{ background:#30404d; color:inherit; } .bp3-popover.bp3-dark .bp3-popover-arrow::before, .bp3-dark .bp3-popover .bp3-popover-arrow::before{ -webkit-box-shadow:1px 1px 6px rgba(16, 22, 26, 0.4); box-shadow:1px 1px 6px rgba(16, 22, 26, 0.4); } .bp3-popover.bp3-dark .bp3-popover-arrow-border, .bp3-dark .bp3-popover .bp3-popover-arrow-border{ fill:#10161a; fill-opacity:0.2; } .bp3-popover.bp3-dark .bp3-popover-arrow-fill, .bp3-dark .bp3-popover .bp3-popover-arrow-fill{ fill:#30404d; } .bp3-popover-arrow::before{ display:block; position:absolute; -webkit-transform:rotate(45deg); transform:rotate(45deg); border-radius:2px; content:\"\"; } .bp3-tether-pinned .bp3-popover-arrow{ display:none; } .bp3-popover-backdrop{ background:rgba(255, 255, 255, 0); } .bp3-transition-container{ opacity:1; display:-webkit-box; display:-ms-flexbox; display:flex; z-index:20; } .bp3-transition-container.bp3-popover-enter, .bp3-transition-container.bp3-popover-appear{ opacity:0; } .bp3-transition-container.bp3-popover-enter-active, .bp3-transition-container.bp3-popover-appear-active{ opacity:1; -webkit-transition-property:opacity; transition-property:opacity; -webkit-transition-duration:100ms; transition-duration:100ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-transition-delay:0; transition-delay:0; } .bp3-transition-container.bp3-popover-exit{ opacity:1; } .bp3-transition-container.bp3-popover-exit-active{ opacity:0; -webkit-transition-property:opacity; transition-property:opacity; -webkit-transition-duration:100ms; transition-duration:100ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-transition-delay:0; transition-delay:0; } .bp3-transition-container:focus{ outline:none; } .bp3-transition-container.bp3-popover-leave .bp3-popover-content{ pointer-events:none; } .bp3-transition-container[data-x-out-of-boundaries]{ display:none; } span.bp3-popover-target{ display:inline-block; } .bp3-popover-wrapper.bp3-fill{ width:100%; } .bp3-portal{ position:absolute; top:0; right:0; left:0; } @-webkit-keyframes linear-progress-bar-stripes{ from{ background-position:0 0; } to{ background-position:30px 0; } } @keyframes linear-progress-bar-stripes{ from{ background-position:0 0; } to{ background-position:30px 0; } } .bp3-progress-bar{ display:block; position:relative; border-radius:40px; background:rgba(92, 112, 128, 0.2); width:100%; height:8px; overflow:hidden; } .bp3-progress-bar .bp3-progress-meter{ position:absolute; border-radius:40px; background:linear-gradient(-45deg, rgba(255, 255, 255, 0.2) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.2) 50%, rgba(255, 255, 255, 0.2) 75%, transparent 75%); background-color:rgba(92, 112, 128, 0.8); background-size:30px 30px; width:100%; height:100%; -webkit-transition:width 200ms cubic-bezier(0.4, 1, 0.75, 0.9); transition:width 200ms cubic-bezier(0.4, 1, 0.75, 0.9); } .bp3-progress-bar:not(.bp3-no-animation):not(.bp3-no-stripes) .bp3-progress-meter{ animation:linear-progress-bar-stripes 300ms linear infinite reverse; } .bp3-progress-bar.bp3-no-stripes .bp3-progress-meter{ background-image:none; } .bp3-dark .bp3-progress-bar{ background:rgba(16, 22, 26, 0.5); } .bp3-dark .bp3-progress-bar .bp3-progress-meter{ background-color:#8a9ba8; } .bp3-progress-bar.bp3-intent-primary .bp3-progress-meter{ background-color:#137cbd; } .bp3-progress-bar.bp3-intent-success .bp3-progress-meter{ background-color:#0f9960; } .bp3-progress-bar.bp3-intent-warning .bp3-progress-meter{ background-color:#d9822b; } .bp3-progress-bar.bp3-intent-danger .bp3-progress-meter{ background-color:#db3737; } @-webkit-keyframes skeleton-glow{ from{ border-color:rgba(206, 217, 224, 0.2); background:rgba(206, 217, 224, 0.2); } to{ border-color:rgba(92, 112, 128, 0.2); background:rgba(92, 112, 128, 0.2); } } @keyframes skeleton-glow{ from{ border-color:rgba(206, 217, 224, 0.2); background:rgba(206, 217, 224, 0.2); } to{ border-color:rgba(92, 112, 128, 0.2); background:rgba(92, 112, 128, 0.2); } } .bp3-skeleton{ border-color:rgba(206, 217, 224, 0.2) !important; border-radius:2px; -webkit-box-shadow:none !important; box-shadow:none !important; background:rgba(206, 217, 224, 0.2); background-clip:padding-box !important; cursor:default; color:transparent !important; -webkit-animation:1000ms linear infinite alternate skeleton-glow; animation:1000ms linear infinite alternate skeleton-glow; pointer-events:none; -webkit-user-select:none; -moz-user-select:none; -ms-user-select:none; user-select:none; } .bp3-skeleton::before, .bp3-skeleton::after, .bp3-skeleton *{ visibility:hidden !important; } .bp3-slider{ width:100%; min-width:150px; height:40px; position:relative; outline:none; cursor:default; -webkit-user-select:none; -moz-user-select:none; -ms-user-select:none; user-select:none; } .bp3-slider:hover{ cursor:pointer; } .bp3-slider:active{ cursor:-webkit-grabbing; cursor:grabbing; } .bp3-slider.bp3-disabled{ opacity:0.5; cursor:not-allowed; } .bp3-slider.bp3-slider-unlabeled{ height:16px; } .bp3-slider-track, .bp3-slider-progress{ top:5px; right:0; left:0; height:6px; position:absolute; } .bp3-slider-track{ border-radius:3px; overflow:hidden; } .bp3-slider-progress{ background:rgba(92, 112, 128, 0.2); } .bp3-dark .bp3-slider-progress{ background:rgba(16, 22, 26, 0.5); } .bp3-slider-progress.bp3-intent-primary{ background-color:#137cbd; } .bp3-slider-progress.bp3-intent-success{ background-color:#0f9960; } .bp3-slider-progress.bp3-intent-warning{ background-color:#d9822b; } .bp3-slider-progress.bp3-intent-danger{ background-color:#db3737; } .bp3-slider-handle{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); background-color:#f5f8fa; background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0))); background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0)); color:#182026; position:absolute; top:0; left:0; border-radius:3px; -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 1px 1px rgba(16, 22, 26, 0.2); cursor:pointer; width:16px; height:16px; } .bp3-slider-handle:hover{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); background-clip:padding-box; background-color:#ebf1f5; } .bp3-slider-handle:active, .bp3-slider-handle.bp3-active{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2); background-color:#d8e1e8; background-image:none; } .bp3-slider-handle:disabled, .bp3-slider-handle.bp3-disabled{ outline:none; -webkit-box-shadow:none; box-shadow:none; background-color:rgba(206, 217, 224, 0.5); background-image:none; cursor:not-allowed; color:rgba(92, 112, 128, 0.6); } .bp3-slider-handle:disabled.bp3-active, .bp3-slider-handle:disabled.bp3-active:hover, .bp3-slider-handle.bp3-disabled.bp3-active, .bp3-slider-handle.bp3-disabled.bp3-active:hover{ background:rgba(206, 217, 224, 0.7); } .bp3-slider-handle:focus{ z-index:1; } .bp3-slider-handle:hover{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 -1px 0 rgba(16, 22, 26, 0.1); background-clip:padding-box; background-color:#ebf1f5; z-index:2; -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 1px 1px rgba(16, 22, 26, 0.2); cursor:-webkit-grab; cursor:grab; } .bp3-slider-handle.bp3-active{ -webkit-box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2); box-shadow:inset 0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 2px rgba(16, 22, 26, 0.2); background-color:#d8e1e8; background-image:none; -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 1px rgba(16, 22, 26, 0.1); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), inset 0 1px 1px rgba(16, 22, 26, 0.1); cursor:-webkit-grabbing; cursor:grabbing; } .bp3-disabled .bp3-slider-handle{ -webkit-box-shadow:none; box-shadow:none; background:#bfccd6; pointer-events:none; } .bp3-dark .bp3-slider-handle{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); background-color:#394b59; background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0))); background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0)); color:#f5f8fa; } .bp3-dark .bp3-slider-handle:hover, .bp3-dark .bp3-slider-handle:active, .bp3-dark .bp3-slider-handle.bp3-active{ color:#f5f8fa; } .bp3-dark .bp3-slider-handle:hover{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.4); background-color:#30404d; } .bp3-dark .bp3-slider-handle:active, .bp3-dark .bp3-slider-handle.bp3-active{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.6), inset 0 1px 2px rgba(16, 22, 26, 0.2); background-color:#202b33; background-image:none; } .bp3-dark .bp3-slider-handle:disabled, .bp3-dark .bp3-slider-handle.bp3-disabled{ -webkit-box-shadow:none; box-shadow:none; background-color:rgba(57, 75, 89, 0.5); background-image:none; color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-slider-handle:disabled.bp3-active, .bp3-dark .bp3-slider-handle.bp3-disabled.bp3-active{ background:rgba(57, 75, 89, 0.7); } .bp3-dark .bp3-slider-handle .bp3-button-spinner .bp3-spinner-head{ background:rgba(16, 22, 26, 0.5); stroke:#8a9ba8; } .bp3-dark .bp3-slider-handle, .bp3-dark .bp3-slider-handle:hover{ background-color:#394b59; } .bp3-dark .bp3-slider-handle.bp3-active{ background-color:#293742; } .bp3-dark .bp3-disabled .bp3-slider-handle{ border-color:#5c7080; -webkit-box-shadow:none; box-shadow:none; background:#5c7080; } .bp3-slider-handle .bp3-slider-label{ margin-left:8px; border-radius:3px; -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2); background:#394b59; color:#f5f8fa; } .bp3-dark .bp3-slider-handle .bp3-slider-label{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); background:#e1e8ed; color:#394b59; } .bp3-disabled .bp3-slider-handle .bp3-slider-label{ -webkit-box-shadow:none; box-shadow:none; } .bp3-slider-handle.bp3-start, .bp3-slider-handle.bp3-end{ width:8px; } .bp3-slider-handle.bp3-start{ border-top-right-radius:0; border-bottom-right-radius:0; } .bp3-slider-handle.bp3-end{ margin-left:8px; border-top-left-radius:0; border-bottom-left-radius:0; } .bp3-slider-handle.bp3-end .bp3-slider-label{ margin-left:0; } .bp3-slider-label{ -webkit-transform:translate(-50%, 20px); transform:translate(-50%, 20px); display:inline-block; position:absolute; padding:2px 5px; vertical-align:top; line-height:1; font-size:12px; } .bp3-slider.bp3-vertical{ width:40px; min-width:40px; height:150px; } .bp3-slider.bp3-vertical .bp3-slider-track, .bp3-slider.bp3-vertical .bp3-slider-progress{ top:0; bottom:0; left:5px; width:6px; height:auto; } .bp3-slider.bp3-vertical .bp3-slider-progress{ top:auto; } .bp3-slider.bp3-vertical .bp3-slider-label{ -webkit-transform:translate(20px, 50%); transform:translate(20px, 50%); } .bp3-slider.bp3-vertical .bp3-slider-handle{ top:auto; } .bp3-slider.bp3-vertical .bp3-slider-handle .bp3-slider-label{ margin-top:-8px; margin-left:0; } .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-end, .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-start{ margin-left:0; width:16px; height:8px; } .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-start{ border-top-left-radius:0; border-bottom-right-radius:3px; } .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-start .bp3-slider-label{ -webkit-transform:translate(20px); transform:translate(20px); } .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-end{ margin-bottom:8px; border-top-left-radius:3px; border-bottom-left-radius:0; border-bottom-right-radius:0; } @-webkit-keyframes pt-spinner-animation{ from{ -webkit-transform:rotate(0deg); transform:rotate(0deg); } to{ -webkit-transform:rotate(360deg); transform:rotate(360deg); } } @keyframes pt-spinner-animation{ from{ -webkit-transform:rotate(0deg); transform:rotate(0deg); } to{ -webkit-transform:rotate(360deg); transform:rotate(360deg); } } .bp3-spinner{ display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-align:center; -ms-flex-align:center; align-items:center; -webkit-box-pack:center; -ms-flex-pack:center; justify-content:center; overflow:visible; vertical-align:middle; } .bp3-spinner svg{ display:block; } .bp3-spinner path{ fill-opacity:0; } .bp3-spinner .bp3-spinner-head{ -webkit-transform-origin:center; transform-origin:center; -webkit-transition:stroke-dashoffset 200ms cubic-bezier(0.4, 1, 0.75, 0.9); transition:stroke-dashoffset 200ms cubic-bezier(0.4, 1, 0.75, 0.9); stroke:rgba(92, 112, 128, 0.8); stroke-linecap:round; } .bp3-spinner .bp3-spinner-track{ stroke:rgba(92, 112, 128, 0.2); } .bp3-spinner-animation{ -webkit-animation:pt-spinner-animation 500ms linear infinite; animation:pt-spinner-animation 500ms linear infinite; } .bp3-no-spin > .bp3-spinner-animation{ -webkit-animation:none; animation:none; } .bp3-dark .bp3-spinner .bp3-spinner-head{ stroke:#8a9ba8; } .bp3-dark .bp3-spinner .bp3-spinner-track{ stroke:rgba(16, 22, 26, 0.5); } .bp3-spinner.bp3-intent-primary .bp3-spinner-head{ stroke:#137cbd; } .bp3-spinner.bp3-intent-success .bp3-spinner-head{ stroke:#0f9960; } .bp3-spinner.bp3-intent-warning .bp3-spinner-head{ stroke:#d9822b; } .bp3-spinner.bp3-intent-danger .bp3-spinner-head{ stroke:#db3737; } .bp3-tabs.bp3-vertical{ display:-webkit-box; display:-ms-flexbox; display:flex; } .bp3-tabs.bp3-vertical > .bp3-tab-list{ -webkit-box-orient:vertical; -webkit-box-direction:normal; -ms-flex-direction:column; flex-direction:column; -webkit-box-align:start; -ms-flex-align:start; align-items:flex-start; } .bp3-tabs.bp3-vertical > .bp3-tab-list .bp3-tab{ border-radius:3px; width:100%; padding:0 10px; } .bp3-tabs.bp3-vertical > .bp3-tab-list .bp3-tab[aria-selected=\"true\"]{ -webkit-box-shadow:none; box-shadow:none; background-color:rgba(19, 124, 189, 0.2); } .bp3-tabs.bp3-vertical > .bp3-tab-list .bp3-tab-indicator-wrapper .bp3-tab-indicator{ top:0; right:0; bottom:0; left:0; border-radius:3px; background-color:rgba(19, 124, 189, 0.2); height:auto; } .bp3-tabs.bp3-vertical > .bp3-tab-panel{ margin-top:0; padding-left:20px; } .bp3-tab-list{ display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-flex:0; -ms-flex:0 0 auto; flex:0 0 auto; -webkit-box-align:end; -ms-flex-align:end; align-items:flex-end; position:relative; margin:0; border:none; padding:0; list-style:none; } .bp3-tab-list > *:not(:last-child){ margin-right:20px; } .bp3-tab{ overflow:hidden; text-overflow:ellipsis; white-space:nowrap; word-wrap:normal; -webkit-box-flex:0; -ms-flex:0 0 auto; flex:0 0 auto; position:relative; cursor:pointer; max-width:100%; vertical-align:top; line-height:30px; color:#182026; font-size:14px; } .bp3-tab a{ display:block; text-decoration:none; color:inherit; } .bp3-tab-indicator-wrapper ~ .bp3-tab{ -webkit-box-shadow:none !important; box-shadow:none !important; background-color:transparent !important; } .bp3-tab[aria-disabled=\"true\"]{ cursor:not-allowed; color:rgba(92, 112, 128, 0.6); } .bp3-tab[aria-selected=\"true\"]{ border-radius:0; -webkit-box-shadow:inset 0 -3px 0 #106ba3; box-shadow:inset 0 -3px 0 #106ba3; } .bp3-tab[aria-selected=\"true\"], .bp3-tab:not([aria-disabled=\"true\"]):hover{ color:#106ba3; } .bp3-tab:focus{ -moz-outline-radius:0; } .bp3-large > .bp3-tab{ line-height:40px; font-size:16px; } .bp3-tab-panel{ margin-top:20px; } .bp3-tab-panel[aria-hidden=\"true\"]{ display:none; } .bp3-tab-indicator-wrapper{ position:absolute; top:0; left:0; -webkit-transform:translateX(0), translateY(0); transform:translateX(0), translateY(0); -webkit-transition:height, width, -webkit-transform; transition:height, width, -webkit-transform; transition:height, transform, width; transition:height, transform, width, -webkit-transform; -webkit-transition-duration:200ms; transition-duration:200ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); pointer-events:none; } .bp3-tab-indicator-wrapper .bp3-tab-indicator{ position:absolute; right:0; bottom:0; left:0; background-color:#106ba3; height:3px; } .bp3-tab-indicator-wrapper.bp3-no-animation{ -webkit-transition:none; transition:none; } .bp3-dark .bp3-tab{ color:#f5f8fa; } .bp3-dark .bp3-tab[aria-disabled=\"true\"]{ color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-tab[aria-selected=\"true\"]{ -webkit-box-shadow:inset 0 -3px 0 #48aff0; box-shadow:inset 0 -3px 0 #48aff0; } .bp3-dark .bp3-tab[aria-selected=\"true\"], .bp3-dark .bp3-tab:not([aria-disabled=\"true\"]):hover{ color:#48aff0; } .bp3-dark .bp3-tab-indicator{ background-color:#48aff0; } .bp3-flex-expander{ -webkit-box-flex:1; -ms-flex:1 1; flex:1 1; } .bp3-tag{ display:-webkit-inline-box; display:-ms-inline-flexbox; display:inline-flex; -webkit-box-orient:horizontal; -webkit-box-direction:normal; -ms-flex-direction:row; flex-direction:row; -webkit-box-align:center; -ms-flex-align:center; align-items:center; position:relative; border:none; border-radius:3px; -webkit-box-shadow:none; box-shadow:none; background-color:#5c7080; min-width:20px; max-width:100%; min-height:20px; padding:2px 6px; line-height:16px; color:#f5f8fa; font-size:12px; } .bp3-tag.bp3-interactive{ cursor:pointer; } .bp3-tag.bp3-interactive:hover{ background-color:rgba(92, 112, 128, 0.85); } .bp3-tag.bp3-interactive.bp3-active, .bp3-tag.bp3-interactive:active{ background-color:rgba(92, 112, 128, 0.7); } .bp3-tag > *{ -webkit-box-flex:0; -ms-flex-positive:0; flex-grow:0; -ms-flex-negative:0; flex-shrink:0; } .bp3-tag > .bp3-fill{ -webkit-box-flex:1; -ms-flex-positive:1; flex-grow:1; -ms-flex-negative:1; flex-shrink:1; } .bp3-tag::before, .bp3-tag > *{ margin-right:4px; } .bp3-tag:empty::before, .bp3-tag > :last-child{ margin-right:0; } .bp3-tag:focus{ outline:rgba(19, 124, 189, 0.6) auto 2px; outline-offset:0; -moz-outline-radius:6px; } .bp3-tag.bp3-round{ border-radius:30px; padding-right:8px; padding-left:8px; } .bp3-dark .bp3-tag{ background-color:#bfccd6; color:#182026; } .bp3-dark .bp3-tag.bp3-interactive{ cursor:pointer; } .bp3-dark .bp3-tag.bp3-interactive:hover{ background-color:rgba(191, 204, 214, 0.85); } .bp3-dark .bp3-tag.bp3-interactive.bp3-active, .bp3-dark .bp3-tag.bp3-interactive:active{ background-color:rgba(191, 204, 214, 0.7); } .bp3-dark .bp3-tag > .bp3-icon, .bp3-dark .bp3-tag .bp3-icon-standard, .bp3-dark .bp3-tag .bp3-icon-large{ fill:currentColor; } .bp3-tag > .bp3-icon, .bp3-tag .bp3-icon-standard, .bp3-tag .bp3-icon-large{ fill:#ffffff; } .bp3-tag.bp3-large, .bp3-large .bp3-tag{ min-width:30px; min-height:30px; padding:0 10px; line-height:20px; font-size:14px; } .bp3-tag.bp3-large::before, .bp3-tag.bp3-large > *, .bp3-large .bp3-tag::before, .bp3-large .bp3-tag > *{ margin-right:7px; } .bp3-tag.bp3-large:empty::before, .bp3-tag.bp3-large > :last-child, .bp3-large .bp3-tag:empty::before, .bp3-large .bp3-tag > :last-child{ margin-right:0; } .bp3-tag.bp3-large.bp3-round, .bp3-large .bp3-tag.bp3-round{ padding-right:12px; padding-left:12px; } .bp3-tag.bp3-intent-primary{ background:#137cbd; color:#ffffff; } .bp3-tag.bp3-intent-primary.bp3-interactive{ cursor:pointer; } .bp3-tag.bp3-intent-primary.bp3-interactive:hover{ background-color:rgba(19, 124, 189, 0.85); } .bp3-tag.bp3-intent-primary.bp3-interactive.bp3-active, .bp3-tag.bp3-intent-primary.bp3-interactive:active{ background-color:rgba(19, 124, 189, 0.7); } .bp3-tag.bp3-intent-success{ background:#0f9960; color:#ffffff; } .bp3-tag.bp3-intent-success.bp3-interactive{ cursor:pointer; } .bp3-tag.bp3-intent-success.bp3-interactive:hover{ background-color:rgba(15, 153, 96, 0.85); } .bp3-tag.bp3-intent-success.bp3-interactive.bp3-active, .bp3-tag.bp3-intent-success.bp3-interactive:active{ background-color:rgba(15, 153, 96, 0.7); } .bp3-tag.bp3-intent-warning{ background:#d9822b; color:#ffffff; } .bp3-tag.bp3-intent-warning.bp3-interactive{ cursor:pointer; } .bp3-tag.bp3-intent-warning.bp3-interactive:hover{ background-color:rgba(217, 130, 43, 0.85); } .bp3-tag.bp3-intent-warning.bp3-interactive.bp3-active, .bp3-tag.bp3-intent-warning.bp3-interactive:active{ background-color:rgba(217, 130, 43, 0.7); } .bp3-tag.bp3-intent-danger{ background:#db3737; color:#ffffff; } .bp3-tag.bp3-intent-danger.bp3-interactive{ cursor:pointer; } .bp3-tag.bp3-intent-danger.bp3-interactive:hover{ background-color:rgba(219, 55, 55, 0.85); } .bp3-tag.bp3-intent-danger.bp3-interactive.bp3-active, .bp3-tag.bp3-intent-danger.bp3-interactive:active{ background-color:rgba(219, 55, 55, 0.7); } .bp3-tag.bp3-fill{ display:-webkit-box; display:-ms-flexbox; display:flex; width:100%; } .bp3-tag.bp3-minimal > .bp3-icon, .bp3-tag.bp3-minimal .bp3-icon-standard, .bp3-tag.bp3-minimal .bp3-icon-large{ fill:#5c7080; } .bp3-tag.bp3-minimal:not([class*=\"bp3-intent-\"]){ background-color:rgba(138, 155, 168, 0.2); color:#182026; } .bp3-tag.bp3-minimal:not([class*=\"bp3-intent-\"]).bp3-interactive{ cursor:pointer; } .bp3-tag.bp3-minimal:not([class*=\"bp3-intent-\"]).bp3-interactive:hover{ background-color:rgba(92, 112, 128, 0.3); } .bp3-tag.bp3-minimal:not([class*=\"bp3-intent-\"]).bp3-interactive.bp3-active, .bp3-tag.bp3-minimal:not([class*=\"bp3-intent-\"]).bp3-interactive:active{ background-color:rgba(92, 112, 128, 0.4); } .bp3-dark .bp3-tag.bp3-minimal:not([class*=\"bp3-intent-\"]){ color:#f5f8fa; } .bp3-dark .bp3-tag.bp3-minimal:not([class*=\"bp3-intent-\"]).bp3-interactive{ cursor:pointer; } .bp3-dark .bp3-tag.bp3-minimal:not([class*=\"bp3-intent-\"]).bp3-interactive:hover{ background-color:rgba(191, 204, 214, 0.3); } .bp3-dark .bp3-tag.bp3-minimal:not([class*=\"bp3-intent-\"]).bp3-interactive.bp3-active, .bp3-dark .bp3-tag.bp3-minimal:not([class*=\"bp3-intent-\"]).bp3-interactive:active{ background-color:rgba(191, 204, 214, 0.4); } .bp3-dark .bp3-tag.bp3-minimal:not([class*=\"bp3-intent-\"]) > .bp3-icon, .bp3-dark .bp3-tag.bp3-minimal:not([class*=\"bp3-intent-\"]) .bp3-icon-standard, .bp3-dark .bp3-tag.bp3-minimal:not([class*=\"bp3-intent-\"]) .bp3-icon-large{ fill:#a7b6c2; } .bp3-tag.bp3-minimal.bp3-intent-primary{ background-color:rgba(19, 124, 189, 0.15); color:#106ba3; } .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive{ cursor:pointer; } .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive:hover{ background-color:rgba(19, 124, 189, 0.25); } .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive.bp3-active, .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive:active{ background-color:rgba(19, 124, 189, 0.35); } .bp3-tag.bp3-minimal.bp3-intent-primary > .bp3-icon, .bp3-tag.bp3-minimal.bp3-intent-primary .bp3-icon-standard, .bp3-tag.bp3-minimal.bp3-intent-primary .bp3-icon-large{ fill:#137cbd; } .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary{ background-color:rgba(19, 124, 189, 0.25); color:#48aff0; } .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive{ cursor:pointer; } .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive:hover{ background-color:rgba(19, 124, 189, 0.35); } .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive.bp3-active, .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive:active{ background-color:rgba(19, 124, 189, 0.45); } .bp3-tag.bp3-minimal.bp3-intent-success{ background-color:rgba(15, 153, 96, 0.15); color:#0d8050; } .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive{ cursor:pointer; } .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive:hover{ background-color:rgba(15, 153, 96, 0.25); } .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive.bp3-active, .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive:active{ background-color:rgba(15, 153, 96, 0.35); } .bp3-tag.bp3-minimal.bp3-intent-success > .bp3-icon, .bp3-tag.bp3-minimal.bp3-intent-success .bp3-icon-standard, .bp3-tag.bp3-minimal.bp3-intent-success .bp3-icon-large{ fill:#0f9960; } .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success{ background-color:rgba(15, 153, 96, 0.25); color:#3dcc91; } .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive{ cursor:pointer; } .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive:hover{ background-color:rgba(15, 153, 96, 0.35); } .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive.bp3-active, .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive:active{ background-color:rgba(15, 153, 96, 0.45); } .bp3-tag.bp3-minimal.bp3-intent-warning{ background-color:rgba(217, 130, 43, 0.15); color:#bf7326; } .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive{ cursor:pointer; } .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive:hover{ background-color:rgba(217, 130, 43, 0.25); } .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive.bp3-active, .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive:active{ background-color:rgba(217, 130, 43, 0.35); } .bp3-tag.bp3-minimal.bp3-intent-warning > .bp3-icon, .bp3-tag.bp3-minimal.bp3-intent-warning .bp3-icon-standard, .bp3-tag.bp3-minimal.bp3-intent-warning .bp3-icon-large{ fill:#d9822b; } .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning{ background-color:rgba(217, 130, 43, 0.25); color:#ffb366; } .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive{ cursor:pointer; } .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive:hover{ background-color:rgba(217, 130, 43, 0.35); } .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive.bp3-active, .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive:active{ background-color:rgba(217, 130, 43, 0.45); } .bp3-tag.bp3-minimal.bp3-intent-danger{ background-color:rgba(219, 55, 55, 0.15); color:#c23030; } .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive{ cursor:pointer; } .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive:hover{ background-color:rgba(219, 55, 55, 0.25); } .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive.bp3-active, .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive:active{ background-color:rgba(219, 55, 55, 0.35); } .bp3-tag.bp3-minimal.bp3-intent-danger > .bp3-icon, .bp3-tag.bp3-minimal.bp3-intent-danger .bp3-icon-standard, .bp3-tag.bp3-minimal.bp3-intent-danger .bp3-icon-large{ fill:#db3737; } .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger{ background-color:rgba(219, 55, 55, 0.25); color:#ff7373; } .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive{ cursor:pointer; } .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive:hover{ background-color:rgba(219, 55, 55, 0.35); } .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive.bp3-active, .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive:active{ background-color:rgba(219, 55, 55, 0.45); } .bp3-tag-remove{ display:-webkit-box; display:-ms-flexbox; display:flex; opacity:0.5; margin-top:-2px; margin-right:-6px !important; margin-bottom:-2px; border:none; background:none; cursor:pointer; padding:2px; padding-left:0; color:inherit; } .bp3-tag-remove:hover{ opacity:0.8; background:none; text-decoration:none; } .bp3-tag-remove:active{ opacity:1; } .bp3-tag-remove:empty::before{ line-height:1; font-family:\"Icons16\", sans-serif; font-size:16px; font-weight:400; font-style:normal; -moz-osx-font-smoothing:grayscale; -webkit-font-smoothing:antialiased; content:\"\ue6d7\"; } .bp3-large .bp3-tag-remove{ margin-right:-10px !important; padding:5px; padding-left:0; } .bp3-large .bp3-tag-remove:empty::before{ line-height:1; font-family:\"Icons20\", sans-serif; font-size:20px; font-weight:400; font-style:normal; } .bp3-tag-input{ display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-orient:horizontal; -webkit-box-direction:normal; -ms-flex-direction:row; flex-direction:row; -webkit-box-align:start; -ms-flex-align:start; align-items:flex-start; cursor:text; height:auto; min-height:30px; padding-right:0; padding-left:5px; line-height:inherit; } .bp3-tag-input > *{ -webkit-box-flex:0; -ms-flex-positive:0; flex-grow:0; -ms-flex-negative:0; flex-shrink:0; } .bp3-tag-input > .bp3-tag-input-values{ -webkit-box-flex:1; -ms-flex-positive:1; flex-grow:1; -ms-flex-negative:1; flex-shrink:1; } .bp3-tag-input .bp3-tag-input-icon{ margin-top:7px; margin-right:7px; margin-left:2px; color:#5c7080; } .bp3-tag-input .bp3-tag-input-values{ display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-orient:horizontal; -webkit-box-direction:normal; -ms-flex-direction:row; flex-direction:row; -ms-flex-wrap:wrap; flex-wrap:wrap; -webkit-box-align:center; -ms-flex-align:center; align-items:center; -ms-flex-item-align:stretch; align-self:stretch; margin-top:5px; margin-right:7px; min-width:0; } .bp3-tag-input .bp3-tag-input-values > *{ -webkit-box-flex:0; -ms-flex-positive:0; flex-grow:0; -ms-flex-negative:0; flex-shrink:0; } .bp3-tag-input .bp3-tag-input-values > .bp3-fill{ -webkit-box-flex:1; -ms-flex-positive:1; flex-grow:1; -ms-flex-negative:1; flex-shrink:1; } .bp3-tag-input .bp3-tag-input-values::before, .bp3-tag-input .bp3-tag-input-values > *{ margin-right:5px; } .bp3-tag-input .bp3-tag-input-values:empty::before, .bp3-tag-input .bp3-tag-input-values > :last-child{ margin-right:0; } .bp3-tag-input .bp3-tag-input-values:first-child .bp3-input-ghost:first-child{ padding-left:5px; } .bp3-tag-input .bp3-tag-input-values > *{ margin-bottom:5px; } .bp3-tag-input .bp3-tag{ overflow-wrap:break-word; } .bp3-tag-input .bp3-tag.bp3-active{ outline:rgba(19, 124, 189, 0.6) auto 2px; outline-offset:0; -moz-outline-radius:6px; } .bp3-tag-input .bp3-input-ghost{ -webkit-box-flex:1; -ms-flex:1 1 auto; flex:1 1 auto; width:80px; line-height:20px; } .bp3-tag-input .bp3-input-ghost:disabled, .bp3-tag-input .bp3-input-ghost.bp3-disabled{ cursor:not-allowed; } .bp3-tag-input .bp3-button, .bp3-tag-input .bp3-spinner{ margin:3px; margin-left:0; } .bp3-tag-input .bp3-button{ min-width:24px; min-height:24px; padding:0 7px; } .bp3-tag-input.bp3-large{ height:auto; min-height:40px; } .bp3-tag-input.bp3-large::before, .bp3-tag-input.bp3-large > *{ margin-right:10px; } .bp3-tag-input.bp3-large:empty::before, .bp3-tag-input.bp3-large > :last-child{ margin-right:0; } .bp3-tag-input.bp3-large .bp3-tag-input-icon{ margin-top:10px; margin-left:5px; } .bp3-tag-input.bp3-large .bp3-input-ghost{ line-height:30px; } .bp3-tag-input.bp3-large .bp3-button{ min-width:30px; min-height:30px; padding:5px 10px; margin:5px; margin-left:0; } .bp3-tag-input.bp3-large .bp3-spinner{ margin:8px; margin-left:0; } .bp3-tag-input.bp3-active{ -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); background-color:#ffffff; } .bp3-tag-input.bp3-active.bp3-intent-primary{ -webkit-box-shadow:0 0 0 1px #106ba3, 0 0 0 3px rgba(16, 107, 163, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px #106ba3, 0 0 0 3px rgba(16, 107, 163, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-tag-input.bp3-active.bp3-intent-success{ -webkit-box-shadow:0 0 0 1px #0d8050, 0 0 0 3px rgba(13, 128, 80, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px #0d8050, 0 0 0 3px rgba(13, 128, 80, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-tag-input.bp3-active.bp3-intent-warning{ -webkit-box-shadow:0 0 0 1px #bf7326, 0 0 0 3px rgba(191, 115, 38, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px #bf7326, 0 0 0 3px rgba(191, 115, 38, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-tag-input.bp3-active.bp3-intent-danger{ -webkit-box-shadow:0 0 0 1px #c23030, 0 0 0 3px rgba(194, 48, 48, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px #c23030, 0 0 0 3px rgba(194, 48, 48, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.2); } .bp3-dark .bp3-tag-input .bp3-tag-input-icon, .bp3-tag-input.bp3-dark .bp3-tag-input-icon{ color:#a7b6c2; } .bp3-dark .bp3-tag-input .bp3-input-ghost, .bp3-tag-input.bp3-dark .bp3-input-ghost{ color:#f5f8fa; } .bp3-dark .bp3-tag-input .bp3-input-ghost::-webkit-input-placeholder, .bp3-tag-input.bp3-dark .bp3-input-ghost::-webkit-input-placeholder{ color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-tag-input .bp3-input-ghost::-moz-placeholder, .bp3-tag-input.bp3-dark .bp3-input-ghost::-moz-placeholder{ color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-tag-input .bp3-input-ghost:-ms-input-placeholder, .bp3-tag-input.bp3-dark .bp3-input-ghost:-ms-input-placeholder{ color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-tag-input .bp3-input-ghost::-ms-input-placeholder, .bp3-tag-input.bp3-dark .bp3-input-ghost::-ms-input-placeholder{ color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-tag-input .bp3-input-ghost::placeholder, .bp3-tag-input.bp3-dark .bp3-input-ghost::placeholder{ color:rgba(167, 182, 194, 0.6); } .bp3-dark .bp3-tag-input.bp3-active, .bp3-tag-input.bp3-dark.bp3-active{ -webkit-box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px #137cbd, 0 0 0 1px #137cbd, 0 0 0 3px rgba(19, 124, 189, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); background-color:rgba(16, 22, 26, 0.3); } .bp3-dark .bp3-tag-input.bp3-active.bp3-intent-primary, .bp3-tag-input.bp3-dark.bp3-active.bp3-intent-primary{ -webkit-box-shadow:0 0 0 1px #106ba3, 0 0 0 3px rgba(16, 107, 163, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px #106ba3, 0 0 0 3px rgba(16, 107, 163, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-tag-input.bp3-active.bp3-intent-success, .bp3-tag-input.bp3-dark.bp3-active.bp3-intent-success{ -webkit-box-shadow:0 0 0 1px #0d8050, 0 0 0 3px rgba(13, 128, 80, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px #0d8050, 0 0 0 3px rgba(13, 128, 80, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-tag-input.bp3-active.bp3-intent-warning, .bp3-tag-input.bp3-dark.bp3-active.bp3-intent-warning{ -webkit-box-shadow:0 0 0 1px #bf7326, 0 0 0 3px rgba(191, 115, 38, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px #bf7326, 0 0 0 3px rgba(191, 115, 38, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); } .bp3-dark .bp3-tag-input.bp3-active.bp3-intent-danger, .bp3-tag-input.bp3-dark.bp3-active.bp3-intent-danger{ -webkit-box-shadow:0 0 0 1px #c23030, 0 0 0 3px rgba(194, 48, 48, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px #c23030, 0 0 0 3px rgba(194, 48, 48, 0.3), inset 0 0 0 1px rgba(16, 22, 26, 0.3), inset 0 1px 1px rgba(16, 22, 26, 0.4); } .bp3-input-ghost{ border:none; -webkit-box-shadow:none; box-shadow:none; background:none; padding:0; } .bp3-input-ghost::-webkit-input-placeholder{ opacity:1; color:rgba(92, 112, 128, 0.6); } .bp3-input-ghost::-moz-placeholder{ opacity:1; color:rgba(92, 112, 128, 0.6); } .bp3-input-ghost:-ms-input-placeholder{ opacity:1; color:rgba(92, 112, 128, 0.6); } .bp3-input-ghost::-ms-input-placeholder{ opacity:1; color:rgba(92, 112, 128, 0.6); } .bp3-input-ghost::placeholder{ opacity:1; color:rgba(92, 112, 128, 0.6); } .bp3-input-ghost:focus{ outline:none !important; } .bp3-toast{ display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-align:start; -ms-flex-align:start; align-items:flex-start; position:relative !important; margin:20px 0 0; border-radius:3px; -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2); background-color:#ffffff; min-width:300px; max-width:500px; pointer-events:all; } .bp3-toast.bp3-toast-enter, .bp3-toast.bp3-toast-appear{ -webkit-transform:translateY(-40px); transform:translateY(-40px); } .bp3-toast.bp3-toast-enter-active, .bp3-toast.bp3-toast-appear-active{ -webkit-transform:translateY(0); transform:translateY(0); -webkit-transition-property:-webkit-transform; transition-property:-webkit-transform; transition-property:transform; transition-property:transform, -webkit-transform; -webkit-transition-duration:300ms; transition-duration:300ms; -webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11); transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11); -webkit-transition-delay:0; transition-delay:0; } .bp3-toast.bp3-toast-enter ~ .bp3-toast, .bp3-toast.bp3-toast-appear ~ .bp3-toast{ -webkit-transform:translateY(-40px); transform:translateY(-40px); } .bp3-toast.bp3-toast-enter-active ~ .bp3-toast, .bp3-toast.bp3-toast-appear-active ~ .bp3-toast{ -webkit-transform:translateY(0); transform:translateY(0); -webkit-transition-property:-webkit-transform; transition-property:-webkit-transform; transition-property:transform; transition-property:transform, -webkit-transform; -webkit-transition-duration:300ms; transition-duration:300ms; -webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11); transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11); -webkit-transition-delay:0; transition-delay:0; } .bp3-toast.bp3-toast-exit{ opacity:1; -webkit-filter:blur(0); filter:blur(0); } .bp3-toast.bp3-toast-exit-active{ opacity:0; -webkit-filter:blur(10px); filter:blur(10px); -webkit-transition-property:opacity, -webkit-filter; transition-property:opacity, -webkit-filter; transition-property:opacity, filter; transition-property:opacity, filter, -webkit-filter; -webkit-transition-duration:300ms; transition-duration:300ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-transition-delay:0; transition-delay:0; } .bp3-toast.bp3-toast-exit ~ .bp3-toast{ -webkit-transform:translateY(0); transform:translateY(0); } .bp3-toast.bp3-toast-exit-active ~ .bp3-toast{ -webkit-transform:translateY(-40px); transform:translateY(-40px); -webkit-transition-property:-webkit-transform; transition-property:-webkit-transform; transition-property:transform; transition-property:transform, -webkit-transform; -webkit-transition-duration:100ms; transition-duration:100ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-transition-delay:50ms; transition-delay:50ms; } .bp3-toast .bp3-button-group{ -webkit-box-flex:0; -ms-flex:0 0 auto; flex:0 0 auto; padding:5px; padding-left:0; } .bp3-toast > .bp3-icon{ margin:12px; margin-right:0; color:#5c7080; } .bp3-toast.bp3-dark, .bp3-dark .bp3-toast{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); background-color:#394b59; } .bp3-toast.bp3-dark > .bp3-icon, .bp3-dark .bp3-toast > .bp3-icon{ color:#a7b6c2; } .bp3-toast[class*=\"bp3-intent-\"] a{ color:rgba(255, 255, 255, 0.7); } .bp3-toast[class*=\"bp3-intent-\"] a:hover{ color:#ffffff; } .bp3-toast[class*=\"bp3-intent-\"] > .bp3-icon{ color:#ffffff; } .bp3-toast[class*=\"bp3-intent-\"] .bp3-button, .bp3-toast[class*=\"bp3-intent-\"] .bp3-button::before, .bp3-toast[class*=\"bp3-intent-\"] .bp3-button .bp3-icon, .bp3-toast[class*=\"bp3-intent-\"] .bp3-button:active{ color:rgba(255, 255, 255, 0.7) !important; } .bp3-toast[class*=\"bp3-intent-\"] .bp3-button:focus{ outline-color:rgba(255, 255, 255, 0.5); } .bp3-toast[class*=\"bp3-intent-\"] .bp3-button:hover{ background-color:rgba(255, 255, 255, 0.15) !important; color:#ffffff !important; } .bp3-toast[class*=\"bp3-intent-\"] .bp3-button:active{ background-color:rgba(255, 255, 255, 0.3) !important; color:#ffffff !important; } .bp3-toast[class*=\"bp3-intent-\"] .bp3-button::after{ background:rgba(255, 255, 255, 0.3) !important; } .bp3-toast.bp3-intent-primary{ background-color:#137cbd; color:#ffffff; } .bp3-toast.bp3-intent-success{ background-color:#0f9960; color:#ffffff; } .bp3-toast.bp3-intent-warning{ background-color:#d9822b; color:#ffffff; } .bp3-toast.bp3-intent-danger{ background-color:#db3737; color:#ffffff; } .bp3-toast-message{ -webkit-box-flex:1; -ms-flex:1 1 auto; flex:1 1 auto; padding:11px; word-break:break-word; } .bp3-toast-container{ display:-webkit-box !important; display:-ms-flexbox !important; display:flex !important; -webkit-box-orient:vertical; -webkit-box-direction:normal; -ms-flex-direction:column; flex-direction:column; -webkit-box-align:center; -ms-flex-align:center; align-items:center; position:fixed; right:0; left:0; z-index:40; overflow:hidden; padding:0 20px 20px; pointer-events:none; } .bp3-toast-container.bp3-toast-container-top{ top:0; bottom:auto; } .bp3-toast-container.bp3-toast-container-bottom{ -webkit-box-orient:vertical; -webkit-box-direction:reverse; -ms-flex-direction:column-reverse; flex-direction:column-reverse; top:auto; bottom:0; } .bp3-toast-container.bp3-toast-container-left{ -webkit-box-align:start; -ms-flex-align:start; align-items:flex-start; } .bp3-toast-container.bp3-toast-container-right{ -webkit-box-align:end; -ms-flex-align:end; align-items:flex-end; } .bp3-toast-container-bottom .bp3-toast.bp3-toast-enter:not(.bp3-toast-enter-active), .bp3-toast-container-bottom .bp3-toast.bp3-toast-enter:not(.bp3-toast-enter-active) ~ .bp3-toast, .bp3-toast-container-bottom .bp3-toast.bp3-toast-appear:not(.bp3-toast-appear-active), .bp3-toast-container-bottom .bp3-toast.bp3-toast-appear:not(.bp3-toast-appear-active) ~ .bp3-toast, .bp3-toast-container-bottom .bp3-toast.bp3-toast-leave-active ~ .bp3-toast{ -webkit-transform:translateY(60px); transform:translateY(60px); } .bp3-tooltip{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 2px 4px rgba(16, 22, 26, 0.2), 0 8px 24px rgba(16, 22, 26, 0.2); -webkit-transform:scale(1); transform:scale(1); } .bp3-tooltip .bp3-popover-arrow{ position:absolute; width:22px; height:22px; } .bp3-tooltip .bp3-popover-arrow::before{ margin:4px; width:14px; height:14px; } .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top > .bp3-tooltip{ margin-top:-11px; margin-bottom:11px; } .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top > .bp3-tooltip > .bp3-popover-arrow{ bottom:-8px; } .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top > .bp3-tooltip > .bp3-popover-arrow svg{ -webkit-transform:rotate(-90deg); transform:rotate(-90deg); } .bp3-tether-element-attached-left.bp3-tether-target-attached-right > .bp3-tooltip{ margin-left:11px; } .bp3-tether-element-attached-left.bp3-tether-target-attached-right > .bp3-tooltip > .bp3-popover-arrow{ left:-8px; } .bp3-tether-element-attached-left.bp3-tether-target-attached-right > .bp3-tooltip > .bp3-popover-arrow svg{ -webkit-transform:rotate(0); transform:rotate(0); } .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom > .bp3-tooltip{ margin-top:11px; } .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom > .bp3-tooltip > .bp3-popover-arrow{ top:-8px; } .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom > .bp3-tooltip > .bp3-popover-arrow svg{ -webkit-transform:rotate(90deg); transform:rotate(90deg); } .bp3-tether-element-attached-right.bp3-tether-target-attached-left > .bp3-tooltip{ margin-right:11px; margin-left:-11px; } .bp3-tether-element-attached-right.bp3-tether-target-attached-left > .bp3-tooltip > .bp3-popover-arrow{ right:-8px; } .bp3-tether-element-attached-right.bp3-tether-target-attached-left > .bp3-tooltip > .bp3-popover-arrow svg{ -webkit-transform:rotate(180deg); transform:rotate(180deg); } .bp3-tether-element-attached-middle > .bp3-tooltip > .bp3-popover-arrow{ top:50%; -webkit-transform:translateY(-50%); transform:translateY(-50%); } .bp3-tether-element-attached-center > .bp3-tooltip > .bp3-popover-arrow{ right:50%; -webkit-transform:translateX(50%); transform:translateX(50%); } .bp3-tether-element-attached-top.bp3-tether-target-attached-top > .bp3-tooltip > .bp3-popover-arrow{ top:-0.22183px; } .bp3-tether-element-attached-right.bp3-tether-target-attached-right > .bp3-tooltip > .bp3-popover-arrow{ right:-0.22183px; } .bp3-tether-element-attached-left.bp3-tether-target-attached-left > .bp3-tooltip > .bp3-popover-arrow{ left:-0.22183px; } .bp3-tether-element-attached-bottom.bp3-tether-target-attached-bottom > .bp3-tooltip > .bp3-popover-arrow{ bottom:-0.22183px; } .bp3-tether-element-attached-top.bp3-tether-element-attached-left > .bp3-tooltip{ -webkit-transform-origin:top left; transform-origin:top left; } .bp3-tether-element-attached-top.bp3-tether-element-attached-center > .bp3-tooltip{ -webkit-transform-origin:top center; transform-origin:top center; } .bp3-tether-element-attached-top.bp3-tether-element-attached-right > .bp3-tooltip{ -webkit-transform-origin:top right; transform-origin:top right; } .bp3-tether-element-attached-middle.bp3-tether-element-attached-left > .bp3-tooltip{ -webkit-transform-origin:center left; transform-origin:center left; } .bp3-tether-element-attached-middle.bp3-tether-element-attached-center > .bp3-tooltip{ -webkit-transform-origin:center center; transform-origin:center center; } .bp3-tether-element-attached-middle.bp3-tether-element-attached-right > .bp3-tooltip{ -webkit-transform-origin:center right; transform-origin:center right; } .bp3-tether-element-attached-bottom.bp3-tether-element-attached-left > .bp3-tooltip{ -webkit-transform-origin:bottom left; transform-origin:bottom left; } .bp3-tether-element-attached-bottom.bp3-tether-element-attached-center > .bp3-tooltip{ -webkit-transform-origin:bottom center; transform-origin:bottom center; } .bp3-tether-element-attached-bottom.bp3-tether-element-attached-right > .bp3-tooltip{ -webkit-transform-origin:bottom right; transform-origin:bottom right; } .bp3-tooltip .bp3-popover-content{ background:#394b59; color:#f5f8fa; } .bp3-tooltip .bp3-popover-arrow::before{ -webkit-box-shadow:1px 1px 6px rgba(16, 22, 26, 0.2); box-shadow:1px 1px 6px rgba(16, 22, 26, 0.2); } .bp3-tooltip .bp3-popover-arrow-border{ fill:#10161a; fill-opacity:0.1; } .bp3-tooltip .bp3-popover-arrow-fill{ fill:#394b59; } .bp3-popover-enter > .bp3-tooltip, .bp3-popover-appear > .bp3-tooltip{ -webkit-transform:scale(0.8); transform:scale(0.8); } .bp3-popover-enter-active > .bp3-tooltip, .bp3-popover-appear-active > .bp3-tooltip{ -webkit-transform:scale(1); transform:scale(1); -webkit-transition-property:-webkit-transform; transition-property:-webkit-transform; transition-property:transform; transition-property:transform, -webkit-transform; -webkit-transition-duration:100ms; transition-duration:100ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-transition-delay:0; transition-delay:0; } .bp3-popover-exit > .bp3-tooltip{ -webkit-transform:scale(1); transform:scale(1); } .bp3-popover-exit-active > .bp3-tooltip{ -webkit-transform:scale(0.8); transform:scale(0.8); -webkit-transition-property:-webkit-transform; transition-property:-webkit-transform; transition-property:transform; transition-property:transform, -webkit-transform; -webkit-transition-duration:100ms; transition-duration:100ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-transition-delay:0; transition-delay:0; } .bp3-tooltip .bp3-popover-content{ padding:10px 12px; } .bp3-tooltip.bp3-dark, .bp3-dark .bp3-tooltip{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 2px 4px rgba(16, 22, 26, 0.4), 0 8px 24px rgba(16, 22, 26, 0.4); } .bp3-tooltip.bp3-dark .bp3-popover-content, .bp3-dark .bp3-tooltip .bp3-popover-content{ background:#e1e8ed; color:#394b59; } .bp3-tooltip.bp3-dark .bp3-popover-arrow::before, .bp3-dark .bp3-tooltip .bp3-popover-arrow::before{ -webkit-box-shadow:1px 1px 6px rgba(16, 22, 26, 0.4); box-shadow:1px 1px 6px rgba(16, 22, 26, 0.4); } .bp3-tooltip.bp3-dark .bp3-popover-arrow-border, .bp3-dark .bp3-tooltip .bp3-popover-arrow-border{ fill:#10161a; fill-opacity:0.2; } .bp3-tooltip.bp3-dark .bp3-popover-arrow-fill, .bp3-dark .bp3-tooltip .bp3-popover-arrow-fill{ fill:#e1e8ed; } .bp3-tooltip.bp3-intent-primary .bp3-popover-content{ background:#137cbd; color:#ffffff; } .bp3-tooltip.bp3-intent-primary .bp3-popover-arrow-fill{ fill:#137cbd; } .bp3-tooltip.bp3-intent-success .bp3-popover-content{ background:#0f9960; color:#ffffff; } .bp3-tooltip.bp3-intent-success .bp3-popover-arrow-fill{ fill:#0f9960; } .bp3-tooltip.bp3-intent-warning .bp3-popover-content{ background:#d9822b; color:#ffffff; } .bp3-tooltip.bp3-intent-warning .bp3-popover-arrow-fill{ fill:#d9822b; } .bp3-tooltip.bp3-intent-danger .bp3-popover-content{ background:#db3737; color:#ffffff; } .bp3-tooltip.bp3-intent-danger .bp3-popover-arrow-fill{ fill:#db3737; } .bp3-tooltip-indicator{ border-bottom:dotted 1px; cursor:help; } .bp3-tree .bp3-icon, .bp3-tree .bp3-icon-standard, .bp3-tree .bp3-icon-large{ color:#5c7080; } .bp3-tree .bp3-icon.bp3-intent-primary, .bp3-tree .bp3-icon-standard.bp3-intent-primary, .bp3-tree .bp3-icon-large.bp3-intent-primary{ color:#137cbd; } .bp3-tree .bp3-icon.bp3-intent-success, .bp3-tree .bp3-icon-standard.bp3-intent-success, .bp3-tree .bp3-icon-large.bp3-intent-success{ color:#0f9960; } .bp3-tree .bp3-icon.bp3-intent-warning, .bp3-tree .bp3-icon-standard.bp3-intent-warning, .bp3-tree .bp3-icon-large.bp3-intent-warning{ color:#d9822b; } .bp3-tree .bp3-icon.bp3-intent-danger, .bp3-tree .bp3-icon-standard.bp3-intent-danger, .bp3-tree .bp3-icon-large.bp3-intent-danger{ color:#db3737; } .bp3-tree-node-list{ margin:0; padding-left:0; list-style:none; } .bp3-tree-root{ position:relative; background-color:transparent; cursor:default; padding-left:0; } .bp3-tree-node-content-0{ padding-left:0px; } .bp3-tree-node-content-1{ padding-left:23px; } .bp3-tree-node-content-2{ padding-left:46px; } .bp3-tree-node-content-3{ padding-left:69px; } .bp3-tree-node-content-4{ padding-left:92px; } .bp3-tree-node-content-5{ padding-left:115px; } .bp3-tree-node-content-6{ padding-left:138px; } .bp3-tree-node-content-7{ padding-left:161px; } .bp3-tree-node-content-8{ padding-left:184px; } .bp3-tree-node-content-9{ padding-left:207px; } .bp3-tree-node-content-10{ padding-left:230px; } .bp3-tree-node-content-11{ padding-left:253px; } .bp3-tree-node-content-12{ padding-left:276px; } .bp3-tree-node-content-13{ padding-left:299px; } .bp3-tree-node-content-14{ padding-left:322px; } .bp3-tree-node-content-15{ padding-left:345px; } .bp3-tree-node-content-16{ padding-left:368px; } .bp3-tree-node-content-17{ padding-left:391px; } .bp3-tree-node-content-18{ padding-left:414px; } .bp3-tree-node-content-19{ padding-left:437px; } .bp3-tree-node-content-20{ padding-left:460px; } .bp3-tree-node-content{ display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-align:center; -ms-flex-align:center; align-items:center; width:100%; height:30px; padding-right:5px; } .bp3-tree-node-content:hover{ background-color:rgba(191, 204, 214, 0.4); } .bp3-tree-node-caret, .bp3-tree-node-caret-none{ min-width:30px; } .bp3-tree-node-caret{ color:#5c7080; -webkit-transform:rotate(0deg); transform:rotate(0deg); cursor:pointer; padding:7px; -webkit-transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9); transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9); transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9); transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9), -webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9); } .bp3-tree-node-caret:hover{ color:#182026; } .bp3-dark .bp3-tree-node-caret{ color:#a7b6c2; } .bp3-dark .bp3-tree-node-caret:hover{ color:#f5f8fa; } .bp3-tree-node-caret.bp3-tree-node-caret-open{ -webkit-transform:rotate(90deg); transform:rotate(90deg); } .bp3-tree-node-caret.bp3-icon-standard::before{ content:\"\ue695\"; } .bp3-tree-node-icon{ position:relative; margin-right:7px; } .bp3-tree-node-label{ overflow:hidden; text-overflow:ellipsis; white-space:nowrap; word-wrap:normal; -webkit-box-flex:1; -ms-flex:1 1 auto; flex:1 1 auto; position:relative; -webkit-user-select:none; -moz-user-select:none; -ms-user-select:none; user-select:none; } .bp3-tree-node-label span{ display:inline; } .bp3-tree-node-secondary-label{ padding:0 5px; -webkit-user-select:none; -moz-user-select:none; -ms-user-select:none; user-select:none; } .bp3-tree-node-secondary-label .bp3-popover-wrapper, .bp3-tree-node-secondary-label .bp3-popover-target{ display:-webkit-box; display:-ms-flexbox; display:flex; -webkit-box-align:center; -ms-flex-align:center; align-items:center; } .bp3-tree-node.bp3-disabled .bp3-tree-node-content{ background-color:inherit; cursor:not-allowed; color:rgba(92, 112, 128, 0.6); } .bp3-tree-node.bp3-disabled .bp3-tree-node-caret, .bp3-tree-node.bp3-disabled .bp3-tree-node-icon{ cursor:not-allowed; color:rgba(92, 112, 128, 0.6); } .bp3-tree-node.bp3-tree-node-selected > .bp3-tree-node-content{ background-color:#137cbd; } .bp3-tree-node.bp3-tree-node-selected > .bp3-tree-node-content, .bp3-tree-node.bp3-tree-node-selected > .bp3-tree-node-content .bp3-icon, .bp3-tree-node.bp3-tree-node-selected > .bp3-tree-node-content .bp3-icon-standard, .bp3-tree-node.bp3-tree-node-selected > .bp3-tree-node-content .bp3-icon-large{ color:#ffffff; } .bp3-tree-node.bp3-tree-node-selected > .bp3-tree-node-content .bp3-tree-node-caret::before{ color:rgba(255, 255, 255, 0.7); } .bp3-tree-node.bp3-tree-node-selected > .bp3-tree-node-content .bp3-tree-node-caret:hover::before{ color:#ffffff; } .bp3-dark .bp3-tree-node-content:hover{ background-color:rgba(92, 112, 128, 0.3); } .bp3-dark .bp3-tree .bp3-icon, .bp3-dark .bp3-tree .bp3-icon-standard, .bp3-dark .bp3-tree .bp3-icon-large{ color:#a7b6c2; } .bp3-dark .bp3-tree .bp3-icon.bp3-intent-primary, .bp3-dark .bp3-tree .bp3-icon-standard.bp3-intent-primary, .bp3-dark .bp3-tree .bp3-icon-large.bp3-intent-primary{ color:#137cbd; } .bp3-dark .bp3-tree .bp3-icon.bp3-intent-success, .bp3-dark .bp3-tree .bp3-icon-standard.bp3-intent-success, .bp3-dark .bp3-tree .bp3-icon-large.bp3-intent-success{ color:#0f9960; } .bp3-dark .bp3-tree .bp3-icon.bp3-intent-warning, .bp3-dark .bp3-tree .bp3-icon-standard.bp3-intent-warning, .bp3-dark .bp3-tree .bp3-icon-large.bp3-intent-warning{ color:#d9822b; } .bp3-dark .bp3-tree .bp3-icon.bp3-intent-danger, .bp3-dark .bp3-tree .bp3-icon-standard.bp3-intent-danger, .bp3-dark .bp3-tree .bp3-icon-large.bp3-intent-danger{ color:#db3737; } .bp3-dark .bp3-tree-node.bp3-tree-node-selected > .bp3-tree-node-content{ background-color:#137cbd; } /*! Copyright 2017-present Palantir Technologies, Inc. All rights reserved. Licensed under the Apache License, Version 2.0. */ .bp3-omnibar{ -webkit-filter:blur(0); filter:blur(0); opacity:1; top:20vh; left:calc(50% - 250px); z-index:21; border-radius:3px; -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 4px 8px rgba(16, 22, 26, 0.2), 0 18px 46px 6px rgba(16, 22, 26, 0.2); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.1), 0 4px 8px rgba(16, 22, 26, 0.2), 0 18px 46px 6px rgba(16, 22, 26, 0.2); background-color:#ffffff; width:500px; } .bp3-omnibar.bp3-overlay-enter, .bp3-omnibar.bp3-overlay-appear{ -webkit-filter:blur(20px); filter:blur(20px); opacity:0.2; } .bp3-omnibar.bp3-overlay-enter-active, .bp3-omnibar.bp3-overlay-appear-active{ -webkit-filter:blur(0); filter:blur(0); opacity:1; -webkit-transition-property:opacity, -webkit-filter; transition-property:opacity, -webkit-filter; transition-property:filter, opacity; transition-property:filter, opacity, -webkit-filter; -webkit-transition-duration:200ms; transition-duration:200ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-transition-delay:0; transition-delay:0; } .bp3-omnibar.bp3-overlay-exit{ -webkit-filter:blur(0); filter:blur(0); opacity:1; } .bp3-omnibar.bp3-overlay-exit-active{ -webkit-filter:blur(20px); filter:blur(20px); opacity:0.2; -webkit-transition-property:opacity, -webkit-filter; transition-property:opacity, -webkit-filter; transition-property:filter, opacity; transition-property:filter, opacity, -webkit-filter; -webkit-transition-duration:200ms; transition-duration:200ms; -webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9); -webkit-transition-delay:0; transition-delay:0; } .bp3-omnibar .bp3-input{ border-radius:0; background-color:transparent; } .bp3-omnibar .bp3-input, .bp3-omnibar .bp3-input:focus{ -webkit-box-shadow:none; box-shadow:none; } .bp3-omnibar .bp3-menu{ border-radius:0; -webkit-box-shadow:inset 0 1px 0 rgba(16, 22, 26, 0.15); box-shadow:inset 0 1px 0 rgba(16, 22, 26, 0.15); background-color:transparent; max-height:calc(60vh - 40px); overflow:auto; } .bp3-omnibar .bp3-menu:empty{ display:none; } .bp3-dark .bp3-omnibar, .bp3-omnibar.bp3-dark{ -webkit-box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 4px 8px rgba(16, 22, 26, 0.4), 0 18px 46px 6px rgba(16, 22, 26, 0.4); box-shadow:0 0 0 1px rgba(16, 22, 26, 0.2), 0 4px 8px rgba(16, 22, 26, 0.4), 0 18px 46px 6px rgba(16, 22, 26, 0.4); background-color:#30404d; } .bp3-omnibar-overlay .bp3-overlay-backdrop{ background-color:rgba(16, 22, 26, 0.2); } .bp3-select-popover .bp3-popover-content{ padding:5px; } .bp3-select-popover .bp3-input-group{ margin-bottom:0; } .bp3-select-popover .bp3-menu{ max-width:400px; max-height:300px; overflow:auto; padding:0; } .bp3-select-popover .bp3-menu:not(:first-child){ padding-top:5px; } .bp3-multi-select{ min-width:150px; } .bp3-multi-select-popover .bp3-menu{ max-width:400px; max-height:300px; overflow:auto; } .bp3-select-popover .bp3-popover-content{ padding:5px; } .bp3-select-popover .bp3-input-group{ margin-bottom:0; } .bp3-select-popover .bp3-menu{ max-width:400px; max-height:300px; overflow:auto; padding:0; } .bp3-select-popover .bp3-menu:not(:first-child){ padding-top:5px; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /* This file was auto-generated by ensureUiComponents() in @jupyterlab/buildutils */ /** * (DEPRECATED) Support for consuming icons as CSS background images */ /* Icons urls */ :root { --jp-icon-add: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDEzaC02djZoLTJ2LTZINXYtMmg2VjVoMnY2aDZ2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=); --jp-icon-bug: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwIDhoLTIuODFjLS40NS0uNzgtMS4wNy0xLjQ1LTEuODItMS45NkwxNyA0LjQxIDE1LjU5IDNsLTIuMTcgMi4xN0MxMi45NiA1LjA2IDEyLjQ5IDUgMTIgNWMtLjQ5IDAtLjk2LjA2LTEuNDEuMTdMOC40MSAzIDcgNC40MWwxLjYyIDEuNjNDNy44OCA2LjU1IDcuMjYgNy4yMiA2LjgxIDhINHYyaDIuMDljLS4wNS4zMy0uMDkuNjYtLjA5IDF2MUg0djJoMnYxYzAgLjM0LjA0LjY3LjA5IDFINHYyaDIuODFjMS4wNCAxLjc5IDIuOTcgMyA1LjE5IDNzNC4xNS0xLjIxIDUuMTktM0gyMHYtMmgtMi4wOWMuMDUtLjMzLjA5LS42Ni4wOS0xdi0xaDJ2LTJoLTJ2LTFjMC0uMzQtLjA0LS42Ny0uMDktMUgyMFY4em0tNiA4aC00di0yaDR2MnptMC00aC00di0yaDR2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=); --jp-icon-build: url(data:image/svg+xml;base64,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); --jp-icon-caret-down-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iOS45LDEzLjYgMy42LDcuNCA0LjQsNi42IDkuOSwxMi4yIDE1LjQsNi43IDE2LjEsNy40ICIvPgoJPC9nPgo8L3N2Zz4K); --jp-icon-caret-down-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNS45TDksOS43bDMuOC0zLjhsMS4yLDEuMmwtNC45LDVsLTQuOS01TDUuMiw1Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K); --jp-icon-caret-down: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNy41TDksMTEuMmwzLjgtMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=); --jp-icon-caret-left: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik0xMC44LDEyLjhMNy4xLDlsMy44LTMuOGwwLDcuNkgxMC44eiIvPgogIDwvZz4KPC9zdmc+Cg==); --jp-icon-caret-right: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik03LjIsNS4yTDEwLjksOWwtMy44LDMuOFY1LjJINy4yeiIvPgogIDwvZz4KPC9zdmc+Cg==); --jp-icon-caret-up-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iMTUuNCwxMy4zIDkuOSw3LjcgNC40LDEzLjIgMy42LDEyLjUgOS45LDYuMyAxNi4xLDEyLjYgIi8+Cgk8L2c+Cjwvc3ZnPgo=); --jp-icon-caret-up: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik01LjIsMTAuNUw5LDYuOGwzLjgsMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=); --jp-icon-case-sensitive: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWFjY2VudDIiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTcuNiw4aDAuOWwzLjUsOGgtMS4xTDEwLDE0SDZsLTAuOSwySDRMNy42LDh6IE04LDkuMUw2LjQsMTNoMy4yTDgsOS4xeiIvPgogICAgPHBhdGggZD0iTTE2LjYsOS44Yy0wLjIsMC4xLTAuNCwwLjEtMC43LDAuMWMtMC4yLDAtMC40LTAuMS0wLjYtMC4yYy0wLjEtMC4xLTAuMi0wLjQtMC4yLTAuNyBjLTAuMywwLjMtMC42LDAuNS0wLjksMC43Yy0wLjMsMC4xLTAuNywwLjItMS4xLDAuMmMtMC4zLDAtMC41LDAtMC43LTAuMWMtMC4yLTAuMS0wLjQtMC4yLTAuNi0wLjNjLTAuMi0wLjEtMC4zLTAuMy0wLjQtMC41IGMtMC4xLTAuMi0wLjEtMC40LTAuMS0wLjdjMC0wLjMsMC4xLTAuNiwwLjItMC44YzAuMS0wLjIsMC4zLTAuNCwwLjQtMC41QzEyLDcsMTIuMiw2LjksMTIuNSw2LjhjMC4yLTAuMSwwLjUtMC4xLDAuNy0wLjIgYzAuMy0wLjEsMC41LTAuMSwwLjctMC4xYzAuMiwwLDAuNC0wLjEsMC42LTAuMWMwLjIsMCwwLjMtMC4xLDAuNC0wLjJjMC4xLTAuMSwwLjItMC4yLDAuMi0wLjRjMC0xLTEuMS0xLTEuMy0xIGMtMC40LDAtMS40LDAtMS40LDEuMmgtMC45YzAtMC40LDAuMS0wLjcsMC4yLTFjMC4xLTAuMiwwLjMtMC40LDAuNS0wLjZjMC4yLTAuMiwwLjUtMC4zLDAuOC0wLjNDMTMuMyw0LDEzLjYsNCwxMy45LDQgYzAuMywwLDAuNSwwLDAuOCwwLjFjMC4zLDAsMC41LDAuMSwwLjcsMC4yYzAuMiwwLjEsMC40LDAuMywwLjUsMC41QzE2LDUsMTYsNS4yLDE2LDUuNnYyLjljMCwwLjIsMCwwLjQsMCwwLjUgYzAsMC4xLDAuMSwwLjIsMC4zLDAuMmMwLjEsMCwwLjIsMCwwLjMsMFY5Ljh6IE0xNS4yLDYuOWMtMS4yLDAuNi0zLjEsMC4yLTMuMSwxLjRjMCwxLjQsMy4xLDEsMy4xLTAuNVY2Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K); --jp-icon-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkgMTYuMTdMNC44MyAxMmwtMS40MiAxLjQxTDkgMTkgMjEgN2wtMS40MS0xLjQxeiIvPgogIDwvZz4KPC9zdmc+Cg==); --jp-icon-circle-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDJDNi40NyAyIDIgNi40NyAyIDEyczQuNDcgMTAgMTAgMTAgMTAtNC40NyAxMC0xMFMxNy41MyAyIDEyIDJ6bTAgMThjLTQuNDEgMC04LTMuNTktOC04czMuNTktOCA4LTggOCAzLjU5IDggOC0zLjU5IDgtOCA4eiIvPgogIDwvZz4KPC9zdmc+Cg==); --jp-icon-circle: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iOSIgY3k9IjkiIHI9IjgiLz4KICA8L2c+Cjwvc3ZnPgo=); --jp-icon-clear: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8bWFzayBpZD0iZG9udXRIb2xlIj4KICAgIDxyZWN0IHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgZmlsbD0id2hpdGUiIC8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSI4IiBmaWxsPSJibGFjayIvPgogIDwvbWFzaz4KCiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxyZWN0IGhlaWdodD0iMTgiIHdpZHRoPSIyIiB4PSIxMSIgeT0iMyIgdHJhbnNmb3JtPSJyb3RhdGUoMzE1LCAxMiwgMTIpIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIxMCIgbWFzaz0idXJsKCNkb251dEhvbGUpIi8+CiAgPC9nPgo8L3N2Zz4K); --jp-icon-close: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1ub25lIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIGpwLWljb24zLWhvdmVyIiBmaWxsPSJub25lIj4KICAgIDxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjExIi8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIGpwLWljb24tYWNjZW50Mi1ob3ZlciIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMTkgNi40MUwxNy41OSA1IDEyIDEwLjU5IDYuNDEgNSA1IDYuNDEgMTAuNTkgMTIgNSAxNy41OSA2LjQxIDE5IDEyIDEzLjQxIDE3LjU5IDE5IDE5IDE3LjU5IDEzLjQxIDEyeiIvPgogIDwvZz4KCiAgPGcgY2xhc3M9ImpwLWljb24tbm9uZSBqcC1pY29uLWJ1c3kiIGZpbGw9Im5vbmUiPgogICAgPGNpcmNsZSBjeD0iMTIiIGN5PSIxMiIgcj0iNyIvPgogIDwvZz4KPC9zdmc+Cg==); --jp-icon-console: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwMCAyMDAiPgogIDxnIGNsYXNzPSJqcC1pY29uLWJyYW5kMSBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMjg4RDEiPgogICAgPHBhdGggZD0iTTIwIDE5LjhoMTYwdjE1OS45SDIweiIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNmZmYiPgogICAgPHBhdGggZD0iTTEwNSAxMjcuM2g0MHYxMi44aC00MHpNNTEuMSA3N0w3NCA5OS45bC0yMy4zIDIzLjMgMTAuNSAxMC41IDIzLjMtMjMuM0w5NSA5OS45IDg0LjUgODkuNCA2MS42IDY2LjV6Ii8+CiAgPC9nPgo8L3N2Zz4K); --jp-icon-copy: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTExLjksMUgzLjJDMi40LDEsMS43LDEuNywxLjcsMi41djEwLjJoMS41VjIuNWg4LjdWMXogTTE0LjEsMy45aC04Yy0wLjgsMC0xLjUsMC43LTEuNSwxLjV2MTAuMmMwLDAuOCwwLjcsMS41LDEuNSwxLjVoOCBjMC44LDAsMS41LTAuNywxLjUtMS41VjUuNEMxNS41LDQuNiwxNC45LDMuOSwxNC4xLDMuOXogTTE0LjEsMTUuNWgtOFY1LjRoOFYxNS41eiIvPgogIDwvZz4KPC9zdmc+Cg==); --jp-icon-cut: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkuNjQgNy42NGMuMjMtLjUuMzYtMS4wNS4zNi0xLjY0IDAtMi4yMS0xLjc5LTQtNC00UzIgMy43OSAyIDZzMS43OSA0IDQgNGMuNTkgMCAxLjE0LS4xMyAxLjY0LS4zNkwxMCAxMmwtMi4zNiAyLjM2QzcuMTQgMTQuMTMgNi41OSAxNCA2IDE0Yy0yLjIxIDAtNCAxLjc5LTQgNHMxLjc5IDQgNCA0IDQtMS43OSA0LTRjMC0uNTktLjEzLTEuMTQtLjM2LTEuNjRMMTIgMTRsNyA3aDN2LTFMOS42NCA3LjY0ek02IDhjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTAgMTJjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTYtNy41Yy0uMjggMC0uNS0uMjItLjUtLjVzLjIyLS41LjUtLjUuNS4yMi41LjUtLjIyLjUtLjUuNXpNMTkgM2wtNiA2IDIgMiA3LTdWM3oiLz4KICA8L2c+Cjwvc3ZnPgo=); --jp-icon-download: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDloLTRWM0g5djZINWw3IDcgNy03ek01IDE4djJoMTR2LTJINXoiLz4KICA8L2c+Cjwvc3ZnPgo=); --jp-icon-edit: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMgMTcuMjVWMjFoMy43NUwxNy44MSA5Ljk0bC0zLjc1LTMuNzVMMyAxNy4yNXpNMjAuNzEgNy4wNGMuMzktLjM5LjM5LTEuMDIgMC0xLjQxbC0yLjM0LTIuMzRjLS4zOS0uMzktMS4wMi0uMzktMS40MSAwbC0xLjgzIDEuODMgMy43NSAzLjc1IDEuODMtMS44M3oiLz4KICA8L2c+Cjwvc3ZnPgo=); --jp-icon-ellipses: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iNSIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxOSIgY3k9IjEyIiByPSIyIi8+CiAgPC9nPgo8L3N2Zz4K); --jp-icon-extension: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwLjUgMTFIMTlWN2MwLTEuMS0uOS0yLTItMmgtNFYzLjVDMTMgMi4xMiAxMS44OCAxIDEwLjUgMVM4IDIuMTIgOCAzLjVWNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAydjMuOEgzLjVjMS40OSAwIDIuNyAxLjIxIDIuNyAyLjdzLTEuMjEgMi43LTIuNyAyLjdIMlYyMGMwIDEuMS45IDIgMiAyaDMuOHYtMS41YzAtMS40OSAxLjIxLTIuNyAyLjctMi43IDEuNDkgMCAyLjcgMS4yMSAyLjcgMi43VjIySDE3YzEuMSAwIDItLjkgMi0ydi00aDEuNWMxLjM4IDAgMi41LTEuMTIgMi41LTIuNVMyMS44OCAxMSAyMC41IDExeiIvPgogIDwvZz4KPC9zdmc+Cg==); --jp-icon-fast-forward: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTQgMThsOC41LTZMNCA2djEyem05LTEydjEybDguNS02TDEzIDZ6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=); --jp-icon-file-upload: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkgMTZoNnYtNmg0bC03LTctNyA3aDR6bS00IDJoMTR2Mkg1eiIvPgogIDwvZz4KPC9zdmc+Cg==); --jp-icon-file: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuMyA4LjJsLTUuNS01LjVjLS4zLS4zLS43LS41LTEuMi0uNUgzLjljLS44LjEtMS42LjktMS42IDEuOHYxNC4xYzAgLjkuNyAxLjYgMS42IDEuNmgxNC4yYy45IDAgMS42LS43IDEuNi0xLjZWOS40Yy4xLS41LS4xLS45LS40LTEuMnptLTUuOC0zLjNsMy40IDMuNmgtMy40VjQuOXptMy45IDEyLjdINC43Yy0uMSAwLS4yIDAtLjItLjJWNC43YzAtLjIuMS0uMy4yLS4zaDcuMnY0LjRzMCAuOC4zIDEuMWMuMy4zIDEuMS4zIDEuMS4zaDQuM3Y3LjJzLS4xLjItLjIuMnoiLz4KPC9zdmc+Cg==); --jp-icon-filter-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEwIDE4aDR2LTJoLTR2MnpNMyA2djJoMThWNkgzem0zIDdoMTJ2LTJINnYyeiIvPgogIDwvZz4KPC9zdmc+Cg==); --jp-icon-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY4YzAtMS4xLS45LTItMi0yaC04bC0yLTJ6Ii8+Cjwvc3ZnPgo=); --jp-icon-html5: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uMCBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMDAiIGQ9Ik0xMDguNCAwaDIzdjIyLjhoMjEuMlYwaDIzdjY5aC0yM1Y0NmgtMjF2MjNoLTIzLjJNMjA2IDIzaC0yMC4zVjBoNjMuN3YyM0gyMjl2NDZoLTIzbTUzLjUtNjloMjQuMWwxNC44IDI0LjNMMzEzLjIgMGgyNC4xdjY5aC0yM1YzNC44bC0xNi4xIDI0LjgtMTYuMS0yNC44VjY5aC0yMi42bTg5LjItNjloMjN2NDYuMmgzMi42VjY5aC01NS42Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI2U0NGQyNiIgZD0iTTEwNy42IDQ3MWwtMzMtMzcwLjRoMzYyLjhsLTMzIDM3MC4yTDI1NS43IDUxMiIvPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNmMTY1MjkiIGQ9Ik0yNTYgNDgwLjVWMTMxaDE0OC4zTDM3NiA0NDciLz4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNlYmViZWIiIGQ9Ik0xNDIgMTc2LjNoMTE0djQ1LjRoLTY0LjJsNC4yIDQ2LjVoNjB2NDUuM0gxNTQuNG0yIDIyLjhIMjAybDMuMiAzNi4zIDUwLjggMTMuNnY0Ny40bC05My4yLTI2Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIiBkPSJNMzY5LjYgMTc2LjNIMjU1Ljh2NDUuNGgxMDkuNm0tNC4xIDQ2LjVIMjU1Ljh2NDUuNGg1NmwtNS4zIDU5LTUwLjcgMTMuNnY0Ny4ybDkzLTI1LjgiLz4KPC9zdmc+Cg==); --jp-icon-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1icmFuZDQganAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNGRkYiIGQ9Ik0yLjIgMi4yaDE3LjV2MTcuNUgyLjJ6Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzNGNTFCNSIgZD0iTTIuMiAyLjJ2MTcuNWgxNy41bC4xLTE3LjVIMi4yem0xMi4xIDIuMmMxLjIgMCAyLjIgMSAyLjIgMi4ycy0xIDIuMi0yLjIgMi4yLTIuMi0xLTIuMi0yLjIgMS0yLjIgMi4yLTIuMnpNNC40IDE3LjZsMy4zLTguOCAzLjMgNi42IDIuMi0zLjIgNC40IDUuNEg0LjR6Ii8+Cjwvc3ZnPgo=); --jp-icon-inspector: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY2YzAtMS4xLS45LTItMi0yem0tNSAxNEg0di00aDExdjR6bTAtNUg0VjloMTF2NHptNSA1aC00VjloNHY5eiIvPgo8L3N2Zz4K); --jp-icon-json: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi13YXJuMSBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNGOUE4MjUiPgogICAgPHBhdGggZD0iTTIwLjIgMTEuOGMtMS42IDAtMS43LjUtMS43IDEgMCAuNC4xLjkuMSAxLjMuMS41LjEuOS4xIDEuMyAwIDEuNy0xLjQgMi4zLTMuNSAyLjNoLS45di0xLjloLjVjMS4xIDAgMS40IDAgMS40LS44IDAtLjMgMC0uNi0uMS0xIDAtLjQtLjEtLjgtLjEtMS4yIDAtMS4zIDAtMS44IDEuMy0yLTEuMy0uMi0xLjMtLjctMS4zLTIgMC0uNC4xLS44LjEtMS4yLjEtLjQuMS0uNy4xLTEgMC0uOC0uNC0uNy0xLjQtLjhoLS41VjQuMWguOWMyLjIgMCAzLjUuNyAzLjUgMi4zIDAgLjQtLjEuOS0uMSAxLjMtLjEuNS0uMS45LS4xIDEuMyAwIC41LjIgMSAxLjcgMXYxLjh6TTEuOCAxMC4xYzEuNiAwIDEuNy0uNSAxLjctMSAwLS40LS4xLS45LS4xLTEuMy0uMS0uNS0uMS0uOS0uMS0xLjMgMC0xLjYgMS40LTIuMyAzLjUtMi4zaC45djEuOWgtLjVjLTEgMC0xLjQgMC0xLjQuOCAwIC4zIDAgLjYuMSAxIDAgLjIuMS42LjEgMSAwIDEuMyAwIDEuOC0xLjMgMkM2IDExLjIgNiAxMS43IDYgMTNjMCAuNC0uMS44LS4xIDEuMi0uMS4zLS4xLjctLjEgMSAwIC44LjMuOCAxLjQuOGguNXYxLjloLS45Yy0yLjEgMC0zLjUtLjYtMy41LTIuMyAwLS40LjEtLjkuMS0xLjMuMS0uNS4xLS45LjEtMS4zIDAtLjUtLjItMS0xLjctMXYtMS45eiIvPgogICAgPGNpcmNsZSBjeD0iMTEiIGN5PSIxMy44IiByPSIyLjEiLz4KICAgIDxjaXJjbGUgY3g9IjExIiBjeT0iOC4yIiByPSIyLjEiLz4KICA8L2c+Cjwvc3ZnPgo=); --jp-icon-jupyter-favicon: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTUyIiBoZWlnaHQ9IjE2NSIgdmlld0JveD0iMCAwIDE1MiAxNjUiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbi13YXJuMCIgZmlsbD0iI0YzNzcyNiI+CiAgICA8cGF0aCB0cmFuc2Zvcm09InRyYW5zbGF0ZSgwLjA3ODk0NywgMTEwLjU4MjkyNykiIGQ9Ik03NS45NDIyODQyLDI5LjU4MDQ1NjEgQzQzLjMwMjM5NDcsMjkuNTgwNDU2MSAxNC43OTY3ODMyLDE3LjY1MzQ2MzQgMCwwIEM1LjUxMDgzMjExLDE1Ljg0MDY4MjkgMTUuNzgxNTM4OSwyOS41NjY3NzMyIDI5LjM5MDQ5NDcsMzkuMjc4NDE3MSBDNDIuOTk5Nyw0OC45ODk4NTM3IDU5LjI3MzcsNTQuMjA2NzgwNSA3NS45NjA1Nzg5LDU0LjIwNjc4MDUgQzkyLjY0NzQ1NzksNTQuMjA2NzgwNSAxMDguOTIxNDU4LDQ4Ljk4OTg1MzcgMTIyLjUzMDY2MywzOS4yNzg0MTcxIEMxMzYuMTM5NDUzLDI5LjU2Njc3MzIgMTQ2LjQxMDI4NCwxNS44NDA2ODI5IDE1MS45MjExNTgsMCBDMTM3LjA4Nzg2OCwxNy42NTM0NjM0IDEwOC41ODI1ODksMjkuNTgwNDU2MSA3NS45NDIyODQyLDI5LjU4MDQ1NjEgTDc1Ljk0MjI4NDIsMjkuNTgwNDU2MSBaIiAvPgogICAgPHBhdGggdHJhbnNmb3JtPSJ0cmFuc2xhdGUoMC4wMzczNjgsIDAuNzA0ODc4KSIgZD0iTTc1Ljk3ODQ1NzksMjQuNjI2NDA3MyBDMTA4LjYxODc2MywyNC42MjY0MDczIDEzNy4xMjQ0NTgsMzYuNTUzNDQxNSAxNTEuOTIxMTU4LDU0LjIwNjc4MDUgQzE0Ni40MTAyODQsMzguMzY2MjIyIDEzNi4xMzk0NTMsMjQuNjQwMTMxNyAxMjIuNTMwNjYzLDE0LjkyODQ4NzggQzEwOC45MjE0NTgsNS4yMTY4NDM5IDkyLjY0NzQ1NzksMCA3NS45NjA1Nzg5LDAgQzU5LjI3MzcsMCA0Mi45OTk3LDUuMjE2ODQzOSAyOS4zOTA0OTQ3LDE0LjkyODQ4NzggQzE1Ljc4MTUzODksMjQuNjQwMTMxNyA1LjUxMDgzMjExLDM4LjM2NjIyMiAwLDU0LjIwNjc4MDUgQzE0LjgzMzA4MTYsMzYuNTg5OTI5MyA0My4zMzg1Njg0LDI0LjYyNjQwNzMgNzUuOTc4NDU3OSwyNC42MjY0MDczIEw3NS45Nzg0NTc5LDI0LjYyNjQwNzMgWiIgLz4KICA8L2c+Cjwvc3ZnPgo=); --jp-icon-jupyter: url(data:image/svg+xml;base64,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); --jp-icon-jupyterlab-wordmark: url(data:image/svg+xml;base64,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); --jp-icon-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgZmlsbD0iIzYxNjE2MSIgZD0iTTE1IDlIOXY2aDZWOXptLTIgNGgtMnYtMmgydjJ6bTgtMlY5aC0yVjdjMC0xLjEtLjktMi0yLTJoLTJWM2gtMnYyaC0yVjNIOXYySDdjLTEuMSAwLTIgLjktMiAydjJIM3YyaDJ2MkgzdjJoMnYyYzAgMS4xLjkgMiAyIDJoMnYyaDJ2LTJoMnYyaDJ2LTJoMmMxLjEgMCAyLS45IDItMnYtMmgydi0yaC0ydi0yaDJ6bS00IDZIN1Y3aDEwdjEweiIvPgo8L3N2Zz4K); --jp-icon-keyboard: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMTdjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY3YzAtMS4xLS45LTItMi0yem0tOSAzaDJ2MmgtMlY4em0wIDNoMnYyaC0ydi0yek04IDhoMnYySDhWOHptMCAzaDJ2Mkg4di0yem0tMSAySDV2LTJoMnYyem0wLTNINVY4aDJ2MnptOSA3SDh2LTJoOHYyem0wLTRoLTJ2LTJoMnYyem0wLTNoLTJWOGgydjJ6bTMgM2gtMnYtMmgydjJ6bTAtM2gtMlY4aDJ2MnoiLz4KPC9zdmc+Cg==); --jp-icon-launcher: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkgMTlINVY1aDdWM0g1YTIgMiAwIDAwLTIgMnYxNGEyIDIgMCAwMDIgMmgxNGMxLjEgMCAyLS45IDItMnYtN2gtMnY3ek0xNCAzdjJoMy41OWwtOS44MyA5LjgzIDEuNDEgMS40MUwxOSA2LjQxVjEwaDJWM2gtN3oiLz4KPC9zdmc+Cg==); --jp-icon-line-form: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGZpbGw9IndoaXRlIiBkPSJNNS44OCA0LjEyTDEzLjc2IDEybC03Ljg4IDcuODhMOCAyMmwxMC0xMEw4IDJ6Ii8+Cjwvc3ZnPgo=); --jp-icon-link: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMuOSAxMmMwLTEuNzEgMS4zOS0zLjEgMy4xLTMuMWg0VjdIN2MtMi43NiAwLTUgMi4yNC01IDVzMi4yNCA1IDUgNWg0di0xLjlIN2MtMS43MSAwLTMuMS0xLjM5LTMuMS0zLjF6TTggMTNoOHYtMkg4djJ6bTktNmgtNHYxLjloNGMxLjcxIDAgMy4xIDEuMzkgMy4xIDMuMXMtMS4zOSAzLjEtMy4xIDMuMWgtNFYxN2g0YzIuNzYgMCA1LTIuMjQgNS01cy0yLjI0LTUtNS01eiIvPgogIDwvZz4KPC9zdmc+Cg==); --jp-icon-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xOSA1djE0SDVWNWgxNG0xLjEtMkgzLjljLS41IDAtLjkuNC0uOS45djE2LjJjMCAuNC40LjkuOS45aDE2LjJjLjQgMCAuOS0uNS45LS45VjMuOWMwLS41LS41LS45LS45LS45ek0xMSA3aDZ2MmgtNlY3em0wIDRoNnYyaC02di0yem0wIDRoNnYyaC02ek03IDdoMnYySDd6bTAgNGgydjJIN3ptMCA0aDJ2Mkg3eiIvPgo8L3N2Zz4=); --jp-icon-listings-info: url(data:image/svg+xml;base64,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); --jp-icon-markdown: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjN0IxRkEyIiBkPSJNNSAxNC45aDEybC02LjEgNnptOS40LTYuOGMwLTEuMy0uMS0yLjktLjEtNC41LS40IDEuNC0uOSAyLjktMS4zIDQuM2wtMS4zIDQuM2gtMkw4LjUgNy45Yy0uNC0xLjMtLjctMi45LTEtNC4zLS4xIDEuNi0uMSAzLjItLjIgNC42TDcgMTIuNEg0LjhsLjctMTFoMy4zTDEwIDVjLjQgMS4yLjcgMi43IDEgMy45LjMtMS4yLjctMi42IDEtMy45bDEuMi0zLjdoMy4zbC42IDExaC0yLjRsLS4zLTQuMnoiLz4KPC9zdmc+Cg==); --jp-icon-new-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwIDZoLThsLTItMkg0Yy0xLjExIDAtMS45OS44OS0xLjk5IDJMMiAxOGMwIDEuMTEuODkgMiAyIDJoMTZjMS4xMSAwIDItLjg5IDItMlY4YzAtMS4xMS0uODktMi0yLTJ6bS0xIDhoLTN2M2gtMnYtM2gtM3YtMmgzVjloMnYzaDN2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=); --jp-icon-not-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI1IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDMgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMTkgMTcuMTg0NCAyLjk2OTY4IDE0LjMwMzIgMS44NjA5NCAxMS40NDA5WiIvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24yIiBzdHJva2U9IiMzMzMzMzMiIHN0cm9rZS13aWR0aD0iMiIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOS4zMTU5MiA5LjMyMDMxKSIgZD0iTTcuMzY4NDIgMEwwIDcuMzY0NzkiLz4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDkuMzE1OTIgMTYuNjgzNikgc2NhbGUoMSAtMSkiIGQ9Ik03LjM2ODQyIDBMMCA3LjM2NDc5Ii8+Cjwvc3ZnPgo=); --jp-icon-notebook: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi13YXJuMCBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNFRjZDMDAiPgogICAgPHBhdGggZD0iTTE4LjcgMy4zdjE1LjRIMy4zVjMuM2gxNS40bTEuNS0xLjVIMS44djE4LjNoMTguM2wuMS0xOC4zeiIvPgogICAgPHBhdGggZD0iTTE2LjUgMTYuNWwtNS40LTQuMy01LjYgNC4zdi0xMWgxMXoiLz4KICA8L2c+Cjwvc3ZnPgo=); --jp-icon-palette: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE4IDEzVjIwSDRWNkg5LjAyQzkuMDcgNS4yOSA5LjI0IDQuNjIgOS41IDRINEMyLjkgNCAyIDQuOSAyIDZWMjBDMiAyMS4xIDIuOSAyMiA0IDIySDE4QzE5LjEgMjIgMjAgMjEuMSAyMCAyMFYxNUwxOCAxM1pNMTkuMyA4Ljg5QzE5Ljc0IDguMTkgMjAgNy4zOCAyMCA2LjVDMjAgNC4wMSAxNy45OSAyIDE1LjUgMkMxMy4wMSAyIDExIDQuMDEgMTEgNi41QzExIDguOTkgMTMuMDEgMTEgMTUuNDkgMTFDMTYuMzcgMTEgMTcuMTkgMTAuNzQgMTcuODggMTAuM0wyMSAxMy40MkwyMi40MiAxMkwxOS4zIDguODlaTTE1LjUgOUMxNC4xMiA5IDEzIDcuODggMTMgNi41QzEzIDUuMTIgMTQuMTIgNCAxNS41IDRDMTYuODggNCAxOCA1LjEyIDE4IDYuNUMxOCA3Ljg4IDE2Ljg4IDkgMTUuNSA5WiIvPgogICAgPHBhdGggZmlsbC1ydWxlPSJldmVub2RkIiBjbGlwLXJ1bGU9ImV2ZW5vZGQiIGQ9Ik00IDZIOS4wMTg5NEM5LjAwNjM5IDYuMTY1MDIgOSA2LjMzMTc2IDkgNi41QzkgOC44MTU3NyAxMC4yMTEgMTAuODQ4NyAxMi4wMzQzIDEySDlWMTRIMTZWMTIuOTgxMUMxNi41NzAzIDEyLjkzNzcgMTcuMTIgMTIuODIwNyAxNy42Mzk2IDEyLjYzOTZMMTggMTNWMjBINFY2Wk04IDhINlYxMEg4VjhaTTYgMTJIOFYxNEg2VjEyWk04IDE2SDZWMThIOFYxNlpNOSAxNkgxNlYxOEg5VjE2WiIvPgogIDwvZz4KPC9zdmc+Cg==); --jp-icon-paste: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE5IDJoLTQuMThDMTQuNC44NCAxMy4zIDAgMTIgMGMtMS4zIDAtMi40Ljg0LTIuODIgMkg1Yy0xLjEgMC0yIC45LTIgMnYxNmMwIDEuMS45IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjRjMC0xLjEtLjktMi0yLTJ6bS03IDBjLjU1IDAgMSAuNDUgMSAxcy0uNDUgMS0xIDEtMS0uNDUtMS0xIC40NS0xIDEtMXptNyAxOEg1VjRoMnYzaDEwVjRoMnYxNnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==); --jp-icon-python: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1icmFuZDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMEQ0N0ExIj4KICAgIDxwYXRoIGQ9Ik0xMS4xIDYuOVY1LjhINi45YzAtLjUgMC0xLjMuMi0xLjYuNC0uNy44LTEuMSAxLjctMS40IDEuNy0uMyAyLjUtLjMgMy45LS4xIDEgLjEgMS45LjkgMS45IDEuOXY0LjJjMCAuNS0uOSAxLjYtMiAxLjZIOC44Yy0xLjUgMC0yLjQgMS40LTIuNCAyLjh2Mi4ySDQuN0MzLjUgMTUuMSAzIDE0IDMgMTMuMVY5Yy0uMS0xIC42LTIgMS44LTIgMS41LS4xIDYuMy0uMSA2LjMtLjF6Ii8+CiAgICA8cGF0aCBkPSJNMTAuOSAxNS4xdjEuMWg0LjJjMCAuNSAwIDEuMy0uMiAxLjYtLjQuNy0uOCAxLjEtMS43IDEuNC0xLjcuMy0yLjUuMy0zLjkuMS0xLS4xLTEuOS0uOS0xLjktMS45di00LjJjMC0uNS45LTEuNiAyLTEuNmgzLjhjMS41IDAgMi40LTEuNCAyLjQtMi44VjYuNmgxLjdDMTguNSA2LjkgMTkgOCAxOSA4LjlWMTNjMCAxLS43IDIuMS0xLjkgMi4xaC02LjJ6Ii8+CiAgPC9nPgo8L3N2Zz4K); --jp-icon-r-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjE5NkYzIiBkPSJNNC40IDIuNWMxLjItLjEgMi45LS4zIDQuOS0uMyAyLjUgMCA0LjEuNCA1LjIgMS4zIDEgLjcgMS41IDEuOSAxLjUgMy41IDAgMi0xLjQgMy41LTIuOSA0LjEgMS4yLjQgMS43IDEuNiAyLjIgMyAuNiAxLjkgMSAzLjkgMS4zIDQuNmgtMy44Yy0uMy0uNC0uOC0xLjctMS4yLTMuN3MtMS4yLTIuNi0yLjYtMi42aC0uOXY2LjRINC40VjIuNXptMy43IDYuOWgxLjRjMS45IDAgMi45LS45IDIuOS0yLjNzLTEtMi4zLTIuOC0yLjNjLS43IDAtMS4zIDAtMS42LjJ2NC41aC4xdi0uMXoiLz4KPC9zdmc+Cg==); --jp-icon-react: url(data:image/svg+xml;base64,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); --jp-icon-refresh: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTkgMTMuNWMtMi40OSAwLTQuNS0yLjAxLTQuNS00LjVTNi41MSA0LjUgOSA0LjVjMS4yNCAwIDIuMzYuNTIgMy4xNyAxLjMzTDEwIDhoNVYzbC0xLjc2IDEuNzZDMTIuMTUgMy42OCAxMC42NiAzIDkgMyA1LjY5IDMgMy4wMSA1LjY5IDMuMDEgOVM1LjY5IDE1IDkgMTVjMi45NyAwIDUuNDMtMi4xNiA1LjktNWgtMS41MmMtLjQ2IDItMi4yNCAzLjUtNC4zOCAzLjV6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=); --jp-icon-regex: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbi1hY2NlbnQyIiBmaWxsPSIjRkZGIj4KICAgIDxjaXJjbGUgY2xhc3M9InN0MiIgY3g9IjUuNSIgY3k9IjE0LjUiIHI9IjEuNSIvPgogICAgPHJlY3QgeD0iMTIiIHk9IjQiIGNsYXNzPSJzdDIiIHdpZHRoPSIxIiBoZWlnaHQ9IjgiLz4KICAgIDxyZWN0IHg9IjguNSIgeT0iNy41IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjg2NiAtMC41IDAuNSAwLjg2NiAtMi4zMjU1IDcuMzIxOSkiIGNsYXNzPSJzdDIiIHdpZHRoPSI4IiBoZWlnaHQ9IjEiLz4KICAgIDxyZWN0IHg9IjEyIiB5PSI0IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjUgLTAuODY2IDAuODY2IDAuNSAtMC42Nzc5IDE0LjgyNTIpIiBjbGFzcz0ic3QyIiB3aWR0aD0iMSIgaGVpZ2h0PSI4Ii8+CiAgPC9nPgo8L3N2Zz4K); --jp-icon-run: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTggNXYxNGwxMS03eiIvPgogICAgPC9nPgo8L3N2Zz4K); --jp-icon-running: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMjU2IDhDMTE5IDggOCAxMTkgOCAyNTZzMTExIDI0OCAyNDggMjQ4IDI0OC0xMTEgMjQ4LTI0OFMzOTMgOCAyNTYgOHptOTYgMzI4YzAgOC44LTcuMiAxNi0xNiAxNkgxNzZjLTguOCAwLTE2LTcuMi0xNi0xNlYxNzZjMC04LjggNy4yLTE2IDE2LTE2aDE2MGM4LjggMCAxNiA3LjIgMTYgMTZ2MTYweiIvPgogIDwvZz4KPC9zdmc+Cg==); --jp-icon-save: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE3IDNINWMtMS4xMSAwLTIgLjktMiAydjE0YzAgMS4xLjg5IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjdsLTQtNHptLTUgMTZjLTEuNjYgMC0zLTEuMzQtMy0zczEuMzQtMyAzLTMgMyAxLjM0IDMgMy0xLjM0IDMtMyAzem0zLTEwSDVWNWgxMHY0eiIvPgogICAgPC9nPgo8L3N2Zz4K); --jp-icon-search: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjEsMTAuOWgtMC43bC0wLjItMC4yYzAuOC0wLjksMS4zLTIuMiwxLjMtMy41YzAtMy0yLjQtNS40LTUuNC01LjRTMS44LDQuMiwxLjgsNy4xczIuNCw1LjQsNS40LDUuNCBjMS4zLDAsMi41LTAuNSwzLjUtMS4zbDAuMiwwLjJ2MC43bDQuMSw0LjFsMS4yLTEuMkwxMi4xLDEwLjl6IE03LjEsMTAuOWMtMi4xLDAtMy43LTEuNy0zLjctMy43czEuNy0zLjcsMy43LTMuN3MzLjcsMS43LDMuNywzLjcgUzkuMiwxMC45LDcuMSwxMC45eiIvPgogIDwvZz4KPC9zdmc+Cg==); --jp-icon-settings: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuNDMgMTIuOThjLjA0LS4zMi4wNy0uNjQuMDctLjk4cy0uMDMtLjY2LS4wNy0uOThsMi4xMS0xLjY1Yy4xOS0uMTUuMjQtLjQyLjEyLS42NGwtMi0zLjQ2Yy0uMTItLjIyLS4zOS0uMy0uNjEtLjIybC0yLjQ5IDFjLS41Mi0uNC0xLjA4LS43My0xLjY5LS45OGwtLjM4LTIuNjVBLjQ4OC40ODggMCAwMDE0IDJoLTRjLS4yNSAwLS40Ni4xOC0uNDkuNDJsLS4zOCAyLjY1Yy0uNjEuMjUtMS4xNy41OS0xLjY5Ljk4bC0yLjQ5LTFjLS4yMy0uMDktLjQ5IDAtLjYxLjIybC0yIDMuNDZjLS4xMy4yMi0uMDcuNDkuMTIuNjRsMi4xMSAxLjY1Yy0uMDQuMzItLjA3LjY1LS4wNy45OHMuMDMuNjYuMDcuOThsLTIuMTEgMS42NWMtLjE5LjE1LS4yNC40Mi0uMTIuNjRsMiAzLjQ2Yy4xMi4yMi4zOS4zLjYxLjIybDIuNDktMWMuNTIuNCAxLjA4LjczIDEuNjkuOThsLjM4IDIuNjVjLjAzLjI0LjI0LjQyLjQ5LjQyaDRjLjI1IDAgLjQ2LS4xOC40OS0uNDJsLjM4LTIuNjVjLjYxLS4yNSAxLjE3LS41OSAxLjY5LS45OGwyLjQ5IDFjLjIzLjA5LjQ5IDAgLjYxLS4yMmwyLTMuNDZjLjEyLS4yMi4wNy0uNDktLjEyLS42NGwtMi4xMS0xLjY1ek0xMiAxNS41Yy0xLjkzIDAtMy41LTEuNTctMy41LTMuNXMxLjU3LTMuNSAzLjUtMy41IDMuNSAxLjU3IDMuNSAzLjUtMS41NyAzLjUtMy41IDMuNXoiLz4KPC9zdmc+Cg==); --jp-icon-spreadsheet: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNENBRjUwIiBkPSJNMi4yIDIuMnYxNy42aDE3LjZWMi4ySDIuMnptMTUuNCA3LjdoLTUuNVY0LjRoNS41djUuNXpNOS45IDQuNHY1LjVINC40VjQuNGg1LjV6bS01LjUgNy43aDUuNXY1LjVINC40di01LjV6bTcuNyA1LjV2LTUuNWg1LjV2NS41aC01LjV6Ii8+Cjwvc3ZnPgo=); --jp-icon-stop: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik02IDZoMTJ2MTJINnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==); --jp-icon-tab: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIxIDNIM2MtMS4xIDAtMiAuOS0yIDJ2MTRjMCAxLjEuOSAyIDIgMmgxOGMxLjEgMCAyLS45IDItMlY1YzAtMS4xLS45LTItMi0yem0wIDE2SDNWNWgxMHY0aDh2MTB6Ii8+CiAgPC9nPgo8L3N2Zz4K); --jp-icon-terminal: url(data:image/svg+xml;base64,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); --jp-icon-text-editor: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTUgMTVIM3YyaDEydi0yem0wLThIM3YyaDEyVjd6TTMgMTNoMTh2LTJIM3Yyem0wIDhoMTh2LTJIM3Yyek0zIDN2MmgxOFYzSDN6Ii8+Cjwvc3ZnPgo=); --jp-icon-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDIgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMiAxNy4xODQ0IDIuOTY5NjggMTQuMzAzMiAxLjg2MDk0IDExLjQ0MDlaIi8+CiAgICA8cGF0aCBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiMzMzMzMzMiIHN0cm9rZT0iIzMzMzMzMyIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOCA5Ljg2NzE5KSIgZD0iTTIuODYwMTUgNC44NjUzNUwwLjcyNjU0OSAyLjk5OTU5TDAgMy42MzA0NUwyLjg2MDE1IDYuMTMxNTdMOCAwLjYzMDg3Mkw3LjI3ODU3IDBMMi44NjAxNSA0Ljg2NTM1WiIvPgo8L3N2Zz4K); --jp-icon-undo: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjUgOGMtMi42NSAwLTUuMDUuOTktNi45IDIuNkwyIDd2OWg5bC0zLjYyLTMuNjJjMS4zOS0xLjE2IDMuMTYtMS44OCA1LjEyLTEuODggMy41NCAwIDYuNTUgMi4zMSA3LjYgNS41bDIuMzctLjc4QzIxLjA4IDExLjAzIDE3LjE1IDggMTIuNSA4eiIvPgogIDwvZz4KPC9zdmc+Cg==); --jp-icon-vega: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbjEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjEyMTIxIj4KICAgIDxwYXRoIGQ9Ik0xMC42IDUuNGwyLjItMy4ySDIuMnY3LjNsNC02LjZ6Ii8+CiAgICA8cGF0aCBkPSJNMTUuOCAyLjJsLTQuNCA2LjZMNyA2LjNsLTQuOCA4djUuNWgxNy42VjIuMmgtNHptLTcgMTUuNEg1LjV2LTQuNGgzLjN2NC40em00LjQgMEg5LjhWOS44aDMuNHY3Ljh6bTQuNCAwaC0zLjRWNi41aDMuNHYxMS4xeiIvPgogIDwvZz4KPC9zdmc+Cg==); --jp-icon-yaml: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1jb250cmFzdDIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjRDgxQjYwIj4KICAgIDxwYXRoIGQ9Ik03LjIgMTguNnYtNS40TDMgNS42aDMuM2wxLjQgMy4xYy4zLjkuNiAxLjYgMSAyLjUuMy0uOC42LTEuNiAxLTIuNWwxLjQtMy4xaDMuNGwtNC40IDcuNnY1LjVsLTIuOS0uMXoiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxNi41IiByPSIyLjEiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxMSIgcj0iMi4xIi8+CiAgPC9nPgo8L3N2Zz4K); } /* Icon CSS class declarations */ .jp-AddIcon { background-image: var(--jp-icon-add); } .jp-BugIcon { background-image: var(--jp-icon-bug); } .jp-BuildIcon { background-image: var(--jp-icon-build); } .jp-CaretDownEmptyIcon { background-image: var(--jp-icon-caret-down-empty); } .jp-CaretDownEmptyThinIcon { background-image: var(--jp-icon-caret-down-empty-thin); } .jp-CaretDownIcon { background-image: var(--jp-icon-caret-down); } .jp-CaretLeftIcon { background-image: var(--jp-icon-caret-left); } .jp-CaretRightIcon { background-image: var(--jp-icon-caret-right); } .jp-CaretUpEmptyThinIcon { background-image: var(--jp-icon-caret-up-empty-thin); } .jp-CaretUpIcon { background-image: var(--jp-icon-caret-up); } .jp-CaseSensitiveIcon { background-image: var(--jp-icon-case-sensitive); } .jp-CheckIcon { background-image: var(--jp-icon-check); } .jp-CircleEmptyIcon { background-image: var(--jp-icon-circle-empty); } .jp-CircleIcon { background-image: var(--jp-icon-circle); } .jp-ClearIcon { background-image: var(--jp-icon-clear); } .jp-CloseIcon { background-image: var(--jp-icon-close); } .jp-ConsoleIcon { background-image: var(--jp-icon-console); } .jp-CopyIcon { background-image: var(--jp-icon-copy); } .jp-CutIcon { background-image: var(--jp-icon-cut); } .jp-DownloadIcon { background-image: var(--jp-icon-download); } .jp-EditIcon { background-image: var(--jp-icon-edit); } .jp-EllipsesIcon { background-image: var(--jp-icon-ellipses); } .jp-ExtensionIcon { background-image: var(--jp-icon-extension); } .jp-FastForwardIcon { background-image: var(--jp-icon-fast-forward); } .jp-FileIcon { background-image: var(--jp-icon-file); } .jp-FileUploadIcon { background-image: var(--jp-icon-file-upload); } .jp-FilterListIcon { background-image: var(--jp-icon-filter-list); } .jp-FolderIcon { background-image: var(--jp-icon-folder); } .jp-Html5Icon { background-image: var(--jp-icon-html5); } .jp-ImageIcon { background-image: var(--jp-icon-image); } .jp-InspectorIcon { background-image: var(--jp-icon-inspector); } .jp-JsonIcon { background-image: var(--jp-icon-json); } .jp-JupyterFaviconIcon { background-image: var(--jp-icon-jupyter-favicon); } .jp-JupyterIcon { background-image: var(--jp-icon-jupyter); } .jp-JupyterlabWordmarkIcon { background-image: var(--jp-icon-jupyterlab-wordmark); } .jp-KernelIcon { background-image: var(--jp-icon-kernel); } .jp-KeyboardIcon { background-image: var(--jp-icon-keyboard); } .jp-LauncherIcon { background-image: var(--jp-icon-launcher); } .jp-LineFormIcon { background-image: var(--jp-icon-line-form); } .jp-LinkIcon { background-image: var(--jp-icon-link); } .jp-ListIcon { background-image: var(--jp-icon-list); } .jp-ListingsInfoIcon { background-image: var(--jp-icon-listings-info); } .jp-MarkdownIcon { background-image: var(--jp-icon-markdown); } .jp-NewFolderIcon { background-image: var(--jp-icon-new-folder); } .jp-NotTrustedIcon { background-image: var(--jp-icon-not-trusted); } .jp-NotebookIcon { background-image: var(--jp-icon-notebook); } .jp-PaletteIcon { background-image: var(--jp-icon-palette); } .jp-PasteIcon { background-image: var(--jp-icon-paste); } .jp-PythonIcon { background-image: var(--jp-icon-python); } .jp-RKernelIcon { background-image: var(--jp-icon-r-kernel); } .jp-ReactIcon { background-image: var(--jp-icon-react); } .jp-RefreshIcon { background-image: var(--jp-icon-refresh); } .jp-RegexIcon { background-image: var(--jp-icon-regex); } .jp-RunIcon { background-image: var(--jp-icon-run); } .jp-RunningIcon { background-image: var(--jp-icon-running); } .jp-SaveIcon { background-image: var(--jp-icon-save); } .jp-SearchIcon { background-image: var(--jp-icon-search); } .jp-SettingsIcon { background-image: var(--jp-icon-settings); } .jp-SpreadsheetIcon { background-image: var(--jp-icon-spreadsheet); } .jp-StopIcon { background-image: var(--jp-icon-stop); } .jp-TabIcon { background-image: var(--jp-icon-tab); } .jp-TerminalIcon { background-image: var(--jp-icon-terminal); } .jp-TextEditorIcon { background-image: var(--jp-icon-text-editor); } .jp-TrustedIcon { background-image: var(--jp-icon-trusted); } .jp-UndoIcon { background-image: var(--jp-icon-undo); } .jp-VegaIcon { background-image: var(--jp-icon-vega); } .jp-YamlIcon { background-image: var(--jp-icon-yaml); } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /** * (DEPRECATED) Support for consuming icons as CSS background images */ :root { --jp-icon-search-white: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjEsMTAuOWgtMC43bC0wLjItMC4yYzAuOC0wLjksMS4zLTIuMiwxLjMtMy41YzAtMy0yLjQtNS40LTUuNC01LjRTMS44LDQuMiwxLjgsNy4xczIuNCw1LjQsNS40LDUuNCBjMS4zLDAsMi41LTAuNSwzLjUtMS4zbDAuMiwwLjJ2MC43bDQuMSw0LjFsMS4yLTEuMkwxMi4xLDEwLjl6IE03LjEsMTAuOWMtMi4xLDAtMy43LTEuNy0zLjctMy43czEuNy0zLjcsMy43LTMuN3MzLjcsMS43LDMuNywzLjcgUzkuMiwxMC45LDcuMSwxMC45eiIvPgogIDwvZz4KPC9zdmc+Cg==); } .jp-Icon, .jp-MaterialIcon { background-position: center; background-repeat: no-repeat; background-size: 16px; min-width: 16px; min-height: 16px; } .jp-Icon-cover { background-position: center; background-repeat: no-repeat; background-size: cover; } /** * (DEPRECATED) Support for specific CSS icon sizes */ .jp-Icon-16 { background-size: 16px; min-width: 16px; min-height: 16px; } .jp-Icon-18 { background-size: 18px; min-width: 18px; min-height: 18px; } .jp-Icon-20 { background-size: 20px; min-width: 20px; min-height: 20px; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /** * Support for icons as inline SVG HTMLElements */ /* recolor the primary elements of an icon */ .jp-icon0[fill] { fill: var(--jp-inverse-layout-color0); } .jp-icon1[fill] { fill: var(--jp-inverse-layout-color1); } .jp-icon2[fill] { fill: var(--jp-inverse-layout-color2); } .jp-icon3[fill] { fill: var(--jp-inverse-layout-color3); } .jp-icon4[fill] { fill: var(--jp-inverse-layout-color4); } .jp-icon0[stroke] { stroke: var(--jp-inverse-layout-color0); } .jp-icon1[stroke] { stroke: var(--jp-inverse-layout-color1); } .jp-icon2[stroke] { stroke: var(--jp-inverse-layout-color2); } .jp-icon3[stroke] { stroke: var(--jp-inverse-layout-color3); } .jp-icon4[stroke] { stroke: var(--jp-inverse-layout-color4); } /* recolor the accent elements of an icon */ .jp-icon-accent0[fill] { fill: var(--jp-layout-color0); } .jp-icon-accent1[fill] { fill: var(--jp-layout-color1); } .jp-icon-accent2[fill] { fill: var(--jp-layout-color2); } .jp-icon-accent3[fill] { fill: var(--jp-layout-color3); } .jp-icon-accent4[fill] { fill: var(--jp-layout-color4); } .jp-icon-accent0[stroke] { stroke: var(--jp-layout-color0); } .jp-icon-accent1[stroke] { stroke: var(--jp-layout-color1); } .jp-icon-accent2[stroke] { stroke: var(--jp-layout-color2); } .jp-icon-accent3[stroke] { stroke: var(--jp-layout-color3); } .jp-icon-accent4[stroke] { stroke: var(--jp-layout-color4); } /* set the color of an icon to transparent */ .jp-icon-none[fill] { fill: none; } .jp-icon-none[stroke] { stroke: none; } /* brand icon colors. Same for light and dark */ .jp-icon-brand0[fill] { fill: var(--jp-brand-color0); } .jp-icon-brand1[fill] { fill: var(--jp-brand-color1); } .jp-icon-brand2[fill] { fill: var(--jp-brand-color2); } .jp-icon-brand3[fill] { fill: var(--jp-brand-color3); } .jp-icon-brand4[fill] { fill: var(--jp-brand-color4); } .jp-icon-brand0[stroke] { stroke: var(--jp-brand-color0); } .jp-icon-brand1[stroke] { stroke: var(--jp-brand-color1); } .jp-icon-brand2[stroke] { stroke: var(--jp-brand-color2); } .jp-icon-brand3[stroke] { stroke: var(--jp-brand-color3); } .jp-icon-brand4[stroke] { stroke: var(--jp-brand-color4); } /* warn icon colors. Same for light and dark */ .jp-icon-warn0[fill] { fill: var(--jp-warn-color0); } .jp-icon-warn1[fill] { fill: var(--jp-warn-color1); } .jp-icon-warn2[fill] { fill: var(--jp-warn-color2); } .jp-icon-warn3[fill] { fill: var(--jp-warn-color3); } .jp-icon-warn0[stroke] { stroke: var(--jp-warn-color0); } .jp-icon-warn1[stroke] { stroke: var(--jp-warn-color1); } .jp-icon-warn2[stroke] { stroke: var(--jp-warn-color2); } .jp-icon-warn3[stroke] { stroke: var(--jp-warn-color3); } /* icon colors that contrast well with each other and most backgrounds */ .jp-icon-contrast0[fill] { fill: var(--jp-icon-contrast-color0); } .jp-icon-contrast1[fill] { fill: var(--jp-icon-contrast-color1); } .jp-icon-contrast2[fill] { fill: var(--jp-icon-contrast-color2); } .jp-icon-contrast3[fill] { fill: var(--jp-icon-contrast-color3); } .jp-icon-contrast0[stroke] { stroke: var(--jp-icon-contrast-color0); } .jp-icon-contrast1[stroke] { stroke: var(--jp-icon-contrast-color1); } .jp-icon-contrast2[stroke] { stroke: var(--jp-icon-contrast-color2); } .jp-icon-contrast3[stroke] { stroke: var(--jp-icon-contrast-color3); } /* CSS for icons in selected items in the settings editor */ #setting-editor .jp-PluginList .jp-mod-selected .jp-icon-selectable[fill] { fill: #fff; } #setting-editor .jp-PluginList .jp-mod-selected .jp-icon-selectable-inverse[fill] { fill: var(--jp-brand-color1); } /* CSS for icons in selected filebrowser listing items */ .jp-DirListing-item.jp-mod-selected .jp-icon-selectable[fill] { fill: #fff; } .jp-DirListing-item.jp-mod-selected .jp-icon-selectable-inverse[fill] { fill: var(--jp-brand-color1); } /* CSS for icons in selected tabs in the sidebar tab manager */ #tab-manager .lm-TabBar-tab.jp-mod-active .jp-icon-selectable[fill] { fill: #fff; } #tab-manager .lm-TabBar-tab.jp-mod-active .jp-icon-selectable-inverse[fill] { fill: var(--jp-brand-color1); } #tab-manager .lm-TabBar-tab.jp-mod-active .jp-icon-hover :hover .jp-icon-selectable[fill] { fill: var(--jp-brand-color1); } #tab-manager .lm-TabBar-tab.jp-mod-active .jp-icon-hover :hover .jp-icon-selectable-inverse[fill] { fill: #fff; } /** * TODO: come up with non css-hack solution for showing the busy icon on top * of the close icon * CSS for complex behavior of close icon of tabs in the sidebar tab manager */ #tab-manager .lm-TabBar-tab.jp-mod-dirty > .lm-TabBar-tabCloseIcon > :not(:hover) > .jp-icon3[fill] { fill: none; } #tab-manager .lm-TabBar-tab.jp-mod-dirty > .lm-TabBar-tabCloseIcon > :not(:hover) > .jp-icon-busy[fill] { fill: var(--jp-inverse-layout-color3); } #tab-manager .lm-TabBar-tab.jp-mod-dirty.jp-mod-active > .lm-TabBar-tabCloseIcon > :not(:hover) > .jp-icon-busy[fill] { fill: #fff; } /** * TODO: come up with non css-hack solution for showing the busy icon on top * of the close icon * CSS for complex behavior of close icon of tabs in the main area tabbar */ .lm-DockPanel-tabBar .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty > .lm-TabBar-tabCloseIcon > :not(:hover) > .jp-icon3[fill] { fill: none; } .lm-DockPanel-tabBar .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty > .lm-TabBar-tabCloseIcon > :not(:hover) > .jp-icon-busy[fill] { fill: var(--jp-inverse-layout-color3); } /* CSS for icons in status bar */ #jp-main-statusbar .jp-mod-selected .jp-icon-selectable[fill] { fill: #fff; } #jp-main-statusbar .jp-mod-selected .jp-icon-selectable-inverse[fill] { fill: var(--jp-brand-color1); } /* special handling for splash icon CSS. While the theme CSS reloads during splash, the splash icon can loose theming. To prevent that, we set a default for its color variable */ :root { --jp-warn-color0: var(--md-orange-700); } /* not sure what to do with this one, used in filebrowser listing */ .jp-DragIcon { margin-right: 4px; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /** * Support for alt colors for icons as inline SVG HTMLElements */ /* alt recolor the primary elements of an icon */ .jp-icon-alt .jp-icon0[fill] { fill: var(--jp-layout-color0); } .jp-icon-alt .jp-icon1[fill] { fill: var(--jp-layout-color1); } .jp-icon-alt .jp-icon2[fill] { fill: var(--jp-layout-color2); } .jp-icon-alt .jp-icon3[fill] { fill: var(--jp-layout-color3); } .jp-icon-alt .jp-icon4[fill] { fill: var(--jp-layout-color4); } .jp-icon-alt .jp-icon0[stroke] { stroke: var(--jp-layout-color0); } .jp-icon-alt .jp-icon1[stroke] { stroke: var(--jp-layout-color1); } .jp-icon-alt .jp-icon2[stroke] { stroke: var(--jp-layout-color2); } .jp-icon-alt .jp-icon3[stroke] { stroke: var(--jp-layout-color3); } .jp-icon-alt .jp-icon4[stroke] { stroke: var(--jp-layout-color4); } /* alt recolor the accent elements of an icon */ .jp-icon-alt .jp-icon-accent0[fill] { fill: var(--jp-inverse-layout-color0); } .jp-icon-alt .jp-icon-accent1[fill] { fill: var(--jp-inverse-layout-color1); } .jp-icon-alt .jp-icon-accent2[fill] { fill: var(--jp-inverse-layout-color2); } .jp-icon-alt .jp-icon-accent3[fill] { fill: var(--jp-inverse-layout-color3); } .jp-icon-alt .jp-icon-accent4[fill] { fill: var(--jp-inverse-layout-color4); } .jp-icon-alt .jp-icon-accent0[stroke] { stroke: var(--jp-inverse-layout-color0); } .jp-icon-alt .jp-icon-accent1[stroke] { stroke: var(--jp-inverse-layout-color1); } .jp-icon-alt .jp-icon-accent2[stroke] { stroke: var(--jp-inverse-layout-color2); } .jp-icon-alt .jp-icon-accent3[stroke] { stroke: var(--jp-inverse-layout-color3); } .jp-icon-alt .jp-icon-accent4[stroke] { stroke: var(--jp-inverse-layout-color4); } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ .jp-icon-hoverShow:not(:hover) svg { display: none !important; } /** * Support for hover colors for icons as inline SVG HTMLElements */ /** * regular colors */ /* recolor the primary elements of an icon */ .jp-icon-hover :hover .jp-icon0-hover[fill] { fill: var(--jp-inverse-layout-color0); } .jp-icon-hover :hover .jp-icon1-hover[fill] { fill: var(--jp-inverse-layout-color1); } .jp-icon-hover :hover .jp-icon2-hover[fill] { fill: var(--jp-inverse-layout-color2); } .jp-icon-hover :hover .jp-icon3-hover[fill] { fill: var(--jp-inverse-layout-color3); } .jp-icon-hover :hover .jp-icon4-hover[fill] { fill: var(--jp-inverse-layout-color4); } .jp-icon-hover :hover .jp-icon0-hover[stroke] { stroke: var(--jp-inverse-layout-color0); } .jp-icon-hover :hover .jp-icon1-hover[stroke] { stroke: var(--jp-inverse-layout-color1); } .jp-icon-hover :hover .jp-icon2-hover[stroke] { stroke: var(--jp-inverse-layout-color2); } .jp-icon-hover :hover .jp-icon3-hover[stroke] { stroke: var(--jp-inverse-layout-color3); } .jp-icon-hover :hover .jp-icon4-hover[stroke] { stroke: var(--jp-inverse-layout-color4); } /* recolor the accent elements of an icon */ .jp-icon-hover :hover .jp-icon-accent0-hover[fill] { fill: var(--jp-layout-color0); } .jp-icon-hover :hover .jp-icon-accent1-hover[fill] { fill: var(--jp-layout-color1); } .jp-icon-hover :hover .jp-icon-accent2-hover[fill] { fill: var(--jp-layout-color2); } .jp-icon-hover :hover .jp-icon-accent3-hover[fill] { fill: var(--jp-layout-color3); } .jp-icon-hover :hover .jp-icon-accent4-hover[fill] { fill: var(--jp-layout-color4); } .jp-icon-hover :hover .jp-icon-accent0-hover[stroke] { stroke: var(--jp-layout-color0); } .jp-icon-hover :hover .jp-icon-accent1-hover[stroke] { stroke: var(--jp-layout-color1); } .jp-icon-hover :hover .jp-icon-accent2-hover[stroke] { stroke: var(--jp-layout-color2); } .jp-icon-hover :hover .jp-icon-accent3-hover[stroke] { stroke: var(--jp-layout-color3); } .jp-icon-hover :hover .jp-icon-accent4-hover[stroke] { stroke: var(--jp-layout-color4); } /* set the color of an icon to transparent */ .jp-icon-hover :hover .jp-icon-none-hover[fill] { fill: none; } .jp-icon-hover :hover .jp-icon-none-hover[stroke] { stroke: none; } /** * inverse colors */ /* inverse recolor the primary elements of an icon */ .jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[fill] { fill: var(--jp-layout-color0); } .jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[fill] { fill: var(--jp-layout-color1); } .jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[fill] { fill: var(--jp-layout-color2); } .jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[fill] { fill: var(--jp-layout-color3); } .jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[fill] { fill: var(--jp-layout-color4); } .jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[stroke] { stroke: var(--jp-layout-color0); } .jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[stroke] { stroke: var(--jp-layout-color1); } .jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[stroke] { stroke: var(--jp-layout-color2); } .jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[stroke] { stroke: var(--jp-layout-color3); } .jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[stroke] { stroke: var(--jp-layout-color4); } /* inverse recolor the accent elements of an icon */ .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[fill] { fill: var(--jp-inverse-layout-color0); } .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[fill] { fill: var(--jp-inverse-layout-color1); } .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[fill] { fill: var(--jp-inverse-layout-color2); } .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[fill] { fill: var(--jp-inverse-layout-color3); } .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[fill] { fill: var(--jp-inverse-layout-color4); } .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[stroke] { stroke: var(--jp-inverse-layout-color0); } .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[stroke] { stroke: var(--jp-inverse-layout-color1); } .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[stroke] { stroke: var(--jp-inverse-layout-color2); } .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[stroke] { stroke: var(--jp-inverse-layout-color3); } .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[stroke] { stroke: var(--jp-inverse-layout-color4); } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /* Sibling imports */ /* Override Blueprint's _reset.scss styles */ html { box-sizing: unset; } *, *::before, *::after { box-sizing: unset; } body { color: unset; font-family: var(--jp-ui-font-family); } p { margin-top: unset; margin-bottom: unset; } small { font-size: unset; } strong { font-weight: unset; } /* Override Blueprint's _typography.scss styles */ a { text-decoration: unset; color: unset; } a:hover { text-decoration: unset; color: unset; } /* Override Blueprint's _accessibility.scss styles */ :focus { outline: unset; outline-offset: unset; -moz-outline-radius: unset; } /* Styles for ui-components */ .jp-Button { border-radius: var(--jp-border-radius); padding: 0px 12px; font-size: var(--jp-ui-font-size1); } /* Use our own theme for hover styles */ button.jp-Button.bp3-button.bp3-minimal:hover { background-color: var(--jp-layout-color2); } .jp-Button.minimal { color: unset !important; } .jp-Button.jp-ToolbarButtonComponent { text-transform: none; } .jp-InputGroup input { box-sizing: border-box; border-radius: 0; background-color: transparent; color: var(--jp-ui-font-color0); box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color); } .jp-InputGroup input:focus { box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-active-box-shadow-color), inset 0 0 0 3px var(--jp-input-active-box-shadow-color); } .jp-InputGroup input::placeholder, input::placeholder { color: var(--jp-ui-font-color3); } .jp-BPIcon { display: inline-block; vertical-align: middle; margin: auto; } /* Stop blueprint futzing with our icon fills */ .bp3-icon.jp-BPIcon > svg:not([fill]) { fill: var(--jp-inverse-layout-color3); } .jp-InputGroupAction { padding: 6px; } .jp-HTMLSelect.jp-DefaultStyle select { background-color: initial; border: none; border-radius: 0; box-shadow: none; color: var(--jp-ui-font-color0); display: block; font-size: var(--jp-ui-font-size1); height: 24px; line-height: 14px; padding: 0 25px 0 10px; text-align: left; -moz-appearance: none; -webkit-appearance: none; } /* Use our own theme for hover and option styles */ .jp-HTMLSelect.jp-DefaultStyle select:hover, .jp-HTMLSelect.jp-DefaultStyle select > option { background-color: var(--jp-layout-color2); color: var(--jp-ui-font-color0); } select { box-sizing: border-box; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /* This file was auto-generated by ensurePackage() in @jupyterlab/buildutils */ /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ .jp-Collapse { display: flex; flex-direction: column; align-items: stretch; border-top: 1px solid var(--jp-border-color2); border-bottom: 1px solid var(--jp-border-color2); } .jp-Collapse-header { padding: 1px 12px; color: var(--jp-ui-font-color1); background-color: var(--jp-layout-color1); font-size: var(--jp-ui-font-size2); } .jp-Collapse-header:hover { background-color: var(--jp-layout-color2); } .jp-Collapse-contents { padding: 0px 12px 0px 12px; background-color: var(--jp-layout-color1); color: var(--jp-ui-font-color1); overflow: auto; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /*----------------------------------------------------------------------------- | Variables |----------------------------------------------------------------------------*/ :root { --jp-private-commandpalette-search-height: 28px; } /*----------------------------------------------------------------------------- | Overall styles |----------------------------------------------------------------------------*/ .lm-CommandPalette { padding-bottom: 0px; color: var(--jp-ui-font-color1); background: var(--jp-layout-color1); /* This is needed so that all font sizing of children done in ems is * relative to this base size */ font-size: var(--jp-ui-font-size1); } /*----------------------------------------------------------------------------- | Search |----------------------------------------------------------------------------*/ .lm-CommandPalette-search { padding: 4px; background-color: var(--jp-layout-color1); z-index: 2; } .lm-CommandPalette-wrapper { overflow: overlay; padding: 0px 9px; background-color: var(--jp-input-active-background); height: 30px; box-shadow: inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color); } .lm-CommandPalette.lm-mod-focused .lm-CommandPalette-wrapper { box-shadow: inset 0 0 0 1px var(--jp-input-active-box-shadow-color), inset 0 0 0 3px var(--jp-input-active-box-shadow-color); } .lm-CommandPalette-wrapper::after { content: ' '; color: white; background-color: var(--jp-brand-color1); position: absolute; top: 4px; right: 4px; height: 30px; width: 10px; padding: 0px 10px; background-image: var(--jp-icon-search-white); background-size: 20px; background-repeat: no-repeat; background-position: center; } .lm-CommandPalette-input { background: transparent; width: calc(100% - 18px); float: left; border: none; outline: none; font-size: var(--jp-ui-font-size1); color: var(--jp-ui-font-color0); line-height: var(--jp-private-commandpalette-search-height); } .lm-CommandPalette-input::-webkit-input-placeholder, .lm-CommandPalette-input::-moz-placeholder, .lm-CommandPalette-input:-ms-input-placeholder { color: var(--jp-ui-font-color3); font-size: var(--jp-ui-font-size1); } /*----------------------------------------------------------------------------- | Results |----------------------------------------------------------------------------*/ .lm-CommandPalette-header:first-child { margin-top: 0px; } .lm-CommandPalette-header { border-bottom: solid var(--jp-border-width) var(--jp-border-color2); color: var(--jp-ui-font-color1); cursor: pointer; display: flex; font-size: var(--jp-ui-font-size0); font-weight: 600; letter-spacing: 1px; margin-top: 8px; padding: 8px 0 8px 12px; text-transform: uppercase; } .lm-CommandPalette-header.lm-mod-active { background: var(--jp-layout-color2); } .lm-CommandPalette-header > mark { background-color: transparent; font-weight: bold; color: var(--jp-ui-font-color1); } .lm-CommandPalette-item { padding: 4px 12px 4px 4px; color: var(--jp-ui-font-color1); font-size: var(--jp-ui-font-size1); font-weight: 400; display: flex; } .lm-CommandPalette-item.lm-mod-disabled { color: var(--jp-ui-font-color3); } .lm-CommandPalette-item.lm-mod-active { background: var(--jp-layout-color3); } .lm-CommandPalette-item.lm-mod-active:hover:not(.lm-mod-disabled) { background: var(--jp-layout-color4); } .lm-CommandPalette-item:hover:not(.lm-mod-active):not(.lm-mod-disabled) { background: var(--jp-layout-color2); } .lm-CommandPalette-itemContent { overflow: hidden; } .lm-CommandPalette-itemLabel > mark { color: var(--jp-ui-font-color0); background-color: transparent; font-weight: bold; } .lm-CommandPalette-item.lm-mod-disabled mark { color: var(--jp-ui-font-color3); } .lm-CommandPalette-item .lm-CommandPalette-itemIcon { margin: 0 4px 0 0; position: relative; width: 16px; top: 2px; flex: 0 0 auto; } .lm-CommandPalette-item.lm-mod-disabled .lm-CommandPalette-itemIcon { opacity: 0.4; } .lm-CommandPalette-item .lm-CommandPalette-itemShortcut { flex: 0 0 auto; } .lm-CommandPalette-itemCaption { display: none; } .lm-CommandPalette-content { background-color: var(--jp-layout-color1); } .lm-CommandPalette-content:empty:after { content: 'No results'; margin: auto; margin-top: 20px; width: 100px; display: block; font-size: var(--jp-ui-font-size2); font-family: var(--jp-ui-font-family); font-weight: lighter; } .lm-CommandPalette-emptyMessage { text-align: center; margin-top: 24px; line-height: 1.32; padding: 0px 8px; color: var(--jp-content-font-color3); } /*----------------------------------------------------------------------------- | Copyright (c) 2014-2017, Jupyter Development Team. | | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ .jp-Dialog { position: absolute; z-index: 10000; display: flex; flex-direction: column; align-items: center; justify-content: center; top: 0px; left: 0px; margin: 0; padding: 0; width: 100%; height: 100%; background: var(--jp-dialog-background); } .jp-Dialog-content { display: flex; flex-direction: column; margin-left: auto; margin-right: auto; background: var(--jp-layout-color1); padding: 24px; padding-bottom: 12px; min-width: 300px; min-height: 150px; max-width: 1000px; max-height: 500px; box-sizing: border-box; box-shadow: var(--jp-elevation-z20); word-wrap: break-word; border-radius: var(--jp-border-radius); /* This is needed so that all font sizing of children done in ems is * relative to this base size */ font-size: var(--jp-ui-font-size1); color: var(--jp-ui-font-color1); } .jp-Dialog-button { overflow: visible; } button.jp-Dialog-button:focus { outline: 1px solid var(--jp-brand-color1); outline-offset: 4px; -moz-outline-radius: 0px; } button.jp-Dialog-button:focus::-moz-focus-inner { border: 0; } .jp-Dialog-header { flex: 0 0 auto; padding-bottom: 12px; font-size: var(--jp-ui-font-size3); font-weight: 400; color: var(--jp-ui-font-color0); } .jp-Dialog-body { display: flex; flex-direction: column; flex: 1 1 auto; font-size: var(--jp-ui-font-size1); background: var(--jp-layout-color1); overflow: auto; } .jp-Dialog-footer { display: flex; flex-direction: row; justify-content: flex-end; flex: 0 0 auto; margin-left: -12px; margin-right: -12px; padding: 12px; } .jp-Dialog-title { overflow: hidden; white-space: nowrap; text-overflow: ellipsis; } .jp-Dialog-body > .jp-select-wrapper { width: 100%; } .jp-Dialog-body > button { padding: 0px 16px; } .jp-Dialog-body > label { line-height: 1.4; color: var(--jp-ui-font-color0); } .jp-Dialog-button.jp-mod-styled:not(:last-child) { margin-right: 12px; } /*----------------------------------------------------------------------------- | Copyright (c) 2014-2016, Jupyter Development Team. | | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ .jp-HoverBox { position: fixed; } .jp-HoverBox.jp-mod-outofview { display: none; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ .jp-IFrame { width: 100%; height: 100%; } .jp-IFrame > iframe { border: none; } /* When drag events occur, `p-mod-override-cursor` is added to the body. Because iframes steal all cursor events, the following two rules are necessary to suppress pointer events while resize drags are occurring. There may be a better solution to this problem. */ body.lm-mod-override-cursor .jp-IFrame { position: relative; } body.lm-mod-override-cursor .jp-IFrame:before { content: ''; position: absolute; top: 0; left: 0; right: 0; bottom: 0; background: transparent; } /*----------------------------------------------------------------------------- | Copyright (c) 2014-2016, Jupyter Development Team. | | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ .jp-MainAreaWidget > :focus { outline: none; } /** * google-material-color v1.2.6 * https://github.com/danlevan/google-material-color */ :root { --md-red-50: #ffebee; --md-red-100: #ffcdd2; --md-red-200: #ef9a9a; --md-red-300: #e57373; --md-red-400: #ef5350; --md-red-500: #f44336; --md-red-600: #e53935; --md-red-700: #d32f2f; --md-red-800: #c62828; --md-red-900: #b71c1c; --md-red-A100: #ff8a80; --md-red-A200: #ff5252; --md-red-A400: #ff1744; --md-red-A700: #d50000; --md-pink-50: #fce4ec; --md-pink-100: #f8bbd0; --md-pink-200: #f48fb1; --md-pink-300: #f06292; --md-pink-400: #ec407a; --md-pink-500: #e91e63; --md-pink-600: #d81b60; --md-pink-700: #c2185b; --md-pink-800: #ad1457; --md-pink-900: #880e4f; --md-pink-A100: #ff80ab; --md-pink-A200: #ff4081; --md-pink-A400: #f50057; --md-pink-A700: #c51162; --md-purple-50: #f3e5f5; --md-purple-100: #e1bee7; --md-purple-200: #ce93d8; --md-purple-300: #ba68c8; --md-purple-400: #ab47bc; --md-purple-500: #9c27b0; --md-purple-600: #8e24aa; --md-purple-700: #7b1fa2; --md-purple-800: #6a1b9a; --md-purple-900: #4a148c; --md-purple-A100: #ea80fc; --md-purple-A200: #e040fb; --md-purple-A400: #d500f9; --md-purple-A700: #aa00ff; --md-deep-purple-50: #ede7f6; --md-deep-purple-100: #d1c4e9; --md-deep-purple-200: #b39ddb; --md-deep-purple-300: #9575cd; --md-deep-purple-400: #7e57c2; --md-deep-purple-500: #673ab7; --md-deep-purple-600: #5e35b1; --md-deep-purple-700: #512da8; --md-deep-purple-800: #4527a0; --md-deep-purple-900: #311b92; --md-deep-purple-A100: #b388ff; --md-deep-purple-A200: #7c4dff; --md-deep-purple-A400: #651fff; --md-deep-purple-A700: #6200ea; --md-indigo-50: #e8eaf6; --md-indigo-100: #c5cae9; --md-indigo-200: #9fa8da; --md-indigo-300: #7986cb; --md-indigo-400: #5c6bc0; --md-indigo-500: #3f51b5; --md-indigo-600: #3949ab; --md-indigo-700: #303f9f; --md-indigo-800: #283593; --md-indigo-900: #1a237e; --md-indigo-A100: #8c9eff; --md-indigo-A200: #536dfe; --md-indigo-A400: #3d5afe; --md-indigo-A700: #304ffe; --md-blue-50: #e3f2fd; --md-blue-100: #bbdefb; --md-blue-200: #90caf9; --md-blue-300: #64b5f6; --md-blue-400: #42a5f5; --md-blue-500: #2196f3; --md-blue-600: #1e88e5; --md-blue-700: #1976d2; --md-blue-800: #1565c0; --md-blue-900: #0d47a1; --md-blue-A100: #82b1ff; --md-blue-A200: #448aff; --md-blue-A400: #2979ff; --md-blue-A700: #2962ff; --md-light-blue-50: #e1f5fe; --md-light-blue-100: #b3e5fc; --md-light-blue-200: #81d4fa; --md-light-blue-300: #4fc3f7; --md-light-blue-400: #29b6f6; --md-light-blue-500: #03a9f4; --md-light-blue-600: #039be5; --md-light-blue-700: #0288d1; --md-light-blue-800: #0277bd; --md-light-blue-900: #01579b; --md-light-blue-A100: #80d8ff; --md-light-blue-A200: #40c4ff; --md-light-blue-A400: #00b0ff; --md-light-blue-A700: #0091ea; --md-cyan-50: #e0f7fa; --md-cyan-100: #b2ebf2; --md-cyan-200: #80deea; --md-cyan-300: #4dd0e1; --md-cyan-400: #26c6da; --md-cyan-500: #00bcd4; --md-cyan-600: #00acc1; --md-cyan-700: #0097a7; --md-cyan-800: #00838f; --md-cyan-900: #006064; --md-cyan-A100: #84ffff; --md-cyan-A200: #18ffff; --md-cyan-A400: #00e5ff; --md-cyan-A700: #00b8d4; --md-teal-50: #e0f2f1; --md-teal-100: #b2dfdb; --md-teal-200: #80cbc4; --md-teal-300: #4db6ac; --md-teal-400: #26a69a; --md-teal-500: #009688; --md-teal-600: #00897b; --md-teal-700: #00796b; --md-teal-800: #00695c; --md-teal-900: #004d40; --md-teal-A100: #a7ffeb; --md-teal-A200: #64ffda; --md-teal-A400: #1de9b6; --md-teal-A700: #00bfa5; --md-green-50: #e8f5e9; --md-green-100: #c8e6c9; --md-green-200: #a5d6a7; --md-green-300: #81c784; --md-green-400: #66bb6a; --md-green-500: #4caf50; --md-green-600: #43a047; --md-green-700: #388e3c; --md-green-800: #2e7d32; --md-green-900: #1b5e20; --md-green-A100: #b9f6ca; --md-green-A200: #69f0ae; --md-green-A400: #00e676; --md-green-A700: #00c853; --md-light-green-50: #f1f8e9; --md-light-green-100: #dcedc8; --md-light-green-200: #c5e1a5; --md-light-green-300: #aed581; --md-light-green-400: #9ccc65; --md-light-green-500: #8bc34a; --md-light-green-600: #7cb342; --md-light-green-700: #689f38; --md-light-green-800: #558b2f; --md-light-green-900: #33691e; --md-light-green-A100: #ccff90; --md-light-green-A200: #b2ff59; --md-light-green-A400: #76ff03; --md-light-green-A700: #64dd17; --md-lime-50: #f9fbe7; --md-lime-100: #f0f4c3; --md-lime-200: #e6ee9c; --md-lime-300: #dce775; --md-lime-400: #d4e157; --md-lime-500: #cddc39; --md-lime-600: #c0ca33; --md-lime-700: #afb42b; --md-lime-800: #9e9d24; --md-lime-900: #827717; --md-lime-A100: #f4ff81; --md-lime-A200: #eeff41; --md-lime-A400: #c6ff00; --md-lime-A700: #aeea00; --md-yellow-50: #fffde7; --md-yellow-100: #fff9c4; --md-yellow-200: #fff59d; --md-yellow-300: #fff176; --md-yellow-400: #ffee58; --md-yellow-500: #ffeb3b; --md-yellow-600: #fdd835; --md-yellow-700: #fbc02d; --md-yellow-800: #f9a825; --md-yellow-900: #f57f17; --md-yellow-A100: #ffff8d; --md-yellow-A200: #ffff00; --md-yellow-A400: #ffea00; --md-yellow-A700: #ffd600; --md-amber-50: #fff8e1; --md-amber-100: #ffecb3; --md-amber-200: #ffe082; --md-amber-300: #ffd54f; --md-amber-400: #ffca28; --md-amber-500: #ffc107; --md-amber-600: #ffb300; --md-amber-700: #ffa000; --md-amber-800: #ff8f00; --md-amber-900: #ff6f00; --md-amber-A100: #ffe57f; --md-amber-A200: #ffd740; --md-amber-A400: #ffc400; --md-amber-A700: #ffab00; --md-orange-50: #fff3e0; --md-orange-100: #ffe0b2; --md-orange-200: #ffcc80; --md-orange-300: #ffb74d; --md-orange-400: #ffa726; --md-orange-500: #ff9800; --md-orange-600: #fb8c00; --md-orange-700: #f57c00; --md-orange-800: #ef6c00; --md-orange-900: #e65100; --md-orange-A100: #ffd180; --md-orange-A200: #ffab40; --md-orange-A400: #ff9100; --md-orange-A700: #ff6d00; --md-deep-orange-50: #fbe9e7; --md-deep-orange-100: #ffccbc; --md-deep-orange-200: #ffab91; --md-deep-orange-300: #ff8a65; --md-deep-orange-400: #ff7043; --md-deep-orange-500: #ff5722; --md-deep-orange-600: #f4511e; --md-deep-orange-700: #e64a19; --md-deep-orange-800: #d84315; --md-deep-orange-900: #bf360c; --md-deep-orange-A100: #ff9e80; --md-deep-orange-A200: #ff6e40; --md-deep-orange-A400: #ff3d00; --md-deep-orange-A700: #dd2c00; --md-brown-50: #efebe9; --md-brown-100: #d7ccc8; --md-brown-200: #bcaaa4; --md-brown-300: #a1887f; --md-brown-400: #8d6e63; --md-brown-500: #795548; --md-brown-600: #6d4c41; --md-brown-700: #5d4037; --md-brown-800: #4e342e; --md-brown-900: #3e2723; --md-grey-50: #fafafa; --md-grey-100: #f5f5f5; --md-grey-200: #eeeeee; --md-grey-300: #e0e0e0; --md-grey-400: #bdbdbd; --md-grey-500: #9e9e9e; --md-grey-600: #757575; --md-grey-700: #616161; --md-grey-800: #424242; --md-grey-900: #212121; --md-blue-grey-50: #eceff1; --md-blue-grey-100: #cfd8dc; --md-blue-grey-200: #b0bec5; --md-blue-grey-300: #90a4ae; --md-blue-grey-400: #78909c; --md-blue-grey-500: #607d8b; --md-blue-grey-600: #546e7a; --md-blue-grey-700: #455a64; --md-blue-grey-800: #37474f; --md-blue-grey-900: #263238; } /*----------------------------------------------------------------------------- | Copyright (c) 2017, Jupyter Development Team. | | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ .jp-Spinner { position: absolute; display: flex; justify-content: center; align-items: center; z-index: 10; left: 0; top: 0; width: 100%; height: 100%; background: var(--jp-layout-color0); outline: none; } .jp-SpinnerContent { font-size: 10px; margin: 50px auto; text-indent: -9999em; width: 3em; height: 3em; border-radius: 50%; background: var(--jp-brand-color3); background: linear-gradient( to right, #f37626 10%, rgba(255, 255, 255, 0) 42% ); position: relative; animation: load3 1s infinite linear, fadeIn 1s; } .jp-SpinnerContent:before { width: 50%; height: 50%; background: #f37626; border-radius: 100% 0 0 0; position: absolute; top: 0; left: 0; content: ''; } .jp-SpinnerContent:after { background: var(--jp-layout-color0); width: 75%; height: 75%; border-radius: 50%; content: ''; margin: auto; position: absolute; top: 0; left: 0; bottom: 0; right: 0; } @keyframes fadeIn { 0% { opacity: 0; } 100% { opacity: 1; } } @keyframes load3 { 0% { transform: rotate(0deg); } 100% { transform: rotate(360deg); } } /*----------------------------------------------------------------------------- | Copyright (c) 2014-2017, Jupyter Development Team. | | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ button.jp-mod-styled { font-size: var(--jp-ui-font-size1); color: var(--jp-ui-font-color0); border: none; box-sizing: border-box; text-align: center; line-height: 32px; height: 32px; padding: 0px 12px; letter-spacing: 0.8px; outline: none; appearance: none; -webkit-appearance: none; -moz-appearance: none; } input.jp-mod-styled { background: var(--jp-input-background); height: 28px; box-sizing: border-box; border: var(--jp-border-width) solid var(--jp-border-color1); padding-left: 7px; padding-right: 7px; font-size: var(--jp-ui-font-size2); color: var(--jp-ui-font-color0); outline: none; appearance: none; -webkit-appearance: none; -moz-appearance: none; } input.jp-mod-styled:focus { border: var(--jp-border-width) solid var(--md-blue-500); box-shadow: inset 0 0 4px var(--md-blue-300); } .jp-select-wrapper { display: flex; position: relative; flex-direction: column; padding: 1px; background-color: var(--jp-layout-color1); height: 28px; box-sizing: border-box; margin-bottom: 12px; } .jp-select-wrapper.jp-mod-focused select.jp-mod-styled { border: var(--jp-border-width) solid var(--jp-input-active-border-color); box-shadow: var(--jp-input-box-shadow); background-color: var(--jp-input-active-background); } select.jp-mod-styled:hover { background-color: var(--jp-layout-color1); cursor: pointer; color: var(--jp-ui-font-color0); background-color: var(--jp-input-hover-background); box-shadow: inset 0 0px 1px rgba(0, 0, 0, 0.5); } select.jp-mod-styled { flex: 1 1 auto; height: 32px; width: 100%; font-size: var(--jp-ui-font-size2); background: var(--jp-input-background); color: var(--jp-ui-font-color0); padding: 0 25px 0 8px; border: var(--jp-border-width) solid var(--jp-input-border-color); border-radius: 0px; outline: none; appearance: none; -webkit-appearance: none; -moz-appearance: none; } /*----------------------------------------------------------------------------- | Copyright (c) 2014-2016, Jupyter Development Team. | | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ :root { --jp-private-toolbar-height: calc( 28px + var(--jp-border-width) ); /* leave 28px for content */ } .jp-Toolbar { color: var(--jp-ui-font-color1); flex: 0 0 auto; display: flex; flex-direction: row; border-bottom: var(--jp-border-width) solid var(--jp-toolbar-border-color); box-shadow: var(--jp-toolbar-box-shadow); background: var(--jp-toolbar-background); min-height: var(--jp-toolbar-micro-height); padding: 2px; z-index: 1; } /* Toolbar items */ .jp-Toolbar > .jp-Toolbar-item.jp-Toolbar-spacer { flex-grow: 1; flex-shrink: 1; } .jp-Toolbar-item.jp-Toolbar-kernelStatus { display: inline-block; width: 32px; background-repeat: no-repeat; background-position: center; background-size: 16px; } .jp-Toolbar > .jp-Toolbar-item { flex: 0 0 auto; display: flex; padding-left: 1px; padding-right: 1px; font-size: var(--jp-ui-font-size1); line-height: var(--jp-private-toolbar-height); height: 100%; } /* Toolbar buttons */ /* This is the div we use to wrap the react component into a Widget */ div.jp-ToolbarButton { color: transparent; border: none; box-sizing: border-box; outline: none; appearance: none; -webkit-appearance: none; -moz-appearance: none; padding: 0px; margin: 0px; } button.jp-ToolbarButtonComponent { background: var(--jp-layout-color1); border: none; box-sizing: border-box; outline: none; appearance: none; -webkit-appearance: none; -moz-appearance: none; padding: 0px 6px; margin: 0px; height: 24px; border-radius: var(--jp-border-radius); display: flex; align-items: center; text-align: center; font-size: 14px; min-width: unset; min-height: unset; } button.jp-ToolbarButtonComponent:disabled { opacity: 0.4; } button.jp-ToolbarButtonComponent span { padding: 0px; flex: 0 0 auto; } button.jp-ToolbarButtonComponent .jp-ToolbarButtonComponent-label { font-size: var(--jp-ui-font-size1); line-height: 100%; padding-left: 2px; color: var(--jp-ui-font-color1); } /*----------------------------------------------------------------------------- | Copyright (c) 2014-2017, Jupyter Development Team. | | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /* This file was auto-generated by ensurePackage() in @jupyterlab/buildutils */ /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Copyright (c) 2014-2017, PhosphorJS Contributors | | Distributed under the terms of the BSD 3-Clause License. | | The full license is in the file LICENSE, distributed with this software. |----------------------------------------------------------------------------*/ /* <DEPRECATED> */ body.p-mod-override-cursor *, /* </DEPRECATED> */ body.lm-mod-override-cursor * { cursor: inherit !important; } /*----------------------------------------------------------------------------- | Copyright (c) 2014-2016, Jupyter Development Team. | | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ .jp-JSONEditor { display: flex; flex-direction: column; width: 100%; } .jp-JSONEditor-host { flex: 1 1 auto; border: var(--jp-border-width) solid var(--jp-input-border-color); border-radius: 0px; background: var(--jp-layout-color0); min-height: 50px; padding: 1px; } .jp-JSONEditor.jp-mod-error .jp-JSONEditor-host { border-color: red; outline-color: red; } .jp-JSONEditor-header { display: flex; flex: 1 0 auto; padding: 0 0 0 12px; } .jp-JSONEditor-header label { flex: 0 0 auto; } .jp-JSONEditor-commitButton { height: 16px; width: 16px; background-size: 18px; background-repeat: no-repeat; background-position: center; } .jp-JSONEditor-host.jp-mod-focused { background-color: var(--jp-input-active-background); border: 1px solid var(--jp-input-active-border-color); box-shadow: var(--jp-input-box-shadow); } .jp-Editor.jp-mod-dropTarget { border: var(--jp-border-width) solid var(--jp-input-active-border-color); box-shadow: var(--jp-input-box-shadow); } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /* This file was auto-generated by ensurePackage() in @jupyterlab/buildutils */ /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /* BASICS */ .CodeMirror { /* Set height, width, borders, and global font properties here */ font-family: monospace; height: 300px; color: black; direction: ltr; } /* PADDING */ .CodeMirror-lines { padding: 4px 0; /* Vertical padding around content */ } .CodeMirror pre.CodeMirror-line, .CodeMirror pre.CodeMirror-line-like { padding: 0 4px; /* Horizontal padding of content */ } .CodeMirror-scrollbar-filler, .CodeMirror-gutter-filler { background-color: white; /* The little square between H and V scrollbars */ } /* GUTTER */ .CodeMirror-gutters { border-right: 1px solid #ddd; background-color: #f7f7f7; white-space: nowrap; } .CodeMirror-linenumbers {} .CodeMirror-linenumber { padding: 0 3px 0 5px; min-width: 20px; text-align: right; color: #999; white-space: nowrap; } .CodeMirror-guttermarker { color: black; } .CodeMirror-guttermarker-subtle { color: #999; } /* CURSOR */ .CodeMirror-cursor { border-left: 1px solid black; border-right: none; width: 0; } /* Shown when moving in bi-directional text */ .CodeMirror div.CodeMirror-secondarycursor { border-left: 1px solid silver; } .cm-fat-cursor .CodeMirror-cursor { width: auto; border: 0 !important; background: #7e7; } .cm-fat-cursor div.CodeMirror-cursors { z-index: 1; } .cm-fat-cursor-mark { background-color: rgba(20, 255, 20, 0.5); -webkit-animation: blink 1.06s steps(1) infinite; -moz-animation: blink 1.06s steps(1) infinite; animation: blink 1.06s steps(1) infinite; } .cm-animate-fat-cursor { width: auto; border: 0; -webkit-animation: blink 1.06s steps(1) infinite; -moz-animation: blink 1.06s steps(1) infinite; animation: blink 1.06s steps(1) infinite; background-color: #7e7; } @-moz-keyframes blink { 0% {} 50% { background-color: transparent; } 100% {} } @-webkit-keyframes blink { 0% {} 50% { background-color: transparent; } 100% {} } @keyframes blink { 0% {} 50% { background-color: transparent; } 100% {} } /* Can style cursor different in overwrite (non-insert) mode */ .CodeMirror-overwrite .CodeMirror-cursor {} .cm-tab { display: inline-block; text-decoration: inherit; } .CodeMirror-rulers { position: absolute; left: 0; right: 0; top: -50px; bottom: 0; overflow: hidden; } .CodeMirror-ruler { border-left: 1px solid #ccc; top: 0; bottom: 0; position: absolute; } /* DEFAULT THEME */ .cm-s-default .cm-header {color: blue;} .cm-s-default .cm-quote {color: #090;} .cm-negative {color: #d44;} .cm-positive {color: #292;} .cm-header, .cm-strong {font-weight: bold;} .cm-em {font-style: italic;} .cm-link {text-decoration: underline;} .cm-strikethrough {text-decoration: line-through;} .cm-s-default .cm-keyword {color: #708;} .cm-s-default .cm-atom {color: #219;} .cm-s-default .cm-number {color: #164;} .cm-s-default .cm-def {color: #00f;} .cm-s-default .cm-variable, .cm-s-default .cm-punctuation, .cm-s-default .cm-property, .cm-s-default .cm-operator {} .cm-s-default .cm-variable-2 {color: #05a;} .cm-s-default .cm-variable-3, .cm-s-default .cm-type {color: #085;} .cm-s-default .cm-comment {color: #a50;} .cm-s-default .cm-string {color: #a11;} .cm-s-default .cm-string-2 {color: #f50;} .cm-s-default .cm-meta {color: #555;} .cm-s-default .cm-qualifier {color: #555;} .cm-s-default .cm-builtin {color: #30a;} .cm-s-default .cm-bracket {color: #997;} .cm-s-default .cm-tag {color: #170;} .cm-s-default .cm-attribute {color: #00c;} .cm-s-default .cm-hr {color: #999;} .cm-s-default .cm-link {color: #00c;} .cm-s-default .cm-error {color: #f00;} .cm-invalidchar {color: #f00;} .CodeMirror-composing { border-bottom: 2px solid; } /* Default styles for common addons */ div.CodeMirror span.CodeMirror-matchingbracket {color: #0b0;} div.CodeMirror span.CodeMirror-nonmatchingbracket {color: #a22;} .CodeMirror-matchingtag { background: rgba(255, 150, 0, .3); } .CodeMirror-activeline-background {background: #e8f2ff;} /* STOP */ /* The rest of this file contains styles related to the mechanics of the editor. You probably shouldn't touch them. */ .CodeMirror { position: relative; overflow: hidden; background: white; } .CodeMirror-scroll { overflow: scroll !important; /* Things will break if this is overridden */ /* 30px is the magic margin used to hide the element's real scrollbars */ /* See overflow: hidden in .CodeMirror */ margin-bottom: -30px; margin-right: -30px; padding-bottom: 30px; height: 100%; outline: none; /* Prevent dragging from highlighting the element */ position: relative; } .CodeMirror-sizer { position: relative; border-right: 30px solid transparent; } /* The fake, visible scrollbars. Used to force redraw during scrolling before actual scrolling happens, thus preventing shaking and flickering artifacts. */ .CodeMirror-vscrollbar, .CodeMirror-hscrollbar, .CodeMirror-scrollbar-filler, .CodeMirror-gutter-filler { position: absolute; z-index: 6; display: none; } .CodeMirror-vscrollbar { right: 0; top: 0; overflow-x: hidden; overflow-y: scroll; } .CodeMirror-hscrollbar { bottom: 0; left: 0; overflow-y: hidden; overflow-x: scroll; } .CodeMirror-scrollbar-filler { right: 0; bottom: 0; } .CodeMirror-gutter-filler { left: 0; bottom: 0; } .CodeMirror-gutters { position: absolute; left: 0; top: 0; min-height: 100%; z-index: 3; } .CodeMirror-gutter { white-space: normal; height: 100%; display: inline-block; vertical-align: top; margin-bottom: -30px; } .CodeMirror-gutter-wrapper { position: absolute; z-index: 4; background: none !important; border: none !important; } .CodeMirror-gutter-background { position: absolute; top: 0; bottom: 0; z-index: 4; } .CodeMirror-gutter-elt { position: absolute; cursor: default; z-index: 4; } .CodeMirror-gutter-wrapper ::selection { background-color: transparent } .CodeMirror-gutter-wrapper ::-moz-selection { background-color: transparent } .CodeMirror-lines { cursor: text; min-height: 1px; /* prevents collapsing before first draw */ } .CodeMirror pre.CodeMirror-line, .CodeMirror pre.CodeMirror-line-like { /* Reset some styles that the rest of the page might have set */ -moz-border-radius: 0; -webkit-border-radius: 0; border-radius: 0; border-width: 0; background: transparent; font-family: inherit; font-size: inherit; margin: 0; white-space: pre; word-wrap: normal; line-height: inherit; color: inherit; z-index: 2; position: relative; overflow: visible; -webkit-tap-highlight-color: transparent; -webkit-font-variant-ligatures: contextual; font-variant-ligatures: contextual; } .CodeMirror-wrap pre.CodeMirror-line, .CodeMirror-wrap pre.CodeMirror-line-like { word-wrap: break-word; white-space: pre-wrap; word-break: normal; } .CodeMirror-linebackground { position: absolute; left: 0; right: 0; top: 0; bottom: 0; z-index: 0; } .CodeMirror-linewidget { position: relative; z-index: 2; padding: 0.1px; /* Force widget margins to stay inside of the container */ } .CodeMirror-widget {} .CodeMirror-rtl pre { direction: rtl; } .CodeMirror-code { outline: none; } /* Force content-box sizing for the elements where we expect it */ .CodeMirror-scroll, .CodeMirror-sizer, .CodeMirror-gutter, .CodeMirror-gutters, .CodeMirror-linenumber { -moz-box-sizing: content-box; box-sizing: content-box; } .CodeMirror-measure { position: absolute; width: 100%; height: 0; overflow: hidden; visibility: hidden; } .CodeMirror-cursor { position: absolute; pointer-events: none; } .CodeMirror-measure pre { position: static; } div.CodeMirror-cursors { visibility: hidden; position: relative; z-index: 3; } div.CodeMirror-dragcursors { visibility: visible; } .CodeMirror-focused div.CodeMirror-cursors { visibility: visible; } .CodeMirror-selected { background: #d9d9d9; } .CodeMirror-focused .CodeMirror-selected { background: #d7d4f0; } .CodeMirror-crosshair { cursor: crosshair; } .CodeMirror-line::selection, .CodeMirror-line > span::selection, .CodeMirror-line > span > span::selection { background: #d7d4f0; } .CodeMirror-line::-moz-selection, .CodeMirror-line > span::-moz-selection, .CodeMirror-line > span > span::-moz-selection { background: #d7d4f0; } .cm-searching { background-color: #ffa; background-color: rgba(255, 255, 0, .4); } /* Used to force a border model for a node */ .cm-force-border { padding-right: .1px; } @media print { /* Hide the cursor when printing */ .CodeMirror div.CodeMirror-cursors { visibility: hidden; } } /* See issue #2901 */ .cm-tab-wrap-hack:after { content: ''; } /* Help users use markselection to safely style text background */ span.CodeMirror-selectedtext { background: none; } .CodeMirror-dialog { position: absolute; left: 0; right: 0; background: inherit; z-index: 15; padding: .1em .8em; overflow: hidden; color: inherit; } .CodeMirror-dialog-top { border-bottom: 1px solid #eee; top: 0; } .CodeMirror-dialog-bottom { border-top: 1px solid #eee; bottom: 0; } .CodeMirror-dialog input { border: none; outline: none; background: transparent; width: 20em; color: inherit; font-family: monospace; } .CodeMirror-dialog button { font-size: 70%; } .CodeMirror-foldmarker { color: blue; text-shadow: #b9f 1px 1px 2px, #b9f -1px -1px 2px, #b9f 1px -1px 2px, #b9f -1px 1px 2px; font-family: arial; line-height: .3; cursor: pointer; } .CodeMirror-foldgutter { width: .7em; } .CodeMirror-foldgutter-open, .CodeMirror-foldgutter-folded { cursor: pointer; } .CodeMirror-foldgutter-open:after { content: \"\\25BE\"; } .CodeMirror-foldgutter-folded:after { content: \"\\25B8\"; } /* Name: material Author: Mattia Astorino (http://github.com/equinusocio) Website: https://material-theme.site/ */ .cm-s-material.CodeMirror { background-color: #263238; color: #EEFFFF; } .cm-s-material .CodeMirror-gutters { background: #263238; color: #546E7A; border: none; } .cm-s-material .CodeMirror-guttermarker, .cm-s-material .CodeMirror-guttermarker-subtle, .cm-s-material .CodeMirror-linenumber { color: #546E7A; } .cm-s-material .CodeMirror-cursor { border-left: 1px solid #FFCC00; } .cm-s-material div.CodeMirror-selected { background: rgba(128, 203, 196, 0.2); } .cm-s-material.CodeMirror-focused div.CodeMirror-selected { background: rgba(128, 203, 196, 0.2); } .cm-s-material .CodeMirror-line::selection, .cm-s-material .CodeMirror-line>span::selection, .cm-s-material .CodeMirror-line>span>span::selection { background: rgba(128, 203, 196, 0.2); } .cm-s-material .CodeMirror-line::-moz-selection, .cm-s-material .CodeMirror-line>span::-moz-selection, .cm-s-material .CodeMirror-line>span>span::-moz-selection { background: rgba(128, 203, 196, 0.2); } .cm-s-material .CodeMirror-activeline-background { background: rgba(0, 0, 0, 0.5); } .cm-s-material .cm-keyword { color: #C792EA; } .cm-s-material .cm-operator { color: #89DDFF; } .cm-s-material .cm-variable-2 { color: #EEFFFF; } .cm-s-material .cm-variable-3, .cm-s-material .cm-type { color: #f07178; } .cm-s-material .cm-builtin { color: #FFCB6B; } .cm-s-material .cm-atom { color: #F78C6C; } .cm-s-material .cm-number { color: #FF5370; } .cm-s-material .cm-def { color: #82AAFF; } .cm-s-material .cm-string { color: #C3E88D; } .cm-s-material .cm-string-2 { color: #f07178; } .cm-s-material .cm-comment { color: #546E7A; } .cm-s-material .cm-variable { color: #f07178; } .cm-s-material .cm-tag { color: #FF5370; } .cm-s-material .cm-meta { color: #FFCB6B; } .cm-s-material .cm-attribute { color: #C792EA; } .cm-s-material .cm-property { color: #C792EA; } .cm-s-material .cm-qualifier { color: #DECB6B; } .cm-s-material .cm-variable-3, .cm-s-material .cm-type { color: #DECB6B; } .cm-s-material .cm-error { color: rgba(255, 255, 255, 1.0); background-color: #FF5370; } .cm-s-material .CodeMirror-matchingbracket { text-decoration: underline; color: white !important; } /** * \" * Using Zenburn color palette from the Emacs Zenburn Theme * https://github.com/bbatsov/zenburn-emacs/blob/master/zenburn-theme.el * * Also using parts of https://github.com/xavi/coderay-lighttable-theme * \" * From: https://github.com/wisenomad/zenburn-lighttable-theme/blob/master/zenburn.css */ .cm-s-zenburn .CodeMirror-gutters { background: #3f3f3f !important; } .cm-s-zenburn .CodeMirror-foldgutter-open, .CodeMirror-foldgutter-folded { color: #999; } .cm-s-zenburn .CodeMirror-cursor { border-left: 1px solid white; } .cm-s-zenburn { background-color: #3f3f3f; color: #dcdccc; } .cm-s-zenburn span.cm-builtin { color: #dcdccc; font-weight: bold; } .cm-s-zenburn span.cm-comment { color: #7f9f7f; } .cm-s-zenburn span.cm-keyword { color: #f0dfaf; font-weight: bold; } .cm-s-zenburn span.cm-atom { color: #bfebbf; } .cm-s-zenburn span.cm-def { color: #dcdccc; } .cm-s-zenburn span.cm-variable { color: #dfaf8f; } .cm-s-zenburn span.cm-variable-2 { color: #dcdccc; } .cm-s-zenburn span.cm-string { color: #cc9393; } .cm-s-zenburn span.cm-string-2 { color: #cc9393; } .cm-s-zenburn span.cm-number { color: #dcdccc; } .cm-s-zenburn span.cm-tag { color: #93e0e3; } .cm-s-zenburn span.cm-property { color: #dfaf8f; } .cm-s-zenburn span.cm-attribute { color: #dfaf8f; } .cm-s-zenburn span.cm-qualifier { color: #7cb8bb; } .cm-s-zenburn span.cm-meta { color: #f0dfaf; } .cm-s-zenburn span.cm-header { color: #f0efd0; } .cm-s-zenburn span.cm-operator { color: #f0efd0; } .cm-s-zenburn span.CodeMirror-matchingbracket { box-sizing: border-box; background: transparent; border-bottom: 1px solid; } .cm-s-zenburn span.CodeMirror-nonmatchingbracket { border-bottom: 1px solid; background: none; } .cm-s-zenburn .CodeMirror-activeline { background: #000000; } .cm-s-zenburn .CodeMirror-activeline-background { background: #000000; } .cm-s-zenburn div.CodeMirror-selected { background: #545454; } .cm-s-zenburn .CodeMirror-focused div.CodeMirror-selected { background: #4f4f4f; } .cm-s-abcdef.CodeMirror { background: #0f0f0f; color: #defdef; } .cm-s-abcdef div.CodeMirror-selected { background: #515151; } .cm-s-abcdef .CodeMirror-line::selection, .cm-s-abcdef .CodeMirror-line > span::selection, .cm-s-abcdef .CodeMirror-line > span > span::selection { background: rgba(56, 56, 56, 0.99); } .cm-s-abcdef .CodeMirror-line::-moz-selection, .cm-s-abcdef .CodeMirror-line > span::-moz-selection, .cm-s-abcdef .CodeMirror-line > span > span::-moz-selection { background: rgba(56, 56, 56, 0.99); } .cm-s-abcdef .CodeMirror-gutters { background: #555; border-right: 2px solid #314151; } .cm-s-abcdef .CodeMirror-guttermarker { color: #222; } .cm-s-abcdef .CodeMirror-guttermarker-subtle { color: azure; } .cm-s-abcdef .CodeMirror-linenumber { color: #FFFFFF; } .cm-s-abcdef .CodeMirror-cursor { border-left: 1px solid #00FF00; } .cm-s-abcdef span.cm-keyword { color: darkgoldenrod; font-weight: bold; } .cm-s-abcdef span.cm-atom { color: #77F; } .cm-s-abcdef span.cm-number { color: violet; } .cm-s-abcdef span.cm-def { color: #fffabc; } .cm-s-abcdef span.cm-variable { color: #abcdef; } .cm-s-abcdef span.cm-variable-2 { color: #cacbcc; } .cm-s-abcdef span.cm-variable-3, .cm-s-abcdef span.cm-type { color: #def; } .cm-s-abcdef span.cm-property { color: #fedcba; } .cm-s-abcdef span.cm-operator { color: #ff0; } .cm-s-abcdef span.cm-comment { color: #7a7b7c; font-style: italic;} .cm-s-abcdef span.cm-string { color: #2b4; } .cm-s-abcdef span.cm-meta { color: #C9F; } .cm-s-abcdef span.cm-qualifier { color: #FFF700; } .cm-s-abcdef span.cm-builtin { color: #30aabc; } .cm-s-abcdef span.cm-bracket { color: #8a8a8a; } .cm-s-abcdef span.cm-tag { color: #FFDD44; } .cm-s-abcdef span.cm-attribute { color: #DDFF00; } .cm-s-abcdef span.cm-error { color: #FF0000; } .cm-s-abcdef span.cm-header { color: aquamarine; font-weight: bold; } .cm-s-abcdef span.cm-link { color: blueviolet; } .cm-s-abcdef .CodeMirror-activeline-background { background: #314151; } /* Name: Base16 Default Light Author: Chris Kempson (http://chriskempson.com) CodeMirror template by Jan T. Sott (https://github.com/idleberg/base16-codemirror) Original Base16 color scheme by Chris Kempson (https://github.com/chriskempson/base16) */ .cm-s-base16-light.CodeMirror { background: #f5f5f5; color: #202020; } .cm-s-base16-light div.CodeMirror-selected { background: #e0e0e0; } .cm-s-base16-light .CodeMirror-line::selection, .cm-s-base16-light .CodeMirror-line > span::selection, .cm-s-base16-light .CodeMirror-line > span > span::selection { background: #e0e0e0; } .cm-s-base16-light .CodeMirror-line::-moz-selection, .cm-s-base16-light .CodeMirror-line > span::-moz-selection, .cm-s-base16-light .CodeMirror-line > span > span::-moz-selection { background: #e0e0e0; } .cm-s-base16-light .CodeMirror-gutters { background: #f5f5f5; border-right: 0px; } .cm-s-base16-light .CodeMirror-guttermarker { color: #ac4142; } .cm-s-base16-light .CodeMirror-guttermarker-subtle { color: #b0b0b0; } .cm-s-base16-light .CodeMirror-linenumber { color: #b0b0b0; } .cm-s-base16-light .CodeMirror-cursor { border-left: 1px solid #505050; } .cm-s-base16-light span.cm-comment { color: #8f5536; } .cm-s-base16-light span.cm-atom { color: #aa759f; } .cm-s-base16-light span.cm-number { color: #aa759f; } .cm-s-base16-light span.cm-property, .cm-s-base16-light span.cm-attribute { color: #90a959; } .cm-s-base16-light span.cm-keyword { color: #ac4142; } .cm-s-base16-light span.cm-string { color: #f4bf75; } .cm-s-base16-light span.cm-variable { color: #90a959; } .cm-s-base16-light span.cm-variable-2 { color: #6a9fb5; } .cm-s-base16-light span.cm-def { color: #d28445; } .cm-s-base16-light span.cm-bracket { color: #202020; } .cm-s-base16-light span.cm-tag { color: #ac4142; } .cm-s-base16-light span.cm-link { color: #aa759f; } .cm-s-base16-light span.cm-error { background: #ac4142; color: #505050; } .cm-s-base16-light .CodeMirror-activeline-background { background: #DDDCDC; } .cm-s-base16-light .CodeMirror-matchingbracket { color: #f5f5f5 !important; background-color: #6A9FB5 !important} /* Name: Base16 Default Dark Author: Chris Kempson (http://chriskempson.com) CodeMirror template by Jan T. Sott (https://github.com/idleberg/base16-codemirror) Original Base16 color scheme by Chris Kempson (https://github.com/chriskempson/base16) */ .cm-s-base16-dark.CodeMirror { background: #151515; color: #e0e0e0; } .cm-s-base16-dark div.CodeMirror-selected { background: #303030; } .cm-s-base16-dark .CodeMirror-line::selection, .cm-s-base16-dark .CodeMirror-line > span::selection, .cm-s-base16-dark .CodeMirror-line > span > span::selection { background: rgba(48, 48, 48, .99); } .cm-s-base16-dark .CodeMirror-line::-moz-selection, .cm-s-base16-dark .CodeMirror-line > span::-moz-selection, .cm-s-base16-dark .CodeMirror-line > span > span::-moz-selection { background: rgba(48, 48, 48, .99); } .cm-s-base16-dark .CodeMirror-gutters { background: #151515; border-right: 0px; } .cm-s-base16-dark .CodeMirror-guttermarker { color: #ac4142; } .cm-s-base16-dark .CodeMirror-guttermarker-subtle { color: #505050; } .cm-s-base16-dark .CodeMirror-linenumber { color: #505050; } .cm-s-base16-dark .CodeMirror-cursor { border-left: 1px solid #b0b0b0; } .cm-s-base16-dark span.cm-comment { color: #8f5536; } .cm-s-base16-dark span.cm-atom { color: #aa759f; } .cm-s-base16-dark span.cm-number { color: #aa759f; } .cm-s-base16-dark span.cm-property, .cm-s-base16-dark span.cm-attribute { color: #90a959; } .cm-s-base16-dark span.cm-keyword { color: #ac4142; } .cm-s-base16-dark span.cm-string { color: #f4bf75; } .cm-s-base16-dark span.cm-variable { color: #90a959; } .cm-s-base16-dark span.cm-variable-2 { color: #6a9fb5; } .cm-s-base16-dark span.cm-def { color: #d28445; } .cm-s-base16-dark span.cm-bracket { color: #e0e0e0; } .cm-s-base16-dark span.cm-tag { color: #ac4142; } .cm-s-base16-dark span.cm-link { color: #aa759f; } .cm-s-base16-dark span.cm-error { background: #ac4142; color: #b0b0b0; } .cm-s-base16-dark .CodeMirror-activeline-background { background: #202020; } .cm-s-base16-dark .CodeMirror-matchingbracket { text-decoration: underline; color: white !important; } /* Name: dracula Author: Michael Kaminsky (http://github.com/mkaminsky11) Original dracula color scheme by Zeno Rocha (https://github.com/zenorocha/dracula-theme) */ .cm-s-dracula.CodeMirror, .cm-s-dracula .CodeMirror-gutters { background-color: #282a36 !important; color: #f8f8f2 !important; border: none; } .cm-s-dracula .CodeMirror-gutters { color: #282a36; } .cm-s-dracula .CodeMirror-cursor { border-left: solid thin #f8f8f0; } .cm-s-dracula .CodeMirror-linenumber { color: #6D8A88; } .cm-s-dracula .CodeMirror-selected { background: rgba(255, 255, 255, 0.10); } .cm-s-dracula .CodeMirror-line::selection, .cm-s-dracula .CodeMirror-line > span::selection, .cm-s-dracula .CodeMirror-line > span > span::selection { background: rgba(255, 255, 255, 0.10); } .cm-s-dracula .CodeMirror-line::-moz-selection, .cm-s-dracula .CodeMirror-line > span::-moz-selection, .cm-s-dracula .CodeMirror-line > span > span::-moz-selection { background: rgba(255, 255, 255, 0.10); } .cm-s-dracula span.cm-comment { color: #6272a4; } .cm-s-dracula span.cm-string, .cm-s-dracula span.cm-string-2 { color: #f1fa8c; } .cm-s-dracula span.cm-number { color: #bd93f9; } .cm-s-dracula span.cm-variable { color: #50fa7b; } .cm-s-dracula span.cm-variable-2 { color: white; } .cm-s-dracula span.cm-def { color: #50fa7b; } .cm-s-dracula span.cm-operator { color: #ff79c6; } .cm-s-dracula span.cm-keyword { color: #ff79c6; } .cm-s-dracula span.cm-atom { color: #bd93f9; } .cm-s-dracula span.cm-meta { color: #f8f8f2; } .cm-s-dracula span.cm-tag { color: #ff79c6; } .cm-s-dracula span.cm-attribute { color: #50fa7b; } .cm-s-dracula span.cm-qualifier { color: #50fa7b; } .cm-s-dracula span.cm-property { color: #66d9ef; } .cm-s-dracula span.cm-builtin { color: #50fa7b; } .cm-s-dracula span.cm-variable-3, .cm-s-dracula span.cm-type { color: #ffb86c; } .cm-s-dracula .CodeMirror-activeline-background { background: rgba(255,255,255,0.1); } .cm-s-dracula .CodeMirror-matchingbracket { text-decoration: underline; color: white !important; } /* Name: Hopscotch Author: Jan T. Sott CodeMirror template by Jan T. Sott (https://github.com/idleberg/base16-codemirror) Original Base16 color scheme by Chris Kempson (https://github.com/chriskempson/base16) */ .cm-s-hopscotch.CodeMirror {background: #322931; color: #d5d3d5;} .cm-s-hopscotch div.CodeMirror-selected {background: #433b42 !important;} .cm-s-hopscotch .CodeMirror-gutters {background: #322931; border-right: 0px;} .cm-s-hopscotch .CodeMirror-linenumber {color: #797379;} .cm-s-hopscotch .CodeMirror-cursor {border-left: 1px solid #989498 !important;} .cm-s-hopscotch span.cm-comment {color: #b33508;} .cm-s-hopscotch span.cm-atom {color: #c85e7c;} .cm-s-hopscotch span.cm-number {color: #c85e7c;} .cm-s-hopscotch span.cm-property, .cm-s-hopscotch span.cm-attribute {color: #8fc13e;} .cm-s-hopscotch span.cm-keyword {color: #dd464c;} .cm-s-hopscotch span.cm-string {color: #fdcc59;} .cm-s-hopscotch span.cm-variable {color: #8fc13e;} .cm-s-hopscotch span.cm-variable-2 {color: #1290bf;} .cm-s-hopscotch span.cm-def {color: #fd8b19;} .cm-s-hopscotch span.cm-error {background: #dd464c; color: #989498;} .cm-s-hopscotch span.cm-bracket {color: #d5d3d5;} .cm-s-hopscotch span.cm-tag {color: #dd464c;} .cm-s-hopscotch span.cm-link {color: #c85e7c;} .cm-s-hopscotch .CodeMirror-matchingbracket { text-decoration: underline; color: white !important;} .cm-s-hopscotch .CodeMirror-activeline-background { background: #302020; } /****************************************************************/ /* Based on mbonaci's Brackets mbo theme */ /* https://github.com/mbonaci/global/blob/master/Mbo.tmTheme */ /* Create your own: http://tmtheme-editor.herokuapp.com */ /****************************************************************/ .cm-s-mbo.CodeMirror { background: #2c2c2c; color: #ffffec; } .cm-s-mbo div.CodeMirror-selected { background: #716C62; } .cm-s-mbo .CodeMirror-line::selection, .cm-s-mbo .CodeMirror-line > span::selection, .cm-s-mbo .CodeMirror-line > span > span::selection { background: rgba(113, 108, 98, .99); } .cm-s-mbo .CodeMirror-line::-moz-selection, .cm-s-mbo .CodeMirror-line > span::-moz-selection, .cm-s-mbo .CodeMirror-line > span > span::-moz-selection { background: rgba(113, 108, 98, .99); } .cm-s-mbo .CodeMirror-gutters { background: #4e4e4e; border-right: 0px; } .cm-s-mbo .CodeMirror-guttermarker { color: white; } .cm-s-mbo .CodeMirror-guttermarker-subtle { color: grey; } .cm-s-mbo .CodeMirror-linenumber { color: #dadada; } .cm-s-mbo .CodeMirror-cursor { border-left: 1px solid #ffffec; } .cm-s-mbo span.cm-comment { color: #95958a; } .cm-s-mbo span.cm-atom { color: #00a8c6; } .cm-s-mbo span.cm-number { color: #00a8c6; } .cm-s-mbo span.cm-property, .cm-s-mbo span.cm-attribute { color: #9ddfe9; } .cm-s-mbo span.cm-keyword { color: #ffb928; } .cm-s-mbo span.cm-string { color: #ffcf6c; } .cm-s-mbo span.cm-string.cm-property { color: #ffffec; } .cm-s-mbo span.cm-variable { color: #ffffec; } .cm-s-mbo span.cm-variable-2 { color: #00a8c6; } .cm-s-mbo span.cm-def { color: #ffffec; } .cm-s-mbo span.cm-bracket { color: #fffffc; font-weight: bold; } .cm-s-mbo span.cm-tag { color: #9ddfe9; } .cm-s-mbo span.cm-link { color: #f54b07; } .cm-s-mbo span.cm-error { border-bottom: #636363; color: #ffffec; } .cm-s-mbo span.cm-qualifier { color: #ffffec; } .cm-s-mbo .CodeMirror-activeline-background { background: #494b41; } .cm-s-mbo .CodeMirror-matchingbracket { color: #ffb928 !important; } .cm-s-mbo .CodeMirror-matchingtag { background: rgba(255, 255, 255, .37); } /* MDN-LIKE Theme - Mozilla Ported to CodeMirror by Peter Kroon <plakroon@gmail.com> Report bugs/issues here: https://github.com/codemirror/CodeMirror/issues GitHub: @peterkroon The mdn-like theme is inspired on the displayed code examples at: https://developer.mozilla.org/en-US/docs/Web/CSS/animation */ .cm-s-mdn-like.CodeMirror { color: #999; background-color: #fff; } .cm-s-mdn-like div.CodeMirror-selected { background: #cfc; } .cm-s-mdn-like .CodeMirror-line::selection, .cm-s-mdn-like .CodeMirror-line > span::selection, .cm-s-mdn-like .CodeMirror-line > span > span::selection { background: #cfc; } .cm-s-mdn-like .CodeMirror-line::-moz-selection, .cm-s-mdn-like .CodeMirror-line > span::-moz-selection, .cm-s-mdn-like .CodeMirror-line > span > span::-moz-selection { background: #cfc; } .cm-s-mdn-like .CodeMirror-gutters { background: #f8f8f8; border-left: 6px solid rgba(0,83,159,0.65); color: #333; } .cm-s-mdn-like .CodeMirror-linenumber { color: #aaa; padding-left: 8px; } .cm-s-mdn-like .CodeMirror-cursor { border-left: 2px solid #222; } .cm-s-mdn-like .cm-keyword { color: #6262FF; } .cm-s-mdn-like .cm-atom { color: #F90; } .cm-s-mdn-like .cm-number { color: #ca7841; } .cm-s-mdn-like .cm-def { color: #8DA6CE; } .cm-s-mdn-like span.cm-variable-2, .cm-s-mdn-like span.cm-tag { color: #690; } .cm-s-mdn-like span.cm-variable-3, .cm-s-mdn-like span.cm-def, .cm-s-mdn-like span.cm-type { color: #07a; } .cm-s-mdn-like .cm-variable { color: #07a; } .cm-s-mdn-like .cm-property { color: #905; } .cm-s-mdn-like .cm-qualifier { color: #690; } .cm-s-mdn-like .cm-operator { color: #cda869; } .cm-s-mdn-like .cm-comment { color:#777; font-weight:normal; } .cm-s-mdn-like .cm-string { color:#07a; font-style:italic; } .cm-s-mdn-like .cm-string-2 { color:#bd6b18; } /*?*/ .cm-s-mdn-like .cm-meta { color: #000; } /*?*/ .cm-s-mdn-like .cm-builtin { color: #9B7536; } /*?*/ .cm-s-mdn-like .cm-tag { color: #997643; } .cm-s-mdn-like .cm-attribute { color: #d6bb6d; } /*?*/ .cm-s-mdn-like .cm-header { color: #FF6400; } .cm-s-mdn-like .cm-hr { color: #AEAEAE; } .cm-s-mdn-like .cm-link { color:#ad9361; font-style:italic; text-decoration:none; } .cm-s-mdn-like .cm-error { border-bottom: 1px solid red; } div.cm-s-mdn-like .CodeMirror-activeline-background { background: #efefff; } div.cm-s-mdn-like span.CodeMirror-matchingbracket { outline:1px solid grey; color: inherit; } .cm-s-mdn-like.CodeMirror { background-image: url(data:image/png;base64,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); } /* Name: seti Author: Michael Kaminsky (http://github.com/mkaminsky11) Original seti color scheme by Jesse Weed (https://github.com/jesseweed/seti-syntax) */ .cm-s-seti.CodeMirror { background-color: #151718 !important; color: #CFD2D1 !important; border: none; } .cm-s-seti .CodeMirror-gutters { color: #404b53; background-color: #0E1112; border: none; } .cm-s-seti .CodeMirror-cursor { border-left: solid thin #f8f8f0; } .cm-s-seti .CodeMirror-linenumber { color: #6D8A88; } .cm-s-seti.CodeMirror-focused div.CodeMirror-selected { background: rgba(255, 255, 255, 0.10); } .cm-s-seti .CodeMirror-line::selection, .cm-s-seti .CodeMirror-line > span::selection, .cm-s-seti .CodeMirror-line > span > span::selection { background: rgba(255, 255, 255, 0.10); } .cm-s-seti .CodeMirror-line::-moz-selection, .cm-s-seti .CodeMirror-line > span::-moz-selection, .cm-s-seti .CodeMirror-line > span > span::-moz-selection { background: rgba(255, 255, 255, 0.10); } .cm-s-seti span.cm-comment { color: #41535b; } .cm-s-seti span.cm-string, .cm-s-seti span.cm-string-2 { color: #55b5db; } .cm-s-seti span.cm-number { color: #cd3f45; } .cm-s-seti span.cm-variable { color: #55b5db; } .cm-s-seti span.cm-variable-2 { color: #a074c4; } .cm-s-seti span.cm-def { color: #55b5db; } .cm-s-seti span.cm-keyword { color: #ff79c6; } .cm-s-seti span.cm-operator { color: #9fca56; } .cm-s-seti span.cm-keyword { color: #e6cd69; } .cm-s-seti span.cm-atom { color: #cd3f45; } .cm-s-seti span.cm-meta { color: #55b5db; } .cm-s-seti span.cm-tag { color: #55b5db; } .cm-s-seti span.cm-attribute { color: #9fca56; } .cm-s-seti span.cm-qualifier { color: #9fca56; } .cm-s-seti span.cm-property { color: #a074c4; } .cm-s-seti span.cm-variable-3, .cm-s-seti span.cm-type { color: #9fca56; } .cm-s-seti span.cm-builtin { color: #9fca56; } .cm-s-seti .CodeMirror-activeline-background { background: #101213; } .cm-s-seti .CodeMirror-matchingbracket { text-decoration: underline; color: white !important; } /* Solarized theme for code-mirror http://ethanschoonover.com/solarized */ /* Solarized color palette http://ethanschoonover.com/solarized/img/solarized-palette.png */ .solarized.base03 { color: #002b36; } .solarized.base02 { color: #073642; } .solarized.base01 { color: #586e75; } .solarized.base00 { color: #657b83; } .solarized.base0 { color: #839496; } .solarized.base1 { color: #93a1a1; } .solarized.base2 { color: #eee8d5; } .solarized.base3 { color: #fdf6e3; } .solarized.solar-yellow { color: #b58900; } .solarized.solar-orange { color: #cb4b16; } .solarized.solar-red { color: #dc322f; } .solarized.solar-magenta { color: #d33682; } .solarized.solar-violet { color: #6c71c4; } .solarized.solar-blue { color: #268bd2; } .solarized.solar-cyan { color: #2aa198; } .solarized.solar-green { color: #859900; } /* Color scheme for code-mirror */ .cm-s-solarized { line-height: 1.45em; color-profile: sRGB; rendering-intent: auto; } .cm-s-solarized.cm-s-dark { color: #839496; background-color: #002b36; text-shadow: #002b36 0 1px; } .cm-s-solarized.cm-s-light { background-color: #fdf6e3; color: #657b83; text-shadow: #eee8d5 0 1px; } .cm-s-solarized .CodeMirror-widget { text-shadow: none; } .cm-s-solarized .cm-header { color: #586e75; } .cm-s-solarized .cm-quote { color: #93a1a1; } .cm-s-solarized .cm-keyword { color: #cb4b16; } .cm-s-solarized .cm-atom { color: #d33682; } .cm-s-solarized .cm-number { color: #d33682; } .cm-s-solarized .cm-def { color: #2aa198; } .cm-s-solarized .cm-variable { color: #839496; } .cm-s-solarized .cm-variable-2 { color: #b58900; } .cm-s-solarized .cm-variable-3, .cm-s-solarized .cm-type { color: #6c71c4; } .cm-s-solarized .cm-property { color: #2aa198; } .cm-s-solarized .cm-operator { color: #6c71c4; } .cm-s-solarized .cm-comment { color: #586e75; font-style:italic; } .cm-s-solarized .cm-string { color: #859900; } .cm-s-solarized .cm-string-2 { color: #b58900; } .cm-s-solarized .cm-meta { color: #859900; } .cm-s-solarized .cm-qualifier { color: #b58900; } .cm-s-solarized .cm-builtin { color: #d33682; } .cm-s-solarized .cm-bracket { color: #cb4b16; } .cm-s-solarized .CodeMirror-matchingbracket { color: #859900; } .cm-s-solarized .CodeMirror-nonmatchingbracket { color: #dc322f; } .cm-s-solarized .cm-tag { color: #93a1a1; } .cm-s-solarized .cm-attribute { color: #2aa198; } .cm-s-solarized .cm-hr { color: transparent; border-top: 1px solid #586e75; display: block; } .cm-s-solarized .cm-link { color: #93a1a1; cursor: pointer; } .cm-s-solarized .cm-special { color: #6c71c4; } .cm-s-solarized .cm-em { color: #999; text-decoration: underline; text-decoration-style: dotted; } .cm-s-solarized .cm-error, .cm-s-solarized .cm-invalidchar { color: #586e75; border-bottom: 1px dotted #dc322f; } .cm-s-solarized.cm-s-dark div.CodeMirror-selected { background: #073642; } .cm-s-solarized.cm-s-dark.CodeMirror ::selection { background: rgba(7, 54, 66, 0.99); } .cm-s-solarized.cm-s-dark .CodeMirror-line::-moz-selection, .cm-s-dark .CodeMirror-line > span::-moz-selection, .cm-s-dark .CodeMirror-line > span > span::-moz-selection { background: rgba(7, 54, 66, 0.99); } .cm-s-solarized.cm-s-light div.CodeMirror-selected { background: #eee8d5; } .cm-s-solarized.cm-s-light .CodeMirror-line::selection, .cm-s-light .CodeMirror-line > span::selection, .cm-s-light .CodeMirror-line > span > span::selection { background: #eee8d5; } .cm-s-solarized.cm-s-light .CodeMirror-line::-moz-selection, .cm-s-ligh .CodeMirror-line > span::-moz-selection, .cm-s-ligh .CodeMirror-line > span > span::-moz-selection { background: #eee8d5; } /* Editor styling */ /* Little shadow on the view-port of the buffer view */ .cm-s-solarized.CodeMirror { -moz-box-shadow: inset 7px 0 12px -6px #000; -webkit-box-shadow: inset 7px 0 12px -6px #000; box-shadow: inset 7px 0 12px -6px #000; } /* Remove gutter border */ .cm-s-solarized .CodeMirror-gutters { border-right: 0; } /* Gutter colors and line number styling based of color scheme (dark / light) */ /* Dark */ .cm-s-solarized.cm-s-dark .CodeMirror-gutters { background-color: #073642; } .cm-s-solarized.cm-s-dark .CodeMirror-linenumber { color: #586e75; text-shadow: #021014 0 -1px; } /* Light */ .cm-s-solarized.cm-s-light .CodeMirror-gutters { background-color: #eee8d5; } .cm-s-solarized.cm-s-light .CodeMirror-linenumber { color: #839496; } /* Common */ .cm-s-solarized .CodeMirror-linenumber { padding: 0 5px; } .cm-s-solarized .CodeMirror-guttermarker-subtle { color: #586e75; } .cm-s-solarized.cm-s-dark .CodeMirror-guttermarker { color: #ddd; } .cm-s-solarized.cm-s-light .CodeMirror-guttermarker { color: #cb4b16; } .cm-s-solarized .CodeMirror-gutter .CodeMirror-gutter-text { color: #586e75; } /* Cursor */ .cm-s-solarized .CodeMirror-cursor { border-left: 1px solid #819090; } /* Fat cursor */ .cm-s-solarized.cm-s-light.cm-fat-cursor .CodeMirror-cursor { background: #77ee77; } .cm-s-solarized.cm-s-light .cm-animate-fat-cursor { background-color: #77ee77; } .cm-s-solarized.cm-s-dark.cm-fat-cursor .CodeMirror-cursor { background: #586e75; } .cm-s-solarized.cm-s-dark .cm-animate-fat-cursor { background-color: #586e75; } /* Active line */ .cm-s-solarized.cm-s-dark .CodeMirror-activeline-background { background: rgba(255, 255, 255, 0.06); } .cm-s-solarized.cm-s-light .CodeMirror-activeline-background { background: rgba(0, 0, 0, 0.06); } .cm-s-the-matrix.CodeMirror { background: #000000; color: #00FF00; } .cm-s-the-matrix div.CodeMirror-selected { background: #2D2D2D; } .cm-s-the-matrix .CodeMirror-line::selection, .cm-s-the-matrix .CodeMirror-line > span::selection, .cm-s-the-matrix .CodeMirror-line > span > span::selection { background: rgba(45, 45, 45, 0.99); } .cm-s-the-matrix .CodeMirror-line::-moz-selection, .cm-s-the-matrix .CodeMirror-line > span::-moz-selection, .cm-s-the-matrix .CodeMirror-line > span > span::-moz-selection { background: rgba(45, 45, 45, 0.99); } .cm-s-the-matrix .CodeMirror-gutters { background: #060; border-right: 2px solid #00FF00; } .cm-s-the-matrix .CodeMirror-guttermarker { color: #0f0; } .cm-s-the-matrix .CodeMirror-guttermarker-subtle { color: white; } .cm-s-the-matrix .CodeMirror-linenumber { color: #FFFFFF; } .cm-s-the-matrix .CodeMirror-cursor { border-left: 1px solid #00FF00; } .cm-s-the-matrix span.cm-keyword { color: #008803; font-weight: bold; } .cm-s-the-matrix span.cm-atom { color: #3FF; } .cm-s-the-matrix span.cm-number { color: #FFB94F; } .cm-s-the-matrix span.cm-def { color: #99C; } .cm-s-the-matrix span.cm-variable { color: #F6C; } .cm-s-the-matrix span.cm-variable-2 { color: #C6F; } .cm-s-the-matrix span.cm-variable-3, .cm-s-the-matrix span.cm-type { color: #96F; } .cm-s-the-matrix span.cm-property { color: #62FFA0; } .cm-s-the-matrix span.cm-operator { color: #999; } .cm-s-the-matrix span.cm-comment { color: #CCCCCC; } .cm-s-the-matrix span.cm-string { color: #39C; } .cm-s-the-matrix span.cm-meta { color: #C9F; } .cm-s-the-matrix span.cm-qualifier { color: #FFF700; } .cm-s-the-matrix span.cm-builtin { color: #30a; } .cm-s-the-matrix span.cm-bracket { color: #cc7; } .cm-s-the-matrix span.cm-tag { color: #FFBD40; } .cm-s-the-matrix span.cm-attribute { color: #FFF700; } .cm-s-the-matrix span.cm-error { color: #FF0000; } .cm-s-the-matrix .CodeMirror-activeline-background { background: #040; } /* Copyright (C) 2011 by MarkLogic Corporation Author: Mike Brevoort <mike@brevoort.com> Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. */ .cm-s-xq-light span.cm-keyword { line-height: 1em; font-weight: bold; color: #5A5CAD; } .cm-s-xq-light span.cm-atom { color: #6C8CD5; } .cm-s-xq-light span.cm-number { color: #164; } .cm-s-xq-light span.cm-def { text-decoration:underline; } .cm-s-xq-light span.cm-variable { color: black; } .cm-s-xq-light span.cm-variable-2 { color:black; } .cm-s-xq-light span.cm-variable-3, .cm-s-xq-light span.cm-type { color: black; } .cm-s-xq-light span.cm-property {} .cm-s-xq-light span.cm-operator {} .cm-s-xq-light span.cm-comment { color: #0080FF; font-style: italic; } .cm-s-xq-light span.cm-string { color: red; } .cm-s-xq-light span.cm-meta { color: yellow; } .cm-s-xq-light span.cm-qualifier { color: grey; } .cm-s-xq-light span.cm-builtin { color: #7EA656; } .cm-s-xq-light span.cm-bracket { color: #cc7; } .cm-s-xq-light span.cm-tag { color: #3F7F7F; } .cm-s-xq-light span.cm-attribute { color: #7F007F; } .cm-s-xq-light span.cm-error { color: #f00; } .cm-s-xq-light .CodeMirror-activeline-background { background: #e8f2ff; } .cm-s-xq-light .CodeMirror-matchingbracket { outline:1px solid grey;color:black !important;background:yellow; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ .CodeMirror { line-height: var(--jp-code-line-height); font-size: var(--jp-code-font-size); font-family: var(--jp-code-font-family); border: 0; border-radius: 0; height: auto; /* Changed to auto to autogrow */ } .CodeMirror pre { padding: 0 var(--jp-code-padding); } .jp-CodeMirrorEditor[data-type='inline'] .CodeMirror-dialog { background-color: var(--jp-layout-color0); color: var(--jp-content-font-color1); } /* This causes https://github.com/jupyter/jupyterlab/issues/522 */ /* May not cause it not because we changed it! */ .CodeMirror-lines { padding: var(--jp-code-padding) 0; } .CodeMirror-linenumber { padding: 0 8px; } .jp-CodeMirrorEditor-static { margin: var(--jp-code-padding); } .jp-CodeMirrorEditor, .jp-CodeMirrorEditor-static { cursor: text; } .jp-CodeMirrorEditor[data-type='inline'] .CodeMirror-cursor { border-left: var(--jp-code-cursor-width0) solid var(--jp-editor-cursor-color); } /* When zoomed out 67% and 33% on a screen of 1440 width x 900 height */ @media screen and (min-width: 2138px) and (max-width: 4319px) { .jp-CodeMirrorEditor[data-type='inline'] .CodeMirror-cursor { border-left: var(--jp-code-cursor-width1) solid var(--jp-editor-cursor-color); } } /* When zoomed out less than 33% */ @media screen and (min-width: 4320px) { .jp-CodeMirrorEditor[data-type='inline'] .CodeMirror-cursor { border-left: var(--jp-code-cursor-width2) solid var(--jp-editor-cursor-color); } } .CodeMirror.jp-mod-readOnly .CodeMirror-cursor { display: none; } .CodeMirror-gutters { border-right: 1px solid var(--jp-border-color2); background-color: var(--jp-layout-color0); } .jp-CollaboratorCursor { border-left: 5px solid transparent; border-right: 5px solid transparent; border-top: none; border-bottom: 3px solid; background-clip: content-box; margin-left: -5px; margin-right: -5px; } .CodeMirror-selectedtext.cm-searching { background-color: var(--jp-search-selected-match-background-color) !important; color: var(--jp-search-selected-match-color) !important; } .cm-searching { background-color: var( --jp-search-unselected-match-background-color ) !important; color: var(--jp-search-unselected-match-color) !important; } .CodeMirror-focused .CodeMirror-selected { background-color: var(--jp-editor-selected-focused-background); } .CodeMirror-selected { background-color: var(--jp-editor-selected-background); } .jp-CollaboratorCursor-hover { position: absolute; z-index: 1; transform: translateX(-50%); color: white; border-radius: 3px; padding-left: 4px; padding-right: 4px; padding-top: 1px; padding-bottom: 1px; text-align: center; font-size: var(--jp-ui-font-size1); white-space: nowrap; } .jp-CodeMirror-ruler { border-left: 1px dashed var(--jp-border-color2); } /** * Here is our jupyter theme for CodeMirror syntax highlighting * This is used in our marked.js syntax highlighting and CodeMirror itself * The string \"jupyter\" is set in ../codemirror/widget.DEFAULT_CODEMIRROR_THEME * This came from the classic notebook, which came form highlight.js/GitHub */ /** * CodeMirror themes are handling the background/color in this way. This works * fine for CodeMirror editors outside the notebook, but the notebook styles * these things differently. */ .CodeMirror.cm-s-jupyter { background: var(--jp-layout-color0); color: var(--jp-content-font-color1); } /* In the notebook, we want this styling to be handled by its container */ .jp-CodeConsole .CodeMirror.cm-s-jupyter, .jp-Notebook .CodeMirror.cm-s-jupyter { background: transparent; } .cm-s-jupyter .CodeMirror-cursor { border-left: var(--jp-code-cursor-width0) solid var(--jp-editor-cursor-color); } .cm-s-jupyter span.cm-keyword { color: var(--jp-mirror-editor-keyword-color); font-weight: bold; } .cm-s-jupyter span.cm-atom { color: var(--jp-mirror-editor-atom-color); } .cm-s-jupyter span.cm-number { color: var(--jp-mirror-editor-number-color); } .cm-s-jupyter span.cm-def { color: var(--jp-mirror-editor-def-color); } .cm-s-jupyter span.cm-variable { color: var(--jp-mirror-editor-variable-color); } .cm-s-jupyter span.cm-variable-2 { color: var(--jp-mirror-editor-variable-2-color); } .cm-s-jupyter span.cm-variable-3 { color: var(--jp-mirror-editor-variable-3-color); } .cm-s-jupyter span.cm-punctuation { color: var(--jp-mirror-editor-punctuation-color); } .cm-s-jupyter span.cm-property { color: var(--jp-mirror-editor-property-color); } .cm-s-jupyter span.cm-operator { color: var(--jp-mirror-editor-operator-color); font-weight: bold; } .cm-s-jupyter span.cm-comment { color: var(--jp-mirror-editor-comment-color); font-style: italic; } .cm-s-jupyter span.cm-string { color: var(--jp-mirror-editor-string-color); } .cm-s-jupyter span.cm-string-2 { color: var(--jp-mirror-editor-string-2-color); } .cm-s-jupyter span.cm-meta { color: var(--jp-mirror-editor-meta-color); } .cm-s-jupyter span.cm-qualifier { color: var(--jp-mirror-editor-qualifier-color); } .cm-s-jupyter span.cm-builtin { color: var(--jp-mirror-editor-builtin-color); } .cm-s-jupyter span.cm-bracket { color: var(--jp-mirror-editor-bracket-color); } .cm-s-jupyter span.cm-tag { color: var(--jp-mirror-editor-tag-color); } .cm-s-jupyter span.cm-attribute { color: var(--jp-mirror-editor-attribute-color); } .cm-s-jupyter span.cm-header { color: var(--jp-mirror-editor-header-color); } .cm-s-jupyter span.cm-quote { color: var(--jp-mirror-editor-quote-color); } .cm-s-jupyter span.cm-link { color: var(--jp-mirror-editor-link-color); } .cm-s-jupyter span.cm-error { color: var(--jp-mirror-editor-error-color); } .cm-s-jupyter span.cm-hr { color: #999; } .cm-s-jupyter span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } .cm-s-jupyter .CodeMirror-activeline-background, .cm-s-jupyter .CodeMirror-gutter { background-color: var(--jp-layout-color2); } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /* This file was auto-generated by ensurePackage() in @jupyterlab/buildutils */ /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /*----------------------------------------------------------------------------- | RenderedText |----------------------------------------------------------------------------*/ .jp-RenderedText { text-align: left; padding-left: var(--jp-code-padding); line-height: var(--jp-code-line-height); font-family: var(--jp-code-font-family); } .jp-RenderedText pre, .jp-RenderedJavaScript pre, .jp-RenderedHTMLCommon pre { color: var(--jp-content-font-color1); font-size: var(--jp-code-font-size); border: none; margin: 0px; padding: 0px; line-height: normal; } .jp-RenderedText pre a:link { text-decoration: none; color: var(--jp-content-link-color); } .jp-RenderedText pre a:hover { text-decoration: underline; color: var(--jp-content-link-color); } .jp-RenderedText pre a:visited { text-decoration: none; color: var(--jp-content-link-color); } /* console foregrounds and backgrounds */ .jp-RenderedText pre .ansi-black-fg { color: #3e424d; } .jp-RenderedText pre .ansi-red-fg { color: #e75c58; } .jp-RenderedText pre .ansi-green-fg { color: #00a250; } .jp-RenderedText pre .ansi-yellow-fg { color: #ddb62b; } .jp-RenderedText pre .ansi-blue-fg { color: #208ffb; } .jp-RenderedText pre .ansi-magenta-fg { color: #d160c4; } .jp-RenderedText pre .ansi-cyan-fg { color: #60c6c8; } .jp-RenderedText pre .ansi-white-fg { color: #c5c1b4; } .jp-RenderedText pre .ansi-black-bg { background-color: #3e424d; } .jp-RenderedText pre .ansi-red-bg { background-color: #e75c58; } .jp-RenderedText pre .ansi-green-bg { background-color: #00a250; } .jp-RenderedText pre .ansi-yellow-bg { background-color: #ddb62b; } .jp-RenderedText pre .ansi-blue-bg { background-color: #208ffb; } .jp-RenderedText pre .ansi-magenta-bg { background-color: #d160c4; } .jp-RenderedText pre .ansi-cyan-bg { background-color: #60c6c8; } .jp-RenderedText pre .ansi-white-bg { background-color: #c5c1b4; } .jp-RenderedText pre .ansi-black-intense-fg { color: #282c36; } .jp-RenderedText pre .ansi-red-intense-fg { color: #b22b31; } .jp-RenderedText pre .ansi-green-intense-fg { color: #007427; } .jp-RenderedText pre .ansi-yellow-intense-fg { color: #b27d12; } .jp-RenderedText pre .ansi-blue-intense-fg { color: #0065ca; } .jp-RenderedText pre .ansi-magenta-intense-fg { color: #a03196; } .jp-RenderedText pre .ansi-cyan-intense-fg { color: #258f8f; } .jp-RenderedText pre .ansi-white-intense-fg { color: #a1a6b2; } .jp-RenderedText pre .ansi-black-intense-bg { background-color: #282c36; } .jp-RenderedText pre .ansi-red-intense-bg { background-color: #b22b31; } .jp-RenderedText pre .ansi-green-intense-bg { background-color: #007427; } .jp-RenderedText pre .ansi-yellow-intense-bg { background-color: #b27d12; } .jp-RenderedText pre .ansi-blue-intense-bg { background-color: #0065ca; } .jp-RenderedText pre .ansi-magenta-intense-bg { background-color: #a03196; } .jp-RenderedText pre .ansi-cyan-intense-bg { background-color: #258f8f; } .jp-RenderedText pre .ansi-white-intense-bg { background-color: #a1a6b2; } .jp-RenderedText pre .ansi-default-inverse-fg { color: var(--jp-ui-inverse-font-color0); } .jp-RenderedText pre .ansi-default-inverse-bg { background-color: var(--jp-inverse-layout-color0); } .jp-RenderedText pre .ansi-bold { font-weight: bold; } .jp-RenderedText pre .ansi-underline { text-decoration: underline; } .jp-RenderedText[data-mime-type='application/vnd.jupyter.stderr'] { background: var(--jp-rendermime-error-background); padding-top: var(--jp-code-padding); } /*----------------------------------------------------------------------------- | RenderedLatex |----------------------------------------------------------------------------*/ .jp-RenderedLatex { color: var(--jp-content-font-color1); font-size: var(--jp-content-font-size1); line-height: var(--jp-content-line-height); } /* Left-justify outputs.*/ .jp-OutputArea-output.jp-RenderedLatex { padding: var(--jp-code-padding); text-align: left; } /*----------------------------------------------------------------------------- | RenderedHTML |----------------------------------------------------------------------------*/ .jp-RenderedHTMLCommon { color: var(--jp-content-font-color1); font-family: var(--jp-content-font-family); font-size: var(--jp-content-font-size1); line-height: var(--jp-content-line-height); /* Give a bit more R padding on Markdown text to keep line lengths reasonable */ padding-right: 20px; } .jp-RenderedHTMLCommon em { font-style: italic; } .jp-RenderedHTMLCommon strong { font-weight: bold; } .jp-RenderedHTMLCommon u { text-decoration: underline; } .jp-RenderedHTMLCommon a:link { text-decoration: none; color: var(--jp-content-link-color); } .jp-RenderedHTMLCommon a:hover { text-decoration: underline; color: var(--jp-content-link-color); } .jp-RenderedHTMLCommon a:visited { text-decoration: none; color: var(--jp-content-link-color); } /* Headings */ .jp-RenderedHTMLCommon h1, .jp-RenderedHTMLCommon h2, .jp-RenderedHTMLCommon h3, .jp-RenderedHTMLCommon h4, .jp-RenderedHTMLCommon h5, .jp-RenderedHTMLCommon h6 { line-height: var(--jp-content-heading-line-height); font-weight: var(--jp-content-heading-font-weight); font-style: normal; margin: var(--jp-content-heading-margin-top) 0 var(--jp-content-heading-margin-bottom) 0; } .jp-RenderedHTMLCommon h1:first-child, .jp-RenderedHTMLCommon h2:first-child, .jp-RenderedHTMLCommon h3:first-child, .jp-RenderedHTMLCommon h4:first-child, .jp-RenderedHTMLCommon h5:first-child, .jp-RenderedHTMLCommon h6:first-child { margin-top: calc(0.5 * var(--jp-content-heading-margin-top)); } .jp-RenderedHTMLCommon h1:last-child, .jp-RenderedHTMLCommon h2:last-child, .jp-RenderedHTMLCommon h3:last-child, .jp-RenderedHTMLCommon h4:last-child, .jp-RenderedHTMLCommon h5:last-child, .jp-RenderedHTMLCommon h6:last-child { margin-bottom: calc(0.5 * var(--jp-content-heading-margin-bottom)); } .jp-RenderedHTMLCommon h1 { font-size: var(--jp-content-font-size5); } .jp-RenderedHTMLCommon h2 { font-size: var(--jp-content-font-size4); } .jp-RenderedHTMLCommon h3 { font-size: var(--jp-content-font-size3); } .jp-RenderedHTMLCommon h4 { font-size: var(--jp-content-font-size2); } .jp-RenderedHTMLCommon h5 { font-size: var(--jp-content-font-size1); } .jp-RenderedHTMLCommon h6 { font-size: var(--jp-content-font-size0); } /* Lists */ .jp-RenderedHTMLCommon ul:not(.list-inline), .jp-RenderedHTMLCommon ol:not(.list-inline) { padding-left: 2em; } .jp-RenderedHTMLCommon ul { list-style: disc; } .jp-RenderedHTMLCommon ul ul { list-style: square; } .jp-RenderedHTMLCommon ul ul ul { list-style: circle; } .jp-RenderedHTMLCommon ol { list-style: decimal; } .jp-RenderedHTMLCommon ol ol { list-style: upper-alpha; } .jp-RenderedHTMLCommon ol ol ol { list-style: lower-alpha; } .jp-RenderedHTMLCommon ol ol ol ol { list-style: lower-roman; } .jp-RenderedHTMLCommon ol ol ol ol ol { list-style: decimal; } .jp-RenderedHTMLCommon ol, .jp-RenderedHTMLCommon ul { margin-bottom: 1em; } .jp-RenderedHTMLCommon ul ul, .jp-RenderedHTMLCommon ul ol, .jp-RenderedHTMLCommon ol ul, .jp-RenderedHTMLCommon ol ol { margin-bottom: 0em; } .jp-RenderedHTMLCommon hr { color: var(--jp-border-color2); background-color: var(--jp-border-color1); margin-top: 1em; margin-bottom: 1em; } .jp-RenderedHTMLCommon > pre { margin: 1.5em 2em; } .jp-RenderedHTMLCommon pre, .jp-RenderedHTMLCommon code { border: 0; background-color: var(--jp-layout-color0); color: var(--jp-content-font-color1); font-family: var(--jp-code-font-family); font-size: inherit; line-height: var(--jp-code-line-height); padding: 0; white-space: pre-wrap; } .jp-RenderedHTMLCommon :not(pre) > code { background-color: var(--jp-layout-color2); padding: 1px 5px; } /* Tables */ .jp-RenderedHTMLCommon table { border-collapse: collapse; border-spacing: 0; border: none; color: var(--jp-ui-font-color1); font-size: 12px; table-layout: fixed; margin-left: auto; margin-right: auto; } .jp-RenderedHTMLCommon thead { border-bottom: var(--jp-border-width) solid var(--jp-border-color1); vertical-align: bottom; } .jp-RenderedHTMLCommon td, .jp-RenderedHTMLCommon th, .jp-RenderedHTMLCommon tr { vertical-align: middle; padding: 0.5em 0.5em; line-height: normal; white-space: normal; max-width: none; border: none; } .jp-RenderedMarkdown.jp-RenderedHTMLCommon td, .jp-RenderedMarkdown.jp-RenderedHTMLCommon th { max-width: none; } :not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon td, :not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon th, :not(.jp-RenderedMarkdown).jp-RenderedHTMLCommon tr { text-align: right; } .jp-RenderedHTMLCommon th { font-weight: bold; } .jp-RenderedHTMLCommon tbody tr:nth-child(odd) { background: var(--jp-layout-color0); } .jp-RenderedHTMLCommon tbody tr:nth-child(even) { background: var(--jp-rendermime-table-row-background); } .jp-RenderedHTMLCommon tbody tr:hover { background: var(--jp-rendermime-table-row-hover-background); } .jp-RenderedHTMLCommon table { margin-bottom: 1em; } .jp-RenderedHTMLCommon p { text-align: left; margin: 0px; } .jp-RenderedHTMLCommon p { margin-bottom: 1em; } .jp-RenderedHTMLCommon img { -moz-force-broken-image-icon: 1; } /* Restrict to direct children as other images could be nested in other content. */ .jp-RenderedHTMLCommon > img { display: block; margin-left: 0; margin-right: 0; margin-bottom: 1em; } /* Change color behind transparent images if they need it... */ [data-jp-theme-light='false'] .jp-RenderedImage img.jp-needs-light-background { background-color: var(--jp-inverse-layout-color1); } [data-jp-theme-light='true'] .jp-RenderedImage img.jp-needs-dark-background { background-color: var(--jp-inverse-layout-color1); } /* ...or leave it untouched if they don't */ [data-jp-theme-light='false'] .jp-RenderedImage img.jp-needs-dark-background { } [data-jp-theme-light='true'] .jp-RenderedImage img.jp-needs-light-background { } .jp-RenderedHTMLCommon img, .jp-RenderedImage img, .jp-RenderedHTMLCommon svg, .jp-RenderedSVG svg { max-width: 100%; height: auto; } .jp-RenderedHTMLCommon img.jp-mod-unconfined, .jp-RenderedImage img.jp-mod-unconfined, .jp-RenderedHTMLCommon svg.jp-mod-unconfined, .jp-RenderedSVG svg.jp-mod-unconfined { max-width: none; } .jp-RenderedHTMLCommon .alert { padding: var(--jp-notebook-padding); border: var(--jp-border-width) solid transparent; border-radius: var(--jp-border-radius); margin-bottom: 1em; } .jp-RenderedHTMLCommon .alert-info { color: var(--jp-info-color0); background-color: var(--jp-info-color3); border-color: var(--jp-info-color2); } .jp-RenderedHTMLCommon .alert-info hr { border-color: var(--jp-info-color3); } .jp-RenderedHTMLCommon .alert-info > p:last-child, .jp-RenderedHTMLCommon .alert-info > ul:last-child { margin-bottom: 0; } .jp-RenderedHTMLCommon .alert-warning { color: var(--jp-warn-color0); background-color: var(--jp-warn-color3); border-color: var(--jp-warn-color2); } .jp-RenderedHTMLCommon .alert-warning hr { border-color: var(--jp-warn-color3); } .jp-RenderedHTMLCommon .alert-warning > p:last-child, .jp-RenderedHTMLCommon .alert-warning > ul:last-child { margin-bottom: 0; } .jp-RenderedHTMLCommon .alert-success { color: var(--jp-success-color0); background-color: var(--jp-success-color3); border-color: var(--jp-success-color2); } .jp-RenderedHTMLCommon .alert-success hr { border-color: var(--jp-success-color3); } .jp-RenderedHTMLCommon .alert-success > p:last-child, .jp-RenderedHTMLCommon .alert-success > ul:last-child { margin-bottom: 0; } .jp-RenderedHTMLCommon .alert-danger { color: var(--jp-error-color0); background-color: var(--jp-error-color3); border-color: var(--jp-error-color2); } .jp-RenderedHTMLCommon .alert-danger hr { border-color: var(--jp-error-color3); } .jp-RenderedHTMLCommon .alert-danger > p:last-child, .jp-RenderedHTMLCommon .alert-danger > ul:last-child { margin-bottom: 0; } .jp-RenderedHTMLCommon blockquote { margin: 1em 2em; padding: 0 1em; border-left: 5px solid var(--jp-border-color2); } a.jp-InternalAnchorLink { visibility: hidden; margin-left: 8px; color: var(--md-blue-800); } h1:hover .jp-InternalAnchorLink, h2:hover .jp-InternalAnchorLink, h3:hover .jp-InternalAnchorLink, h4:hover .jp-InternalAnchorLink, h5:hover .jp-InternalAnchorLink, h6:hover .jp-InternalAnchorLink { visibility: visible; } .jp-RenderedHTMLCommon kbd { background-color: var(--jp-rendermime-table-row-background); border: 1px solid var(--jp-border-color0); border-bottom-color: var(--jp-border-color2); border-radius: 3px; box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.25); display: inline-block; font-size: 0.8em; line-height: 1em; padding: 0.2em 0.5em; } /* Most direct children of .jp-RenderedHTMLCommon have a margin-bottom of 1.0. * At the bottom of cells this is a bit too much as there is also spacing * between cells. Going all the way to 0 gets too tight between markdown and * code cells. */ .jp-RenderedHTMLCommon > *:last-child { margin-bottom: 0.5em; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /* This file was auto-generated by ensurePackage() in @jupyterlab/buildutils */ /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ .jp-MimeDocument { outline: none; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /* This file was auto-generated by ensurePackage() in @jupyterlab/buildutils */ /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /*----------------------------------------------------------------------------- | Variables |----------------------------------------------------------------------------*/ :root { --jp-private-filebrowser-button-height: 28px; --jp-private-filebrowser-button-width: 48px; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ .jp-FileBrowser { display: flex; flex-direction: column; color: var(--jp-ui-font-color1); background: var(--jp-layout-color1); /* This is needed so that all font sizing of children done in ems is * relative to this base size */ font-size: var(--jp-ui-font-size1); } .jp-FileBrowser-toolbar.jp-Toolbar { border-bottom: none; height: auto; margin: var(--jp-toolbar-header-margin); box-shadow: none; } .jp-BreadCrumbs { flex: 0 0 auto; margin: 4px 12px; } .jp-BreadCrumbs-item { margin: 0px 2px; padding: 0px 2px; border-radius: var(--jp-border-radius); cursor: pointer; } .jp-BreadCrumbs-item:hover { background-color: var(--jp-layout-color2); } .jp-BreadCrumbs-item:first-child { margin-left: 0px; } .jp-BreadCrumbs-item.jp-mod-dropTarget { background-color: var(--jp-brand-color2); opacity: 0.7; } /*----------------------------------------------------------------------------- | Buttons |----------------------------------------------------------------------------*/ .jp-FileBrowser-toolbar.jp-Toolbar { padding: 0px; } .jp-FileBrowser-toolbar.jp-Toolbar { justify-content: space-evenly; } .jp-FileBrowser-toolbar.jp-Toolbar .jp-Toolbar-item { flex: 1; } .jp-FileBrowser-toolbar.jp-Toolbar .jp-ToolbarButtonComponent { width: 100%; } /*----------------------------------------------------------------------------- | DirListing |----------------------------------------------------------------------------*/ .jp-DirListing { flex: 1 1 auto; display: flex; flex-direction: column; outline: 0; } .jp-DirListing-header { flex: 0 0 auto; display: flex; flex-direction: row; overflow: hidden; border-top: var(--jp-border-width) solid var(--jp-border-color2); border-bottom: var(--jp-border-width) solid var(--jp-border-color1); box-shadow: var(--jp-toolbar-box-shadow); z-index: 2; } .jp-DirListing-headerItem { padding: 4px 12px 2px 12px; font-weight: 500; } .jp-DirListing-headerItem:hover { background: var(--jp-layout-color2); } .jp-DirListing-headerItem.jp-id-name { flex: 1 0 84px; } .jp-DirListing-headerItem.jp-id-modified { flex: 0 0 112px; border-left: var(--jp-border-width) solid var(--jp-border-color2); text-align: right; } .jp-DirListing-narrow .jp-id-modified, .jp-DirListing-narrow .jp-DirListing-itemModified { display: none; } .jp-DirListing-headerItem.jp-mod-selected { font-weight: 600; } /* increase specificity to override bundled default */ .jp-DirListing-content { flex: 1 1 auto; margin: 0; padding: 0; list-style-type: none; overflow: auto; background-color: var(--jp-layout-color1); } /* Style the directory listing content when a user drops a file to upload */ .jp-DirListing.jp-mod-native-drop .jp-DirListing-content { outline: 5px dashed rgba(128, 128, 128, 0.5); outline-offset: -10px; cursor: copy; } .jp-DirListing-item { display: flex; flex-direction: row; padding: 4px 12px; -webkit-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; } .jp-DirListing-item.jp-mod-selected { color: white; background: var(--jp-brand-color1); } .jp-DirListing-item.jp-mod-dropTarget { background: var(--jp-brand-color3); } .jp-DirListing-item:hover:not(.jp-mod-selected) { background: var(--jp-layout-color2); } .jp-DirListing-itemIcon { flex: 0 0 20px; margin-right: 4px; } .jp-DirListing-itemText { flex: 1 0 64px; white-space: nowrap; overflow: hidden; text-overflow: ellipsis; user-select: none; } .jp-DirListing-itemModified { flex: 0 0 125px; text-align: right; } .jp-DirListing-editor { flex: 1 0 64px; outline: none; border: none; } .jp-DirListing-item.jp-mod-running .jp-DirListing-itemIcon:before { color: limegreen; content: '\\25CF'; font-size: 8px; position: absolute; left: -8px; } .jp-DirListing-item.lm-mod-drag-image, .jp-DirListing-item.jp-mod-selected.lm-mod-drag-image { font-size: var(--jp-ui-font-size1); padding-left: 4px; margin-left: 4px; width: 160px; background-color: var(--jp-ui-inverse-font-color2); box-shadow: var(--jp-elevation-z2); border-radius: 0px; color: var(--jp-ui-font-color1); transform: translateX(-40%) translateY(-58%); } .jp-DirListing-deadSpace { flex: 1 1 auto; margin: 0; padding: 0; list-style-type: none; overflow: auto; background-color: var(--jp-layout-color1); } .jp-Document { min-width: 120px; min-height: 120px; outline: none; } .jp-FileDialog.jp-mod-conflict input { color: red; } .jp-FileDialog .jp-new-name-title { margin-top: 12px; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /* This file was auto-generated by ensurePackage() in @jupyterlab/buildutils */ /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /*----------------------------------------------------------------------------- | Private CSS variables |----------------------------------------------------------------------------*/ :root { } /*----------------------------------------------------------------------------- | Main OutputArea | OutputArea has a list of Outputs |----------------------------------------------------------------------------*/ .jp-OutputArea { overflow-y: auto; } .jp-OutputArea-child { display: flex; flex-direction: row; } .jp-OutputPrompt { flex: 0 0 var(--jp-cell-prompt-width); color: var(--jp-cell-outprompt-font-color); font-family: var(--jp-cell-prompt-font-family); padding: var(--jp-code-padding); letter-spacing: var(--jp-cell-prompt-letter-spacing); line-height: var(--jp-code-line-height); font-size: var(--jp-code-font-size); border: var(--jp-border-width) solid transparent; opacity: var(--jp-cell-prompt-opacity); /* Right align prompt text, don't wrap to handle large prompt numbers */ text-align: right; white-space: nowrap; overflow: hidden; text-overflow: ellipsis; /* Disable text selection */ -webkit-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; } .jp-OutputArea-output { height: auto; overflow: auto; user-select: text; -moz-user-select: text; -webkit-user-select: text; -ms-user-select: text; } .jp-OutputArea-child .jp-OutputArea-output { flex-grow: 1; flex-shrink: 1; } /** * Isolated output. */ .jp-OutputArea-output.jp-mod-isolated { width: 100%; display: block; } /* When drag events occur, `p-mod-override-cursor` is added to the body. Because iframes steal all cursor events, the following two rules are necessary to suppress pointer events while resize drags are occurring. There may be a better solution to this problem. */ body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated { position: relative; } body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated:before { content: ''; position: absolute; top: 0; left: 0; right: 0; bottom: 0; background: transparent; } /* pre */ .jp-OutputArea-output pre { border: none; margin: 0px; padding: 0px; overflow-x: auto; overflow-y: auto; word-break: break-all; word-wrap: break-word; white-space: pre-wrap; } /* tables */ .jp-OutputArea-output.jp-RenderedHTMLCommon table { margin-left: 0; margin-right: 0; } /* description lists */ .jp-OutputArea-output dl, .jp-OutputArea-output dt, .jp-OutputArea-output dd { display: block; } .jp-OutputArea-output dl { width: 100%; overflow: hidden; padding: 0; margin: 0; } .jp-OutputArea-output dt { font-weight: bold; float: left; width: 20%; padding: 0; margin: 0; } .jp-OutputArea-output dd { float: left; width: 80%; padding: 0; margin: 0; } /* Hide the gutter in case of * - nested output areas (e.g. in the case of output widgets) * - mirrored output areas */ .jp-OutputArea .jp-OutputArea .jp-OutputArea-prompt { display: none; } /*----------------------------------------------------------------------------- | executeResult is added to any Output-result for the display of the object | returned by a cell |----------------------------------------------------------------------------*/ .jp-OutputArea-output.jp-OutputArea-executeResult { margin-left: 0px; flex: 1 1 auto; } .jp-OutputArea-executeResult.jp-RenderedText { padding-top: var(--jp-code-padding); } /*----------------------------------------------------------------------------- | The Stdin output |----------------------------------------------------------------------------*/ .jp-OutputArea-stdin { line-height: var(--jp-code-line-height); padding-top: var(--jp-code-padding); display: flex; } .jp-Stdin-prompt { color: var(--jp-content-font-color0); padding-right: var(--jp-code-padding); vertical-align: baseline; flex: 0 0 auto; } .jp-Stdin-input { font-family: var(--jp-code-font-family); font-size: inherit; color: inherit; background-color: inherit; width: 42%; min-width: 200px; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; flex: 0 0 70%; } .jp-Stdin-input:focus { box-shadow: none; } /*----------------------------------------------------------------------------- | Output Area View |----------------------------------------------------------------------------*/ .jp-LinkedOutputView .jp-OutputArea { height: 100%; display: block; } .jp-LinkedOutputView .jp-OutputArea-output:only-child { height: 100%; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /* This file was auto-generated by ensurePackage() in @jupyterlab/buildutils */ /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ .jp-Collapser { flex: 0 0 var(--jp-cell-collapser-width); padding: 0px; margin: 0px; border: none; outline: none; background: transparent; border-radius: var(--jp-border-radius); opacity: 1; } .jp-Collapser-child { display: block; width: 100%; box-sizing: border-box; /* height: 100% doesn't work because the height of its parent is computed from content */ position: absolute; top: 0px; bottom: 0px; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /*----------------------------------------------------------------------------- | Header/Footer |----------------------------------------------------------------------------*/ /* Hidden by zero height by default */ .jp-CellHeader, .jp-CellFooter { height: 0px; width: 100%; padding: 0px; margin: 0px; border: none; outline: none; background: transparent; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /*----------------------------------------------------------------------------- | Input |----------------------------------------------------------------------------*/ /* All input areas */ .jp-InputArea { display: flex; flex-direction: row; } .jp-InputArea-editor { flex: 1 1 auto; } .jp-InputArea-editor { /* This is the non-active, default styling */ border: var(--jp-border-width) solid var(--jp-cell-editor-border-color); border-radius: 0px; background: var(--jp-cell-editor-background); } .jp-InputPrompt { flex: 0 0 var(--jp-cell-prompt-width); color: var(--jp-cell-inprompt-font-color); font-family: var(--jp-cell-prompt-font-family); padding: var(--jp-code-padding); letter-spacing: var(--jp-cell-prompt-letter-spacing); opacity: var(--jp-cell-prompt-opacity); line-height: var(--jp-code-line-height); font-size: var(--jp-code-font-size); border: var(--jp-border-width) solid transparent; opacity: var(--jp-cell-prompt-opacity); /* Right align prompt text, don't wrap to handle large prompt numbers */ text-align: right; white-space: nowrap; overflow: hidden; text-overflow: ellipsis; /* Disable text selection */ -webkit-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /*----------------------------------------------------------------------------- | Placeholder |----------------------------------------------------------------------------*/ .jp-Placeholder { display: flex; flex-direction: row; flex: 1 1 auto; } .jp-Placeholder-prompt { box-sizing: border-box; } .jp-Placeholder-content { flex: 1 1 auto; border: none; background: transparent; height: 20px; box-sizing: border-box; } .jp-Placeholder-content .jp-MoreHorizIcon { width: 32px; height: 16px; border: 1px solid transparent; border-radius: var(--jp-border-radius); } .jp-Placeholder-content .jp-MoreHorizIcon:hover { border: 1px solid var(--jp-border-color1); box-shadow: 0px 0px 2px 0px rgba(0, 0, 0, 0.25); background-color: var(--jp-layout-color0); } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /*----------------------------------------------------------------------------- | Private CSS variables |----------------------------------------------------------------------------*/ :root { --jp-private-cell-scrolling-output-offset: 5px; } /*----------------------------------------------------------------------------- | Cell |----------------------------------------------------------------------------*/ .jp-Cell { padding: var(--jp-cell-padding); margin: 0px; border: none; outline: none; background: transparent; } /*----------------------------------------------------------------------------- | Common input/output |----------------------------------------------------------------------------*/ .jp-Cell-inputWrapper, .jp-Cell-outputWrapper { display: flex; flex-direction: row; padding: 0px; margin: 0px; /* Added to reveal the box-shadow on the input and output collapsers. */ overflow: visible; } /* Only input/output areas inside cells */ .jp-Cell-inputArea, .jp-Cell-outputArea { flex: 1 1 auto; } /*----------------------------------------------------------------------------- | Collapser |----------------------------------------------------------------------------*/ /* Make the output collapser disappear when there is not output, but do so * in a manner that leaves it in the layout and preserves its width. */ .jp-Cell.jp-mod-noOutputs .jp-Cell-outputCollapser { border: none !important; background: transparent !important; } .jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputCollapser { min-height: var(--jp-cell-collapser-min-height); } /*----------------------------------------------------------------------------- | Output |----------------------------------------------------------------------------*/ /* Put a space between input and output when there IS output */ .jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputWrapper { margin-top: 5px; } /* Text output with the Out[] prompt needs a top padding to match the * alignment of the Out[] prompt itself. */ .jp-OutputArea-executeResult .jp-RenderedText.jp-OutputArea-output { padding-top: var(--jp-code-padding); } .jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea { overflow-y: auto; max-height: 200px; box-shadow: inset 0 0 6px 2px rgba(0, 0, 0, 0.3); margin-left: var(--jp-private-cell-scrolling-output-offset); } .jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-prompt { flex: 0 0 calc( var(--jp-cell-prompt-width) - var(--jp-private-cell-scrolling-output-offset) ); } /*----------------------------------------------------------------------------- | CodeCell |----------------------------------------------------------------------------*/ /*----------------------------------------------------------------------------- | MarkdownCell |----------------------------------------------------------------------------*/ .jp-MarkdownOutput { flex: 1 1 auto; margin-top: 0; margin-bottom: 0; padding-left: var(--jp-code-padding); } .jp-MarkdownOutput.jp-RenderedHTMLCommon { overflow: auto; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /* This file was auto-generated by ensurePackage() in @jupyterlab/buildutils */ /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /*----------------------------------------------------------------------------- | Variables |----------------------------------------------------------------------------*/ /*----------------------------------------------------------------------------- /*----------------------------------------------------------------------------- | Styles |----------------------------------------------------------------------------*/ .jp-NotebookPanel-toolbar { padding: 2px; } .jp-Toolbar-item.jp-Notebook-toolbarCellType .jp-select-wrapper.jp-mod-focused { border: none; box-shadow: none; } .jp-Notebook-toolbarCellTypeDropdown select { height: 24px; font-size: var(--jp-ui-font-size1); line-height: 14px; border-radius: 0; display: block; } .jp-Notebook-toolbarCellTypeDropdown span { top: 5px !important; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /*----------------------------------------------------------------------------- | Private CSS variables |----------------------------------------------------------------------------*/ :root { --jp-private-notebook-dragImage-width: 304px; --jp-private-notebook-dragImage-height: 36px; --jp-private-notebook-selected-color: var(--md-blue-400); --jp-private-notebook-active-color: var(--md-green-400); } /*----------------------------------------------------------------------------- | Imports |----------------------------------------------------------------------------*/ /*----------------------------------------------------------------------------- | Notebook |----------------------------------------------------------------------------*/ .jp-NotebookPanel { display: block; height: 100%; } .jp-NotebookPanel.jp-Document { min-width: 240px; min-height: 120px; } .jp-Notebook { padding: var(--jp-notebook-padding); outline: none; overflow: auto; background: var(--jp-layout-color0); } .jp-Notebook.jp-mod-scrollPastEnd::after { display: block; content: ''; min-height: var(--jp-notebook-scroll-padding); } .jp-Notebook .jp-Cell { overflow: visible; } .jp-Notebook .jp-Cell .jp-InputPrompt { cursor: move; } /*----------------------------------------------------------------------------- | Notebook state related styling | | The notebook and cells each have states, here are the possibilities: | | - Notebook | - Command | - Edit | - Cell | - None | - Active (only one can be active) | - Selected (the cells actions are applied to) | - Multiselected (when multiple selected, the cursor) | - No outputs |----------------------------------------------------------------------------*/ /* Command or edit modes */ .jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-InputPrompt { opacity: var(--jp-cell-prompt-not-active-opacity); color: var(--jp-cell-prompt-not-active-font-color); } .jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-OutputPrompt { opacity: var(--jp-cell-prompt-not-active-opacity); color: var(--jp-cell-prompt-not-active-font-color); } /* cell is active */ .jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser { background: var(--jp-brand-color1); } /* collapser is hovered */ .jp-Notebook .jp-Cell .jp-Collapser:hover { box-shadow: var(--jp-elevation-z2); background: var(--jp-brand-color1); opacity: var(--jp-cell-collapser-not-active-hover-opacity); } /* cell is active and collapser is hovered */ .jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser:hover { background: var(--jp-brand-color0); opacity: 1; } /* Command mode */ .jp-Notebook.jp-mod-commandMode .jp-Cell.jp-mod-selected { background: var(--jp-notebook-multiselected-color); } .jp-Notebook.jp-mod-commandMode .jp-Cell.jp-mod-active.jp-mod-selected:not(.jp-mod-multiSelected) { background: transparent; } /* Edit mode */ .jp-Notebook.jp-mod-editMode .jp-Cell.jp-mod-active .jp-InputArea-editor { border: var(--jp-border-width) solid var(--jp-cell-editor-active-border-color); box-shadow: var(--jp-input-box-shadow); background-color: var(--jp-cell-editor-active-background); } /*----------------------------------------------------------------------------- | Notebook drag and drop |----------------------------------------------------------------------------*/ .jp-Notebook-cell.jp-mod-dropSource { opacity: 0.5; } .jp-Notebook-cell.jp-mod-dropTarget, .jp-Notebook.jp-mod-commandMode .jp-Notebook-cell.jp-mod-active.jp-mod-selected.jp-mod-dropTarget { border-top-color: var(--jp-private-notebook-selected-color); border-top-style: solid; border-top-width: 2px; } .jp-dragImage { display: flex; flex-direction: row; width: var(--jp-private-notebook-dragImage-width); height: var(--jp-private-notebook-dragImage-height); border: var(--jp-border-width) solid var(--jp-cell-editor-border-color); background: var(--jp-cell-editor-background); overflow: visible; } .jp-dragImage-singlePrompt { box-shadow: 2px 2px 4px 0px rgba(0, 0, 0, 0.12); } .jp-dragImage .jp-dragImage-content { flex: 1 1 auto; z-index: 2; font-size: var(--jp-code-font-size); font-family: var(--jp-code-font-family); line-height: var(--jp-code-line-height); padding: var(--jp-code-padding); border: var(--jp-border-width) solid var(--jp-cell-editor-border-color); background: var(--jp-cell-editor-background-color); color: var(--jp-content-font-color3); text-align: left; margin: 4px 4px 4px 0px; } .jp-dragImage .jp-dragImage-prompt { flex: 0 0 auto; min-width: 36px; color: var(--jp-cell-inprompt-font-color); padding: var(--jp-code-padding); padding-left: 12px; font-family: var(--jp-cell-prompt-font-family); letter-spacing: var(--jp-cell-prompt-letter-spacing); line-height: 1.9; font-size: var(--jp-code-font-size); border: var(--jp-border-width) solid transparent; } .jp-dragImage-multipleBack { z-index: -1; position: absolute; height: 32px; width: 300px; top: 8px; left: 8px; background: var(--jp-layout-color2); border: var(--jp-border-width) solid var(--jp-input-border-color); box-shadow: 2px 2px 4px 0px rgba(0, 0, 0, 0.12); } /*----------------------------------------------------------------------------- | Cell toolbar |----------------------------------------------------------------------------*/ .jp-NotebookTools { display: block; min-width: var(--jp-sidebar-min-width); color: var(--jp-ui-font-color1); background: var(--jp-layout-color1); /* This is needed so that all font sizing of children done in ems is * relative to this base size */ font-size: var(--jp-ui-font-size1); overflow: auto; } .jp-NotebookTools-tool { padding: 0px 12px 0 12px; } .jp-ActiveCellTool { padding: 12px; background-color: var(--jp-layout-color1); border-top: none !important; } .jp-ActiveCellTool .jp-InputArea-prompt { flex: 0 0 auto; padding-left: 0px; } .jp-ActiveCellTool .jp-InputArea-editor { flex: 1 1 auto; background: var(--jp-cell-editor-background); border-color: var(--jp-cell-editor-border-color); } .jp-ActiveCellTool .jp-InputArea-editor .CodeMirror { background: transparent; } .jp-MetadataEditorTool { flex-direction: column; padding: 12px 0px 12px 0px; } .jp-RankedPanel > :not(:first-child) { margin-top: 12px; } .jp-KeySelector select.jp-mod-styled { font-size: var(--jp-ui-font-size1); color: var(--jp-ui-font-color0); border: var(--jp-border-width) solid var(--jp-border-color1); } .jp-KeySelector label, .jp-MetadataEditorTool label { line-height: 1.4; } /*----------------------------------------------------------------------------- | Presentation Mode (.jp-mod-presentationMode) |----------------------------------------------------------------------------*/ .jp-mod-presentationMode .jp-Notebook { --jp-content-font-size1: var(--jp-content-presentation-font-size1); --jp-code-font-size: var(--jp-code-presentation-font-size); } .jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-InputPrompt, .jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-OutputPrompt { flex: 0 0 110px; } /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /* This file was auto-generated by ensurePackage() in @jupyterlab/buildutils */ /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /* The following CSS variables define the main, public API for styling JupyterLab. These variables should be used by all plugins wherever possible. In other words, plugins should not define custom colors, sizes, etc unless absolutely necessary. This enables users to change the visual theme of JupyterLab by changing these variables. Many variables appear in an ordered sequence (0,1,2,3). These sequences are designed to work well together, so for example, `--jp-border-color1` should be used with `--jp-layout-color1`. The numbers have the following meanings: * 0: super-primary, reserved for special emphasis * 1: primary, most important under normal situations * 2: secondary, next most important under normal situations * 3: tertiary, next most important under normal situations Throughout JupyterLab, we are mostly following principles from Google's Material Design when selecting colors. We are not, however, following all of MD as it is not optimized for dense, information rich UIs. */ :root { /* Elevation * * We style box-shadows using Material Design's idea of elevation. These particular numbers are taken from here: * * https://github.com/material-components/material-components-web * https://material-components-web.appspot.com/elevation.html */ --jp-shadow-base-lightness: 0; --jp-shadow-umbra-color: rgba( var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), 0.2 ); --jp-shadow-penumbra-color: rgba( var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), 0.14 ); --jp-shadow-ambient-color: rgba( var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), 0.12 ); --jp-elevation-z0: none; --jp-elevation-z1: 0px 2px 1px -1px var(--jp-shadow-umbra-color), 0px 1px 1px 0px var(--jp-shadow-penumbra-color), 0px 1px 3px 0px var(--jp-shadow-ambient-color); --jp-elevation-z2: 0px 3px 1px -2px var(--jp-shadow-umbra-color), 0px 2px 2px 0px var(--jp-shadow-penumbra-color), 0px 1px 5px 0px var(--jp-shadow-ambient-color); --jp-elevation-z4: 0px 2px 4px -1px var(--jp-shadow-umbra-color), 0px 4px 5px 0px var(--jp-shadow-penumbra-color), 0px 1px 10px 0px var(--jp-shadow-ambient-color); --jp-elevation-z6: 0px 3px 5px -1px var(--jp-shadow-umbra-color), 0px 6px 10px 0px var(--jp-shadow-penumbra-color), 0px 1px 18px 0px var(--jp-shadow-ambient-color); --jp-elevation-z8: 0px 5px 5px -3px var(--jp-shadow-umbra-color), 0px 8px 10px 1px var(--jp-shadow-penumbra-color), 0px 3px 14px 2px var(--jp-shadow-ambient-color); --jp-elevation-z12: 0px 7px 8px -4px var(--jp-shadow-umbra-color), 0px 12px 17px 2px var(--jp-shadow-penumbra-color), 0px 5px 22px 4px var(--jp-shadow-ambient-color); --jp-elevation-z16: 0px 8px 10px -5px var(--jp-shadow-umbra-color), 0px 16px 24px 2px var(--jp-shadow-penumbra-color), 0px 6px 30px 5px var(--jp-shadow-ambient-color); --jp-elevation-z20: 0px 10px 13px -6px var(--jp-shadow-umbra-color), 0px 20px 31px 3px var(--jp-shadow-penumbra-color), 0px 8px 38px 7px var(--jp-shadow-ambient-color); --jp-elevation-z24: 0px 11px 15px -7px var(--jp-shadow-umbra-color), 0px 24px 38px 3px var(--jp-shadow-penumbra-color), 0px 9px 46px 8px var(--jp-shadow-ambient-color); /* Borders * * The following variables, specify the visual styling of borders in JupyterLab. */ --jp-border-width: 1px; --jp-border-color0: var(--md-grey-400); --jp-border-color1: var(--md-grey-400); --jp-border-color2: var(--md-grey-300); --jp-border-color3: var(--md-grey-200); --jp-border-radius: 2px; /* UI Fonts * * The UI font CSS variables are used for the typography all of the JupyterLab * user interface elements that are not directly user generated content. * * The font sizing here is done assuming that the body font size of --jp-ui-font-size1 * is applied to a parent element. When children elements, such as headings, are sized * in em all things will be computed relative to that body size. */ --jp-ui-font-scale-factor: 1.2; --jp-ui-font-size0: 0.83333em; --jp-ui-font-size1: 13px; /* Base font size */ --jp-ui-font-size2: 1.2em; --jp-ui-font-size3: 1.44em; --jp-ui-font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; /* * Use these font colors against the corresponding main layout colors. * In a light theme, these go from dark to light. */ /* Defaults use Material Design specification */ --jp-ui-font-color0: rgba(0, 0, 0, 1); --jp-ui-font-color1: rgba(0, 0, 0, 0.87); --jp-ui-font-color2: rgba(0, 0, 0, 0.54); --jp-ui-font-color3: rgba(0, 0, 0, 0.38); /* * Use these against the brand/accent/warn/error colors. * These will typically go from light to darker, in both a dark and light theme. */ --jp-ui-inverse-font-color0: rgba(255, 255, 255, 1); --jp-ui-inverse-font-color1: rgba(255, 255, 255, 1); --jp-ui-inverse-font-color2: rgba(255, 255, 255, 0.7); --jp-ui-inverse-font-color3: rgba(255, 255, 255, 0.5); /* Content Fonts * * Content font variables are used for typography of user generated content. * * The font sizing here is done assuming that the body font size of --jp-content-font-size1 * is applied to a parent element. When children elements, such as headings, are sized * in em all things will be computed relative to that body size. */ --jp-content-line-height: 1.6; --jp-content-font-scale-factor: 1.2; --jp-content-font-size0: 0.83333em; --jp-content-font-size1: 14px; /* Base font size */ --jp-content-font-size2: 1.2em; --jp-content-font-size3: 1.44em; --jp-content-font-size4: 1.728em; --jp-content-font-size5: 2.0736em; /* This gives a magnification of about 125% in presentation mode over normal. */ --jp-content-presentation-font-size1: 17px; --jp-content-heading-line-height: 1; --jp-content-heading-margin-top: 1.2em; --jp-content-heading-margin-bottom: 0.8em; --jp-content-heading-font-weight: 500; /* Defaults use Material Design specification */ --jp-content-font-color0: rgba(0, 0, 0, 1); --jp-content-font-color1: rgba(0, 0, 0, 0.87); --jp-content-font-color2: rgba(0, 0, 0, 0.54); --jp-content-font-color3: rgba(0, 0, 0, 0.38); --jp-content-link-color: var(--md-blue-700); --jp-content-font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; /* * Code Fonts * * Code font variables are used for typography of code and other monospaces content. */ --jp-code-font-size: 13px; --jp-code-line-height: 1.3077; /* 17px for 13px base */ --jp-code-padding: 5px; /* 5px for 13px base, codemirror highlighting needs integer px value */ --jp-code-font-family-default: Menlo, Consolas, 'DejaVu Sans Mono', monospace; --jp-code-font-family: var(--jp-code-font-family-default); /* This gives a magnification of about 125% in presentation mode over normal. */ --jp-code-presentation-font-size: 16px; /* may need to tweak cursor width if you change font size */ --jp-code-cursor-width0: 1.4px; --jp-code-cursor-width1: 2px; --jp-code-cursor-width2: 4px; /* Layout * * The following are the main layout colors use in JupyterLab. In a light * theme these would go from light to dark. */ --jp-layout-color0: white; --jp-layout-color1: white; --jp-layout-color2: var(--md-grey-200); --jp-layout-color3: var(--md-grey-400); --jp-layout-color4: var(--md-grey-600); /* Inverse Layout * * The following are the inverse layout colors use in JupyterLab. In a light * theme these would go from dark to light. */ --jp-inverse-layout-color0: #111111; --jp-inverse-layout-color1: var(--md-grey-900); --jp-inverse-layout-color2: var(--md-grey-800); --jp-inverse-layout-color3: var(--md-grey-700); --jp-inverse-layout-color4: var(--md-grey-600); /* Brand/accent */ --jp-brand-color0: var(--md-blue-700); --jp-brand-color1: var(--md-blue-500); --jp-brand-color2: var(--md-blue-300); --jp-brand-color3: var(--md-blue-100); --jp-brand-color4: var(--md-blue-50); --jp-accent-color0: var(--md-green-700); --jp-accent-color1: var(--md-green-500); --jp-accent-color2: var(--md-green-300); --jp-accent-color3: var(--md-green-100); /* State colors (warn, error, success, info) */ --jp-warn-color0: var(--md-orange-700); --jp-warn-color1: var(--md-orange-500); --jp-warn-color2: var(--md-orange-300); --jp-warn-color3: var(--md-orange-100); --jp-error-color0: var(--md-red-700); --jp-error-color1: var(--md-red-500); --jp-error-color2: var(--md-red-300); --jp-error-color3: var(--md-red-100); --jp-success-color0: var(--md-green-700); --jp-success-color1: var(--md-green-500); --jp-success-color2: var(--md-green-300); --jp-success-color3: var(--md-green-100); --jp-info-color0: var(--md-cyan-700); --jp-info-color1: var(--md-cyan-500); --jp-info-color2: var(--md-cyan-300); --jp-info-color3: var(--md-cyan-100); /* Cell specific styles */ --jp-cell-padding: 5px; --jp-cell-collapser-width: 8px; --jp-cell-collapser-min-height: 20px; --jp-cell-collapser-not-active-hover-opacity: 0.6; --jp-cell-editor-background: var(--md-grey-100); --jp-cell-editor-border-color: var(--md-grey-300); --jp-cell-editor-box-shadow: inset 0 0 2px var(--md-blue-300); --jp-cell-editor-active-background: var(--jp-layout-color0); --jp-cell-editor-active-border-color: var(--jp-brand-color1); --jp-cell-prompt-width: 64px; --jp-cell-prompt-font-family: 'Source Code Pro', monospace; --jp-cell-prompt-letter-spacing: 0px; --jp-cell-prompt-opacity: 1; --jp-cell-prompt-not-active-opacity: 0.5; --jp-cell-prompt-not-active-font-color: var(--md-grey-700); /* A custom blend of MD grey and blue 600 * See https://meyerweb.com/eric/tools/color-blend/#546E7A:1E88E5:5:hex */ --jp-cell-inprompt-font-color: #307fc1; /* A custom blend of MD grey and orange 600 * https://meyerweb.com/eric/tools/color-blend/#546E7A:F4511E:5:hex */ --jp-cell-outprompt-font-color: #bf5b3d; /* Notebook specific styles */ --jp-notebook-padding: 10px; --jp-notebook-select-background: var(--jp-layout-color1); --jp-notebook-multiselected-color: var(--md-blue-50); /* The scroll padding is calculated to fill enough space at the bottom of the notebook to show one single-line cell (with appropriate padding) at the top when the notebook is scrolled all the way to the bottom. We also subtract one pixel so that no scrollbar appears if we have just one single-line cell in the notebook. This padding is to enable a 'scroll past end' feature in a notebook. */ --jp-notebook-scroll-padding: calc( 100% - var(--jp-code-font-size) * var(--jp-code-line-height) - var(--jp-code-padding) - var(--jp-cell-padding) - 1px ); /* Rendermime styles */ --jp-rendermime-error-background: #fdd; --jp-rendermime-table-row-background: var(--md-grey-100); --jp-rendermime-table-row-hover-background: var(--md-light-blue-50); /* Dialog specific styles */ --jp-dialog-background: rgba(0, 0, 0, 0.25); /* Console specific styles */ --jp-console-padding: 10px; /* Toolbar specific styles */ --jp-toolbar-border-color: var(--jp-border-color1); --jp-toolbar-micro-height: 8px; --jp-toolbar-background: var(--jp-layout-color1); --jp-toolbar-box-shadow: 0px 0px 2px 0px rgba(0, 0, 0, 0.24); --jp-toolbar-header-margin: 4px 4px 0px 4px; --jp-toolbar-active-background: var(--md-grey-300); /* Input field styles */ --jp-input-box-shadow: inset 0 0 2px var(--md-blue-300); --jp-input-active-background: var(--jp-layout-color1); --jp-input-hover-background: var(--jp-layout-color1); --jp-input-background: var(--md-grey-100); --jp-input-border-color: var(--jp-border-color1); --jp-input-active-border-color: var(--jp-brand-color1); --jp-input-active-box-shadow-color: rgba(19, 124, 189, 0.3); /* General editor styles */ --jp-editor-selected-background: #d9d9d9; --jp-editor-selected-focused-background: #d7d4f0; --jp-editor-cursor-color: var(--jp-ui-font-color0); /* Code mirror specific styles */ --jp-mirror-editor-keyword-color: #008000; --jp-mirror-editor-atom-color: #88f; --jp-mirror-editor-number-color: #080; --jp-mirror-editor-def-color: #00f; --jp-mirror-editor-variable-color: var(--md-grey-900); --jp-mirror-editor-variable-2-color: #05a; --jp-mirror-editor-variable-3-color: #085; --jp-mirror-editor-punctuation-color: #05a; --jp-mirror-editor-property-color: #05a; --jp-mirror-editor-operator-color: #aa22ff; --jp-mirror-editor-comment-color: #408080; --jp-mirror-editor-string-color: #ba2121; --jp-mirror-editor-string-2-color: #708; --jp-mirror-editor-meta-color: #aa22ff; --jp-mirror-editor-qualifier-color: #555; --jp-mirror-editor-builtin-color: #008000; --jp-mirror-editor-bracket-color: #997; --jp-mirror-editor-tag-color: #170; --jp-mirror-editor-attribute-color: #00c; --jp-mirror-editor-header-color: blue; --jp-mirror-editor-quote-color: #090; --jp-mirror-editor-link-color: #00c; --jp-mirror-editor-error-color: #f00; --jp-mirror-editor-hr-color: #999; /* Vega extension styles */ --jp-vega-background: white; /* Sidebar-related styles */ --jp-sidebar-min-width: 180px; /* Search-related styles */ --jp-search-toggle-off-opacity: 0.5; --jp-search-toggle-hover-opacity: 0.8; --jp-search-toggle-on-opacity: 1; --jp-search-selected-match-background-color: rgb(245, 200, 0); --jp-search-selected-match-color: black; --jp-search-unselected-match-background-color: var( --jp-inverse-layout-color0 ); --jp-search-unselected-match-color: var(--jp-ui-inverse-font-color0); /* Icon colors that work well with light or dark backgrounds */ --jp-icon-contrast-color0: var(--md-purple-600); --jp-icon-contrast-color1: var(--md-green-600); --jp-icon-contrast-color2: var(--md-pink-600); --jp-icon-contrast-color3: var(--md-blue-600); } /* Reset */ *, *::before, *::after { -webkit-box-sizing: border-box; box-sizing: border-box; } /* Reset */ .jp-Notebook { padding: unset; /* outline: none; */ /* overflow: auto; */ background: unset; } /* Reset */ .jp-RenderedHTMLCommon { padding: 0; } /* Anchor links */ .anchor-link { display: none; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link { display: inline-block; } /* Markdown cells: Remove prompt */ .jp-Cell-inputWrapper .jp-InputPrompt { display: none; } /* Code cells: Show prompt */ .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt { display: block; } /* Prompt fixes */ .jp-InputPrompt, .jp-OutputPrompt{ overflow: visible; } /* Lab uses move for the input prompts */ .jp-Notebook .jp-Cell .jp-InputPrompt { cursor: default; } /* Editor max-width */ .jp-InputArea-editor { width: 1px; } /* Editor colors */ .jp-Editor .highlight { margin: 0.4em; background: var(--jp-cell-editor-background); color: var(--jp-mirror-editor-variable-color); } /* Editor colors */ .jp-Editor .highlight pre { background: none; border: none; margin: 0; overflow-x: auto; } /* Material mkdocs */ .jp-RenderedHTMLCommon p { font-size: .8rem; line-height: 1.6; } .jp-RenderedHTMLCommon li { font-size: .8rem; line-height: 1.6; } .jp-RenderedHTMLCommon h1 { margin: 0 0 1.25em; color: var(--md-default-fg-color--light); font-weight: 300; font-size: 2em; line-height: 1.3; letter-spacing: -0.01em; } .jp-RenderedHTMLCommon h2 { margin: 1.6em 0 .64em; font-weight: 300; font-size: 1.965em; line-height: 1.4; letter-spacing: -0.01em; } .jp-RenderedHTMLCommon h3 { margin: 1.6em 0 .8em; font-weight: 400; font-size: 1.57em; line-height: 1.5; letter-spacing: -0.01em; } .jp-RenderedHTMLCommon hr { border: none; } /* Tags on cells */ /* Mostly copying this from straight Jupyter */ .celltoolbar { border: none; background: #EEE; border-radius: 2px 2px 0px 0px; width: 100%; height: 29px; padding-right: 4px; box-orient: horizontal; box-align: stretch; display: flex; flex-direction: row; align-items: stretch; box-pack: end; justify-content: flex-start; display: -webkit-flex; } .celltoolbar .tags_button_container { display: flex; } .celltoolbar .tags_button_container .tag-container { display: flex; flex-direction: row; flex-grow: 1; overflow: hidden; position: relative; } .celltoolbar .tags_button_container .tag-container .cell-tag { background-color: #fff; white-space: nowrap; margin: 3px 4px; padding: 0 4px; border-radius: 1px; border: 1px solid #ccc; box-shadow: none; width: inherit; font-size: 11px; font-family: \"Roboto Mono\",SFMono-Regular,Consolas,Menlo,monospace; height: 22px; /* line-height: 22px; */ display: inline-block; } /* Bokeh plots table no hover */ .bk-plot-wrapper tbody tr { background: none !important; } .bk-plot-wrapper tbody tr:hover { background: none !important; } init_mathjax = function() { if (window.MathJax) { // MathJax loaded MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: \"AMS\", useLabelIds: true } }, tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ], processEscapes: true, processEnvironments: true }, displayAlign: 'center', CommonHTML: { linebreaks: { automatic: true } }, \"HTML-CSS\": { linebreaks: { automatic: true } } }); MathJax.Hub.Queue([\"Typeset\", MathJax.Hub]); } } init_mathjax(); Getting Started: Low Level API \u00b6 Elegy's low-level API works similar to the Pytorch Lightning API or Keras Custom Training and has the following features: Its functional: it uses a simple functional state management pattern so it should be compatible with all jax features. Its framework agnostic: since its just Jax you can use any Jax framework like Flax, Haiku, Objax, etc, or even just use pure Jax. Its very flexible: you get to define everything from how predictions, losses, metrics, gradients, parameters update, etc, are calculated, this is more ideal for research or less standar training procedures like Adversarial Training. Before getting started lets install some dependencies and define some styles for the notebook: In [ ]: ! pip install -- upgrade pip ! pip install elegy dataget matplotlib # For GPU install proper version of your CUDA, following will work in COLAB: # ! pip install --upgrade jax jaxlib==0.1.59+cuda101 -f https://storage.googleapis.com/jax-releases/jax_releases.html In [7]: %% html < style > table { margin - left : 0 ! important ;} </ style > table {margin-left: 0 !important;} Note: that Elegy depends on the jax CPU version hosted on Pypi, if you want to run jax on GPU you will need to install it separately. If you are running this example on Colab, jax is already preinstalled but you can uncomment the last line of the previous cell if you want to update it. Loading the Data \u00b6 In this tutorial we will train a Neural Network on the MNIST dataset, for this we will first need to download and load the data into memory. Here we will use dataget for simplicity but you can use you favorite datasets library. In [1]: import dataget X_train , y_train , X_test , y_test = dataget . image . mnist ( global_cache = True ) . get () print ( \"X_train:\" , X_train . shape , X_train . dtype ) print ( \"y_train:\" , y_train . shape , y_train . dtype ) print ( \"X_test:\" , X_test . shape , X_test . dtype ) print ( \"y_test:\" , y_test . shape , y_test . dtype ) X_train: (60000, 28, 28) uint8 y_train: (60000,) uint8 X_test: (10000, 28, 28) uint8 y_test: (10000,) uint8 In this case dataget loads the data from Yann LeCun's website. Defining a simple Model \u00b6 The low-level API lets you redefine what happens during the various stages of training, evaluation and inference by implementing some methods in a custom class. Here is the list of methods you can define along with the high-level method that uses it: Low-level Method High-level Method pred_step predict test_step evaluate grad_step NA train_step fit Check out the guides on the low-level API for more information. In this tutorial we are going to implement Linear Classifier using pure Jax by overriding Model.test_step which defines loss and metrics of our model. test_step returns a tuple with: loss : the scalar loss use to calculate the gradient logs : a dictionary with the logs to be reported during training states : a elegy.States namedtuple that contains the states for thing like network trainable parameter, network states, metrics states, optimizer states, rng state. Since Jax is functional you will find that low-level API is very explicit with state management, that is, you always get the currrent state as input and you return the new state as output. Lets define test_step to make things clearer: In [2]: import elegy , jax import numpy as np import jax.numpy as jnp class LinearClassifier ( elegy . Model ): # request parameters by name via depending injection. # possible: net_params, x, y_true, net_states, metrics_states, sample_weight, class_weight, rng, states, initializing def test_step ( self , x , # inputs y_true , # labels states : elegy . States , # model state initializing : bool , # if True we should initialize our parameters ): rng : elegy . RNGSeq = states . rng # rng.next() ~ jax.random.split(...) # flatten + scale x = jnp . reshape ( x , ( x . shape [ 0 ], - 1 )) / 255 # maybe init if initializing : w = jax . random . uniform ( rng . next (), shape = [ np . prod ( x . shape [ 1 :]), 10 ], minval =- 1 , maxval = 1 ) b = jax . random . uniform ( rng . next (), shape = [ 1 ], minval =- 1 , maxval = 1 ) else : w , b = states . net_params # model logits = jnp . dot ( x , w ) + b # crossentropy loss labels = jax . nn . one_hot ( y_true , 10 ) loss = jnp . mean ( - jnp . sum ( labels * jax . nn . log_softmax ( logits ), axis =- 1 )) accuracy = jnp . mean ( jnp . argmax ( logits , axis =- 1 ) == y_true ) # metrics logs = dict ( accuracy = accuracy , loss = loss , ) return loss , logs , states . update ( net_params = ( w , b )) Notice the following: We define a bunch of arguments with specific names, Elegy uses Dependency Injection so you can just request what you need. initializing tells use if we should initialize our parameters, here we are directly creating them ourselves but if you use a Module system you can conditionally call its init method here. Our model is defined by a simple linear function. Defined a simple crossentropy loss and an accuracy metric, we added both the the logs. We set the updated States.net_params with the w and b parameters so we get them as an input on the next run after they are initialized. States.update ofers a clean way inmutably update the states without having to copy all fields to a new States structure. Remember test_step only defines what happens what happens during evaluate , however, Elegy's Model default implementation has a structure where on method is defined in terms of another: pred_step \u2b05 test_step \u2b05 grad_step \u2b05 train_step Because of this, we get the train_step / fit for free if we just pass an optimizer to the the constructor as we are going to do next: In [3]: import optax model = LinearClassifier ( optimizer = optax . adam ( 1e-3 ) ) If we try to get the summaries we will get an error as we haven't defined pred_step which is a depencency: In [5]: try : model . summary ( X_train [: 64 ]) except BaseException as e : print ( e ) Trying run default `pred_step` on a Model with no `module`, try overriding `pred_step`. Training the Model \u00b6 Now that we have our model we can just call fit . The following code will train our model for 100 epochs while limiting each epoch to 200 steps and using a batch size of 64 : In [ ]: history = model . fit ( x = X_train , y = y_train , epochs = 100 , steps_per_epoch = 200 , batch_size = 64 , validation_data = ( X_test , y_test ), shuffle = True , callbacks = [ elegy . callbacks . ModelCheckpoint ( \"models/low-level\" , save_best_only = True )], ) ... Epoch 99/100 200/200 [==============================] - 0s 680us/step - accuracy: 0.9375 - loss: 0.2508 - val_accuracy: 1.0000 - val_loss: 0.0878 Epoch 100/100 200/200 [==============================] - 0s 618us/step - accuracy: 0.9219 - loss: 0.2440 - val_accuracy: 0.8750 - val_loss: 0.1224 The elegy.callbacks.ModelCheckpoint callback will periodicall saves the model a folder called \"models/low-level\" during training which is very useful if we want to load it after the process is finished, we will use it later. fit returns a History object which which contains information the time series for the values of the losses and metrics throughout training, we can use it to generate some nice plots of the evolution of our training: In [7]: import matplotlib.pyplot as plt def plot_history ( history ): n_plots = len ( history . history . keys ()) // 2 plt . figure ( figsize = ( 14 , 24 )) for i , key in enumerate ( list ( history . history . keys ())[: n_plots ]): metric = history . history [ key ] val_metric = history . history [ f \"val_ { key } \" ] plt . subplot ( n_plots , 1 , i + 1 ) plt . plot ( metric , label = f \"Training { key } \" ) plt . plot ( val_metric , label = f \"Validation { key } \" ) plt . legend ( loc = \"lower right\" ) plt . ylabel ( key ) plt . title ( f \"Training and Validation { key } \" ) plt . show () plot_history ( history ) Notice that the logs are very noisy, this is because for this example we didn't use cummulative metrics so the reported value is just the value for the last batch of that epoch, not the value for the entire epoch. To fix this we could use some of the modules in elegy.metrics . Generating Predictions \u00b6 Having trained our model we can now get some samples from the test set and generate some predictions. First we will just pick some random samples using numpy : In [8]: import numpy as np idxs = np . random . randint ( 0 , 10000 , size = ( 9 ,)) x_sample = X_test [ idxs ] Here we selected 9 random images. Now if we had implemented pred_step we could've used predict , instead we are just going to the the calculation by hand. In [9]: x = x_sample . reshape ( x_sample . shape [ 0 ], - 1 ) w , b = model . states . net_params y_pred = jnp . dot ( x , w ) + b y_pred . shape Out[9]: (9, 10) Easy right? Finally lets plot the results to see if they are accurate. In [10]: plt . figure ( figsize = ( 12 , 12 )) for i in range ( 3 ): for j in range ( 3 ): k = 3 * i + j plt . subplot ( 3 , 3 , k + 1 ) plt . title ( f \" { np . argmax ( y_pred [ k ]) } \" ) plt . imshow ( x_sample [ k ], cmap = \"gray\" ) Good enough! Serialization \u00b6 To serialize the Model you can use the model.save(...) , this will create a folder with some files that contain the model's code plus all parameters and states, however since we had previously used the ModelCheckpoint callback we can load it using elegy.load . Lets get a new model reference containing the same weights and call its evaluate method to verify it loaded correctly: In [11]: # You can use can use `save` but `ModelCheckpoint already serialized the model # model.save(\"model\") # current model reference print ( \"current model id:\" , id ( model )) # load model from disk model = elegy . load ( \"models/low-level\" ) # new model reference print ( \"new model id: \" , id ( model )) # check that it works! model . evaluate ( x = X_test , y = y_test ) current model id: 139835452734576 new model id: 139834008582896 313/313 [==============================] - 1s 2ms/step - accuracy: 0.9116 - loss: 0.3216 Out[11]: {'accuracy': DeviceArray(1., dtype=float32), 'loss': DeviceArray(0.05334431, dtype=float32), 'size': 32} Excellent! We hope you've enjoyed this tutorial.","title":"Low Level API"},{"location":"getting-started/low-level-api/#getting-started-low-level-api","text":"Elegy's low-level API works similar to the Pytorch Lightning API or Keras Custom Training and has the following features: Its functional: it uses a simple functional state management pattern so it should be compatible with all jax features. Its framework agnostic: since its just Jax you can use any Jax framework like Flax, Haiku, Objax, etc, or even just use pure Jax. Its very flexible: you get to define everything from how predictions, losses, metrics, gradients, parameters update, etc, are calculated, this is more ideal for research or less standar training procedures like Adversarial Training. Before getting started lets install some dependencies and define some styles for the notebook: In [ ]: ! pip install -- upgrade pip ! pip install elegy dataget matplotlib # For GPU install proper version of your CUDA, following will work in COLAB: # ! pip install --upgrade jax jaxlib==0.1.59+cuda101 -f https://storage.googleapis.com/jax-releases/jax_releases.html In [7]: %% html < style > table { margin - left : 0 ! important ;} </ style > table {margin-left: 0 !important;} Note: that Elegy depends on the jax CPU version hosted on Pypi, if you want to run jax on GPU you will need to install it separately. If you are running this example on Colab, jax is already preinstalled but you can uncomment the last line of the previous cell if you want to update it.","title":"Getting Started: Low Level API"},{"location":"getting-started/low-level-api/#loading-the-data","text":"In this tutorial we will train a Neural Network on the MNIST dataset, for this we will first need to download and load the data into memory. Here we will use dataget for simplicity but you can use you favorite datasets library. In [1]: import dataget X_train , y_train , X_test , y_test = dataget . image . mnist ( global_cache = True ) . get () print ( \"X_train:\" , X_train . shape , X_train . dtype ) print ( \"y_train:\" , y_train . shape , y_train . dtype ) print ( \"X_test:\" , X_test . shape , X_test . dtype ) print ( \"y_test:\" , y_test . shape , y_test . dtype ) X_train: (60000, 28, 28) uint8 y_train: (60000,) uint8 X_test: (10000, 28, 28) uint8 y_test: (10000,) uint8 In this case dataget loads the data from Yann LeCun's website.","title":"Loading the Data"},{"location":"getting-started/low-level-api/#defining-a-simple-model","text":"The low-level API lets you redefine what happens during the various stages of training, evaluation and inference by implementing some methods in a custom class. Here is the list of methods you can define along with the high-level method that uses it: Low-level Method High-level Method pred_step predict test_step evaluate grad_step NA train_step fit Check out the guides on the low-level API for more information. In this tutorial we are going to implement Linear Classifier using pure Jax by overriding Model.test_step which defines loss and metrics of our model. test_step returns a tuple with: loss : the scalar loss use to calculate the gradient logs : a dictionary with the logs to be reported during training states : a elegy.States namedtuple that contains the states for thing like network trainable parameter, network states, metrics states, optimizer states, rng state. Since Jax is functional you will find that low-level API is very explicit with state management, that is, you always get the currrent state as input and you return the new state as output. Lets define test_step to make things clearer: In [2]: import elegy , jax import numpy as np import jax.numpy as jnp class LinearClassifier ( elegy . Model ): # request parameters by name via depending injection. # possible: net_params, x, y_true, net_states, metrics_states, sample_weight, class_weight, rng, states, initializing def test_step ( self , x , # inputs y_true , # labels states : elegy . States , # model state initializing : bool , # if True we should initialize our parameters ): rng : elegy . RNGSeq = states . rng # rng.next() ~ jax.random.split(...) # flatten + scale x = jnp . reshape ( x , ( x . shape [ 0 ], - 1 )) / 255 # maybe init if initializing : w = jax . random . uniform ( rng . next (), shape = [ np . prod ( x . shape [ 1 :]), 10 ], minval =- 1 , maxval = 1 ) b = jax . random . uniform ( rng . next (), shape = [ 1 ], minval =- 1 , maxval = 1 ) else : w , b = states . net_params # model logits = jnp . dot ( x , w ) + b # crossentropy loss labels = jax . nn . one_hot ( y_true , 10 ) loss = jnp . mean ( - jnp . sum ( labels * jax . nn . log_softmax ( logits ), axis =- 1 )) accuracy = jnp . mean ( jnp . argmax ( logits , axis =- 1 ) == y_true ) # metrics logs = dict ( accuracy = accuracy , loss = loss , ) return loss , logs , states . update ( net_params = ( w , b )) Notice the following: We define a bunch of arguments with specific names, Elegy uses Dependency Injection so you can just request what you need. initializing tells use if we should initialize our parameters, here we are directly creating them ourselves but if you use a Module system you can conditionally call its init method here. Our model is defined by a simple linear function. Defined a simple crossentropy loss and an accuracy metric, we added both the the logs. We set the updated States.net_params with the w and b parameters so we get them as an input on the next run after they are initialized. States.update ofers a clean way inmutably update the states without having to copy all fields to a new States structure. Remember test_step only defines what happens what happens during evaluate , however, Elegy's Model default implementation has a structure where on method is defined in terms of another: pred_step \u2b05 test_step \u2b05 grad_step \u2b05 train_step Because of this, we get the train_step / fit for free if we just pass an optimizer to the the constructor as we are going to do next: In [3]: import optax model = LinearClassifier ( optimizer = optax . adam ( 1e-3 ) ) If we try to get the summaries we will get an error as we haven't defined pred_step which is a depencency: In [5]: try : model . summary ( X_train [: 64 ]) except BaseException as e : print ( e ) Trying run default `pred_step` on a Model with no `module`, try overriding `pred_step`.","title":"Defining a simple Model"},{"location":"getting-started/low-level-api/#training-the-model","text":"Now that we have our model we can just call fit . The following code will train our model for 100 epochs while limiting each epoch to 200 steps and using a batch size of 64 : In [ ]: history = model . fit ( x = X_train , y = y_train , epochs = 100 , steps_per_epoch = 200 , batch_size = 64 , validation_data = ( X_test , y_test ), shuffle = True , callbacks = [ elegy . callbacks . ModelCheckpoint ( \"models/low-level\" , save_best_only = True )], ) ... Epoch 99/100 200/200 [==============================] - 0s 680us/step - accuracy: 0.9375 - loss: 0.2508 - val_accuracy: 1.0000 - val_loss: 0.0878 Epoch 100/100 200/200 [==============================] - 0s 618us/step - accuracy: 0.9219 - loss: 0.2440 - val_accuracy: 0.8750 - val_loss: 0.1224 The elegy.callbacks.ModelCheckpoint callback will periodicall saves the model a folder called \"models/low-level\" during training which is very useful if we want to load it after the process is finished, we will use it later. fit returns a History object which which contains information the time series for the values of the losses and metrics throughout training, we can use it to generate some nice plots of the evolution of our training: In [7]: import matplotlib.pyplot as plt def plot_history ( history ): n_plots = len ( history . history . keys ()) // 2 plt . figure ( figsize = ( 14 , 24 )) for i , key in enumerate ( list ( history . history . keys ())[: n_plots ]): metric = history . history [ key ] val_metric = history . history [ f \"val_ { key } \" ] plt . subplot ( n_plots , 1 , i + 1 ) plt . plot ( metric , label = f \"Training { key } \" ) plt . plot ( val_metric , label = f \"Validation { key } \" ) plt . legend ( loc = \"lower right\" ) plt . ylabel ( key ) plt . title ( f \"Training and Validation { key } \" ) plt . show () plot_history ( history ) Notice that the logs are very noisy, this is because for this example we didn't use cummulative metrics so the reported value is just the value for the last batch of that epoch, not the value for the entire epoch. To fix this we could use some of the modules in elegy.metrics .","title":"Training the Model"},{"location":"getting-started/low-level-api/#generating-predictions","text":"Having trained our model we can now get some samples from the test set and generate some predictions. First we will just pick some random samples using numpy : In [8]: import numpy as np idxs = np . random . randint ( 0 , 10000 , size = ( 9 ,)) x_sample = X_test [ idxs ] Here we selected 9 random images. Now if we had implemented pred_step we could've used predict , instead we are just going to the the calculation by hand. In [9]: x = x_sample . reshape ( x_sample . shape [ 0 ], - 1 ) w , b = model . states . net_params y_pred = jnp . dot ( x , w ) + b y_pred . shape Out[9]: (9, 10) Easy right? Finally lets plot the results to see if they are accurate. In [10]: plt . figure ( figsize = ( 12 , 12 )) for i in range ( 3 ): for j in range ( 3 ): k = 3 * i + j plt . subplot ( 3 , 3 , k + 1 ) plt . title ( f \" { np . argmax ( y_pred [ k ]) } \" ) plt . imshow ( x_sample [ k ], cmap = \"gray\" ) Good enough!","title":"Generating Predictions"},{"location":"getting-started/low-level-api/#serialization","text":"To serialize the Model you can use the model.save(...) , this will create a folder with some files that contain the model's code plus all parameters and states, however since we had previously used the ModelCheckpoint callback we can load it using elegy.load . Lets get a new model reference containing the same weights and call its evaluate method to verify it loaded correctly: In [11]: # You can use can use `save` but `ModelCheckpoint already serialized the model # model.save(\"model\") # current model reference print ( \"current model id:\" , id ( model )) # load model from disk model = elegy . load ( \"models/low-level\" ) # new model reference print ( \"new model id: \" , id ( model )) # check that it works! model . evaluate ( x = X_test , y = y_test ) current model id: 139835452734576 new model id: 139834008582896 313/313 [==============================] - 1s 2ms/step - accuracy: 0.9116 - loss: 0.3216 Out[11]: {'accuracy': DeviceArray(1., dtype=float32), 'loss': DeviceArray(0.05334431, dtype=float32), 'size': 32} Excellent! We hope you've enjoyed this tutorial.","title":"Serialization"},{"location":"guides/contributing/","text":"Contributing This is a short guide on how to start contributing to Elegy along with some best practices for the project. Setup We use poetry >= 1.1.4 , the easiest way to setup a development environment is run: poetry config virtualenvs.in-project true --local poetry install In order for Jax to recognize your GPU, you will probably have to install it again using the command below. PYTHON_VERSION = cp38 CUDA_VERSION = cuda101 # alternatives: cuda100, cuda101, cuda102, cuda110, check your cuda version PLATFORM = manylinux2010_x86_64 # alternatives: manylinux2010_x86_64 BASE_URL = 'https://storage.googleapis.com/jax-releases' pip install --upgrade $BASE_URL / $CUDA_VERSION /jaxlib-0.1.55- $PYTHON_VERSION -none- $PLATFORM .whl pip install --upgrade jax Gitpod An alternative way to contribute is using gitpod which creates a vscode-based cloud development enviroment. To get started just login at gitpod, grant the appropriate permissions to github, and open the following link: https://gitpod.io/#https://github.com/poets-ai/elegy We have built a python 3.8 enviroment and all development dependencies will install when the enviroment starts. Creating Losses and Metrics For this you can follow these guidelines: Each loss / metric should be defined in its own file. Inherit from either elegy.losses.loss.Loss or elegy.metrics.metric.Metric or an existing class that inherits from them. Try to use an existing metric or loss as a template You must provide documentation for the following: The class definition. The __init__ method. The call method. Try to port the documentation + signature from its Keras counter part. If so you must give credits to the original source file. You must include tests. If you there exists an equivalent loss/metric in Keras you must test numerical equivalence between both. Testing To execute all the tests just run pytest Documentation We use mkdocs . If you create a new object that requires documentation please do the following: Add a markdown file inside /docs/api in the appropriate location according to the project's structure. This file must: Contain the path of function / class as header Use mkdocstring to render the API information. Example: # elegy.losses.BinaryCrossentropy ::: elegy.losses.BinaryCrossentropy selection: inherited_members: true members: - call - __init__ Add and entry to mkdocs.yml inside nav pointing to this file. Checkout mkdocs.yml . To build and visualize the documentation locally run mkdocs serve Creating a PR Before sending a pull request make sure all test run and code is formatted with black : black . Changelog CHANGELOG.md is automatically generated using github-changelog-generator , to update the changelog just run: docker run -it --rm -v ( pwd ) :/usr/local/src/your-app ferrarimarco/github-changelog-generator -u poets-ai -p elegy -t <TOKEN> where <TOKEN> token can be obtained from Github at Personal access tokens , you only have to give permission for the repo section.","title":"Contributing"},{"location":"guides/contributing/#contributing","text":"This is a short guide on how to start contributing to Elegy along with some best practices for the project.","title":"Contributing"},{"location":"guides/contributing/#setup","text":"We use poetry >= 1.1.4 , the easiest way to setup a development environment is run: poetry config virtualenvs.in-project true --local poetry install In order for Jax to recognize your GPU, you will probably have to install it again using the command below. PYTHON_VERSION = cp38 CUDA_VERSION = cuda101 # alternatives: cuda100, cuda101, cuda102, cuda110, check your cuda version PLATFORM = manylinux2010_x86_64 # alternatives: manylinux2010_x86_64 BASE_URL = 'https://storage.googleapis.com/jax-releases' pip install --upgrade $BASE_URL / $CUDA_VERSION /jaxlib-0.1.55- $PYTHON_VERSION -none- $PLATFORM .whl pip install --upgrade jax","title":"Setup"},{"location":"guides/contributing/#gitpod","text":"An alternative way to contribute is using gitpod which creates a vscode-based cloud development enviroment. To get started just login at gitpod, grant the appropriate permissions to github, and open the following link: https://gitpod.io/#https://github.com/poets-ai/elegy We have built a python 3.8 enviroment and all development dependencies will install when the enviroment starts.","title":"Gitpod"},{"location":"guides/contributing/#creating-losses-and-metrics","text":"For this you can follow these guidelines: Each loss / metric should be defined in its own file. Inherit from either elegy.losses.loss.Loss or elegy.metrics.metric.Metric or an existing class that inherits from them. Try to use an existing metric or loss as a template You must provide documentation for the following: The class definition. The __init__ method. The call method. Try to port the documentation + signature from its Keras counter part. If so you must give credits to the original source file. You must include tests. If you there exists an equivalent loss/metric in Keras you must test numerical equivalence between both.","title":"Creating Losses and Metrics"},{"location":"guides/contributing/#testing","text":"To execute all the tests just run pytest","title":"Testing"},{"location":"guides/contributing/#documentation","text":"We use mkdocs . If you create a new object that requires documentation please do the following: Add a markdown file inside /docs/api in the appropriate location according to the project's structure. This file must: Contain the path of function / class as header Use mkdocstring to render the API information. Example: # elegy.losses.BinaryCrossentropy ::: elegy.losses.BinaryCrossentropy selection: inherited_members: true members: - call - __init__ Add and entry to mkdocs.yml inside nav pointing to this file. Checkout mkdocs.yml . To build and visualize the documentation locally run mkdocs serve","title":"Documentation"},{"location":"guides/contributing/#creating-a-pr","text":"Before sending a pull request make sure all test run and code is formatted with black : black .","title":"Creating a PR"},{"location":"guides/contributing/#changelog","text":"CHANGELOG.md is automatically generated using github-changelog-generator , to update the changelog just run: docker run -it --rm -v ( pwd ) :/usr/local/src/your-app ferrarimarco/github-changelog-generator -u poets-ai -p elegy -t <TOKEN> where <TOKEN> token can be obtained from Github at Personal access tokens , you only have to give permission for the repo section.","title":"Changelog"},{"location":"low-level-api/basics/","text":"Low-level API Elegy's low-level API allows you to override some core methods in Model that specify what happens during training, inference, etc. This approach is perfect when you want to do things that are hard or simply not possible with the high-level API as it gives you the flexibility to do anything inside these methods as long as you return the expected types. Methods This is the list of all the overrideable methods: Caller Method predict pred_step evaluate test_step grad_step fit train_step init init_step summary summary_step states_step jit_step Each method has a default implementation which is what gives rise to the high-level API. Example Most overrideable methods take some input & state, perform some jax operations & updates the state, and returns some outputs & the new state. Lets see a simple example of a linear classifier using test_step : class LinearClassifier (elegy . Model): def test_step ( self , x, y_true, states, initializing): x = jnp . reshape(x, (x . shape[ 0 ], -1 )) / 255 # initialize or use existing parameters if initializing: w = jax . random . uniform( jax . random . PRNGKey( 42 ), shape = [np . prod(x . shape[ 1 :]), 10 ] ) b = jax . random . uniform(jax . random . PRNGKey( 69 ), shape = [ 1 ]) else : w, b = states . net_params # model logits = jnp . dot(x, w) + b # categorical crossentropy loss labels = jax . nn . one_hot(y_true, 10 ) loss = jnp . mean( - jnp . sum(labels * jax . nn . log_softmax(logits), axis =-1 )) accuracy = jnp . mean(jnp . argmax(logits, axis =-1 ) == y_true) # metrics logs = dict (accuracy = accuracy, loss = loss) # update states states = states . update(net_params = (w, b)) return loss, logs, states model = LinearClassifier( optimizer = optax . adam( 1e-3 ) ) model . fit( x = X_train, y = y_train, epochs =100 , batch_size =64 , ) As you see here we perform everything from parameter initialization, modeling, calculating the main loss, and logging some metrics. Some notes about the previous example: The states argument of type elegy.States is an immutable Mapping which you add / update fields via its update method. net_params is one of the names used by the default implementation, check the States guide for more information. initializing tells you whether to initialize the parameters of the model or fetch the current ones from states , if you are using a Module framework this usually tells you whether to call init or apply . test_step should returns 3 specific outputs ( loss , logs , states ), you should check the docs for each method to know what to return.","title":"Basics"},{"location":"low-level-api/basics/#low-level-api","text":"Elegy's low-level API allows you to override some core methods in Model that specify what happens during training, inference, etc. This approach is perfect when you want to do things that are hard or simply not possible with the high-level API as it gives you the flexibility to do anything inside these methods as long as you return the expected types.","title":"Low-level API"},{"location":"low-level-api/basics/#methods","text":"This is the list of all the overrideable methods: Caller Method predict pred_step evaluate test_step grad_step fit train_step init init_step summary summary_step states_step jit_step Each method has a default implementation which is what gives rise to the high-level API.","title":"Methods"},{"location":"low-level-api/basics/#example","text":"Most overrideable methods take some input & state, perform some jax operations & updates the state, and returns some outputs & the new state. Lets see a simple example of a linear classifier using test_step : class LinearClassifier (elegy . Model): def test_step ( self , x, y_true, states, initializing): x = jnp . reshape(x, (x . shape[ 0 ], -1 )) / 255 # initialize or use existing parameters if initializing: w = jax . random . uniform( jax . random . PRNGKey( 42 ), shape = [np . prod(x . shape[ 1 :]), 10 ] ) b = jax . random . uniform(jax . random . PRNGKey( 69 ), shape = [ 1 ]) else : w, b = states . net_params # model logits = jnp . dot(x, w) + b # categorical crossentropy loss labels = jax . nn . one_hot(y_true, 10 ) loss = jnp . mean( - jnp . sum(labels * jax . nn . log_softmax(logits), axis =-1 )) accuracy = jnp . mean(jnp . argmax(logits, axis =-1 ) == y_true) # metrics logs = dict (accuracy = accuracy, loss = loss) # update states states = states . update(net_params = (w, b)) return loss, logs, states model = LinearClassifier( optimizer = optax . adam( 1e-3 ) ) model . fit( x = X_train, y = y_train, epochs =100 , batch_size =64 , ) As you see here we perform everything from parameter initialization, modeling, calculating the main loss, and logging some metrics. Some notes about the previous example: The states argument of type elegy.States is an immutable Mapping which you add / update fields via its update method. net_params is one of the names used by the default implementation, check the States guide for more information. initializing tells you whether to initialize the parameters of the model or fetch the current ones from states , if you are using a Module framework this usually tells you whether to call init or apply . test_step should returns 3 specific outputs ( loss , logs , states ), you should check the docs for each method to know what to return.","title":"Example"},{"location":"low-level-api/default-implementation/","text":"Default Implementation Methods The default implementation favors composition by implementing a method in term of another, especifically if follows this call graph: summary predict evalutate fit init \u2b07\ufe0f \u2b07\ufe0f \u2b07\ufe0f \u2b07\ufe0f \u2b07\ufe0f call_summary_step call_pred_step call_test_step call_train_step call_init_step \u2b07\ufe0f \u2b07\ufe0f \u2b07\ufe0f \u2b07\ufe0f \u2b07\ufe0f summary_step \u27a1\ufe0f pred_step \u2b05 test_step \u2b05 grad_step \u2b05 train_step \u2b05 init_step This structure allows you to for example override test_step and still be able to use use fit since train_step (called by fit ) will call your test_step via grad_step . It also means that if you implement test_step but not pred_step there is a high chance both predict and summary will not work. call_* methods The call_<method> method family are entrypoints that usually just redirect to their inputs to <method> , you choose to override these if you need to perform some some computation only when method in question is the entry point i.e. when its not called by other methods in the bottom path. For example if you want to change the behavior of evaluate without affecting the behavior of fit while preserving most of the default implementation you can override call_step_step to do the corresponding adjustments and then call test_step . Since train_step does not depend on call_step_step then the change will manifest during evaluate but not during fit .","title":"Default Implementation"},{"location":"low-level-api/default-implementation/#default-implementation","text":"","title":"Default Implementation"},{"location":"low-level-api/default-implementation/#methods","text":"The default implementation favors composition by implementing a method in term of another, especifically if follows this call graph: summary predict evalutate fit init \u2b07\ufe0f \u2b07\ufe0f \u2b07\ufe0f \u2b07\ufe0f \u2b07\ufe0f call_summary_step call_pred_step call_test_step call_train_step call_init_step \u2b07\ufe0f \u2b07\ufe0f \u2b07\ufe0f \u2b07\ufe0f \u2b07\ufe0f summary_step \u27a1\ufe0f pred_step \u2b05 test_step \u2b05 grad_step \u2b05 train_step \u2b05 init_step This structure allows you to for example override test_step and still be able to use use fit since train_step (called by fit ) will call your test_step via grad_step . It also means that if you implement test_step but not pred_step there is a high chance both predict and summary will not work.","title":"Methods"},{"location":"low-level-api/default-implementation/#call_-methods","text":"The call_<method> method family are entrypoints that usually just redirect to their inputs to <method> , you choose to override these if you need to perform some some computation only when method in question is the entry point i.e. when its not called by other methods in the bottom path. For example if you want to change the behavior of evaluate without affecting the behavior of fit while preserving most of the default implementation you can override call_step_step to do the corresponding adjustments and then call test_step . Since train_step does not depend on call_step_step then the change will manifest during evaluate but not during fit .","title":"call_* methods"},{"location":"low-level-api/states/","text":"States elegy.States is an immutable Mapping that contains all the states needed in Model , the low-level API provides a simple state management system by passing the states parameter (of type elegy.States ) to all methods. Basic usage The most common way to use States is via its update method you can use to set or update field: states = states . update(some_field = some_value) You can access a field via index or field access notation: some_value = states[ \"some_field\" ] some_value = states . some_field Default Implementation The default implementation uses the following fields: name description rng contains an elegy.RNGSeq instance you can you to request random state. net_params the trainable parameters of the model. net_states the non-trainable parameters of the model. metrics_states the states used to calculate cumulative metrics. optimizer_states the states for the optimizer.","title":"States"},{"location":"low-level-api/states/#states","text":"elegy.States is an immutable Mapping that contains all the states needed in Model , the low-level API provides a simple state management system by passing the states parameter (of type elegy.States ) to all methods.","title":"States"},{"location":"low-level-api/states/#basic-usage","text":"The most common way to use States is via its update method you can use to set or update field: states = states . update(some_field = some_value) You can access a field via index or field access notation: some_value = states[ \"some_field\" ] some_value = states . some_field","title":"Basic usage"},{"location":"low-level-api/states/#default-implementation","text":"The default implementation uses the following fields: name description rng contains an elegy.RNGSeq instance you can you to request random state. net_params the trainable parameters of the model. net_states the non-trainable parameters of the model. metrics_states the states used to calculate cumulative metrics. optimizer_states the states for the optimizer.","title":"Default Implementation"},{"location":"low-level-api/methods/pred_step/","text":"pred_step The pred_step method computes the predictions of the main model, by overriding this method you can directly influence what happens during predict . Inputs Any of following input arguments are available for pred_step : name type x Any Input data states States Current state of the model initializing bool Whether the model is initializing or not training bool Whether the model is training or not You must request the arguments you want by name . Outputs pred_step must output a tuple with the following values: name type y_pred Any The predictions of the model states States The new state of the model Callers method when predict always test_step default implementation summary_step default implementation Examples If for some reason you wish to create a pure jax / Module-less model, you can define your own Model that implements pred_step like this: class LinearClassifier (elegy . Model): def pred_step ( self , x, y_true, states, initializing): x = jnp . reshape(x, (x . shape[ 0 ], -1 )) / 255 # initialize or use existing parameters if initializing: w = jax . random . uniform( jax . random . PRNGKey( 42 ), shape = [np . prod(x . shape[ 1 :]), 10 ] ) b = jax . random . uniform(jax . random . PRNGKey( 69 ), shape = [ 1 ]) else : w, b = states . net_params # model y_pred = jnp . dot(x, w) + b return y_pred, states . update(net_params = (w, b)) model = LinearClassifier( optimizer = optax . adam( 1e-3 ), loss = elegy . losses . SparseCategoricalCrossentropy(from_logits = True ), metrics = elegy . metrics . SparseCategoricalAccuracy(), ) model . fit( x = X_train, y = y_train, epochs =100 , batch_size =64 , ) Here we implement the same LinearClassifier from the basics section but we extracted the definition of the model to pred_step and we let the basic implementation of test_step take care of the loss and metrics which we provide to the LinearClassifier 's constructor. Default Implementation The default implementation of pred_step does the following: Calls api_module.init or api_module.apply depending on state of initializing . api_module of type GeneralizedModule is a wrapper over the module object passed by the user to the Model s constructor.","title":"pred_step"},{"location":"low-level-api/methods/pred_step/#pred_step","text":"The pred_step method computes the predictions of the main model, by overriding this method you can directly influence what happens during predict .","title":"pred_step"},{"location":"low-level-api/methods/pred_step/#inputs","text":"Any of following input arguments are available for pred_step : name type x Any Input data states States Current state of the model initializing bool Whether the model is initializing or not training bool Whether the model is training or not You must request the arguments you want by name .","title":"Inputs"},{"location":"low-level-api/methods/pred_step/#outputs","text":"pred_step must output a tuple with the following values: name type y_pred Any The predictions of the model states States The new state of the model","title":"Outputs"},{"location":"low-level-api/methods/pred_step/#callers","text":"method when predict always test_step default implementation summary_step default implementation","title":"Callers"},{"location":"low-level-api/methods/pred_step/#examples","text":"If for some reason you wish to create a pure jax / Module-less model, you can define your own Model that implements pred_step like this: class LinearClassifier (elegy . Model): def pred_step ( self , x, y_true, states, initializing): x = jnp . reshape(x, (x . shape[ 0 ], -1 )) / 255 # initialize or use existing parameters if initializing: w = jax . random . uniform( jax . random . PRNGKey( 42 ), shape = [np . prod(x . shape[ 1 :]), 10 ] ) b = jax . random . uniform(jax . random . PRNGKey( 69 ), shape = [ 1 ]) else : w, b = states . net_params # model y_pred = jnp . dot(x, w) + b return y_pred, states . update(net_params = (w, b)) model = LinearClassifier( optimizer = optax . adam( 1e-3 ), loss = elegy . losses . SparseCategoricalCrossentropy(from_logits = True ), metrics = elegy . metrics . SparseCategoricalAccuracy(), ) model . fit( x = X_train, y = y_train, epochs =100 , batch_size =64 , ) Here we implement the same LinearClassifier from the basics section but we extracted the definition of the model to pred_step and we let the basic implementation of test_step take care of the loss and metrics which we provide to the LinearClassifier 's constructor.","title":"Examples"},{"location":"low-level-api/methods/pred_step/#default-implementation","text":"The default implementation of pred_step does the following: Calls api_module.init or api_module.apply depending on state of initializing . api_module of type GeneralizedModule is a wrapper over the module object passed by the user to the Model s constructor.","title":"Default Implementation"},{"location":"low-level-api/methods/test_step/","text":"test_step The test_step computes the main loss of the model along with some logs for reporting, by overriding this method you can directly influence what happens during evaluate . Inputs Any of following input arguments are available for test_step : name type x Any Input data y_true Any The target labels sample_weight Optional[ndarray] The weight of each sample in the total loss class_weight Optional[ndarray] The weight of each class in the total loss states States Current state of the model initializing bool Whether the model is initializing or not training bool Whether the model is training or not You must request the arguments you want by name . Outputs pred_step must output a tuple with the following values: name type loss ndarray The loss of the model over the data logs Dict[str, ndarray] A dictionary with a set of values to report states States The new state of the model Callers method when evaluate always grad_step default implementation train_step default implementation during initialization only Examples Lets review the example of test_step found in basics : class LinearClassifier (elegy . Model): def test_step ( self , x, y_true, states, initializing): x = jnp . reshape(x, (x . shape[ 0 ], -1 )) / 255 # initialize or use existing parameters if initializing: w = jax . random . uniform( jax . random . PRNGKey( 42 ), shape = [np . prod(x . shape[ 1 :]), 10 ] ) b = jax . random . uniform(jax . random . PRNGKey( 69 ), shape = [ 1 ]) else : w, b = states . net_params # model logits = jnp . dot(x, w) + b # categorical crossentropy loss labels = jax . nn . one_hot(y_true, 10 ) loss = jnp . mean( - jnp . sum(labels * jax . nn . log_softmax(logits), axis =-1 )) accuracy = jnp . mean(jnp . argmax(logits, axis =-1 ) == y_true) # metrics logs = dict (accuracy = accuracy, loss = loss) # update states states = states . update(net_params = (w, b)) return loss, logs, states model = LinearClassifier( optimizer = optax . adam( 1e-3 ) ) model . fit( x = X_train, y = y_train, epochs =100 , batch_size =64 , ) In this case test_step is defining both the \"forward\" pass of the model and calculating the losses and metrics in a single place. However, since we are not defining pred_step we loose the power to call predict which might not be desirable. The optimimal way to fix this is to extract the calculation of the logits into pred_step and call this from test_step : class LinearClassifier (elegy . Model): def test_step ( self , x, states, initializing): x = jnp . reshape(x, (x . shape[ 0 ], -1 )) / 255 # initialize or use existing parameters if initializing: w = jax . random . uniform( jax . random . PRNGKey( 42 ), shape = [np . prod(x . shape[ 1 :]), 10 ] ) b = jax . random . uniform(jax . random . PRNGKey( 69 ), shape = [ 1 ]) else : w, b = states . net_params # model logits = jnp . dot(x, w) + b return logits, states . update(net_params = (w, b)) def test_step ( self , x, y_true, states, initializing): # call pred_step logits, states = self . pred_step((x, states, initializing) # categorical crossentropy loss labels = jax . nn . one_hot(y_true, 10 ) loss = jnp . mean( - jnp . sum(labels * jax . nn . log_softmax(logits), axis =-1 )) accuracy = jnp . mean(jnp . argmax(logits, axis =-1 ) == y_true) # metrics logs = dict (accuracy = accuracy, loss = loss) # update states states = states . update(net_params = (w, b)) return loss, logs, states model = LinearClassifier( optimizer = optax . adam( 1e-3 ), ) model . fit( x = X_train, y = y_train, epochs =100 , batch_size =64 , ) This not only creates a separation of concerns, it also favors code reuse, and we can now use predict , evaluate , and fit as intended. There are cases however where you might want to implement a forward pass inside test_step that is different from what you would define in pred_step , for example you can create a VAE or GAN Models that use multiple modules to calculate the loss inside test_step (e.g. encoder, decoder, and discriminator) but only use the decoder inside pred_step to generate samples. Default Implementation The default implementation of pred_step does the following: Call pred_step to get y_pred . Calls api_loss.init or api_loss.apply depending on state of initializing . api_loss of type Losses computes the aggregated batch loss from the loss functions passed by the user through the loss argument in the Model s constructor, and also computes a running mean of each loss individually which is passed for reporting to logs . Calls api_metrics.init or api_metrics.apply depending on state of initializing . api_metrics of type Metrics calculates the metrics passed by the user through the metrics argument in the Model s constructor and passes their values to logs for reporting.","title":"test_step"},{"location":"low-level-api/methods/test_step/#test_step","text":"The test_step computes the main loss of the model along with some logs for reporting, by overriding this method you can directly influence what happens during evaluate .","title":"test_step"},{"location":"low-level-api/methods/test_step/#inputs","text":"Any of following input arguments are available for test_step : name type x Any Input data y_true Any The target labels sample_weight Optional[ndarray] The weight of each sample in the total loss class_weight Optional[ndarray] The weight of each class in the total loss states States Current state of the model initializing bool Whether the model is initializing or not training bool Whether the model is training or not You must request the arguments you want by name .","title":"Inputs"},{"location":"low-level-api/methods/test_step/#outputs","text":"pred_step must output a tuple with the following values: name type loss ndarray The loss of the model over the data logs Dict[str, ndarray] A dictionary with a set of values to report states States The new state of the model","title":"Outputs"},{"location":"low-level-api/methods/test_step/#callers","text":"method when evaluate always grad_step default implementation train_step default implementation during initialization only","title":"Callers"},{"location":"low-level-api/methods/test_step/#examples","text":"Lets review the example of test_step found in basics : class LinearClassifier (elegy . Model): def test_step ( self , x, y_true, states, initializing): x = jnp . reshape(x, (x . shape[ 0 ], -1 )) / 255 # initialize or use existing parameters if initializing: w = jax . random . uniform( jax . random . PRNGKey( 42 ), shape = [np . prod(x . shape[ 1 :]), 10 ] ) b = jax . random . uniform(jax . random . PRNGKey( 69 ), shape = [ 1 ]) else : w, b = states . net_params # model logits = jnp . dot(x, w) + b # categorical crossentropy loss labels = jax . nn . one_hot(y_true, 10 ) loss = jnp . mean( - jnp . sum(labels * jax . nn . log_softmax(logits), axis =-1 )) accuracy = jnp . mean(jnp . argmax(logits, axis =-1 ) == y_true) # metrics logs = dict (accuracy = accuracy, loss = loss) # update states states = states . update(net_params = (w, b)) return loss, logs, states model = LinearClassifier( optimizer = optax . adam( 1e-3 ) ) model . fit( x = X_train, y = y_train, epochs =100 , batch_size =64 , ) In this case test_step is defining both the \"forward\" pass of the model and calculating the losses and metrics in a single place. However, since we are not defining pred_step we loose the power to call predict which might not be desirable. The optimimal way to fix this is to extract the calculation of the logits into pred_step and call this from test_step : class LinearClassifier (elegy . Model): def test_step ( self , x, states, initializing): x = jnp . reshape(x, (x . shape[ 0 ], -1 )) / 255 # initialize or use existing parameters if initializing: w = jax . random . uniform( jax . random . PRNGKey( 42 ), shape = [np . prod(x . shape[ 1 :]), 10 ] ) b = jax . random . uniform(jax . random . PRNGKey( 69 ), shape = [ 1 ]) else : w, b = states . net_params # model logits = jnp . dot(x, w) + b return logits, states . update(net_params = (w, b)) def test_step ( self , x, y_true, states, initializing): # call pred_step logits, states = self . pred_step((x, states, initializing) # categorical crossentropy loss labels = jax . nn . one_hot(y_true, 10 ) loss = jnp . mean( - jnp . sum(labels * jax . nn . log_softmax(logits), axis =-1 )) accuracy = jnp . mean(jnp . argmax(logits, axis =-1 ) == y_true) # metrics logs = dict (accuracy = accuracy, loss = loss) # update states states = states . update(net_params = (w, b)) return loss, logs, states model = LinearClassifier( optimizer = optax . adam( 1e-3 ), ) model . fit( x = X_train, y = y_train, epochs =100 , batch_size =64 , ) This not only creates a separation of concerns, it also favors code reuse, and we can now use predict , evaluate , and fit as intended. There are cases however where you might want to implement a forward pass inside test_step that is different from what you would define in pred_step , for example you can create a VAE or GAN Models that use multiple modules to calculate the loss inside test_step (e.g. encoder, decoder, and discriminator) but only use the decoder inside pred_step to generate samples.","title":"Examples"},{"location":"low-level-api/methods/test_step/#default-implementation","text":"The default implementation of pred_step does the following: Call pred_step to get y_pred . Calls api_loss.init or api_loss.apply depending on state of initializing . api_loss of type Losses computes the aggregated batch loss from the loss functions passed by the user through the loss argument in the Model s constructor, and also computes a running mean of each loss individually which is passed for reporting to logs . Calls api_metrics.init or api_metrics.apply depending on state of initializing . api_metrics of type Metrics calculates the metrics passed by the user through the metrics argument in the Model s constructor and passes their values to logs for reporting.","title":"Default Implementation"}]}